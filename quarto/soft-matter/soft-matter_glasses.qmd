---
title: "Arrested states"
pyodide:
    packages:
        - numpy
        - matplotlib
        - scipy

bibliography: references.bib

---

In these chapters we have been considering systems implicitly (or explicitly) in thermal equilibrium with a surrounding environment. This is typically some kind of dispersion medium (a solvent). But we also also been more demanding: we have required that the dispersed phase (the colloids, the polymers the anisotropic particles of liquid crystals) have explored exhaustively their free energy options and that they are truly in some macrostate corresponding to a global, stable thermal equilibrium state.

Equilibrium signifies time reversibility.

::: {.column-margin} 
At equilibrium, the entropy change $\Delta S$ of a system between two times $t_1, t_2$ or equivalently the entropy production $\frac{dS}{dt}$ are **zero**. Hence the transformation between $t_1$ and $t_2$ has to be **reversible**.
:::


Many fluids, whether simple liquids like argon or complex liquids such as colloids and polymers, can be rapidly cooled (quenched) to temperatures well below their equilibrium freezing point without crystallization occurring on experimental timescales. In thermodynamic terms this is interpreted as a failure of the system to reach its true equilibrium (minimum free energy) state, namely the ordered crystalline phase. At these low temperatures, the dynamics slows down and the large scale structrues remain disordered as the parent fluids.

In this chapter we are going to consider two paradigmatic cases of such nonequilibrium dynamic

- **physical gels**, where the system forms a disordered, arrested state due to the formation of a percolating network of reversible bonds (e.g., hydrogen bonds or van der Waals interactions) between particles or polymers,
- **glasses**, where the system falls out of equilibrium due to a dramatic slowing down of dynamics as temperature decreases or density increases, leading to a rigid but disordered structure.


## Physical gels

The word *gel* signifies many different things to different scientific communities. In the context of soft matter physics, a physical gel is typically understood as a system in which the constituent particles or polymers are connected via reversible, non-covalent bonds to form a percolating network that spans the entire sample. This network imparts solid-like mechanical properties to the material, even though the underlying structure remains disordered and fluid-like on a microscopic scale. 

Physical gels differ from chemical gels, where the network is formed by irreversible covalent bonds. In physical gels, the bonds can break and reform dynamically,  have an energy of oder $1 k_BT$, allowing the system to respond to external stresses and sometimes to self-heal. Examples include gelatin desserts, agarose gels, and certain colloidal suspensions where attractive interactions lead to network formation, see @Zaccarelli2007 for a comprehensive review.

The transition from a fluid to a gel state is often associated with the appearance of a system-spanning cluster, which can be described using concepts from **percolation theory**. The mechanical rigidity of the gel arises when this cluster forms, leading to a dramatic increase in viscosity and the emergence of an elastic response.

::: {.callout-note collapse="true"}
## Percolation

Percolation is the study of an apparently innocent mathematical problem: take a square grid if $L\times L$ sites and randomly label $N$ of them; how does the likelihood of forming a cluster that spans across the grid (i.e. *percolates*) depend on the probability $p=N/L^2$ of having a labelled square?

The problem is interesting for its various applications across domains of science as diverse as networks formation, transport, material science, epidemiologu and ecology. It is a meaneable to an analytical treatment and can be solved via direct simulations. Percolation is a classic example of a phase transition and critical phenomena. The probability $p_c$ at which a spanning cluster first appears is called the **percolation threshold**. For a large 2D square lattice, $p_c \approx 0.5927$. Below $p_c$, only small, disconnected clusters exist; above $p_c$, a giant connected component spans the system. The transitions is **continuous**, i.e. second order.

Analytically, percolation is tractable and exhibits universal critical exponents near $p_c$, making it a cornerstone of statistical physics and network theory.

You can play with percolation on a squared lattice with the code below.

```{pyodide}
#| autorun: true
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import label

def sample_percolation(
    L = 50,  # grid size
    p = 0.2  # occupation probability
):
    # Generate random grid
    grid = np.random.rand(L, L) < p

    # Label clusters
    structure = np.array([[0,1,0],[1,1,1],[0,1,0]], dtype=int) 
    labeled, num_features = label(grid, structure=structure)

    # Find largest cluster
    sizes = np.bincount(labeled.ravel())
    sizes[0] = 0  # background is label 0
    largest_label = sizes.argmax()
    
    # Mask for largest cluster
    mask = labeled == largest_label

    return grid, mask

# sample
p = 0.2
L = 120
grid, mask = sample_percolation(L,p)
# Plot
plt.figure(figsize=(6, 6))
# Show all clusters in gray
plt.imshow(grid, cmap='gray', interpolation='none')

plt.imshow(np.ma.masked_where(~mask, mask), cmap='autumn', alpha=0.8, interpolation='none')
plt.axis('off')
plt.title(f'Percolation in 2D (p={p})\nLargest cluster highlighted in red')
plt.show()
```


```{pyodide}
#| autorun: true 

ps = np.linspace(0.1, 1.0, 50)
L = 500
largest_sizes = []
repeats = 3
for p in ps:
    grid, mask = sample_percolation(L, p)
    sizes = []
    for r in range(repeats):
        sizes.append(mask.sum() / (L * L))  # Fraction of sites in largest cluster
    largest_sizes.append(np.mean(sizes))

plt.figure(figsize=(7, 4))
plt.plot(ps, largest_sizes, marker='o')
plt.xlabel('Occupation probability $p$')
plt.ylabel('Fraction in largest cluster')
plt.title('Percolation transition in 2D')
plt.show()
```

The following table illustrates that various values of the critical percolation probability are known for various lattices, either exactly or via simulation. Notice that this non-universal feature, the critical $p_c$, decreases with increasing dimensionality: at higher dimensions, the connectivity is naturally higher and so a comparitively smaller fraction of the system is require in order to find a percolating path.

| Lattice Type    | Dimension | Coordination $z$ | $p_c^{site}$ | $p_c^{bond}$ |
| --------------- | --------- | ---------------- | ------------ | ------------ |
| Square          | 2         | 4                | 0.592746     | **0.5**      |
| Triangular      | 2         | 6                | **0.5**      | 0.347296     |
| Honeycomb       | 2         | 3                | 0.697043     | **0.652703** |
| Kagome          | 2         | 4                | 0.6527       | 0.5244       |
| Union Jack      | 2         | 8                | 0.379        | 0.429        |
| Cubic           | 3         | 6                | 0.3116       | 0.2488       |
| FCC             | 3         | 12               | 0.199        | 0.120        |
| BCC             | 3         | 8                | 0.245        | 0.180        |
| Diamond         | 3         | 4                | 0.43         | 0.389        |
| Hypercubic (4D) | 4         | 8                | 0.197        | 0.160        |
| Hypercubic (5D) | 5         | 10               | 0.141        | 0.118        |
| Hypercubic (6D) | 6         | 12               | 0.109        | 0.094        |
| Hypercubic (7D) | 7         | 14               | 0.091        | 0.079        |
| Hypercubic (8D) | 8         | 16               | 0.078        | 0.069        |


Instead, other properties of percolation are **universal**: these are the critical exponents of observables on the lattice,and do not depend on the lattice detail, such as the probability to find a site in a percolating cluster $P_{\infty}(p)\propto (p-p_c)^{\beta}$

| Dimension $d$   | $\beta$ (Percolation) |
| --------------- | --------------------- |
| 2               | **5/36 ≈ 0.1389**     |
| 3               | ≈ 0.41                |
| 4               | ≈ 0.65                |
| 5               | ≈ 0.81                |
| 6               | ≈ 0.96                |
| ≥6 (Mean-field) | **1**                 |


:::

In the case of physical gels, percolation is a required ingredient for mechanical stability: in the absence of a large spanning percolating network, it is not possible for the gel to sustain external stresses or even self-generated stresses such as its own weight under gravity.

![Molecular dynamics simulation of the gradual formation of a very dilute colloidal gel by the growth of a system spanning (percolating) cluster (the largest cluster is highlighted in red). ](./figs/m.gif)




### Colloid-polymer mixtures as an example 

A way to realise robust physical gels in experiments is to use colloid-polymer mixtures, which we have seen previously when we talked about depletion. When the colloids are significantly larger than the polymers, the resulting depletion interaction is attractive and very short ranged: it is a so-called **sticky** interaction, ideally suited to form sobust physical bonds where, the colloids cluster in a nonequilibrium, branched, dense phase.

The resulting phase diagrams different significantly for the ones of simple liquids (e.g., Lennard-Jones interactions): the liquid-gas coexistence curve (the binodal) becomes metastable.


![Phase diagrams of colloid-polymer mixtures, from @anderson2002insights]

Given the very short range of the interactions, models of colloid polymer mixtures can approximate the Asakura-Oosawa potential using even simpler pair-wise interactions. For example, for Monte-Carlo simulations qand theoretical calculations one often uses the **square-well** model,

The square-well potential $U(r)$ is defined as:

$$
U(r) = 
\begin{cases}
\infty & r < \sigma \\
-\epsilon & \sigma \leq r < \lambda \sigma \\
0 & r \geq \lambda \sigma
\end{cases}
$$

where $\sigma$ is the particle diameter (hard core), $\epsilon$ is the well depth (attractive strength), and $\lambda$ controls the range of the attraction.

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

# Parameters
sigma = 1.0         # Hard core diameter
epsilon = 1.0       # Well depth
lambda_ = 1.2       # Range parameter

# r values
r = np.linspace(0.7, 2.0, 500)
U = np.zeros_like(r)

# Square well potential
U[r < sigma] = np.inf
U[(r >= sigma) & (r < lambda_ * sigma)] = -epsilon
U[r >= lambda_ * sigma] = 0

# Plot
plt.figure(figsize=(7,4))
plt.axhline(0, color='k', lw=0.5)
plt.plot(r, U, label='Square-well potential $U(r)$', color='C0')
# Shade area for hard core region
plt.fill_between(r, -1.2*epsilon, 2, where=(r < sigma), color='gray', alpha=0.3, label='Hard core ($r<\sigma$)')
# plt.axvline(sigma, color='k', linestyle='--', label=r'Hard core $r=\sigma$')
plt.axvline(lambda_ * sigma, color='C1', linestyle='--', label=r'Well edge $r=\lambda\sigma$')
plt.axhline(-epsilon, color='C2', linestyle=':', label=r'Well depth $-\epsilon$')

plt.ylim(-1.2*epsilon, 2)
plt.xlim(0.7, 2.0)
plt.xlabel(r'$r$')
plt.ylabel(r'$U(r)$')
plt.title('Square-well potential')
plt.legend(frameon=False)
plt.show()
```

For molecular dynamics one often employs the so-called **Morse potential**, which combines fast exponentially decaying tails and a repulsive core in a simple analytical form 

$$
U_{\mathrm{Morse}}(r) = D \left[ e^{-2\alpha (r - r_0)} - 2 e^{-\alpha (r - r_0)} \right]
$$

where $D$ is the well depth, $\alpha$ controls the range (steepness) of the potential, and $r_0$ is the equilibrium bond distance.

```{python}
#| echo: false
# Morse potential parameters`
D = 1.0        # Depth of the potential well
alpha = 10.0   # Controls the range (steepness)
r0 = 1.0       # Equilibrium bond distance

# r values (reuse from above)
U_morse = D * (np.exp(-2 * alpha * (r - r0)) - 2 * np.exp(-alpha * (r - r0)))

# Plot Morse potential
plt.figure(figsize=(7,4))
plt.axhline(0, color='k', lw=0.5)
plt.plot(r, U_morse, label='Morse potential $U_{\\mathrm{Morse}}(r)$', color='C3')
plt.axvline(r0, color='k', linestyle='--', label=r'Equilibrium $r_0$')
plt.axhline(-D, color='C2', linestyle=':', label=r'Well depth $-D$')
plt.xlabel(r'$r$')
plt.ylabel(r'$U(r)$')
plt.title('Morse potential')
plt.legend(frameon=False)
plt.ylim(-2,5)
plt.show()
```

#### Pair correlations {.unnumbered}

In many situation, pair-wise correlation functions are sufficient to capture gel formation. Both the radial distribution function and the structure factor are able to describe gel formation, but they focus (and are accurate) in different regimes:

The radial distribution function $g(r)$ has best statistics at short distances, capturing the features of the clusters that form the branched network.

![The changes in the radial distribution fucntion $g(r)$ as time evolves and the frozen structure of a simulated colloidal gel emerges. The systems is at an overall dilute concetration, so clusters appear as very sharp peaks at short distances, and their structure in terms of nearest and second-nearest neighbour shells are reflected in the first few peaks. From @griffiths2017local ](./figs/grtime.svg){width=60%}


In gels near the percolation threshold, the structure factor $S(q)$ exhibits **scale-free (power-law) behavior** at low $q$:

$$
S(q) \sim q^{-D_f}
$$

where $D_f$ is the **fractal dimension** of the gel network. This power-law regime reflects the absence of a characteristic length scale in the structure—clusters are **self-similar** over a range of sizes.

- For $q$ much smaller than the inverse cluster size, $S(q)$ flattens (finite-size effects).
- For intermediate $q$, $S(q) \sim q^{-D_f}$, indicating fractal geometry.
- For large $q$, $S(q)$ reflects local (non-fractal) structure.

![The same changes of the previous figures as seen in changes in the structure factor $S(q)$ as time evolves and the frozen structure of a colloidal gel emerges. At small q, a chracteristic power law behaviour emerges, while at higher q the mistructure changes are subtly capture by the reorganisation or splitting of the peaks.  ](./figs/sq_time.svg){width=60%}

The exponent $D_f$ is related to the spatial scaling of mass with size in the fractal cluster: $M(R) \sim R^{D_f}$. Typical values for percolation clusters in 3D are $D_f \approx 2.5$.

Thus, by analyzing the low-$q$ behavior of $S(q)$, one can extract the fractal dimension and confirm scale-free, self-similar structure in gels.

::: {.callout-note collapse="true"}

# Measuring fractal dimensions: the box-counting algorithm

Fractal dimensions are in practice quite tricky to measure. 
A common method is the **box-counting algorithm**:

1. Overlay a grid of boxes of size $\ell$ over the structure (e.g., a cluster or network).
2. Count the number $N(\ell)$ of boxes that contain any part of the structure.
3. Repeat for different box sizes $\ell$.
4. Plot $\log N(\ell)$ versus $\log(1/\ell)$. For a fractal, this plot is linear over some range, and the slope gives the fractal dimension $D_f$:
$$
N(\ell) \sim \ell^{-D_f}
$$

This method is widely used for experimental images and simulation data to estimate the fractal dimension of clusters, aggregates, or networks.


Below, illustrate this using a known fractal, the [Sierpinski carpet](https://en.wikipedia.org/wiki/Sierpiński_carpet){target="_blank"}, whose fractal dimensions is exactly $D_f = \log{8}/\log{3}\approx 1.8929$

```{python}
# Example: Box-counting fractal dimension for the Sierpinski carpet

import numpy as np
import matplotlib.pyplot as plt

def sierpinski_carpet(n):
    """Generate a Sierpinski carpet of order n as a 2D boolean array."""
    size = 3**n
    carpet = np.ones((size, size), dtype=bool)
    for i in range(n):
        step = 3**i
        for x in range(0, size, 3*step):
            for y in range(0, size, 3*step):
                carpet[x+step:x+2*step, y+step:y+2*step] = False
    return carpet

def box_count(img, box_sizes):
    """Count number of boxes containing any part of the structure for each box size using histogramdd."""
    N = []
    coords = np.argwhere(img)
    for box in box_sizes:
        bins = [img.shape[0] // box, img.shape[1] // box]
        # Assign each coordinate to a box
        hist, _ = np.histogramdd(coords, bins=bins, range=[[0, img.shape[0]], [0, img.shape[1]]])
        N.append(np.count_nonzero(hist))
    return np.array(N)

# Generate Sierpinski carpet
order = 5
carpet = sierpinski_carpet(order)

# Box sizes (powers of 3)
box_sizes = [3**i for i in range(order, 0, -1)]
N_boxes = box_count(carpet, box_sizes)

# Plot Sierpinski carpet
plt.figure(figsize=(4,4))
plt.imshow(carpet, cmap='binary')
plt.title('Sierpinski carpet (order %d)' % order)
plt.axis('off')
plt.show()

# Box-counting plot
plt.figure(figsize=(5,4))
plt.plot(np.log(1/np.array(box_sizes)), np.log(N_boxes), 'o-', label='Box counting')
# Linear fit for slope (fractal dimension)
slope, intercept = np.polyfit(np.log(1/np.array(box_sizes)), np.log(N_boxes), 1)
plt.plot(np.log(1/np.array(box_sizes)), slope*np.log(1/np.array(box_sizes))+intercept, '--', label=f'Fit: $D_f$={slope:.4f}')
plt.xlabel(r'$\log(1/\ell)$')
plt.ylabel(r'$\log N(\ell)$')
plt.title('Box-counting for Sierpinski carpet')
plt.legend(frameon=False)
plt.show()
```

:::


{{< video https://www.youtube.com/watch?v=QOa2qhDdywc >}}

### Arrested spinodal scenario

But how do physical gels come about? A viable scenario is the so-called **arrested spinodal decomposition** scenario. Here, a system is rapidly quenched into a region of its phase diagram where it would normally phase separate into two distinct phases (e.g., a dense and a dilute phase) via spinodal decomposition. However, before the phase separation can complete, the dynamics of the dense regions slow down dramatically—often due to glassy or jamming behavior—leading to a dynamically arrested, bicontinuous structure.

This process is relevant for colloid-polymer mixtures, protein solutions, and some polymer blends. The resulting gels are characterized by a network-like structure that reflects the early stages of spinodal decomposition, "frozen in" by the arrest of particle motion.

For the arested spinodal scenario to hold, the system evolves following these steps:
- The system is quenched inside the spinodal region, where spontaneous fluctuations grow.
- Domains of different densities form, but the dense domains become dynamically arrested before macroscopic phase separation completes.
- The final structure is a bicontinuous, percolating network with solid-like mechanical properties.




![The arrested spinodal scenario mapped on the phase diagram of the dispersed particles (e.g. colloids).](./figs/arrestedSpinodal.svg){ width=70%}

For more details, see @Zaccarelli2007.



## Glasses

In terms of their structure (e.g. as measured by a radial distribution function, $g(r)$) they are practically indistinguishable from a liquids, i.e. they exhibit some short ranged order, but no long ranged order. However, their dynamic behaviour is quite different from that of liquids; glasses exhibit very slow relaxation because molecules cannot easily diffuse. As a result, glasses do not flow (on experimental timescales) and therefore their mechanical properties, i.e. their response to stress and shearing, is more akin to that of solids.



<!--  Working minimal simulation from python to OJS
```{pyodide}
#| autorun: true

#| define:
#|   - particle_data
import numpy as np
from pyodide.ffi import to_js


# Flatten: each element contains [frame, particle_id, x, y, radius, hue]
particle_data = []

n_particles = 50
n_frames = 100
width, height = 400, 400

for frame in range(n_frames):
    for i in range(n_particles):
        angle = (frame * 0.1) + (i * 2 * np.pi / n_particles)
        radius = 50 + 30 * np.sin(frame * 0.05 + i * 0.1)
        
        x = width/2 + radius * np.cos(angle)
        y = height/2 + radius * np.sin(angle)
        r = 3 + 2 * np.sin(frame * 0.08 + i * 0.2)
        hue = (i/n_particles*360 + frame*2) % 360
        
        particle_data.extend([frame, i, x, y, r, hue])

particle_data = np.array(particle_data).tolist()
particle_data = to_js(particle_data)
```

```{ojs}
{
  const width = 400;
  const height = 400;
  const context = DOM.context2d(width, height);
  
  const n_particles = 50;
  const n_frames = 100;
  
  while (true) {
    const currentFrame = Math.floor(Date.now() / 50) % n_frames;
    
    context.clearRect(0, 0, width, height);
    
    // Extract particles for current frame
    for (let i = 0; i < particle_data.length; i += 6) {
      const frame = particle_data[i];
      const particle_id = particle_data[i + 1];
      const x = particle_data[i + 2];
      const y = particle_data[i + 3];
      const radius = particle_data[i + 4];
      const hue = particle_data[i + 5];
      
      if (frame === currentFrame) {
        context.fillStyle = `hsl(${hue}, 70%, 60%)`;
        context.beginPath();
        context.arc(x, y, radius, 0, 2 * Math.PI);
        context.fill();
      }
    }
    
    yield context.canvas;
  }
}

``` -->
### Glass forming systems

Glasses may be formed if the cooling rate is so fast that the liquid does not have time to crystallize , or alternatively because the molecules have some feature to their structure or bonding that inhibits the formation of a crystalline phase. Many glass forming materials are found in nature. Below we describe just a few.

Elements: Sulphur, Selenium and Phosphorous readily form glasses.
Metallic alloys: liquid metals can form glasses if quenched very rapidly ( $10^{6} \mathrm{~K} \mathrm{~s}^{-1}$ ). Such glasses find applications in recording heads and electrical transformers.
Polymer glasses: Due to entanglement effects, polymers form glasses very easily. Examples from everyday life are polycarbonates (eyeglasses, shatterproof windows) and polymethacrylate (plastic pipes and tubes).
Oxide glasses: Familiar examples are $\mathrm{SiO}_{2}, \mathrm{Na}_{2} \mathrm{O}$ and CaO (all components of window glass).
Organic glasses: Sucrose solution forms a glass used in boiled sweets. Toluene and methanol also readily form glasses.

### Relaxation time and viscosity

Let us first consider relaxation in liquids. We have already seen that at high volume fractions a particle is effectively trapped by its nearest neighbours, within a 'cage'. To escape the cage, a particle must jump. Let us now consider the characteristic time $\tau$ between such jumps and its dependence on temperature. Empirically, one finds that

$$
\tau^{-1} \sim v \exp \left(-\frac{\varepsilon}{k_{B} T}\right)
$$

Here $v$ is the typical vibration frequency for a particle rattling around in its cage. This can also be regarded as the frequency with which the particle attempts to escape its cage. The probability of an escape attempt succeeding depends on a Boltzmann factor, i.e. the exponential of the ratio of the energy cost associated with climbing the cage "walls", $\varepsilon$, to the typical thermal energy $k_{B} T$.

It can be shown (cf. PH10002), that the relaxation time $\tau$ is proportional to another quantity, the viscosity $\eta$, which is a measure of the response of a material to a shear stress, or more loosely speaking a measure of the ability of a liquid to flow. Thus $\tau \propto \eta$ and it follows that

$$
\eta=\frac{G_{0}}{v} \exp \left(\frac{\varepsilon}{k_{B} T}\right)
$$

where $G_{0}$ is a proportionality constant. This characteristic exponential temperature dependence is known as Arrhenius behaviour. Loosely speaking the viscosity provides a measure of the ability of a liquid to flow.

While Arrhenius behaviour is observed for the viscosity of liquids at high temperature, things are very different at low temperature. The experimentally measured viscosity and the associated configurational relaxation time exhibits a temperature dependence that strongly departs from that implied by the characteristic frequency of vibrations $v$

$$
\eta=\eta_{0} \exp \left(\frac{B}{T-T_{0}}\right)
$$

The empirical finding, known as the Vogel-Fulcher law implies that the viscosity diverges exponentially at the finite temperature $T_{0}$. In practical terms, this means that as $T_{0}$ is approached from above, one reaches some temperature $T_{g}$ (the glass transition temperature) at which the time for a configuration to relax becomes
comparable to the timescale of the experiment itself $\tau_{\exp }$. At this point, the system falls out of equilibrium with respect to rearrangements of the particle configurations.

### Characterising the glass transition

The glass transition temperature $T_{g}$ is that temperature at which the system falls out of equilibrium on the experimental timescale $\tau_{\text {exp. }}$. Clearly therefore, if one does a long experiment, in which the temperature is lowered slowly, the system will have a greater time to relax at each successive temperature and will stay in equilibrium to a lower temperature. Thus the experimental glass transition temperature depends on the rate at which we do the experiment. To illustrate this consider a measurement of some structural quantity (such as the sample volume) as a function of temperature. Various scenarios for the behaviour of the volume as the temperature are shown in the following figure
![](https://cdn.mathpix.com/cropped/2025_01_27_a9af280ad9752fe3d0c0g-37.jpg?height=875&width=1231&top_left_y=1113&top_left_x=344)

At the glass transition temperature, the curve exhibits a well defined discontinuity in it slope, rather like what happens at an equilibrium phase transition such as freezing or boiling. The reason why the glass transition is not a true thermodynamic phase transition is that the temperature at which the discontinuity occurs depends on the history of the sample, i.e. how rapidly it is cooled. We say that the glass transition is a kinetic phase transition brought on by the divergence of the structural relaxation time.

### A simple picture of the glass transition

Theories of the glass transition are a matter of intense current research and it is probably fair to say that the detailed physics is still not well understood in a comprehensive fashion. Nevertheless simple models offer some insight, in qualitative if not quantitative agreement with experiment. One such theory is based on the idea of cooperativity. In a liquid at high temperatures, a molecule can diffuse simply by moving to occupy the space made available by the random local motions of its neighbours. At low temperatures and high volume fractions, the local motion of neighbours is insufficient to allow diffusion. Instead a number of neighbours must move cooperatively in order to make space for a molecule to move. The minimum number of molecules that have to move in unison in order for diffusion of a molecule to take place gives rise to the concept of a cooperative rearranging region. As the temperature is lowered the size of this region increases, diverging (with the viscosity) at the Vogel-Fulcher temperature $T_{0}$
![](https://cdn.mathpix.com/cropped/2025_01_27_a9af280ad9752fe3d0c0g-38.jpg?height=597&width=621&top_left_y=1152&top_left_x=295)
![](https://cdn.mathpix.com/cropped/2025_01_27_a9af280ad9752fe3d0c0g-38.jpg?height=577&width=604&top_left_y=1162&top_left_x=1086)

If we suppose that the energy barrier for a single molecule to move is $\mu$ and there are $z$ molecules in the cooperatively rearranging region, then the thermal activation barrier for motion is

$$
\tau^{-1} \sim v \exp \left(-\frac{z \mu}{k_{B} T}\right)
$$

with $v$ the microscopic vibration frequency. Within this model, the non Arrhenius behaviour of the relaxation time (see above) derives from the increase in the number of molecules that have to move cooperatively as temperature decreases, i.e. the $T$ dependence of $z$.
