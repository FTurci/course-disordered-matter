[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "",
    "text": "Welcome to the Complex Disordered Matter Course!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Overview",
    "text": "Overview\nThis course introduces your to the theoretical, computational and experimental aspects of the physics of complex disordered matter.\n\nComplex disordered matter is the study of wide range of systems like polymers, colloids, glasses, gels, and emulsions, which lack long-range order but exhibit intricate behaviour. Colloids, suspensions of microscopic particles in a fluid, are useful for studying disordered structures due to their observable dynamics. Similarly, polymer systems can form amorphous solids or glasses when densely packed or cooled, showing solid-like rigidity despite their disordered structure. These materials often undergo phase transitions, such as demixing and crystallisation, and near these transitions, they can display critical phenomena with extensive fluctuations and correlations.\nThese various systems are examples of soft matter systems. In such systems, the interplay between disorder, softness, and phase behavior leads to rich physical phenomena, particularly near critical points where even small changes in external conditions can trigger large-scale reorganisations and universal behaviour. Glasses, for instance, exhibit slow relaxation and memory effects, while colloidal systems may crystallize, phase separate, or become jammed depending on particle interactions and concentration. Understanding such behaviors involves studying how microscopic interactions and thermal fluctuations influence macroscopic properties, especially in non-equilibrium conditions. Through techniques like scattering, microscopy, rheology, and simulation, one can explore how disordered soft materials respond to stress, age, or undergo transitions—insights that are vital for applications in materials design, biotechnology, and beyond.\nThis course is organized into three interconnected parts, each offering a distinct perspective on the study of complex disordered matter.\n\nPart 1: Unifying concepts (Nigel Wilding) introduces the theoretical framework for rationalising complex disordered matter which is grounded in statistical mechanics and thermodynamics. We emphasize the theory of phase transitions, thermal fluctuations, critical phenomena, and stochastic dynamics—providing the essential theoretical tools needed to describe and predict the behavior of soft and disordered systems.\n\nPart 2: Complex disordered matter (Francesco Turci) explores the phenomenology of key examples of complex disordered soft matter systems, including colloids, polymers, liquid crystals, glasses, gels, and active matter. These systems will be analyzed using the theoretical concepts introduced in Part 1, highlighting how disorder, interactions, and fluctuations shape their macroscopic behavior.\n\nPart 3: Experimental techniques (Adrian Barnes) focuses on the methods of microscopy, and scattering via x-rays, neutrons and light that are used to study complex disordered matter, offering insight into how their properties are measured and understood in real-world contexts.\n\nIn addition to theory and experiment, computer simulation plays a central role in soft matter research. This course includes a substantial coursework component consisting of two computational projects. These exercises will allow you to apply state-of-the-art simulation techniques to investigate the complex behavior of disordered systems, bridging theory and observation through hands-on exploration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#delivery-and-format",
    "href": "index.html#delivery-and-format",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Delivery and format",
    "text": "Delivery and format\n\nDetailed e-notes (accessible via Blackboard) can be viewed on a variety of devices. Pdf is also available.\nWe will give ‘traditional’ lectures (Tuesdays, Wednesdays, Fridays) in which we use slides to summarise and explain the lecture content. Questions are welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to the explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are problem classes (Thursdays) where you can try problem sheets and seek help. Lecturers may go over some problems with the class.\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Intended learning outcomes",
    "text": "Intended learning outcomes\nThe course will\n\nIntroduce you to the qualitative features of a range of complex and disordered systems and the experimental techniques used to study them.\nIntroduce you to a range of model systems and theoretical techniques used to elucidate the physics of complex disordered matter.\nProvide you with elementary computational tools to model complex disordered systems numerically and predict their properties.\nAllow you to apply your physics background to understand a variety of systems of inter-disciplinary relevance.\nConnect with the most recent advances in the research on complex disordered matter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Contact details",
    "text": "Contact details\nThe course will be taught by\n\nProf Nigel B. Wilding (unit director): nigel.wilding@bristol.ac.uk\nDr Francesco Turci: F.Turci@bristol.ac.uk\nDr Adrian Barnes: a.c.barnes@bristol.ac.uk",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#questions-and-comments",
    "href": "index.html#questions-and-comments",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Questions and comments",
    "text": "Questions and comments\nIf you have any questions about the course, please don’t hesitate to contact the relevant lecturer, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025/26. If you find any errors or mistakes or something which isn’t clear, please let us know by email, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Recommended texts and literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course course is the absence of a single wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking.\n\nRevision on thermodynamics and statistical mechanics\nSee your year two Thermal Physics notes. Also\n\nF. Mandl: Statistical Physics\n\n\n\nPhase transitions and critical phenomena\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\n\n\nStochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry\n\n\n\nSoft matter and glasses\nThe best overall text for part 2 of the course is:\n\nR.A.L Jones, Soft Condensed Matter.\n\nAdditionally, the following more specialised texts (which include information on experimental techniques) might be useful.\n\nColloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet\nR.J.Hunter: Introduction to Modern Colloid Science\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions\nD.H.Everett: Basic Principles of Colloid Science\n\n\n\nPolymers and surfactants\n\nR.J. Young and P.A. Lovell: Introduction to polymers\nM. Doi: Introduction to polymer physics\nJ.Israelachvili, Intermolecular and Surface Forces\n\n\n\nGlasses\n\nJ. Zarzycki; Glasses and the vitreous state",
    "crumbs": [
      "Recommended texts"
    ]
  },
  {
    "objectID": "phase-transitions/introduction.html",
    "href": "phase-transitions/introduction.html",
    "title": "1  Introduction to phase behaviour and enhanced fluctuations",
    "section": "",
    "text": "A phase transition can be defined as a macroscopic rearrangment of the internal constituents of a system in response to a change in the thermodynamic conditions to which they are subject. A wide variety of physical systems undergo such transitions. Understanding the properties of phase transitions is fundamental to the study of soft and complex matter, as these systems often exhibit rich and subtle transformations between different states of organization. Whether in colloidal suspensions, polymer blends, liquid crystals, or biological materials, phase transitions underpin a wide range of physical behaviours, from self-assembly and pattern formation to critical phenomena and dynamical arrest. By analysing how macroscopic phases emerge from microscopic interactions and external conditions, one gains crucial insight into the principles that govern structure, stability, and functionality in these intricate systems. As such, an understanding of phase transitions not only enriches theoretical understanding but also informs practical applications across materials science, biophysics, and nanotechnology. For these reasons we will devote a large proportion of this course to the study of phase transitions.\nTwo classic examples of systems displaying phase transitions are the ferromagnet and fluid systems. For the magnet, a key observable is the magnetisation defined as the magnetic moment per spin, given by \\(m=M/N\\), with \\(N\\) the number of spins. \\(m\\) can be positive or negative, dependent on whether the spins are aligned ‘up’ or ‘down’. As the temperature of a ferromagnet is increased, its net magnetisation \\(|m|\\) is observed to decrease smoothly, until at a certain temperature known as the critical temperature, \\(T_c\\), it vanishes altogether (see left part of Figure 1.1). We define the magnetisation to be the order parameter of this phase transition.\nOne can also envisage applying a magnetic field \\(H\\) to the system which, depending on its sign (i.e. whether it is aligned (positive) or anti-aligned (negative) relative to the magnetisation axis), favours up or down spin states respectively, as shown schematically in Figure 1.1 (right part). Changing the sign of the magnetic field \\(H\\) for \\(T&lt;T_c\\) leads to a phase transition chacterised by a discontinuous jump in \\(m\\). We shall explore this behaviour in more detail in section 5.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of a simple magnet (schematic). Left: magnetisation as a function of temperature for zero applied magnetic field, \\(H=0\\). Right: Applying a magnetic field that is aligned or antialigned with the direction of the magnetisation leads to a phase transition. The \\(H=0\\) axis at \\(T&lt;T_c\\) is the coexistence curve for which positive and negative magnetisations are equally likely.\n\n\n\nSimilarly, a change of state from liquid to gas can be induced in a fluid system (though not in an ideal gas) simply by raising the temperature. Typically the liquid-vapour transition is abrupt, reflecting the large number density difference between the states either side of the transition. However the abruptness of this transition can be reduced by applying pressure. At one particular pressure and temperature the discontinuity in the density difference between the two states vanishes and the two phases coalesce. These conditions of pressure and temperature serve to locate the critical point for the fluid. We define the density difference \\(\\rho_{liq}-\\rho_{vap}\\) to be the order parameter for the liquid-gas phase transition. We shall meet order parameters for other, more complex, systems in section 5,\n\n\n\n\n\n\nFigure 1.2: Phase diagram of a simple fluid (schematic)\n\n\n\nIn the vicinity of a critical point, a system displays a host of remarkable behaviors known as critical phenomena. Chief among these is the divergence of thermal response functions—such as specific heat, compressibility, or magnetic susceptibility—which signal an enhanced sensitivity to external perturbations. These singularities arise from the emergence of large-scale cooperative interactions among the system’s microscopic constituents, as measured by a diverging correlation length (see Chapter 2). One visually striking manifestation of this is critical opalescence, particularly observed in fluids like CO\\(_2\\). As carbon dioxide nears its critical temperature and pressure, the distinction between its liquid and gas phases vanishes, giving rise to huge fluctuations in density. These fluctuations scatter visible light, rendering the fluid milky or opalescent. This scattering effect directly reflects the long-range correlations developing within the fluid. The movie below illustrates the effect as the critical temperature of CO\\(_2\\) is approached from above. Note the appearence of a liquid-vapour interface (meniscus) as the system enters the two-phase region.\n\nThe recalcitrant problem posed by the critical region is how best to incorporate such collective effects within the framework of a rigorous mathematical theory that affords both physical insight and quantitative explanation of the observed phenomena. This matter has been (and still is!) the subject of intense theoretical activity.\nThe importance of the critical point stems largely from the fact that many of the phenomena observed in its vicinity are believed to be common to a whole range of apparently quite disparate physical systems. Systems such as liquid mixtures, superconductors, liquid crystals, ferromagnets, antiferromagnets and molecular crystals may display identical behaviour near criticality. This observation implies a profound underlying similarity among physical systems at criticality, regardless of many aspects of their distinctive microscopic nature. These ideas have found formal expression in the so-called ‘universality hypothesis’ which, since its inception in the 1970s, has enjoyed considerable success.\nIn the next few lectures, principal aspects of the contemporary theoretical viewpoint of phase transitions and critical phenomena will be reviewed. Mean field theories of phase transitions will be discussed and their inadequacies in the critical region will be exposed. The phenomenology of the critical region will we described including power laws, critical exponents and their relationship to scaling phenomena. These will be set within the context of the powerful renormalisation group technique. The notion of universality as a phenomenological hypothesis will be introduced and its implications for real and model systems will be explored. Finally, the utility of finite-size scaling methods for computer studies of critical phenomena will be discussed, culminating in the introduction of a specific technique suitable for exposing universality in model systems. Thereafter we will consider some foundational concepts in the dynamics of complex disordered matter. We shall look at the processes by which one phase transform into another and introduce differential equations that allow us to deal with the inherent stochasticity of thermal systems. The wider applicability of these unifying concepts to complex disordered systems such as colloids, polymers, liquid crystals and glasses will be covered in part 2 of the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to phase behaviour</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html",
    "href": "phase-transitions/background.html",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "2.1 Observables and expectation values\nIn seeking to describe phase transition and critical phenomena, it is useful to have a quantitative measure of the difference between the phases: this is the role of the order parameter, \\(Q\\). In the case of the fluid, the order parameter is taken as the difference between the densities of the liquid and vapour phases. In the ferromagnet it is taken as the magnetisation. As its name suggest, the order parameter serves as a measure of the kind of orderliness that sets in when the temperature is cooled below a critical temperature.\nOur first task is to give some feeling for the principles which underlie the ordering process. Referring back to Section 1.2, the probability \\(p_a\\) that a physical system at temperature \\(T\\) will have a particular microscopic arrangement (alternatively referred to as a ‘configuration’ or ‘state’), labelled \\(a\\), of energy \\(E_a\\) is\n\\[\np_a=\\frac{1}{Z}e^{-E_a/k_BT}\n\\tag{2.1}\\]\nThe prefactor \\(Z^{-1}\\) is the partition function: since the system must always have some specific arrangement, the sum of the probabilities \\(p_a\\) must be unity, implying that\n\\[\nZ=\\sum_ae^{-E_a/k_BT}\n\\tag{2.2}\\] where the sum extends over all possible microscopic arrangements.\nThese equations assume that physical system evolves rapidly (on the timescale of typical observations) amongst all its allowed arrangements, sampling them with the probabilities Equation 2.1 the expectation value of any physical observable \\(O\\) will thus be given by averaging \\(O\\) over all the arrangements \\(a\\), weighting each contribution by the appropriate probability:\n\\[\\overline {O}=\\frac{1}{Z}\\sum_a O_a e^{-E_a/k_BT}\n\\tag{2.3}\\]\nSums like Equation 2.3 are not easily evaluated because the number of terms grows exponentially in the system size. Nevertheless, some important insights follow painlessly. Consider the case where the observable of interest is the order parameter, or more specifically the magnetisation of a ferromagnet.\n\\[\nQ=\\frac{1}{Z}\\sum_a Q_a e^{-E_a/k_BT}\n\\tag{2.4}\\]\nIt is clear from Equation 2.1 that at very low temperature the system will be overwhelmingly likely to be found in its minimum energy arrangements (ground states). For the ferromagnet, these are the fully ordered spin arrangements having magnetisation \\(+1\\), or \\(-1\\).\nNow consider the high temperature limit. The enhanced weight that the fully ordered arrangement carries in the sum of Equation 2.4 by virtue of its low energy, is now no longer sufficient to offset the fact that arrangements in which \\(Q_a\\) has some intermediate value, though each carry a smaller weight, are vastly greater in number. A little thought shows that the arrangements which have essentially zero magnetisation (equal populations of up and down spins) are by far the most numerous. At high temperature, these disordered arrangements dominate the sum in Equation 2.4 and the order parameter is zero.\nThe competition between energy-of-arrangements weighting (or simply ‘energy’) and the ‘number of arrangements’ weighting (or ‘entropy’) is then the key principle at work here. The distinctive feature of a system with a critical point is that, in the course of this competition, the system is forced to choose amongst a number of macroscopically different sets of microscopic arrangements.\nFinally in this section, we note that the probabilistic (statistical mechanics) approach to thermal systems outlined above is completely compatible with classical thermodynamics. Specifically, the bridge between the two disciplines is provided by the following equation\n\\[\nF=-k_BT \\ln Z\n\\tag{2.5}\\]\nwhere \\(F\\) is the “Helmholtz free energy”. All thermodynamic observables, for example the order parameter \\(Q\\), and response functions such as the specific heat or magnetic susceptibility are obtainable as appropriate derivatives of the free energy. For instance, utilizing Equation 2.2, one can readily verify (try it as an exercise!) that the average internal energy is given by\n\\[\\overline{E}=-\\frac{\\partial \\ln Z}{\\partial \\beta},\\]\nwhere \\(\\beta=(k_BT)^{-1}\\).\nThe relationship between other thermodynamic quantities and derivatives of the free energy are given in fig. Figure 2.1",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#observables-and-expectation-values",
    "href": "phase-transitions/background.html#observables-and-expectation-values",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "Figure 2.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#sec-correlations",
    "href": "phase-transitions/background.html#sec-correlations",
    "title": "2  Key concepts for phase transitions",
    "section": "2.2 Correlations",
    "text": "2.2 Correlations\n\n2.2.1 Spatial correlations\nThe two-point connected correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where the correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nIn isotropic fluids and particle systems, a closely related and more directly measurable quantity (particularly in simulations) is the radial distribution function \\(g(r)\\), which describes how particle density varies as a function of distance from a reference particle. For such systems, the two-point correlation function of the number density field \\(\\rho(\\vec{r})\\) is related to \\(g(r)\\) as follows:\n\\[\ng(r) = 1+\\frac{C(r)}{\\rho^2},\n\\] where \\(\\rho\\) is the average number density. This relation shows that \\(g(r)\\) encodes the same spatial correlations as \\(C(r)\\), but in a form that is more natural for discrete particle systems. Note that by definition \\(g(r)\\to 1\\) in the absence of correlations ie. when \\(C(r)=0\\). This is typically the case for \\(r\\gg\\xi\\).\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector.\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.\n\n\n2.2.2 Temporal correlations\nConsider a thermodynamic variable \\(x\\) with zero mean that fluctuates over time. Examples include the local magnetization in a magnetic system or the local density in a fluid. Here, \\(x\\) represents a deviation from the average value — a fluctuation.\nWe’re interested in how such fluctuations are correlated over time when the system is in thermal equilibrium. For instance, if \\(x\\) is positive at some time \\(t\\), it’s more likely to remain positive shortly after.\nThese temporal correlations are characterized by the two-time correlation function (also known as an auto-correlation function):\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle\n\\]\nIn equilibrium, the correlation function must be independent of the starting time \\(\\tau\\). Therefore, we define:\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle = M_{xx}(t)\n\\]\nThat is, \\(M_{xx}(t)\\) depends only on the time difference \\(t\\).\nWe typically expect \\(M_{xx}(t)\\) to decay exponentially over a characteristic correlation time \\(t_c\\):\n\\[\nM_{xx}(t) \\sim \\exp(-t / t_c)\n\\]\n\n\n\n\n\n\nFigure 2.2: Sketch of \\(M_{xx}(t)\\) against \\(t\\)\n\n\n\nThis exponential decay reflects how the memory of fluctuations fades with time.\nNow consider two different fluctuating variables, \\(x\\) and \\(y\\) (e.g., local magnetizations at different positions). Their cross-correlation function is defined as:\n\\[\n\\langle x(\\tau) y(\\tau + t) \\rangle = M_{xy}(t)\n\\]\nThis defines the elements of a dynamic correlation matrix, of which \\(M_{xx}(t)\\) is the diagonal.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/approach-to-criticality.html",
    "href": "phase-transitions/approach-to-criticality.html",
    "title": "3  The approach to criticality",
    "section": "",
    "text": "It is a matter of experimental fact that the approach to criticality in a given system is characterized by the divergence of various thermodynamic observables. Let us remain with the archetypal example of a critical system, the ferromagnet, whose critical temperature will be denoted as \\(T_c\\). For temperatures close to \\(T_c\\), the magnetic response functions (the magnetic susceptibility \\(\\chi\\) and the specific heat) are found to be singular functions, diverging as a power of the reduced (dimensionless) temperature \\(t \\equiv\n(T-T_c)/T_c\\):-\n\\[\n\\chi \\equiv \\frac{\\partial M}{\\partial H}\\propto t^{-\\gamma} ~~~~ (H=0)\n\\tag{3.1}\\]\n(where \\(M=mN\\)), \\[\nC_H \\equiv \\frac{\\partial E}{\\partial T}\\propto t^{-\\alpha} ~~~~ (H=\\textrm{ constant})\n\\tag{3.2}\\]\nAnother key quantity is the correlation length \\(\\xi\\), which measures the distance over which fluctuations of the magnetic moments are correlated. This is observed to diverge near the critical point with an exponent \\(\\nu\\).\n\\[\n\\xi \\propto t^{-\\nu} ~~~~ (T &gt; T_c,\\: H=0)\n\\tag{3.3}\\]\nSimilar power law behaviour is found for the order parameter \\(Q\\) (in this case the magnetisation) which vanishes in a singular fashion (it has infinite gradient) as the critical point is is approached as a function of temperature:\n\\[\nm \\propto t^{\\beta} ~~~~ (T &lt; T_c,\\: H=0)\n\\tag{3.4}\\] (here the symbol \\(\\beta\\), is not to be confused with \\(\\beta=1/k_BT\\)– this unfortunately is the standard notation.)\nFinally, as a function of magnetic field:\n\\[m \\propto h^{1/\\delta} ~~~~ (T = T_c,\\: H&gt;0) . \\tag{3.5}\\] with \\(h=(H-H_c)/H_c\\), the reduced magnetic field.\nAs examples, the behaviour of the magnetisation and correlation length are plotted in Figure 3.1 as a function of \\(t\\).\n\n\n\n\n\n\nFigure 3.1: Singular behaviour of the correlation length and order parameter in the vicinity of the critical point as a function of the reduced temperature \\(t\\).\n\n\n\nThe quantities \\(\\gamma, \\alpha, \\nu, \\beta\\) in the above equations are known as critical exponents. They serve to control the rate at which the various thermodynamic quantities change on the approach to criticality.\nRemarkably, the form of singular behaviour observed at criticality for the example ferromagnet also occurs in qualitatively quite different systems such as the fluid. All that is required to obtain the corresponding power law relationships for the fluid is to substitute the analogous thermodynamic quantities in to the above equations. Accordingly the magnetisation order parameter is replaced by the density difference \\(\\rho_{liq}-\\rho_{gas}\\) while the susceptibility is replaced by the isothermal compressibility and the specific heat capacity at constant field is replaced by the specific heat capacity at constant volume. The approach to criticality in a variety of qualitatively quite different systems can therefore be expressed in terms of a set of critical exponents describing the power law behaviour for that system (see the book by Yeomans for examples).\nEven more remarkable is the experimental observation that the values of the critical exponents for a whole range of fluids and magnets (and indeed many other systems with critical points) are identical. This is the phenomenon of universality. It implies a deep underlying physical similarity between ostensibly disparate critical systems. The principal aim of theories of critical point phenomena is to provide a sound theoretical basis for the existence of power law behaviour, the factors governing the observed values of critical exponents and the universality phenomenon. Ultimately this basis is provided by the Renormalisation Group (RG) theory, for which K.G. Wilson was awarded the Nobel Prize in Physics in 1982.\nMore about the scientists mentioned in this chapter:\nKenneth Wilson",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The approach to criticality</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html",
    "href": "phase-transitions/Ising-model.html",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "4.1 The 2D Ising model\nIn order to probe the properties of the critical region, it is common to appeal to simplified model systems whose behaviour parallels that of real materials. The sophistication of any particular model depends on the properties of the system it is supposed to represent. The simplest model to exhibit critical phenomena is the two-dimensional Ising model of a ferromagnet. Actual physical realizations of 2-d magnetic systems do exist in the form of layered ferromagnets such as K\\(_2\\)CoF\\(_4\\), so the 2-d Ising model is of more than just technical relevance.\nThe 2-d spin-\\(\\frac{1}{2}\\) Ising model envisages a regular arrangement of magnetic moments or ‘spins’ on an infinite plane. Each spin can take two values, \\(+1\\) (‘up’ spins) or \\(-1\\) (‘down’ spins) and is assumed to interact with its nearest neighbours according to the Hamiltonian\n\\[\n{\\cal H}_I=-J\\sum_{&lt;ij&gt;}s_is_j - H\\sum_i s_i\n\\tag{4.1}\\]\nwhere \\(J&gt;0\\) measures the strength of the coupling between spins and the sum extends over nearest neighbour spins \\(s_i\\) and \\(s_j\\), i.e it is a sum of the bonds of the lattice. \\(H\\) is a magnetic field term which can be positive or negative (although for the time being we will set it equal to zero). The order parameter is simply the average magnetisation:\n\\[m=\\frac{1}{N} \\langle \\sum_i s_i \\rangle\\:,\\] where \\(\\langle\\cdot\\rangle\\) means an average over typical configurations corresponding to the prescribed value of \\(J/k_BT\\).\nThe fact that the Ising model displays a phase transition was argued in Chapter 2. Thus at low temperatures for which there is little thermal disorder, there is a preponderance of aligned spins and hence a net spontaneous magnetic moment (ie. the system is ferromagnetic). As the temperature is raised, thermal disorder increases until at a certain temperature \\(T_c\\), entropy drives the system through a continuous phase transition to a disordered spin arrangement with zero net magnetisation (ie. the system is paramagnetic). These trends are visible in configurational snapshots from computer simulations of the 2D Ising model (see Figure 4.1). Although each spin interacts only with its nearest neighbours, the phase transition occurs due to cooperative effects among a large number of spins.\nAn interactive Monte Carlo simulation of the Ising model demonstrates the phenomenology, By altering the temperature you will be able to observe for yourself how the typical spin arrangements change as one traverses the critical region. Pay particular attention to the configurations near the critical point. They have very interesting properties. We will return to them later!\nAlthough the 2-d Ising model may appear at first sight to be an excessively simplistic portrayal of a real magnetic system, critical point universality implies that many physical observables such as critical exponents are not materially influenced by the actual nature of the microscopic interactions. The Ising model therefore provides a simple, yet quantitatively accurate representation of the critical properties of a whole range of real magnetic (and indeed fluid) systems. This universal feature of the model is largely responsible for its ubiquity in the field of critical phenomena. We shall explore these ideas in more detail later in the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "href": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "(a) \\(T=1.2T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(T=T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(T=0.95T_c\\)\n\n\n\n\n\n\n\nFigure 4.1: Configurations of the 2d Ising model. The patterns depict typical arrangements of the spins (white=+1, black=−1) generated in a computer simulation of the Ising model on a square lattice of \\(N=512\\) sites, at temperatures (from left to right) of \\(T= 1.2T_c\\), \\(T=T_c\\), and \\(T=0.95T_c\\). In each case only a portion of the system containing \\(128\\) sites in shown. The typical island size is a measure of the correlation length \\(\\xi\\): the excess of black over white (below \\(T_c\\) is a measure of the order parameter.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "href": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "4.2 Exact solutions: the one dimensional Ising chain",
    "text": "4.2 Exact solutions: the one dimensional Ising chain\nOne might well ask why the 2D Ising model is the simplest model to exhibit a phase transition. What about the one-dimensional Ising model (ie. spins on a line)? In fact in one dimension, the Ising model can be solved exactly. It turns out that the system is paramagnetic for all \\(T&gt;0\\), so there is no phase transition at any finite temperature. To see this, consider the ground state of the system in zero external field. This will have all spins aligned the same way (say up), and hence be ferromagnetic. Now consider a configuration with a various “domain walls” dividing spin up and spin down regions:\n\n\n\n\n\n\nFigure 4.2: (a) Schematic of an Ising chain at \\(T=0\\). (b) At a small finite temperature the chain is split into domains of spins ordered in the same direction. Domains are separated by notional domain “walls”, which cost energy \\(\\Delta=2J\\). Periodic boundary conditions are assumed.\n\n\n\nInstead of considering the underlying spin configurations, we shall describe the system in terms of the statistics of its domain walls. The energy cost of a wall is \\(\\Delta = 2J\\), independent of position. Domain walls can occupy the bonds of the lattice, of which there are \\(N-1\\). Moreover, the walls are noninteracting, except that you cannot have two of them on the same bond. (Check through these ideas if you are unsure.)\nIn this representation, the partition function involves a count over all possible domain wall arrangements. Since the domain walls are non interacting (eg it doesn’t cost energy for one to move along the chain) they can be treated as independent. Independent contributions to the partition function simply multiply. So we can calculate \\(Z\\) by considering the partition function associated with a single domain wall being present or absent on some given bond, and then simply raise to the power of the number of bonds:\n\\[Z=Z_1^{N-1}\\]\nwhere\n\\[Z_1=e^{\\beta J} + e^{\\beta (J-\\Delta)}=e^{\\beta J}(1+e^{-\\beta\\Delta})\\] is the domain wall partition function for a single bond and represent the sum over the two possible states: domain wall absent or present. Then the free energy per bond of the system is\n\\[\\beta f\\equiv \\beta F/(N-1)=-\\ln Z_1=-\\beta J-\\ln(1+e^{-\\beta\\Delta})\\]\nThe first term on the RHS is simply the energy per spin of the ferromagnetic (ordered) phase, while the second term arises from the free energy of domain walls. Clearly for any finite temperature (ie. for \\(\\beta&lt;\\infty\\)), this second term is finite and negative. Hence the free energy will always be lowered by having a finite concentration of domain walls in the system. Since these domain walls disorder the system, leading to a zero average magnetisation, the 1D system is paramagnetic for all finite temperatures. Exercise: Explain why this argument works only in 1D.\nThe animation below lets you see qualitatively how the typical number of domain walls varies with temperature. If you’d lke to explore more quantitatively, a python code performing a Monte Carlo simulation is available. You will learn about Monte Carlo simulation in the coursework and in later parts of the course.\n\n\nShow python code\n#Monte Carlo simulation of the 1d Ising chain with periodic bounary conditions\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Slider\n\n# Number of spins\nN = 20 \n\n# Initialize spins (+1 or -1)\nspins = np.random.choice([-1, 1], size=N)\n\n# Initial temperature\nT = 2.0\n\n# Set up figure and axis for the spins\nfig, ax = plt.subplots(figsize=(10, 2))\nplt.subplots_adjust(bottom=0.25)  # make room for slider\nax.set_xlim(-0.5, N - 0.5)\nax.set_ylim(-1, 1)\nax.axis('off')\n\n# Create text objects for each spin\ntexts = []\nfor i in range(N):\n    arrow = '↑' if spins[i] == 1 else '↓'\n    t = ax.text(i, 0, arrow, fontsize=24, ha='center', va='center')\n    texts.append(t)\n\ndef update(frame):\n    \"\"\"Perform Metropolis updates over all spins, then refresh display.\"\"\"\n    global spins, T\n    for _ in range(N):\n        i = np.random.randint(N)\n        left = spins[(i - 1) % N]\n        right = spins[(i + 1) % N]\n        deltaE = 2 * spins[i] * (left + right)\n        # Metropolis criterion ensures configurations appear with the correct Boltzmann probability\n        if deltaE &lt; 0 or np.random.rand() &lt; np.exp(-deltaE / T):\n            spins[i] *= -1\n    # Update arrows on screen\n    for idx, t in enumerate(texts):\n        t.set_text('↑' if spins[idx] == 1 else '↓')\n    return texts\n\n# Create the animation with caching disabled and blit turned off\nani = FuncAnimation(\n    fig,\n    update,\n    interval=200,\n    blit=False,\n    cache_frame_data=False\n)\n\n# Add a temperature slider\nax_T = plt.axes([0.2, 0.1, 0.6, 0.03], facecolor='lightgray')\nslider_T = Slider(ax_T, 'Temperature T', 0.1, 5.0, valinit=T)\n\ndef on_T_change(val):\n    \"\"\"Callback to update T when the slider changes.\"\"\"\n    global T\n    T = val\n\nslider_T.on_changed(on_T_change)\n\n# Show the plot (ani is kept in scope so it won't be deleted)\nplt.show()\n\n\n\n\n\n Temperature T =\n\n2.0\n\n \n\n\n4.2.1 More general 1D spins systems: transfer matrix method\nGenerally speaking one-dimensional systems lend themselves to a degree of analytic tractability not found in most higher dimensional models. Indeed for the case of a 1-d assembly of \\(N\\) spins each having \\(m\\) discrete energy states, and in the presence of a magnetic field, it is possible to reduce the evaluation of the partition function to the calculation of the eigenvalues of a matrix–the so called transfer matrix.\nLet us start by assuming that the assembly has cyclic boundary conditions, then the total energy of configuration \\(\\{s\\}\\) is \\[\n\\begin{aligned}\n{\\cal H}(\\{s\\})=&-\\sum_{i=1}^N (J s_i s_{i+1}+Hs_i)\\\\\n\\:=&-\\sum_{i=1}^N (J s_i s_{i+1}+H(s_i+s_{i+1})/2)\\\\\n\\:=&\\sum_{i=1}^N E(s_i,s_{i+1})\n\\end{aligned}\n\\]\nwhere we have defined \\(E(s_i,s_{i+1})=-J s_i s_{i+1}-H(s_i+s_{i+1})/2\\).\nNow the partition function may be written\n\\[\\begin{aligned}\nZ_N =& \\sum_{\\{s\\}}\\exp\\left(-\\beta {\\cal H}(\\{s\\})\\right)\\nonumber \\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta[E(s_1,s_2)+E(s_2,s_3)+....E(s_N,s_1)]\\right) \\nonumber\\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta E(s_1,s_2)\\right)\\exp\\left(-\\beta E(s_2,s_3)\\right)....\\exp\\left(-\\beta E(s_N,s_1)\\right) \\nonumber\\\\\n=&\\sum_{i,j,...,l=1}^m V_{ij}V_{jk}...V_{li}\n\\end{aligned} \\tag{4.2}\\]\nwhere the \\(V_{ij}=\\exp(-\\beta E_{ij})\\) are elements of an \\(m \\times m\\) matrix \\(\\mathbf{V}\\), known as the transfer matrix (\\(i,j,k\\) etc are dummy indices that run over the matrix elements). You should see that the sum over the product of matrix elements picks up all the terms in the partition function and therefore Equation 4.2 is an alternative way of writing the partition function.\nThe reason it is useful to transform to a matrix representation is that it transpires that the sum over the product of matrix elements in Equation 4.2 is simply just the trace of \\(\\mathbf{V}^N\\) (check this yourself for a short periodic chain), given by the sum of its eigenvalues:-\n\\[Z_N=\\lambda_1^N+\\lambda_2^N+...\\lambda_m^N\\] For very large \\(N\\), this expression simplifies further because the largest eigenvalue \\(\\lambda_1\\) dominates the behaviour since \\((\\lambda_2/\\lambda_1)^N\\) vanishes as \\(N\\rightarrow \\infty\\). Consequently in the thermodynamic limit one may put \\(Z_N=\\lambda_1^N\\) and the problem reduces to identifying the largest eigenvalue of the transfer matrix.\nSpecializing to the case of the simple Ising model in the presence of an applied field \\(H\\), the transfer matrix takes the form\n\\[\\mathbf{V}(H)=\\left(\n\\begin{array}{cc}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{array} \\right)\\]\nThis matrix has two eigenvalues which can be readily calculated in the usual fashion as the roots of the characteristic polynomial \\(|\\mathbf{V}-\\lambda\\mathbf{I}|\\). They are\n\\[\\lambda_{\\pm}=e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\\]\nHence the free energy per spin \\(f=-k_BT\\ln \\lambda_+\\) is\n\\[f=-k_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\\]\nThe Ising model in 2D can also be solved exactly, as was done by Lars Onsager in 1940. The solution is extremely complicated and is regarded as one of the pinnacles of statistical mechanics. In 3D no exact solution is known.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html",
    "href": "phase-transitions/precourse-reading.html",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Ensembles and free energies\nComplex disordered systems are composed of an enormous number of interacting components—typically on the order of \\(\\sim 10^{23}\\). These interactions can lead to fascinating emergent behaviour, but they also render the systems analytically intractable; it is clearly impossible to solve Newton’s equations for such vast numbers of particles. To address this difficulty, we turn to Statistical Mechanics, which you first encountered in your second year. Statistical Mechanics provides the essential framework for connecting the microscopic behaviour of individual constituents with the macroscopic thermodynamic and dynamical properties of the system as a whole.\nIn this section, we will revisit and expand upon key concepts relevant to our discussion, with particular emphasis on the free energy—a central quantity that captures the balance between energy minimisation and entropy maximisation in determining the system’s equilibrium state. If any of these ideas feel unfamiliar, you may find it useful to revise the Statistical Mechanics material from your Year 2 Thermal Physics course notes.\nStatistical mechanics can be formulated in a variety of ensembles reflecting the relationship between the system and its environment. In what follows we summarise the formalism, focussing on the case of a particle fluid. Analogous equations apply to lattice spin models (see lectures and the book by Yeomans). Key ensembles are:",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "href": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Microcanonical ensemble\nApplies to a system of \\(N\\) particles (or spins) in a fixed volume \\(V\\) having adiabatic walls so that the internal energy \\(E\\) is constant. Denoted as constant-\\(NVE\\). Let \\(\\Omega\\) be the number of (micro)states having the prescribed energy:\n\\[\n\\Omega=\\sum_\\textrm{all states having energy E}\n\\]\nThermodynamically, the states favored in the canonical ensemble are those that maximise the entropy:\n\\[\nS=k_B\\ln \\Omega\\: .\n\\]\nwhere \\(k_B\\) is Boltzmann’s constant The microcanonical ensemble is useful for defining the entropy, but is little used in practice.\n\n\nCanonical ensemble\nApplies to a system of \\(N\\) particles in a fixed volume \\(V\\) and coupled to a heat bath at temperature \\(T\\). Denoted as constant-\\(NVT\\). A central quantity is the partition function\n\\[\nZ_{NVT}=\\sum_\\textrm{ all states i}e^{-\\beta E_i},~~~~~\\beta=1/(k_BT)\n\\tag{1}\\] which is a weighted sum over the states. The partition function provides the normalisation constant in the probability of finding the system in a given state \\(i\\).\n\\[\nP_i=\\frac{e^{-\\beta E_i}}{Z_{NVT}}.\n\\tag{2}\\]\nThe states favored in the canonical ensemble are those that minimise the free energy:\n\\[\nF_{NVT}=-\\beta^{-1}\\ln Z_{NVT}\\:.\n\\]\n\\(F_{NVT}\\) is known as the Helmholtz free energy. Thermodynamics also supplies a relation for the Helmholtz free energy:\n\\[\nF_{NVT}=E-TS\\:,\n\\] where \\(E\\) is the average internal energy. In minimising the free energy, the system strikes a compromise between low energy and high entropy. The temperature plays the role of arbiter, favouring high entropy at high \\(T\\), and low energy at low \\(T\\). The canonical ensemble is usually used to describe systems such as magnets, or a fluid held at constant volume. It is the ensemble we shall use most in this course.\n\n\nGrand canonical ensemble\nApplies to a system with a variable number of particle in a fixed volume \\(V\\) coupled to both a heat bath at temperature \\(T\\) and a particle reservoir with chemical potential \\(\\mu\\) (which is the field conjugate to \\(N\\)). Denoted as constant-\\(\\mu VT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty e^{\\beta\\mu N}Z_{NVT}\n\\] and a state probability analogous to Equation 2 holds. One can recast this in a form similar to Equation 1:\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty\\:\\sum_\\textrm{all~states~i}e^{-\\beta {\\cal H}_i},\n\\tag{3}\\] where \\({\\cal H}_i=E_i-\\mu N\\) is the form of the Hamiltonian in the grand canonical ensemble.\nStatistically, the states favored in the grand canonical ensemble are those that minimise the free energy:\n\\[\nF_{\\mu VT}=-\\beta^{-1}\\ln Z_{\\mu VT}\n\\] \\(F_{\\mu VT}\\) is known as the grand potential. It can also be derived from thermodynamics, from which one finds\n\\[\nF_{\\mu VT}=E-TS-\\mu N=-pV,\n\\] where \\(p\\) is the pressure.\nThe grand canonical ensemble is usually used to describe systems such as fluid connected to a particle reservoir. Sometimes for a magnet we consider the effects of an applied magnetic field, which is analogous to working in the grand canonical ensemble: the magnetic field (which is conjugate to the magnetisation) plays a similar role to the chemical potential in a fluid.\n\n\nIsothermal-isobaric ensemble\nApplies to a system with a fixed number of particles \\(N\\) that is coupled to a heat bath at temperature \\(T\\) and a reservoir that exerts a constant pressure \\(p\\) which allows the sample volume to fluctuate. Denoted as constant-\\(NpT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}\n\\] or \\[\nZ_{NpT}=\\int_0^\\infty dV\\:\\sum_\\textrm{i}e^{-\\beta {\\cal H}_i},\n\\tag{4}\\] where \\({\\cal H}_i=E_i+pV\\) is the form of the Hamiltonian in the constant-\\(NpT\\) ensemble. Again a state probability analogous to Equation 2 holds.\nStatistically, the states favored in the constant-\\(NpT\\) ensemble are those that minimise the free energy:\n\\[\nF_{NpT}=-\\beta^{-1}\\ln Z_{NpT}\n\\] \\(F_{NpT}\\) is known as the Gibb’s free energy (often denoted \\(G\\)). It can also be derived from thermodynamics, from which one finds\n\\[\nF_{NpT}=E-TS+pV=\\mu N\n\\]\nThe constant-\\(NpT\\) ensemble is usually used to describe systems such as a fluid subject to a variable pressure, or a magnet coupled to a magnetic field \\(H\\). In the latter case the quantity \\(HM\\) plays the role of \\(pV\\) and\n\\[\nF_{NpT}=E-TS-MH\\:,\n\\] with \\(M\\) the total magnetisation.",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "href": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "title": "Tools for understanding complex disordered matter",
    "section": "From free energies to observables",
    "text": "From free energies to observables\nFree energies are not directly observable quantities. However, all physical observables can be expressed in terms of derivatives of the free energy. One can derive the appropriate relations either from Thermodynamics, or the corresponding statistical mechanics (Revise your year-2 Thermal Physics notes on this if necessary). As an example let us consider a fluid in the isothermal-isobaric ensemble for which the appropriate free energy is \\(F_{NpT}=E-TS+pV\\), and where the volume fluctuates in response to the prescribed pressure. We shall seek an expression for the average volume in terms of the free energy. First lets us take the thermodynamic route. Differentiating the free energy and applying the chain rule we have:\n\\[\ndF=dE-TdS-sdT+pdV+VdP\\:.\n\\] But from the first law of thermodynamics, \\(dE=TdS-pdV\\), so\n\\[\ndF=-SdT+Vdp\\:,\n\\] and rearranging yields \\[\nV=\\left(\\frac{\\partial F}{\\partial p}\\right)_T\\:.\n\\]\nWe can now show that this result is consistent with the definition of \\(F_{NpT}\\) in terms of the partition function. Write\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}=\\int_0^\\infty dV\\sum_{all~states~i}e^{-\\beta (p V_i+E_i)}\n\\]\nThen\n\\[\\begin{align}\n\\left(\\frac{\\partial F}{\\partial p}\\right)_T\n&= -\\frac{1}{\\beta} \\left(\\frac{\\partial \\ln Z_{NpT}}{\\partial p}\\right)_T \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\frac{\\partial Z_{NpT}}{\\partial p} \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\int_0^\\infty dV \\int_{\\text{all states}} (-\\beta V) e^{-\\beta (p V + E)} \\\\\n&= \\langle V \\rangle_T \\,.\n\\end{align}\\]\nwhere in the last step we have used the fact that the probability of a state is defined to be \\(e^{-\\beta (p V_i+E_i)}/Z_{NpT}\\).\nExercise. Repeat these manipulations to find an expression for the mean particle number \\(N\\) in the grand canonical ensemble\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn the grand canonical ensemble (GCE), the relevant free energy is\n\\[\nF_{\\mu VT} = E - TS - \\mu N\n\\]\nFrom the first law of thermodynamics changes in the internal energy are given by:\n\\[\ndE = TdS - PdV + \\mu dN=TdS+\\mu dN\n\\] where we have used the fact that \\(V\\) is fixed in the GCE, so \\(dV=0\\).\nDifferentiating \\(F_{\\mu VT}\\):\n\\[\ndF_{\\mu VT} = dE - TdS - SdT - \\mu dN - N d\\mu\n= -S dT - N d\\mu\n\\] where for the last equality we have substitued for \\(dE\\) from above.\nThus \\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V} = -N\n\\quad \\Rightarrow \\quad\n\\langle N \\rangle = -\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V}\n\\]\nNow consider the statistical mechanics route to calculate \\(\\langle N\\rangle\\):\n\\[\nZ_{\\mu V T} = \\sum_{N=0}^{\\infty} \\sum_{\\text{states}} e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nThe grand potential (now written as \\(F_{\\mu VT}\\)) is:\n\\[\nF_{\\mu VT} = -k_B T \\ln Z_{\\mu V T}\n\\]\nWe now differentiate:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\left( \\frac{1}{Z} \\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} \\right)\n\\]\nFrom the partition function\n\\[\n\\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} = \\sum_{N=0}^\\infty \\sum_{\\text{states}} \\left( \\beta N \\right) e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nSubstitute:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\cdot \\beta \\cdot \\frac{1}{Z_{\\mu V T}} \\sum_{N=0}^\\infty \\sum_{\\text{states}} N\\, e^{-\\beta (E_{N,i} - \\mu N)} = - \\langle N \\rangle\n\\]\nwhere in the last step we have used the fact that in the GCE the Boltzmann probability of a microstate is defined to be \\(e^{-\\beta (E_{N,i}-\\mu N)}/Z_{\\mu VT}\\).",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html",
    "href": "phase-transitions/problems.html",
    "title": "Unifying concepts: Problems",
    "section": "",
    "text": "Although you should try all of these questions, some of them are deliberately quite challenging. If you don’t get very far with some, don’t worry. We’ll be going over them in problems classes, so you can just regard them as worked examples.\n\n1. Existence of a phase transition in \\(d=2\\).\nIn lectures it was argued that no long ranged order occurs at finite-temperatures in a one dimensional system because of the presence of domain walls. Were macroscopic domain walls to exist in two dimensions at finite temperature, they would similarly destroy long ranged order and prevent a phase transition. By calculating the free energy of a 2D domain wall for an Ising lattice, show that domain walls do not in fact exist for sufficiently low \\(T\\).\n(Hint: Model the domain wall as a non-reversing \\(N\\)-step random walk on the lattice and find an expression for its energy and -from the number of random walk configurations- its entropy.)\n\n\n\n2. Correlation Length\nFor a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small (as occurs at low \\(T\\))?\n\n\n\n3. A model fluid\nThe van der Waals (vdW) equation of state is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nWrite a Python script to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of \\(P\\) vs \\(V\\) on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\n4. Mean field theory of the Ising model heat capacity\nUsing results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle E\\rangle/\\partial T\\) has the behaviour\n\\[\\begin{align*}\nC_H=& 0 \\quad T&gt;T_c\\\\\nC_H=& 3Nk_B/2 \\quad T\\le T_c\n\\end{align*}\\]\n\n\n\n5. Magnetisation and fluctuations\nA system of spins on a lattice, has, in the absence of an applied field, a Hamiltonian \\({\\cal H}\\). In the presence of a field \\(h\\) the Hamiltonian becomes \\[\n\\tilde {\\cal H}={\\cal H}-hM\n\\] where \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\n\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\n\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\n6. Spin-1 Ising model\nA set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j + h\\sum_i s_i\\]\nFind the partition function and hence show that in the mean field approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\n7. Transfer Matrix.\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\n8. Landau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\n9. Scaling equation of state\nConsider a Landau expression for the free energy of a magnetic system having magnetisation \\(m\\):\n\\[\nF=F_0+\\tilde{a}_2tm^2+a_4m^4-Hm\\:,\n\\] where \\(t=T-T_c\\) and \\(H\\) is an applied magnetic field; \\(\\tilde{a}_2\\) and \\(a_4\\) are positive constants and \\(F_0\\) is a constant background term.\nShow that the equation of state for the model is\n\\[\nH=2\\tilde{a}_2tm+4a_4m^3\\:.\n\\]\nUse the near-critical power law behaviour of \\(m\\) to show that the equation of state may be written in the scaling form\n\\[\n\\frac{H}{m^\\delta}=g\\left(\\frac{t}{m^{1/\\beta}}\\right)\\:,\n\\] and find the (mean field) values of the critical exponents \\(\\delta\\) and \\(\\beta\\).\nDeduce that \\(g(x)=x+1\\) up to a choice of scale for \\(\\tilde{a}_2\\) and \\(a_4\\).\n\n\n\n10. Scaling laws\nUsing the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\n\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\n\\]\nHence derive the scaling laws among the critical exponents:\n\\[\\begin{align*}\n\\alpha+\\beta(\\delta+1)=& 2 \\\\\n\\alpha+2\\beta+\\gamma =& 2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{align*}\\]\n(Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 8: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\n\n\n\n11. Classical nucleation theory\nA supercooled liquid metal is undergoing solidification. According to classical nucleation theory, the Gibbs free energy change \\(\\Delta G\\) for forming a spherical solid nucleus of radius \\(r\\) in the liquid is given by:\n\\[\n\\Delta G(r) = \\frac{4}{3}\\pi r^3 \\Delta G_v + 4\\pi r^2 \\gamma\n\\] where \\(\\Delta G_v &lt; 0\\) is the free energy change per unit volume due to the phase change, and \\(\\gamma &gt; 0\\) is the interfacial energy between the solid and liquid phases.\n(a) Derive the expression for the critical radius \\(r^*\\) at which the nucleus becomes stable and begins to grow.\n(b) Show that the critical energy barrier for nucleation \\(\\Delta G^*\\) is given by:\n\\[\n\\Delta G^* = \\frac{16\\pi \\gamma^3}{3 (\\Delta G_v)^2}\n\\]\n(c) Explain qualitatively how the degree of undercooling \\(\\Delta T\\) affects the rate of nucleation. You may use the fact that \\(\\Delta G_v \\propto \\Delta T\\) to support your answer.\n\n\n\n12. Colloidal diffusion\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + \\frac{e^{-\\gamma t}}{M} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{kT}{M} + \\left( u_0^2 - \\frac{kT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\n\n13. Einstein’s expression for the diffusion coefficient\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle [X(t) - X(0)]^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nUse the fact that \\(X(t) - X(0)\n= \\int_{0}^{t} u(t')\\,\\mathrm{d}t'\\) to show that the Einstein relation may be written as\n\\[\n\\gamma = \\frac{1}{\\mu} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\n\n14. Life in one dimension\nA particle lives on the sites of a one-dimensional lattice. At any instant it has probability \\(\\alpha\\) per unit time that it will hop to the site on its right and probability \\(\\alpha\\) per unit time of hopping to the site on its left.\nWrite down the master equation for the set of probabilities \\(p_n(t)\\) of finding the particle at the \\(n^{\\text{th}}\\) site, where \\(-\\infty &lt; n &lt; \\infty\\).\nSolve the master equation for the \\(p_n\\), subject to the initial condition that the particle was at the site \\(n = 0\\) at time \\(t = 0\\). Hence obtain the mean position \\(\\langle n \\rangle\\) and root mean square deviation from the mean, both as functions of time.\nHint: The second part of the question is most easily done by introducing the generating function\n\\[\nF(z, t) = \\sum_{n=-\\infty}^{\\infty} p_n(t) z^n.\n\\]\n\n\n\n15. Master equation\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\n\n16. Detailed balance\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\n\n17. Jump processes\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump from state 1 to 2 and 2 to 3 but not directly from 1 to 3. Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the rate matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  }
]