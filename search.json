[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "",
    "text": "Welcome to the Complex Disordered Matter course!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Overview",
    "text": "Overview\nThis course introduces your to the theoretical, computational and experimental aspects of the physics of complex disordered matter.\n\nComplex disordered matter is the study of wide range of systems like polymers, colloids, glasses, gels, and emulsions, which lack long-range order but exhibit intricate behaviour. Colloids, suspensions of microscopic particles in a fluid, are useful for studying disordered structures due to their observable dynamics. Similarly, polymer systems can form amorphous solids or glasses when densely packed or cooled, showing solid-like rigidity despite their disordered structure. These materials often undergo phase transitions, such as demixing and crystallisation, and near these transitions, they can display critical phenomena with extensive fluctuations and correlations.\nThese various systems are examples of soft matter systems. In such systems, the interplay between disorder, softness, and phase behavior leads to rich physical phenomena, particularly near critical points where even small changes in external conditions can trigger large-scale reorganisations and universal behaviour. Glasses, for instance, exhibit slow relaxation and memory effects, while colloidal systems may crystallize, phase separate, or become jammed depending on particle interactions and concentration. Understanding such behaviors involves studying how microscopic interactions and thermal fluctuations influence macroscopic properties, especially in non-equilibrium conditions. Through techniques like scattering, microscopy, rheology, and simulation, one can explore how disordered soft materials respond to stress, age, or undergo transitions—insights that are vital for applications in materials design, biotechnology, and beyond.\nThis course is organized into three interconnected parts, each offering a distinct perspective on the study of complex disordered matter.\n\nPart 1: Unifying concepts (Nigel Wilding) introduces the theoretical framework for rationalising complex disordered matter which is grounded in statistical mechanics and thermodynamics. We emphasize the theory of phase transitions, thermal fluctuations, critical phenomena, and stochastic dynamics—providing the essential theoretical tools needed to describe and predict the behavior of soft and disordered systems.\n\nPart 2: Complex disordered matter (Francesco Turci) explores the phenomenology of key examples of complex disordered soft matter systems, including colloids, polymers, liquid crystals, glasses, gels, and active matter. These systems will be analyzed using the theoretical concepts introduced in Part 1, highlighting how disorder, interactions, and fluctuations shape their macroscopic behavior.\n\nPart 3: Experimental techniques (Adrian Barnes) focuses on the methods of microscopy, and scattering via x-rays, neutrons and light that are used to study complex disordered matter, offering insight into how their properties are measured and understood in real-world contexts.\n\nIn addition to theory and experiment, computer simulation plays a central role in soft matter research. This course includes a substantial coursework component consisting of a computational project. This exercise will allow you to apply state-of-the-art simulation techniques to investigate the complex behavior of disordered systems, bridging theory and observation through hands-on exploration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#delivery-and-format",
    "href": "index.html#delivery-and-format",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Delivery and format",
    "text": "Delivery and format\n\nDetailed e-notes (accessible via Blackboard) can be viewed on a variety of devices. Pdf is also available.\nWe will give ‘traditional’ lectures (Tuesdays, Wednesdays, Fridays) in which we use slides to summarise and explain the lecture content. Questions are welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to the explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are problem classes (Thursdays) where you can try problem sheets and seek help. Lecturers may go over some problems with the class.\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Intended learning outcomes",
    "text": "Intended learning outcomes\nThe course will\n\nIntroduce you to the qualitative features of a range of complex and disordered systems and the experimental techniques used to study them.\nIntroduce you to a range of model systems and theoretical techniques used to elucidate the physics of complex disordered matter.\nProvide you with elementary computational tools to model complex disordered systems numerically and predict their properties.\nAllow you to apply your physics background to understand a variety of systems of inter-disciplinary relevance.\nConnect with the most recent advances in the research on complex disordered matter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Contact details",
    "text": "Contact details\nThe course will be taught by\n\nProf Nigel B. Wilding (unit director): nigel.wilding@bristol.ac.uk\nDr Francesco Turci: F.Turci@bristol.ac.uk\nDr Adrian Barnes: a.c.barnes@bristol.ac.uk",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#questions-and-comments",
    "href": "index.html#questions-and-comments",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Questions and comments",
    "text": "Questions and comments\nIf you have any questions about the course, please don’t hesitate to contact the relevant lecturer, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025/26. If you find any errors or mistakes or something which isn’t clear, please let us know by email, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Recommended texts and literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course course is the absence of a single wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking.\n\nRevision on thermodynamics and statistical mechanics\nSee your year two Thermal Physics notes. Also\n\nF. Mandl: Statistical Physics\n\n\n\nPhase transitions and critical phenomena\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\n\n\nStochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry\n\n\n\nSoft matter and glasses\nThe best overall text for part 2 of the course is:\n\nR.A.L Jones, Soft Condensed Matter.\n\nAdditionally, the following more specialised texts (which include information on experimental techniques) might be useful.\n\nColloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet\nR.J.Hunter: Introduction to Modern Colloid Science\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions\nD.H.Everett: Basic Principles of Colloid Science\n\n\n\nPolymers and surfactants\n\nR.J. Young and P.A. Lovell: Introduction to polymers\nM. Doi: Introduction to polymer physics\nJ.Israelachvili, Intermolecular and Surface Forces\n\n\n\nGlasses\n\nJ. Zarzycki; Glasses and the vitreous state",
    "crumbs": [
      "Recommended texts"
    ]
  },
  {
    "objectID": "phase-transitions/introduction.html",
    "href": "phase-transitions/introduction.html",
    "title": "1  Introduction to phase behaviour and enhanced fluctuations",
    "section": "",
    "text": "A phase transition can be defined as a macroscopic rearrangment of the internal constituents of a system in response to a change in the thermodynamic conditions to which they are subject. A wide variety of physical systems undergo such transitions. Understanding the properties of phase transitions is fundamental to the study of soft and complex matter, as these systems often exhibit rich and subtle transformations between different states of organization. Whether in colloidal suspensions, polymer blends, liquid crystals, or biological materials, phase transitions underpin a wide range of physical behaviours, from self-assembly and pattern formation to critical phenomena and dynamical arrest. By analysing how macroscopic phases emerge from microscopic interactions and external conditions, one gains crucial insight into the principles that govern structure, stability, and functionality in these intricate systems. As such, an understanding of phase transitions not only enriches theoretical understanding but also informs practical applications across materials science, biophysics, and nanotechnology. For these reasons we will devote a large proportion of this course to the study of phase transitions.\nTwo classic examples of systems displaying phase transitions are the ferromagnet and fluid systems. For the magnet, a key observable is the magnetisation defined as the magnetic moment per spin, given by \\(m=M/N\\), with \\(N\\) the number of spins. \\(m\\) can be positive or negative, dependent on whether the spins are aligned ‘up’ or ‘down’. As the temperature of a ferromagnet is increased, its net magnetisation \\(|m|\\) is observed to decrease smoothly, until at a certain temperature known as the critical temperature, \\(T_c\\), it vanishes altogether (see left part of Figure 1.1). We define the magnetisation to be the order parameter of this phase transition.\nOne can also envisage applying a magnetic field \\(H\\) to the system which, depending on its sign (i.e. whether it is aligned (positive) or anti-aligned (negative) relative to the magnetisation axis), favours up or down spin states respectively, as shown schematically in Figure 1.1 (right part). Changing the sign of the magnetic field \\(H\\) for \\(T&lt;T_c\\) leads to a phase transition chacterised by a discontinuous jump in \\(m\\). We shall explore this behaviour in more detail in section 5.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of a simple magnet (schematic). Left: magnetisation as a function of temperature for zero applied magnetic field, \\(H=0\\). Right: Applying a magnetic field that is aligned or antialigned with the direction of the magnetisation leads to a phase transition. The \\(H=0\\) axis at \\(T&lt;T_c\\) is the coexistence curve for which positive and negative magnetisations are equally likely.\n\n\n\nSimilarly, a change of state from liquid to gas can be induced in a fluid system (though not in an ideal gas) simply by raising the temperature. Typically the liquid-vapour transition is abrupt, reflecting the large number density difference between the states either side of the transition. However the abruptness of this transition can be reduced by applying pressure. At one particular pressure and temperature the discontinuity in the density difference between the two states vanishes and the two phases coalesce. These conditions of pressure and temperature serve to locate the critical point for the fluid. We define the density difference \\(\\rho_{liq}-\\rho_{vap}\\) to be the order parameter for the liquid-gas phase transition. We shall meet order parameters for other, more complex, systems in section 5,\n\n\n\n\n\n\nFigure 1.2: Phase diagram of a simple fluid (schematic)\n\n\n\nIn the vicinity of a critical point, a system displays a host of remarkable behaviors known as critical phenomena. Chief among these is the divergence of thermal response functions—such as specific heat, compressibility, or magnetic susceptibility—which signal an enhanced sensitivity to external perturbations. These singularities arise from the emergence of large-scale cooperative interactions among the system’s microscopic constituents, as measured by a diverging correlation length (see Chapter 2). One visually striking manifestation of this is critical opalescence, particularly observed in fluids like CO\\(_2\\). As carbon dioxide nears its critical temperature and pressure, the distinction between its liquid and gas phases vanishes, giving rise to huge fluctuations in density. These fluctuations scatter visible light, rendering the fluid milky or opalescent. This scattering effect directly reflects the long-range correlations developing within the fluid. The movie below illustrates the effect as the critical temperature of CO\\(_2\\) is approached from above. Note the appearence of a liquid-vapour interface (meniscus) as the system enters the two-phase region.\n\nThe recalcitrant problem posed by the critical region is how best to incorporate such collective effects within the framework of a rigorous mathematical theory that affords both physical insight and quantitative explanation of the observed phenomena. This matter has been (and still is!) the subject of intense theoretical activity.\nThe importance of the critical point stems largely from the fact that many of the phenomena observed in its vicinity are believed to be common to a whole range of apparently quite disparate physical systems. Systems such as liquid mixtures, superconductors, liquid crystals, ferromagnets, antiferromagnets and molecular crystals may display identical behaviour near criticality. This observation implies a profound underlying similarity among physical systems at criticality, regardless of many aspects of their distinctive microscopic nature. These ideas have found formal expression in the so-called ‘universality hypothesis’ which, since its inception in the 1970s, has enjoyed considerable success.\nIn the next few lectures, principal aspects of the contemporary theoretical viewpoint of phase transitions and critical phenomena will be reviewed. Mean field theories of phase transitions will be discussed and their inadequacies in the critical region will be exposed. The phenomenology of the critical region will we described including power laws, critical exponents and their relationship to scaling phenomena. These will be set within the context of the powerful renormalisation group technique. The notion of universality as a phenomenological hypothesis will be introduced and its implications for real and model systems will be explored. Finally, the utility of finite-size scaling methods for computer studies of critical phenomena will be discussed, culminating in the introduction of a specific technique suitable for exposing universality in model systems. Thereafter we will consider some foundational concepts in the dynamics of complex disordered matter. We shall look at the processes by which one phase transform into another and introduce differential equations that allow us to deal with the inherent stochasticity of thermal systems. The wider applicability of these unifying concepts to complex disordered systems such as colloids, polymers, liquid crystals and glasses will be covered in part 2 of the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to phase behaviour</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html",
    "href": "phase-transitions/background.html",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "2.1 Observables and expectation values\nIn seeking to describe phase transition and critical phenomena, it is useful to have a quantitative measure of the difference between the phases: this is the role of the order parameter, \\(Q\\). In the case of the fluid, the order parameter is taken as the difference between the densities of the liquid and vapour phases. In the ferromagnet it is taken as the magnetisation. As its name suggest, the order parameter serves as a measure of the kind of orderliness that sets in when the temperature is cooled below a critical temperature.\nOur first task is to give some feeling for the principles which underlie the ordering process. Referring back to Section 1.2, the probability \\(p_a\\) that a physical system at temperature \\(T\\) will have a particular microscopic arrangement (alternatively referred to as a ‘configuration’ or ‘state’), labelled \\(a\\), of energy \\(E_a\\) is\n\\[\np_a=\\frac{1}{Z}e^{-E_a/k_BT}\n\\tag{2.1}\\]\nThe prefactor \\(Z^{-1}\\) is the partition function: since the system must always have some specific arrangement, the sum of the probabilities \\(p_a\\) must be unity, implying that\n\\[\nZ=\\sum_ae^{-E_a/k_BT}\n\\tag{2.2}\\] where the sum extends over all possible microscopic arrangements.\nThese equations assume that physical system evolves rapidly (on the timescale of typical observations) amongst all its allowed arrangements, sampling them with the probabilities Equation 2.1 the expectation value of any physical observable \\(O\\) will thus be given by averaging \\(O\\) over all the arrangements \\(a\\), weighting each contribution by the appropriate probability:\n\\[\\overline {O}=\\frac{1}{Z}\\sum_a O_a e^{-E_a/k_BT}\n\\tag{2.3}\\]\nSums like Equation 2.3 are not easily evaluated because the number of terms grows exponentially in the system size. Nevertheless, some important insights follow painlessly. Consider the case where the observable of interest is the order parameter, or more specifically the magnetisation of a ferromagnet.\n\\[\nQ=\\frac{1}{Z}\\sum_a Q_a e^{-E_a/k_BT}\n\\tag{2.4}\\]\nIt is clear from Equation 2.1 that at very low temperature the system will be overwhelmingly likely to be found in its minimum energy arrangements (ground states). For the ferromagnet, these are the fully ordered spin arrangements having magnetisation \\(+1\\), or \\(-1\\).\nNow consider the high temperature limit. The enhanced weight that the fully ordered arrangement carries in the sum of Equation 2.4 by virtue of its low energy, is now no longer sufficient to offset the fact that arrangements in which \\(Q_a\\) has some intermediate value, though each carry a smaller weight, are vastly greater in number. A little thought shows that the arrangements which have essentially zero magnetisation (equal populations of up and down spins) are by far the most numerous. At high temperature, these disordered arrangements dominate the sum in Equation 2.4 and the order parameter is zero.\nThe competition between energy-of-arrangements weighting (or simply ‘energy’) and the ‘number of arrangements’ weighting (or ‘entropy’) is then the key principle at work here. The distinctive feature of a system with a critical point is that, in the course of this competition, the system is forced to choose amongst a number of macroscopically different sets of microscopic arrangements.\nFinally in this section, we note that the probabilistic (statistical mechanics) approach to thermal systems outlined above is completely compatible with classical thermodynamics. Specifically, the bridge between the two disciplines is provided by the following equation\n\\[\nF=-k_BT \\ln Z\n\\tag{2.5}\\]\nwhere \\(F\\) is the “Helmholtz free energy”. All thermodynamic observables, for example the order parameter \\(Q\\), and response functions such as the specific heat or magnetic susceptibility are obtainable as appropriate derivatives of the free energy. For instance, utilizing Equation 2.2, one can readily verify (try it as an exercise!) that the average internal energy is given by\n\\[\\overline{E}=-\\frac{\\partial \\ln Z}{\\partial \\beta},\\]\nwhere \\(\\beta=(k_BT)^{-1}\\).\nThe relationship between other thermodynamic quantities and derivatives of the free energy are given in fig. Figure 2.1",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#observables-and-expectation-values",
    "href": "phase-transitions/background.html#observables-and-expectation-values",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "Figure 2.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#sec-correlations",
    "href": "phase-transitions/background.html#sec-correlations",
    "title": "2  Key concepts for phase transitions",
    "section": "2.2 Correlations",
    "text": "2.2 Correlations\n\n2.2.1 Spatial correlations\nThe two-point connected correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where the correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nIn isotropic fluids and particle systems, a closely related and more directly measurable quantity (particularly in simulations) is the radial distribution function \\(g(r)\\), which describes how particle density varies as a function of distance from a reference particle. For such systems, the two-point correlation function of the number density field \\(\\rho(\\vec{r})\\) is related to \\(g(r)\\) as follows:\n\\[\ng(r) = 1+\\frac{C(r)}{\\rho^2},\n\\] where \\(\\rho\\) is the average number density. This relation shows that \\(g(r)\\) encodes the same spatial correlations as \\(C(r)\\), but in a form that is more natural for discrete particle systems. Note that by definition \\(g(r)\\to 1\\) in the absence of correlations ie. when \\(C(r)=0\\). This is typically the case for \\(r\\gg\\xi\\).\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector and \\(d^dr\\) refers to the elemental volume (eg. \\(d^3r\\) in three dimensions).\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.\n\n\n2.2.2 Temporal correlations\nConsider a thermodynamic variable \\(x\\) with zero mean that fluctuates over time. Examples include the local magnetization in a magnetic system or the local density in a fluid. Here, \\(x\\) represents a deviation from the average value — a fluctuation.\nWe’re interested in how such fluctuations are correlated over time when the system is in thermal equilibrium. For instance, if \\(x\\) is positive at some time \\(t\\), it’s more likely to remain positive shortly after.\nThese temporal correlations are characterized by the two-time correlation function (also known as an auto-correlation function):\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle\n\\]\nIn equilibrium, the correlation function must be independent of the starting time \\(\\tau\\). Therefore, we define:\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle = M_{xx}(t)\n\\]\nThat is, \\(M_{xx}(t)\\) depends only on the time difference \\(t\\).\nWe typically expect \\(M_{xx}(t)\\) to decay exponentially over a characteristic correlation time \\(t_c\\):\n\\[\nM_{xx}(t) \\sim \\exp(-t / t_c)\n\\]\n\n\n\n\n\n\nFigure 2.2: Sketch of \\(M_{xx}(t)\\) against \\(t\\)\n\n\n\nThis exponential decay reflects how the memory of fluctuations fades with time.\nNow consider two different fluctuating variables, \\(x\\) and \\(y\\) (e.g., local magnetizations at different positions). Their cross-correlation function is defined as:\n\\[\n\\langle x(\\tau) y(\\tau + t) \\rangle = M_{xy}(t)\n\\]\nThis defines the elements of a dynamic correlation matrix, of which \\(M_{xx}(t)\\) is the diagonal.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/approach-to-criticality.html",
    "href": "phase-transitions/approach-to-criticality.html",
    "title": "3  The approach to criticality",
    "section": "",
    "text": "It is a matter of experimental fact that the approach to criticality in a given system is characterized by the divergence of various thermodynamic observables. Let us remain with the archetypal example of a critical system, the ferromagnet, whose critical temperature will be denoted as \\(T_c\\). For temperatures close to \\(T_c\\), the magnetic response functions (the magnetic susceptibility \\(\\chi\\) and the specific heat) are found to be singular functions, diverging as a power of the reduced (dimensionless) temperature \\(t \\equiv\n(T-T_c)/T_c\\):-\n\\[\n\\chi \\equiv \\frac{\\partial M}{\\partial H}\\propto t^{-\\gamma} ~~~~ (H=0)\n\\tag{3.1}\\]\n(where \\(M=mN\\)), \\[\nC_H \\equiv \\frac{\\partial E}{\\partial T}\\propto t^{-\\alpha} ~~~~ (H=\\textrm{ constant})\n\\tag{3.2}\\]\nAnother key quantity is the correlation length \\(\\xi\\), which measures the distance over which fluctuations of the magnetic moments are correlated. This is observed to diverge near the critical point with an exponent \\(\\nu\\).\n\\[\n\\xi \\propto t^{-\\nu} ~~~~ (T &gt; T_c,\\: H=0)\n\\tag{3.3}\\]\nSimilar power law behaviour is found for the order parameter \\(Q\\) (in this case the magnetisation) which vanishes in a singular fashion (it has infinite gradient) as the critical point is is approached as a function of temperature:\n\\[\nm \\propto t^{\\beta} ~~~~ (T &lt; T_c,\\: H=0)\n\\tag{3.4}\\] (here the symbol \\(\\beta\\), is not to be confused with \\(\\beta=1/k_BT\\)– this unfortunately is the standard notation.)\nFinally, as a function of magnetic field:\n\\[m \\propto h^{1/\\delta} ~~~~ (T = T_c,\\: |H|&gt;0) . \\tag{3.5}\\] with \\(h=(H-H_c)/H_c\\), the reduced magnetic field.\nAs examples, the behaviour of the magnetisation and correlation length are plotted in Figure 3.1 as a function of \\(t\\).\n\n\n\n\n\n\nFigure 3.1: Singular behaviour of the correlation length and order parameter in the vicinity of the critical point as a function of the reduced temperature \\(t\\).\n\n\n\nThe quantities \\(\\gamma, \\alpha, \\nu, \\beta\\) in the above equations are known as critical exponents. They serve to control the rate at which the various thermodynamic quantities change on the approach to criticality.\nRemarkably, the form of singular behaviour observed at criticality for the example ferromagnet also occurs in qualitatively quite different systems such as the fluid. All that is required to obtain the corresponding power law relationships for the fluid is to substitute the analogous thermodynamic quantities in to the above equations. Accordingly the magnetisation order parameter is replaced by the density difference \\(\\rho_{liq}-\\rho_{gas}\\) while the susceptibility is replaced by the isothermal compressibility and the specific heat capacity at constant field is replaced by the specific heat capacity at constant volume. The approach to criticality in a variety of qualitatively quite different systems can therefore be expressed in terms of a set of critical exponents describing the power law behaviour for that system (see the book by Yeomans for examples).\nEven more remarkable is the experimental observation that the values of the critical exponents for a whole range of fluids and magnets (and indeed many other systems with critical points) are identical. This is the phenomenon of universality. It implies a deep underlying physical similarity between ostensibly disparate critical systems. The principal aim of theories of critical point phenomena is to provide a sound theoretical basis for the existence of power law behaviour, the factors governing the observed values of critical exponents and the universality phenomenon. Ultimately this basis is provided by the Renormalisation Group (RG) theory, for which K.G. Wilson was awarded the Nobel Prize in Physics in 1982.\nMore about the scientists mentioned in this chapter:\nKenneth Wilson",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The approach to criticality</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html",
    "href": "phase-transitions/Ising-model.html",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "4.1 The 2D Ising model\nIn order to probe the properties of the critical region, it is common to appeal to simplified model systems whose behaviour parallels that of real materials. The sophistication of any particular model depends on the properties of the system it is supposed to represent. The simplest model to exhibit critical phenomena is the two-dimensional Ising model of a ferromagnet. Actual physical realizations of 2-d magnetic systems do exist in the form of layered ferromagnets such as K\\(_2\\)CoF\\(_4\\), so the 2-d Ising model is of more than just technical relevance.\nThe 2-d spin-\\(\\frac{1}{2}\\) Ising model envisages a regular arrangement of magnetic moments or ‘spins’ on an infinite plane. Each spin can take two values, \\(+1\\) (‘up’ spins) or \\(-1\\) (‘down’ spins) and is assumed to interact with its nearest neighbours according to the Hamiltonian\n\\[\n{\\cal H}_I=-J\\sum_{&lt;ij&gt;}s_is_j - H\\sum_i s_i\n\\tag{4.1}\\]\nwhere \\(J&gt;0\\) measures the strength of the coupling between spins and the sum extends over nearest neighbour spins \\(s_i\\) and \\(s_j\\), i.e it is a sum of the bonds of the lattice. \\(H\\) is a magnetic field term which can be positive or negative (although for the time being we will set it equal to zero). The order parameter is simply the average magnetisation:\n\\[m=\\frac{1}{N} \\langle \\sum_i s_i \\rangle\\:,\\] where \\(\\langle\\cdot\\rangle\\) means an average over typical configurations corresponding to the prescribed value of \\(J/k_BT\\).\nThe fact that the Ising model displays a phase transition was argued in Chapter 2. Thus at low temperatures for which there is little thermal disorder, there is a preponderance of aligned spins and hence a net spontaneous magnetic moment (ie. the system is ferromagnetic). As the temperature is raised, thermal disorder increases until at a certain temperature \\(T_c\\), entropy drives the system through a continuous phase transition to a disordered spin arrangement with zero net magnetisation (ie. the system is paramagnetic). These trends are visible in configurational snapshots from computer simulations of the 2D Ising model (see Figure 4.1). Although each spin interacts only with its nearest neighbours, the phase transition occurs due to cooperative effects among a large number of spins.\nAn interactive Monte Carlo simulation of the Ising model demonstrates the phenomenology, By altering the temperature you will be able to observe for yourself how the typical spin arrangements change as one traverses the critical region. Pay particular attention to the configurations near the critical point. They have very interesting properties. We will return to them later!\nAlthough the 2-d Ising model may appear at first sight to be an excessively simplistic portrayal of a real magnetic system, critical point universality implies that many physical observables such as critical exponents are not materially influenced by the actual nature of the microscopic interactions. The Ising model therefore provides a simple, yet quantitatively accurate representation of the critical properties of a whole range of real magnetic (and indeed fluid) systems. This universal feature of the model is largely responsible for its ubiquity in the field of critical phenomena. We shall explore these ideas in more detail later in the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "href": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "(a) \\(T=1.2T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(T=T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(T=0.95T_c\\)\n\n\n\n\n\n\n\nFigure 4.1: Configurations of the 2d Ising model. The patterns depict typical arrangements of the spins (white=+1, black=−1) generated in a computer simulation of the Ising model on a square lattice of \\(N=512\\) sites, at temperatures (from left to right) of \\(T= 1.2T_c\\), \\(T=T_c\\), and \\(T=0.95T_c\\). In each case only a portion of the system containing \\(128\\) sites in shown. The typical island size is a measure of the correlation length \\(\\xi\\): the excess of black over white (below \\(T_c\\) is a measure of the order parameter.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "href": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "4.2 Exact solutions: the one dimensional Ising chain",
    "text": "4.2 Exact solutions: the one dimensional Ising chain\nOne might well ask why the 2D Ising model is the simplest model to exhibit a phase transition. What about the one-dimensional Ising model (ie. spins on a line)? In fact in one dimension, the Ising model can be solved exactly. It turns out that the system is paramagnetic for all \\(T&gt;0\\), so there is no phase transition at any finite temperature. To see this, consider the ground state of the system in zero external field. This will have all spins aligned the same way (say up), and hence be ferromagnetic. Now consider a configuration with a various “domain walls” dividing spin up and spin down regions:\n\n\n\n\n\n\nFigure 4.2: (a) Schematic of an Ising chain at \\(T=0\\). (b) At a small finite temperature the chain is split into domains of spins ordered in the same direction. Domains are separated by notional domain “walls”, which cost energy \\(\\Delta=2J\\). Periodic boundary conditions are assumed.\n\n\n\nInstead of considering the underlying spin configurations, we shall describe the system in terms of the statistics of its domain walls. The energy cost of a wall is \\(\\Delta = 2J\\), independent of position. Domain walls can occupy the bonds of the lattice, of which there are \\(N-1\\). Moreover, the walls are noninteracting, except that you cannot have two of them on the same bond. (Check through these ideas if you are unsure.)\nIn this representation, the partition function involves a count over all possible domain wall arrangements. Since the domain walls are non interacting (eg it doesn’t cost energy for one to move along the chain) they can be treated as independent. Independent contributions to the partition function simply multiply. So we can calculate \\(Z\\) by considering the partition function associated with a single domain wall being present or absent on some given bond, and then simply raise to the power of the number of bonds:\n\\[Z=Z_1^{N-1}\\]\nwhere\n\\[Z_1=e^{\\beta J} + e^{\\beta (J-\\Delta)}=e^{\\beta J}(1+e^{-\\beta\\Delta})\\] is the domain wall partition function for a single bond and represent the sum over the two possible states: domain wall absent or present. Then the free energy per bond of the system is\n\\[\\beta f\\equiv \\beta F/(N-1)=-\\ln Z_1=-\\beta J-\\ln(1+e^{-\\beta\\Delta})\\]\nThe first term on the RHS is simply the energy per spin of the ferromagnetic (ordered) phase, while the second term arises from the free energy of domain walls. Clearly for any finite temperature (ie. for \\(\\beta&lt;\\infty\\)), this second term is finite and negative. Hence the free energy will always be lowered by having a finite concentration of domain walls in the system. Since these domain walls disorder the system, leading to a zero average magnetisation, the 1D system is paramagnetic for all finite temperatures. Exercise: Explain why this argument works only in 1D.\nThe animation below lets you see qualitatively how the typical number of domain walls varies with temperature. If you’d lke to explore more quantitatively, a python code performing a Monte Carlo simulation is available. You will learn about Monte Carlo simulation in the coursework and in later parts of the course.\n\n\nShow python code\n#Monte Carlo simulation of the 1d Ising chain with periodic bounary conditions\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Slider\n\n# Number of spins\nN = 20 \n\n# Initialize spins (+1 or -1)\nspins = np.random.choice([-1, 1], size=N)\n\n# Initial temperature\nT = 2.0\n\n# Set up figure and axis for the spins\nfig, ax = plt.subplots(figsize=(10, 2))\nplt.subplots_adjust(bottom=0.25)  # make room for slider\nax.set_xlim(-0.5, N - 0.5)\nax.set_ylim(-1, 1)\nax.axis('off')\n\n# Create text objects for each spin\ntexts = []\nfor i in range(N):\n    arrow = '↑' if spins[i] == 1 else '↓'\n    t = ax.text(i, 0, arrow, fontsize=24, ha='center', va='center')\n    texts.append(t)\n\ndef update(frame):\n    \"\"\"Perform Metropolis updates over all spins, then refresh display.\"\"\"\n    global spins, T\n    for _ in range(N):\n        i = np.random.randint(N)\n        left = spins[(i - 1) % N]\n        right = spins[(i + 1) % N]\n        deltaE = 2 * spins[i] * (left + right)\n        # Metropolis criterion ensures configurations appear with the correct Boltzmann probability\n        if deltaE &lt; 0 or np.random.rand() &lt; np.exp(-deltaE / T):\n            spins[i] *= -1\n    # Update arrows on screen\n    for idx, t in enumerate(texts):\n        t.set_text('↑' if spins[idx] == 1 else '↓')\n    return texts\n\n# Create the animation with caching disabled and blit turned off\nani = FuncAnimation(\n    fig,\n    update,\n    interval=200,\n    blit=False,\n    cache_frame_data=False\n)\n\n# Add a temperature slider\nax_T = plt.axes([0.2, 0.1, 0.6, 0.03], facecolor='lightgray')\nslider_T = Slider(ax_T, 'Temperature T', 0.1, 5.0, valinit=T)\n\ndef on_T_change(val):\n    \"\"\"Callback to update T when the slider changes.\"\"\"\n    global T\n    T = val\n\nslider_T.on_changed(on_T_change)\n\n# Show the plot (ani is kept in scope so it won't be deleted)\nplt.show()\n\n\n\n\n\n Temperature T =\n\n2.0\n\n \n\n\n4.2.1 More general 1D spins systems: transfer matrix method\nGenerally speaking one-dimensional systems lend themselves to a degree of analytic tractability not found in most higher dimensional models. Indeed for the case of a 1-d assembly of \\(N\\) spins each having \\(m\\) discrete energy states, and in the presence of a magnetic field, it is possible to reduce the evaluation of the partition function to the calculation of the eigenvalues of a matrix–the so called transfer matrix.\nLet us start by assuming that the assembly has cyclic boundary conditions, then the total energy of configuration \\(\\{s\\}\\) is \\[\n\\begin{aligned}\n{\\cal H}(\\{s\\})=&-\\sum_{i=1}^N (J s_i s_{i+1}+Hs_i)\\\\\n\\:=&-\\sum_{i=1}^N (J s_i s_{i+1}+H(s_i+s_{i+1})/2)\\\\\n\\:=&\\sum_{i=1}^N E(s_i,s_{i+1})\n\\end{aligned}\n\\]\nwhere we have defined \\(E(s_i,s_{i+1})=-J s_i s_{i+1}-H(s_i+s_{i+1})/2\\).\nNow the partition function may be written\n\\[\\begin{aligned}\nZ_N =& \\sum_{\\{s\\}}\\exp\\left(-\\beta {\\cal H}(\\{s\\})\\right)\\nonumber \\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta[E(s_1,s_2)+E(s_2,s_3)+....E(s_N,s_1)]\\right) \\nonumber\\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta E(s_1,s_2)\\right)\\exp\\left(-\\beta E(s_2,s_3)\\right)....\\exp\\left(-\\beta E(s_N,s_1)\\right) \\nonumber\\\\\n=&\\sum_{i,j,...,l=1}^m V_{ij}V_{jk}...V_{li}\n\\end{aligned} \\tag{4.2}\\]\nwhere the \\(V_{ij}=\\exp(-\\beta E_{ij})\\) are elements of an \\(m \\times m\\) matrix \\(\\mathbf{V}\\), known as the transfer matrix (\\(i,j,k\\) etc are dummy indices that run over the matrix elements). You should see that the sum over the product of matrix elements picks up all the terms in the partition function and therefore Equation 4.2 is an alternative way of writing the partition function.\nThe reason it is useful to transform to a matrix representation is that it transpires that the sum over the product of matrix elements in Equation 4.2 is simply just the trace of \\(\\mathbf{V}^N\\) (check this yourself for a short periodic chain), given by the sum of its eigenvalues:-\n\n\n\n\n\n\nProof (non examinable)\n\n\n\n\n\nLet \\(V\\) be an \\(n \\times n\\) matrix, and let \\(\\lambda_1, \\dots, \\lambda_n\\) denote its eigenvalues.\nEvery square matrix \\(V\\) can be written as \\[\nV = Q T Q^\\dagger,\n\\] where \\(Q\\) is a unitary matrix (\\(Q^\\dagger Q = I\\)), and \\(T\\) is upper triangular with the eigenvalues of \\(V\\) on its diagonal: \\[\nT = \\begin{pmatrix}\n\\lambda_1 & * & \\cdots & * \\\\\n0 & \\lambda_2 & \\cdots & * \\\\\n\\vdots & & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & \\lambda_n\n\\end{pmatrix}.\n\\]\nRaising both sides to the \\(N\\)th power gives \\[\nV^N = (Q T Q^\\dagger)^N = Q \\, T^N \\, Q^\\dagger.\n\\] The trace (ie. the sum of diagonal elements) is invariant under similarity transformations: \\[\n\\mathrm{Tr}(V^N) = \\mathrm{Tr}(T^N).\n\\]\nSince \\(T\\) is upper triangular, so is \\(T^N\\). The diagonal elements of \\(T^N\\) are simply the \\(N\\)th powers of the diagonal elements of \\(T\\), i.e. \\[\n(T^N)_{ii} = (T_{ii})^N = \\lambda_i^N.\n\\]\nTherefore, \\[\n\\mathrm{Tr}(T^N) = \\sum_{i=1}^n (T^N)_{ii} = \\sum_{i=1}^n \\lambda_i^N.\n\\]\n\n\n\n\\[Z_N=\\lambda_1^N+\\lambda_2^N+...\\lambda_m^N\\] For very large \\(N\\), this expression simplifies further because the largest eigenvalue \\(\\lambda_1\\) dominates the behaviour since \\((\\lambda_2/\\lambda_1)^N\\) vanishes as \\(N\\rightarrow \\infty\\). Consequently in the thermodynamic limit one may put \\(Z_N=\\lambda_1^N\\) and the problem reduces to identifying the largest eigenvalue of the transfer matrix.\nSpecializing to the case of the simple Ising model in the presence of an applied field \\(H\\), the transfer matrix takes the form\n\\[\\mathbf{V}(H)=\\left(\n\\begin{array}{cc}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{array} \\right)\\]\nThis matrix has two eigenvalues which can be readily calculated in the usual fashion as the roots of the characteristic polynomial \\(|\\mathbf{V}-\\lambda\\mathbf{I}|\\). They are\n\\[\\lambda_{\\pm}=e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\\]\nHence the free energy per spin \\(f=-k_BT\\ln \\lambda_+\\) is\n\\[f=-k_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\\]\nThe Ising model in 2D can also be solved exactly, as was done by Lars Onsager in 1940. The solution is extremely complicated and is regarded as one of the pinnacles of statistical mechanics. In 3D no exact solution is known.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html",
    "href": "phase-transitions/mean-field-theory.html",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "",
    "text": "5.1 Mean field solution of the Ising model\nOf the wide variety of models of interest to the critical point theorist, the majority have shown themselves intractable to direct analytic (pen and paper) assault. In a very limited number of instances models have been solved exactly, yielding the phase coexistence parameters, critical exponents and the critical temperature. The 2-d spin-\\(\\frac{1}{2}\\) Ising model is certainly the most celebrated such example, its principal critical exponents are found to be \\(\\beta=\\frac{1}{8}, \\nu=1, \\gamma=\\frac{7}{4}\\). Its critical temperature is \\(-2J/\\ln(\\sqrt{2}-1)\\approx 2.269J\\). Unfortunately such solutions rarely afford deep insight to the general framework of criticality although they do act as an invaluable test-bed for new and existing theories.\nThe inability to solve many models exactly often means that one must resort to approximations. One such approximation scheme is mean field theory.\nLet us look for a mean field expression for the free energy of the Ising model whose Hamiltonian is given in Equation 4.1 . Write\n\\[s_i=\\langle s_i\\rangle+(s_i-\\langle s_i\\rangle)=m+(s_i-m)=m+\\delta s_i\\]\nThen \\[\\begin{aligned}\n{\\cal H}_I=&-J\\sum_{&lt;i,j&gt;}[m+(s_i-m)][m+(s_j-m)]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{&lt;i,j&gt;}[m^2+m(s_i-m)+m(s_j-m)+\\delta s_i\\delta s_j]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{i}(qms_i-qm^2/2)-H\\sum_i s_i-J\\sum_{&lt;i,j&gt;}\\delta s_i\\delta s_j\n\\end{aligned} \\tag{5.1}\\] where in the last line we trnsformed from a sum over bonds to a sum over sites. Doing so makes use of the fact that when for each site \\(i\\) we perform the sum \\(\\sum_{&lt;i,j&gt;}\\) over bonds of a quantity which is independent of \\(s_j\\), then the result is just the number of bonds per site times that quantity. Since the number of bonds on a lattice of \\(N\\) sites of coordination \\(q\\) is \\(Nq/2\\) (because each bond is shared between two sites), there are therefore \\(q/2\\) bonds per site.\nNow the mean field approximation is to ignore the last term in the last line of Equation 5.1 giving the configurational energy as\n\\[\n{\\cal H}_{mf}=-\\sum_{i}H_{mf}s_i+NqJm^2/2\n\\] with \\(H_{mf}\\equiv qJm+ H\\) the “mean field” seen by spin \\(s_i\\). As all the spins are decoupled (independent) in this approximation we can write down the partition function, which follows by taking the partition function for a single spin (by summing the Boltzmann factor for \\(s_i=\\pm 1\\)) and raising to the power \\(N\\) to find\n\\[\nZ=e^{-\\beta qJm^2N/2}[2\\cosh(\\beta(qJm+H))]^N\n\\]\nThe free energy follows as\n\\[F(m)=NJqm^2/2-Nk_BT\\ln[2\\cosh(\\beta (qJm+H)]\\:.\\]\nand the magnetisation as\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\tanh(\\beta(qJm+H)),\n\\] where the first term drops out because we treat \\(m\\) as an independent variable when differentiating w.r.t. \\(H\\).\nThis is a self consistent equation because \\(m\\) appears on both the left and the right hand sides. To find \\(m(H,T)\\), we must numerically solve this last equation-self consistently. You will meet such an equation again later when you learn about mean field theories for liquid crystals.\nNote that we can obtain \\(m\\) in a different way. Consider some arbitary spin, \\(s_i\\) say. Then this spin has an energy \\({\\cal H}_{mf}(s_i)\\). Considering this energy for both cases \\(s_i=\\pm 1\\) and the probability \\(p(s_i)=e^{-\\beta{\\cal H}_{mf}(s_i)}/Z\\) of each, we have that\n\\[\\langle s_i\\rangle=\\sum_{s_i=\\pm 1}s_ip(s_i)\\] but for consistancy, \\(\\langle s_i\\rangle=m\\). Thus\n\\[\n\\begin{aligned}\nm & = \\sum_{s_i=\\pm 1}s_ip(s_i)\\nonumber\\\\\n\\: & = \\frac{e^{\\beta(qJm+H)}-e^{-\\beta(qJm+H)}} {e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\\nonumber\\\\\n\\: & = \\tanh(\\beta(qJm+H))\n\\end{aligned} \\tag{5.2}\\] as before.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-mfising",
    "href": "phase-transitions/mean-field-theory.html#sec-mfising",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "",
    "text": "Why self-consistent?\n\n\n\n\n\nIn mean-field theory, the many-body interaction is replaced by an effective one-body problem in which each degree of freedom experiences an average field generated by all the others. The quantity that characterises the ordered phase—the order parameter—is precisely this average. Because the effective (mean-field) Hamiltonian is constructed using a presumed value of that average, internal consistency requires that the order parameter obtained by solving the effective problem match the value assumed to define it. Enforcing this equality yields a self-consistency condition for the order parameter. In practice: choose the effective field determined by the putative order parameter, compute the corresponding thermal average, and require that the two coincide.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-breaking",
    "href": "phase-transitions/mean-field-theory.html#sec-breaking",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.2 Spontaneous symmetry breaking",
    "text": "5.2 Spontaneous symmetry breaking\n\n\n\n\n\n\nFigure 5.1: Schematic of the form of the free energy for a critical, subcritical and supercritical temperature\n\n\n\nThis mean field analysis reveals what is happening in the Ising model near the critical temperature \\(T_c\\). Figure 5.1 shows sketches for \\(\\beta F(m)/N\\) as a function of temperature, where for f simplicity we restrict attention to \\(H=0\\). In this case \\(F(m)\\) is symmetric in \\(m\\), Moreover, at high \\(T\\), the entropy dominates and there is a single minimum in \\(F(m)\\) at \\(m=0\\). As \\(T\\) is lowered, there comes a point (\\(T=T_c=qJ/k_B\\)) where the curvature of \\(F(m)\\) at the origin changes sign; precisely at this point\n\\[\\frac{\\partial^2 F}{\\partial m^2}=0.\\] At lower temperature, there are instead two minima at nonzero \\(m=\\pm m^\\star\\), where the equilibrium magnetisation \\(m^\\star\\) is the positive root (calculated explicitly below) of\n\\[m^\\star=\\tanh(\\beta Jqm^\\star)= \\tanh(\\frac{m^\\star T_c}{T})\\] The point \\(m=0\\) which remains a root of this equation, is clearly an unstable point for \\(T&lt;T_c\\) (since \\(F\\) has a maximum there).\nThis is an example of spontaneous symmetry breaking. In the absence of an external field, the Hamiltonian (and therefore the free energy) is symmetric under \\(m\\to -m\\). Accordingly, one might expect the actual state of the system to also show this symmetry. This is true at high temperature, but spontaneously breaks down at low ones. Instead there are a pair of ferromagnetic states (spins mostly up, or spins mostly down) which – by symmetry– have the same free energy, lower than the unmagnetized state.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#phase-diagram",
    "href": "phase-transitions/mean-field-theory.html#phase-diagram",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.3 Phase diagram",
    "text": "5.3 Phase diagram\nThe resulting zero-field magnetisation curve \\(m(T,H=0)\\) looks like Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: Phase diagram of a simple magnet in the \\(m\\)-\\(T\\) plane.\n\n\n\nThis shows the sudden change of behaviour at \\(T_c\\) (phase transition). For \\(T&lt;T_c\\) it is arbitrary which of the two roots \\(\\pm m^\\star\\) is chosen; typically it will be different in different parts of the sample (giving macroscopic “magnetic domains”). But this behaviour with temperature is qualitatively modified by the presence of a field \\(H\\), however small. In that case, there is always a slight magnetization, even far above \\(T_c\\) and the curves becomes smoothed out, as shown. There is no doubt which root will be chosen, and no sudden change of the behaviour (no phase transition). Spontaneous symmetry breaking does not occur, because the symmetry is already broken by \\(H\\). (The curve \\(F(m)\\) is lopsided, rather than symmetrical about \\(m=0\\).)\nOn the other hand, if we sit below \\(T_c\\) in a positive field (say) and gradually reduce \\(H\\) through zero so that it becomes negative, there is a very sudden change of behaviour at \\(h=0\\): the equilibrium state jumps discontinuously from \\(m=m^\\star\\) to \\(m=-m^\\star\\).\n\n\n\n\n\n\nFigure 5.3: Phase diagram of a simple magnet in the \\(H\\)-\\(T\\) plane.\n\n\n\nThis is called a first order phase transition as opposed to the “second order” or continuous transition that occurs at \\(T_c\\) in zero field. The definitions are:\nFirst order transition: magnetisation (or similar order parameter) depends discontinuously on a field variable (such as \\(h\\) or \\(T\\)).\nContinuous transition (criticality): Change of functional form, but no discontinuity in \\(m\\); typically, however, \\((\\partial m/\\partial T)_h\\) (or similar) is either discontinuous, or diverges with an integrable singularity.\nIn this terminology, we can say that the phase diagram of the magnet in the \\(H,T\\) plane shows a line of first order phase transitions, terminating at a continuous transition, which is the critical point.\n\n\n\n\n\n\nAside on Quantum Criticality\n\n\n\n\n\nIn some magnetic systems such as \\(CePd_2Si_2\\), one can, by applying pressure or altering the chemical composition, depress the critical temperature all the way to abolute zero! This may seem counterintuitive, after all at \\(T=0\\) one should expect perfect ordering, not the large fluctuations that accompany criticality. It turns out that the source of the fluctuations that drive the system critical is zero point motion associated with the Heisenberg uncertainty principle. Quantum criticality is a matter of ongoing active research, and open questions concern the nature of the phase diagrams and the relationship to superconductivity. Although the subject goes beyond the scope of this course, there is an accessible article here if you want to learn more.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "href": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.4 A closer look: critical exponents",
    "text": "5.4 A closer look: critical exponents\nLet us now see how we can calculate critical exponents within the mean field approximation.\n\n5.4.1 Zero H solution and the order parameter exponent\nIn zero field\n\\[m=\\tanh(\\frac{mT_c}{T})\\] where \\(T_c=qJ/k_B\\) is the critical temperature at which \\(m\\) first goes to zero.\nWe look for a solution where \\(m\\) is small (\\(\\ll 1\\)). Expanding the tanh function and replacing \\(\\beta=(k_BT)^{-1}\\) yields\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +O(m^5)\\:.\\] Then \\(m=0\\) is one solution. The other solution is given by\n\\[m^2=3\\left(\\frac{T}{T_c} \\right)^3\\left(\\frac{T_c}{T} -1\\right)\\]\nNow, considering temperatures close to \\(T_c\\) to guarantee small \\(m\\), and employing the reduced temperature \\(t=(T-T_c)/T_c\\), one finds\n\\[m^2\\simeq -3t\\]\nHence\n\\[\\begin{aligned}\nm= 0  &    ~~~\\textrm{for } T&gt;T_c \\:\\:\\:  \\textrm{since otherwise $m$ imaginary}\\\\\nm= \\pm\\sqrt{-3t} & ~~\\textrm{ for}  \\:\\:\\: T&lt;T_c ~~\\textrm{ real}\n\\end{aligned}  \\tag{5.3}\\] This result implies that (within the mean field approximation) the critical exponent \\(\\beta=1/2\\).\n\n\n5.4.2 Finite (but small) field solution: the susceptibility exponent\nIn a finite, but small field we can expand Equation 5.2 thus:\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +\\frac{H}{kT}\\]\nConsider now the isothermal susceptibility\n\\[\n\\begin{aligned}\n\\chi  \\equiv & \\left(\\frac{\\partial m}{\\partial H}\\right)_T\\\\\n      =     & \\frac{T_c}{T}\\chi - \\left(\\frac{T_c}{T}\\right)^3 \\chi m^2 + \\frac{1}{k_BT}  \n\\end{aligned}\n\\]\nThen\n\\[\\chi \\left[ 1-\\frac{T_c}{T} +\\left(\\frac{T_c}{T}\\right)^3m^2  \\right]=\\frac{1}{k_BT}\\]\nHence near \\(T_c\\)\n\\[\\chi=\\frac{1}{k_BT_c}\\left(\\frac{1}{t+m^2}\\right)\\]\nThen using the results of Equation 5.3\n\\[\n\\begin{aligned}\n\\chi= (k_BT_ct)^{-1} & \\textrm{ for} ~~~ T&gt; T_c \\\\\n\\chi= (-2k_BT_ct)^{-1} & \\textrm{ for}  ~~~T \\le T_c\n\\end{aligned}\n\\]\nwhere one has to take the non-zero value for \\(m\\) below \\(T_c\\) to ensure +ve \\(\\chi\\), i.e. thermodynamic stability. This result implies that (within the mean field approximation) the critical exponent \\(\\gamma=1\\).\nThe schematic behaviour of the Ising order parameter and susceptibility are shown in Figure 5.5 (a) and (b) respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mean field behaviour of the Ising magnetisation (schematic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Mean field behaviour of the Ising susceptibility (schematic)\n\n\n\n\n\n\n\nFigure 5.4",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "href": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.5 Landau theory",
    "text": "5.5 Landau theory\nLandau theory is a slightly more general type of mean field theory than that discussed in the previous subsection because it is not based on a particular microscopic model. Its starting point is the Helmholtz free energy, which Landau asserted can be written in terms of power series expansion of the order parameter \\(\\phi\\):\n\\[\nF_(\\phi)=\\sum_{i=0}^{\\infty}a_i\\phi^i\n\\] The equilibrium value of \\(\\rho\\) is that which minimises the Landau free energy.\n\n\n\n\n\n\nA note on order parameters\n\n\n\n\n\nWe have already seen examples of these in earlier sections, e.g., for the liquid-gas transition this was \\[\n\\rho_{liq} - \\rho_{gas}: \\quad \\textrm{difference in density of two coexisting phases},\n\\] while for the Ising magnet it is the magnetisation \\(m\\). Both quantities vanish at the critical point. These are examples of scalar order parameters – a single number is required to represent the degree of order (\\(n = 1\\)).\nIn the absence of a symmetry-breaking field, the Landau free-energy density \\(f_L\\) must have symmetry \\(f_L(-\\phi) = f_L(\\phi)\\) (Ising case).\nFor some other systems, \\(n\\) component vectors are required in order to represent the order:\n\\[\n\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_n)\n\\]\nThen \\(f_L(\\boldsymbol{\\phi})\\) should be symmetric under \\(O(n)\\) rotations in \\(n\\)-component \\(\\phi\\)-space.\nThe table below lists examples of order parameters for various physical systems.\n\n\n\n\n\n\n\n\nPhysical System\nOrder Parameter \\(\\varphi\\)\nSymmetry Group\n\n\n\n\nUniaxial (Ising) ferromagnet\nMagnetisation per spin, \\(m\\)\n\\(O(1)\\)\n\n\nFluid (liquid-gas)\nDensity difference, \\(\\rho - \\rho_c\\)\n\\(O(1)\\)\n\n\nLiquid mixtures\nConcentration difference, \\(c - c_c\\)\n\\(O(1)\\)\n\n\nBinary (AB) alloy (e.g., \\(\\beta\\)-brass)\nConcentration of one of the species, \\(c\\)\n\\(O(1)\\)\n\n\nIsotropic (vector) ferromagnet\n\\(n\\)-component magnetisation, \\(\\mathbf{m} = (m_1, m_2, \\dots, m_n)\\)\n\\(O(n)\\)\n\n\n\n\\(n = 2\\): xy model\n\\(O(2)\\)\n\n\n\n\\(n = 3\\): Heisenberg model\n\\(O(3)\\)\n\n\nSuperfluid He\\(^4\\)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nSuperconductor (s-wave)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nNematic liquid crystal\nOrientational order, \\(\\langle P_2(\\cos \\theta)\\rangle\\)\n\n\n\nSmectic A liquid crystal\n1-dimensional periodic density\n\n\n\nCrystal\n3-dimensional periodic density\n\n\n\n\nNotes:\n\nIn superfluid \\(^4He\\) the order parameter is\n\n\\[\n\\Psi = |\\Psi| e^{i\\theta},\n\\]\nthe complex wavefunction of the macroscopic condensate. Both the amplitude \\(|\\Psi|\\) and phase \\(\\theta\\) must be specified, so this corresponds to \\(n = 2\\).\nSuperconductors also correspond to \\(n = 2\\).\n\nIn a nematic liquid crystal, the orientational order parameter is\n\n\\[\n\\langle P_2(\\cos \\theta) \\rangle \\equiv \\frac{1}{2}\\langle 3\\cos^2 \\theta - 1\\rangle,\n\\]\nwhere \\(\\theta\\) is the angle a molecule makes with the average direction of the long axes of the molecules (known as the director \\(\\hat{n}\\)). Rotational symmetry is broken. For the case of an \\(n\\) component vector, the free energy should be a function of:\n\\[\n\\phi^2 \\equiv |\\boldsymbol{\\phi}|^2 = \\phi_1^2 + \\phi_2^2 + \\dots + \\phi_n^2 = \\sum_{i=1}^n \\phi_i^2\n\\] in the absence of a symmetry breaking field. Rotational symmetry is incorporated into the theory.\n\n\n\n\n\n\n\n\n\n\n\n(a) Schematic of the isotropic liquid phase of a system of elongated molcules.\n\n\n\n\n\n\n\n\n\n\n\n(b) Schematic of the nematic liquid phase of a system elongated molcules. This phase has uniaxial ordering.\n\n\n\n\n\n\n\nFigure 5.5: Isotropic and uniaxially ordered (nematic) phases of liquid crystal molecules.\n\n\n\n\n\n\nTo exemplify the approach, let us specialise to the case of a ferromagnet where \\(\\phi=m\\), the magnetisation and write the Landau free energy as\n\\[\nF(m)=F_0+a_2m^2+a_4m^4\n\\tag{5.4}\\]\nHere only the terms compatible with the order parameter symmetry are included in the expansion and we truncate the series at the 4th power because this is all that is necessary to yield the essential phenomenology. On symmetry grounds, the free energy of a ferromagnet should be invariant under a reversal of the sign of the magnetisation. Terms linear and cubic in \\(m\\) are not invariant under \\(m\\to -m\\), and so do not feature.\nOne can understand how the Landau free energy can give rise to a critical point and coexistence values of the magnetisation, by plotting \\(F(m)\\) for various values of \\(a_2\\) with \\(a_4\\) assumed positive (which ensures that the magnetisation remains bounded). This is shown in the following movie:\n\n\nThe situation is qualitatively similar to that discussed in Section 5.2. Thermodynamics tells us that the system adopts the state of lowest free energy. From the movie, we see that for \\(a_2&gt;0\\), the system will have \\(m=0\\), i.e. will be in the disordered (or paramagnetic) phase. For \\(a_2&lt;0\\), the minimum in the free energy occurs at a finite value of \\(m\\), indicating that the ordered (ferromagnetic) phase is the stable one. In fact, the physical (up-down) spin symmetry built into \\(F\\) indicates that there are two equivalent stable states at \\(m=\\pm m^\\star\\). \\(a_2=0\\) corresponds to the critical point which marks the border between the ordered and disordered phases. Note that it is an inflexion point, so has \\(\\frac{d^2F}{dm^2}=0\\).\nClearly \\(a_2\\) controls the deviation from the critical temperature, and accordingly we may write\n\\[a_2=\\tilde{a_2} t\\] where \\(t\\) is the reduced temperature. Thus we see that the trajectory of the minima as a function of \\(a_2&lt;0\\) in the above movie effective traces out the coexistence curve in the \\(m-T\\) plane.\nWe can now attempt to calculate critical exponents. Restricting ourselves first to the magnetisation exponent \\(\\beta\\) defined by \\(m=t^\\beta\\), we first find the equilibrium magnetisation, corresponding to the minimum of the Landau free energy:\n\\[\n\\frac{dF}{dm}=2\\tilde{a_2} tm+4a_4m^3=0\n\\tag{5.5}\\]\nwhich implies\n\\[m\\propto (-t)^{1/2},\\] so \\(\\beta=1/2\\), which is again a mean field result.\nLikewise we can calculate the effect of a small field \\(H\\) if we sit at the critical temperature \\(T_c\\). Since \\(a_2=0\\), we have\n\\[F(m)=F_0+a_4m^4-Hm\\]\n\\[\\frac{\\partial F}{\\partial m}=0 \\Rightarrow m(H,T_c)=\\left(\\frac{H}{4a_4}\\right)^{1/3}\\]\nor\n\\[H \\sim m^\\delta ~~~~~ \\delta=3\\] which defines a second critical exponent.\nNote that at the critical point, a small applied field causes a very big increase in magnetisation; formally, \\((\\partial m/\\partial H)_T\\) is infinite at \\(T=T_c\\).\nA third critical exponent can be defined from the magnetic susceptibility at zero field\n\\[\\chi=\\left(\\frac{\\partial m}{\\partial H}\\right)_{T,V} \\sim |T-T_c|^{-\\gamma}\\]\nExercise: Show that the Landau expansion predicts \\(\\gamma=1\\).\nFinally we define a fourth critical exponent via the variation of the heat capacity (per site or per unit volume) \\(C_H\\), in fixed external field \\(H=0\\):\n\\[C_H \\sim |T-T_c|^{-\\alpha}\\]\nBy convention, \\(\\alpha\\) is defined to be positive for systems where there is a divergence of the heat capacity at the critical point (very often the case). The heat capacity can be calculated from\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\]\nFrom the minimization over \\(m\\) Equation 5.5 one finds (exercise: check this) \\[\n\\begin{aligned}\nF = & 0 ~~~~T&gt;T_c\\nonumber\\\\\nF = & -a_2^2/4a_4 ~~~~ T &lt; T_c\n\\end{aligned}\n\\]\nUsing the fact that \\(a_2\\) varies linearly with \\(T\\), we have\n\\[\n\\begin{aligned}\nC_H =& 0 ~~~~ T\\to T_c^+\\nonumber\\\\\nC_H =& \\frac{T\\tilde a_2^2}{2a_4} ~~~~ T \\to T_c^-\\:,\n\\end{aligned}\n\\]\nwhich is actually a step discontinuity in specific heat. Since for positive \\(\\alpha\\) the heat capacity is divergent, and for negative \\(\\alpha\\) it is continuous, this behaviour formally corresponds to \\(\\alpha=0\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "href": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.6 Shortcomings of mean field theory",
    "text": "5.6 Shortcomings of mean field theory\nWhile mean field theories provide a useful route to understanding qualitatively the phenomenology of phase transitions, in real ferromagnets, as well as in more sophisticated theories, the critical exponents are not the simple fraction and integers found here. This failure of mean field theory to predict the correct exponents is of course traceable to their neglect of correlations. In later sections we shall start to take the first steps to including the effects of long range correlations.\n\n\n\nComparison of true Ising critical exponents with their mean field theory predictions in a number of dimensions.\n\n\n\\(\\:\\)\nMean Field\n\\(d=1\\)\n\\(d=2\\)\n\\(d=3\\)\n\n\nCritical temperature \\(k_BT/qJ\\)\n\\(1\\)\n\\(0\\)\n\\(0.5673\\)\n\\(0.754\\)\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\frac{1}{2}\\)\n-\n\\(\\frac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(1\\)\n\\(\\infty\\)\n\\(\\frac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(\\frac{1}{2}\\)\n\\(\\infty\\)\n\\(1\\)\n\\(0.63\\pm 0.001\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html",
    "href": "phase-transitions/scaling.html",
    "title": "6  The Static Scaling Hypothesis",
    "section": "",
    "text": "6.1 Experimental Verification of Scaling\nHistorically, the first step towards properly elucidating near-critical behaviour was taken with the static scaling hypothesis. This is essentially a plausible conjecture concerning the origin of power law behaviour which appears to be consistent with observed phenomena. According to the hypothesis, the basis for power law behaviour (and associated scale invariance or “scaling”) in near-critical systems is expressed in the claim that: in the neighbourhood of a critical point, the basic thermodynamic functions (most notably the free energy) are generalized homogeneous functions of their variables. For such functions one can always deduce a scaling law such that by an appropriate change of scale, the dependence on two variables (e.g. the temperature and applied field) can be reduced to dependence on one new variable. This claim may be warranted by the following general argument.\nA function of two variables \\(g(u,v)\\) is called a generalized homogeneous function if it has the property\n\\[g(\\lambda^au,\\lambda^bv)=\\lambda g(u,v)\\] for all \\(\\lambda\\), where the parameters \\(a\\) and \\(b\\) (known as scaling parameters) are constants. An example of such a function is \\(g(u,v)=u^3+v^2\\) with \\(a=1/3, b=1/2\\).\nNow, the arbitrary scale factor \\(\\lambda\\) can be redefined without loss of generality as \\(\\lambda^a=u^{-1}\\) giving\n\\[g(u,v)=u^{1/a}g(1,\\frac{v}{u^{b/a}})\\] A corresponding relation is obtained by choosing the rescaling to be \\(\\lambda^b=v^{-1}\\).\n\\[\\label{eq:sca2}\ng(u,v)=v^{1/b}g(\\frac{u}{v^{a/b}},1)\\]\nThis equation demonstrates that \\(g(u,v)\\) indeed satisfies a simple power law in \\(\\mathit{one}\\) variable, subject to the constraint that \\(u/v^{a/b}\\) is a constant. It should be stressed, however, that such a scaling relation specifies neither the function \\(g\\) nor the parameters \\(a\\) and \\(b\\).\nNow, the static scaling hypothesis asserts that in the critical region, the free energy \\(F\\) is a generalized homogeneous function of the (reduced) thermodynamic fields \\(t=(T-T_c)/T_c\\) and \\(h=(H-H_c)\\). Remaining with the example ferromagnet, the following scaling assumption can then be made:\n\\[F(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h) \\:.\n\\label{eq:scagibbs}\\]\nWithout loss of generality, we can set \\(\\lambda^a=t^{-1}\\), implying \\(\\lambda=t^{-1/a}\\) and \\(\\lambda^b=t^{-b/a}\\).\nThen \\[F(t,h)=t^{1/a}F(1,t^{-b/a}h)\\] where our choice of \\(\\lambda\\) ensures that \\(F\\) on the rhs is now a function of a single variable \\(t^{-b/a}h\\).\nNow, as stated in Chapter 2, the free energy provides the route to all thermodynamic functions of interest. An expression for the magnetisation can be obtained simply by taking the field derivative of \\(F\\) (cf. Figure 2.1)\n\\[m(t,h)=-t^{(1-b)/a}m(1,t^{-b/a}h)\n\\tag{6.1}\\]\nIn zero applied field \\(h=0\\), this reduces to\n\\[m(t,0)=(-t)^{(1-b)/a}m(1,0)\\] where the r.h.s. is a power law in \\(t\\). Equation 3.4 then allows identification of the exponent \\(\\beta\\) in terms of the scaling parameters \\(a\\) and \\(b\\).\n\\[\\beta=\\frac{1-b}{a}\\]\nBy taking further appropriate derivatives of the free energy, other relations between scaling parameters and critical exponents may be deduced. Such calculations (Exercise: try to derive them) yield the results \\(\\delta =\nb/(1-b)\\),\\(\\gamma = (2b-1)/a\\), and \\(\\alpha =(2a-1)/a\\) . Relationships between the critical exponents themselves can be obtained trivially by eliminating the scaling parameters from these equations. The principal results (known as “scaling laws”) are:- \\[\n\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\n\\end{aligned}\n\\]\nThus, provided all critical exponents can be expressed in terms of the scaling parameters \\(a\\) and \\(b\\), then only two critical exponents need be specified, for all others to be deduced. Of course these scaling laws are also expected to hold for the appropriate thermodynamic functions of analogous systems such as the liquid-gas critical point.\nThe validity of the scaling hypothesis finds startling verification in experiment. To facilitate contact with experimental data for real systems, consider again Equation 6.1. Eliminating the scaling parameters \\(a\\) and \\(b\\) in favour of the exponents \\(\\beta\\) and \\(\\delta\\) gives\n\\[\n\\frac{m(t,h)}{t^{\\beta}}=m(1,\\frac{h}{t^{\\beta\\delta}})\n\\] where the RHS of this last equation can be regarded as a function of the single scaled variable \\(\\tilde{H} \\equiv t^{-\\beta\\delta} h(t,M)\\).\nFor some particular magnetic system, one can perform an experiment in which one measures \\(m\\) vs \\(h\\) for various fixed temperatures. This allows one to draw a set of isotherms, i.e. \\(m-h\\) curves of constant \\(t\\). These can be used to demonstrate scaling by plotting the data against the scaling variables \\(M=t^{-\\beta}m(t,h)\\) and \\(\\tilde{H}=t^{-\\beta\\delta}h(t,M)\\). Under this scale transformation, it is found that all isotherms (for \\(t\\) close to zero) coincide to within experimental error. Reassuringly, similar results are found using the scaled equation of state of simple fluid systems such as He\\(^3\\) or Xe.\nIn summary, the static scaling hypothesis is remarkably successful in providing a foundation for the observation of power laws and scaling phenomena. However, it furnishes little or no guidance regarding the role of co-operative phenomena at the critical point. In particular it provides no means for calculating the values of the critical exponents appropriate to given model systems.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "href": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "title": "6  The Static Scaling Hypothesis",
    "section": "",
    "text": "Figure 6.1: Magnetisation of CrBr\\(_3\\) in the critical region plotted in scaled form (see text). From Ho and Lister (1969).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#sec-compsim",
    "href": "phase-transitions/scaling.html#sec-compsim",
    "title": "6  The Static Scaling Hypothesis",
    "section": "6.2 Computer simulation",
    "text": "6.2 Computer simulation\nIn seeking to employ simulation to obtain estimates of bulk critical point properties (such as the location of a critical point and the values of its associated exponents), one is immediately confronted with a difficulty. The problem is that simulations are necessarily restricted to dealing with systems of finite-size and cannot therefore accommodate the truly long ranged fluctuations that characterize the near-critical regime. As a consequence, the critical singularities in \\(C_v\\), order parameter, etc. appear rounded and shifted in a simulation study. Figure 6.2 shows a schematic example for the susceptibility of a magnet.\n\n\n\n\n\n\nFigure 6.2: Schematic of the near-critical temperature dependence of the magnet susceptibility in a finite-sized system.\n\n\n\nThus the position of the peak in a response function (such as \\(C_v\\)) measured for a finite-sized system does not provide an accurate estimate of the critical temperature. Although the degree of rounding and shifting reduces with system size, it is often the case, that computational constraints prevent access to the largest system sizes which would provide accurate estimates of critical parameters. To help deal with this difficulty, finite-size scaling (FSS) methods have been developed to allow extraction of bulk critical properties from simulations of finite size. FSS will be discussed in section 7.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html",
    "href": "phase-transitions/rg.html",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "7.1 The critical point: A many length scale problem\nOwing the the absence of a wholly appropriate textbook for the material covered in this section, I have supplied more detailed notes than used in other parts of the unit.\nThe critical region is characterised by correlated microstructure on \\(\\underline{all}\\) length-scales up to and including the correlation length.\nSuch a profusion of degrees of freedom can only be accurately characterized by a very large number of variables. Mean field theories and approximation schemes fail in the critical region because they at best incorporate interactions among only a few spins, while neglecting correlations over larger distances. Similarly, the scaling hypothesis fails to provide more than a qualitative insight into the nature of criticality because it focuses on only one length-scale, namely the correlation length itself. Evidently a fuller understanding of the critical region may only be attained by taking account of the existence of structure on all length-scales. Such a scheme is provided by the renormalisation group method, which stands today as the cornerstone of the modern theory of critical phenomena.\nA near critical system can be characterized by three important length scales, namely\nThe authentic critical region is defined by a window condition:\n\\[L_\\textrm{ max} \\gg \\xi \\gg L_\\textrm{ min}\\]\nThe physics of this regime is hard to tackle by analytic theory because it is characterized by configurational structure on all scales between \\(L_\\textrm{ min}\\) and \\(\\xi\\) (in fact it turns out that the near critical configurational patterns are fractal-like, cf. Figure 4.1 (b). Moreover different length scales are correlated with one another, giving rise to a profusion of coupled variables in any theoretical description. The window regime is also not easily accessed by computer simulation because it entails studying very large system sizes \\(L_\\textrm{\nmax}\\), often requiring considerable computing resources.\nA nice illustration of critical point scale invariance in the Ising model can be viewed here.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "href": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "The correlation length, \\(\\xi\\), ie the size of correlated microstructure.\nMinimum length scale \\(L_\\textrm{ min}\\), i.e. the smallest length in the microscopics of the problem, e.g. lattice spacing of a magnet or the particle size in a fluid.\nMacroscopic size \\(L_{max}\\) eg. size of the system.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-rgmethod",
    "href": "phase-transitions/rg.html#sec-rgmethod",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.2 Methodology of the RG",
    "text": "7.2 Methodology of the RG\nThe central idea of the renormalisation group (RG) method is a stepwise elimination of the degrees of freedom of the system on successively larger length-scales. To achieve this one introduces a fourth length scale \\(L\\). In contrast to the other three, which characterize the system itself, \\(L\\) characterises the description of the system. It may be thought of as typifying the size of the smallest resolvable detail in a description of the system’s microstructure.\nConsider the Ising model arrangements displayed in Figure 4.1. These pictures contain all the details of each configuration shown: the resolution length \\(L\\) in this case has its smallest possible value, coinciding with the lattice spacing i.e. \\(L=L_{\\min}\\). In the present context, the most detailed description is not the most useful: the essential signals with which we are concerned are hidden in a noise of relevant detail. A clue to eliminating this noise lies in the nature of the correlation length, i.e. the size of the largest droplets. The explicit form of the small scale microstructure is irrelevant to the behaviour of \\(\\xi\\). The small scale microstructure is the noise. To eliminate it, we simply select a larger value of the resolution length (or ‘coarse-graining’ length) \\(L\\).\nThere are many ways of implementing this coarse-graining procedure. We adopt a simple strategy in which we divide our sample into blocks of side \\(L\\), each of which contains \\(L^d\\) sites, with \\(d\\) the space dimensions . The centres of the blocks define a lattice of points indexed by \\(I=1,2,\\ldots,N/L^d\\). We associate with each block lattice point centre, \\(I\\), a coarse-grained or block variable \\(S(L)\\) defined as the spatial average of the local variables it contains:\n\\[\nS(L)=L^{-d}\\sum_i^Is_i\n\\tag{7.1}\\] where the sum extends over the \\(L^d\\) sites in the block \\(I\\). The set of coarse grained coordinates \\(\\{S(L)\\}\\) are the basic ingredients of a picture of the system having spatial resolution of order \\(L\\).\nThe coarse graining operation is easily implemented on a computer. In so doing one is faced with the fact that while the underlying Ising spins can only take two possible values, the block variables \\(S(L)\\) have \\(L^d+1\\) possible values. Accordingly in displaying the consequences of the blocking procedure, we need a more elaborate colour convention than that used in Figure 4.1. We will associate with each block a shade of grey drawn from a spectrum ranging from black to white.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(aiv)\n\n\n\n\n\n\n\n(biv)\n\n\n\n\n\n\nFigure 7.1: See text for details\n\n\n\nThe results of coarse-graining configurations typical of three different temperatures are shown in Figure 7.1 and Figure 7.2. Two auxiliary operations are implicit in these results. The first operation is a length scaling: the lattice spacing on each blocked lattice has been scaled to the same size as that of the original lattice, making possible the display of correspondingly larger portions of the physical system. The second operation is a variable scaling: loosely speaking, we have adjusted the scale (‘contrast’) of the block variable so as to match the spectrum of block variable values to the spectrum of shades at our disposal.\nConsider first a system marginally above its critical point at a temperature \\(T\\) chosen so that the correlation length \\(\\xi\\) is approximately 6 lattice spacing units. A typical arrangement (without coarse-graining) is shown in Figure 7.1(ai). The succeeding panels, (aii) and (aiii), show the result of coarse-graining with block sizes \\(L=4\\) and \\(L=8\\), respectively. A clear trend is apparent. The coarse-graining amplifies the consequences of the small deviation of \\(T\\) from \\(T_c\\). As \\(L\\) is increased, the ratio of the size of the largest configurational features (\\(\\xi\\)) to the size of the smallest (\\(L\\)) is reduced. The ratio \\(\\xi/L\\) provides a natural measure of how ‘critical’ is a configuration. Thus the coarse-graining operation generates a representation of the system that is effectively less critical the larger the coarse-graining length. The limit point of this trend is the effectively fully disordered arrangement shown in Figure 7.1(aiii) and in an alternative form in Figure 7.1(aiv), which shows the limiting distribution of the coarse grained variables, averaged over many realizations of the underlying configurations: the distribution is a Gaussian which is narrow (even more so the larger the \\(L\\) value) and centred on zero. This limit is easily understood. When the system is viewed on a scaled \\(L\\) larger than \\(\\xi\\), the correlated microstructure is no longer explicitly apparent; each coarse-grained variable is essentially independent of the others.\nA similar trend is apparent below the critical point. Figure 7.1(bi) show a typical arrangement at a temperature \\(T&lt;T_c\\) such that again \\(\\xi\\) is approximately \\(6\\) lattice spacings. Coarse-graining with \\(L=4\\) and \\(L=8\\) again generates representations which are effectively less critical as shown in panels (bii) and (biii)). This time the coarse-graining smoothes out the microstructure which makes the order incomplete. The limit point of this procedure is a homogeneously ordered arrangement in which the block variables have a random (Gaussian) distribution centred on the order parameter (Figure 7.1(biv)).\nConsider now the situation at the critical point. Figure 7.2(ai) shows a typical arrangement; panels (aii) and (aiii) show the results of coarse-graining with \\(L=4\\) and \\(L=8\\) respectively. Since the \\(\\xi\\) is as large as the system itself the coarse graining does not produce less critical representations of the physical system: each of the figures displays structure over all length scales between the lower limit set by \\(L\\) and the upper limit set by the size of the display itself. A limiting trend is nevertheless apparent. Although the \\(L=4\\) pattern is qualitatively quite different from the pattern of the local variables, the \\(L=4\\) and \\(L=8\\) patterns display qualitatively similar features. These similarities are more profound than is immediately apparent. A statistical analysis of the spectrum of \\(L=4\\) configurations (generated as the local variables evolve in time) shows (Figure 7.2(iv)) that it is almost identical to that of the \\(L=8\\) configurations (given the block variable scaling). The implication of this limiting behaviour is clear: the patterns formed by the ordering variable at criticality look the same (in a statistical sense) when viewed on all sufficiently large length scales.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(iv)\n\n\n\n\n\n\nFigure 7.2: See text for details\n\n\n\nLet us summarize. Under the coarse-graining operation there is an evolution or flow of the system’s configuration spectrum. The flow tends to a limit, or fixed point, such that the pattern spectrum does not change under further coarse-graining. These scale-invariant limits have a trivial character for \\(T&gt;T_c\\), (a perfectly disordered arrangement) and \\(T&lt; T_c\\), (a perfectly ordered arrangement). The hallmark of the critical point is the existence of a scale-invariant limit which is neither fully ordered nor fully disordered but which possesses structure on all length scales.\nA nice illustration of these points made by my former postdoc Douglas Ashton can be viewed here. (Note this video uses a different coarse-graining scheme called a “majority rule”: blocks variables are assigned to be \\(+1\\) or \\(-1\\) depending on whether spin-up or spin-down is in the majority in the underlying block. Thus in contrast to our scheme, there is no gray scale.)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#universality-and-scaling",
    "href": "phase-transitions/rg.html#universality-and-scaling",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.3 Universality and Scaling",
    "text": "7.3 Universality and Scaling\nEquipped with the coarse-graining technique, we now address the universality phenomenon. We aim to understand how it is that systems that are different microscopically can nevertheless display critical point behaviour which (in certain respects) is quantitatively identical.\nTo obtain initial insight we introduce a spin-1 Ising model in which the spins take on three values (\\(s_i=1,0,-1\\)), in contrast to the two values (\\(s_i=1,-1\\)) of the spin-1/2 Ising model. The two models have properties which are different: for example, \\(T_c\\) for the three-state model is some \\(30\\%\\) lower than that of the two-state model (for the same coupling \\(J\\)). However, there is abundant evidence that the two models have the same universal properties.\nLet us explore what is the same and what is different in the configurations of the two models at criticality. The configurations of the local variables \\(s_i\\) are clearly qualitatively different for the two models. Now consider the coarse-grained configurations (with \\(L=4\\) and \\(L=8\\) respectively) for the three-state model at the critical point. We have already seen that the coarse-graining operation bears the configuration spectrum of the critical two-state Ising model to a non-trivial scale-invariant limit. It is scarcely surprising that the same is true for the three-state model. What is remarkable is that the two limits are the same! This is expressed in Figure 7.2(iv), which shows the near coincidence of the distribution of block variables (grey-levels) for the two different coarse-graining lengths. Thus the coarse-graining erases the physical differences apparent in configurations where the local behaviour is resolvable, and exposes a profound configurational similarity.\n\n7.3.1 Fluid-magnet universality\nLet us now turn to fluid-magnet universality. In a magnet, the relevant configurations are those formed by the coarse-grained magnetisation (the magnetic moment averaged over a block of side \\(L\\)). In a fluid, the relevant configurations are those of the coarse-grained density (the mass averaged over a block if side \\(L\\)) or more precisely, its fluctuation from its macroscopic average (Figure 7.3). The patterns in the latter (bubbles of liquid or vapour) may be matched to pattern in the former (microdomains of the magnetisation), given appropriate scaling operations to camouflage the differences between the length scales and the differences between the variable scales.\n\n\n\n\n\n\nFigure 7.3: Schematic representation of the coarse graining operation via which the universal properties of fluids and magnets may be exposed.\n\n\n\nThe results is illustrated in Figure 7.4.\n\n\n\n\n\n\n\n\n2D critical Ising model and 2d critical Lennard-Jones fluid at small lengthscales\n\n\n\n\n\n\n\n\n\nSame models as above, but viewed at large lengthscales\n\n\n\n\n\n\nFigure 7.4: Snapshot configurations of the 2D critical Ising model (left) and the 2D critical Lennard-Jones fluid (right). When viewed on sufficiently large length scales the configurational patterns appear universal and self similar.\n\n\n\nA movie in which we progressively zoom out shows how the loss of microscopic details reveals the large lengthscale universal features.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#near-critical-scaling",
    "href": "phase-transitions/rg.html#near-critical-scaling",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.4 Near critical scaling",
    "text": "7.4 Near critical scaling\nThe similarity of coarse-grained configurations of different systems is not restricted to the critical temperature itself. Suppose we have a two state spin model and a three state spin model each somewhat above their critical points at reduced temperature \\(t\\). The two systems will have somewhat different correlation lengths, \\(\\xi_1\\) and \\(\\xi_2\\) say. Suppose however, we choose coarse-graining lengths \\(L_1\\) for \\(L_2\\) for the two models such that \\(\\xi_1/L_1=\\xi_2/L_2\\). We adjust the scales of the block variables (our grey level control) so that the typical variable value is the same for the two systems. We adjust the length scale of the systems (stretch or shrink our snapshots) so that the sizes of the minimum-length-scale structure (set by \\(L_1\\) and \\(L_2\\)) looks the same for each system. Precisely what they look like depends upon our choice of \\(\\xi/L\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-unipics",
    "href": "phase-transitions/rg.html#sec-unipics",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.5 Universality classes",
    "text": "7.5 Universality classes\nCoarse graining does not erase all differences between the physical properties of critical systems. Differences in the space dimension \\(d\\) of two critical systems will lead to different universal properties such as critical exponents. Thus, for instance, the critical exponents of the 2D magnet, match those of the 2d fluid, but they are different to those of 3d magnets and fluids.\n\n\n\n\n\n\n\n\n\n\\(d=2\\)\n\\(d=3\\)\n\n\n\n\nCritical temperature\n0.5673\n0.75\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\tfrac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(\\tfrac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(1\\)\n\\(0.63 \\pm 0.001\\)\n\n\n\nIn fact the space dimension is one of a small set of qualitative features of a critical system which are sufficiently deep-seated to survive coarse graining and which together serve to define the system’s universal behaviour, or universality class. The constituents of this set are not all identifiable a priori. They include the number of components \\(n\\) of the order parameter. Up to now, we have only considered order parameters which are scalar (for a fluid the density, for a magnet the magnetisation), for which \\(n=1\\). In some ferromagnets, the order parameter may have components along two axes, or three axes, implying a vector order parameter, with \\(n=2\\) (the so called XY model) or \\(n=3\\) (Heisenberg model), respectively. It is clear that the order-parameter \\(n\\)-value will be reflected in the nature of the coarse-grained configurations, and thus in the universal observables they imply.\nA third important feature which can change the universality class of a critical system is the range of the interaction potential between its constituent particles. Clearly for the Ising model, interactions between spins are inherently nearest neighbour in character. Most fluids interact via dispersion forces (such as the Lennard-Jones potential) which is also short ranged owing to the \\(r^{-6}\\) attractive interaction. However some systems have much longer ranged interactions. Notable here are systems of charged particles which interact via a Coulomb potential. The long ranged nature of the Coulomb potential (which decays like \\(r^{-1}\\)) means that charged systems often do not have the same critical exponents as the Ising model and fluid.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#critical-exponents",
    "href": "phase-transitions/rg.html#critical-exponents",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.6 Critical exponents",
    "text": "7.6 Critical exponents\nWe consider now how the critical exponents, may be computed via the coarse-graining procedure. In what follows we will refer only to the behaviour of a single typical coarse grained variable, which we shall denote \\(S(L)\\). We suppose that \\(t\\) is sufficiently small that \\(\\xi \\gg\nL_\\textrm{ min}\\). Universality and scaling may be expressed in the claim that, for any \\(L\\) and \\(t\\), scale factors \\(a(L)\\) and \\(b(L)\\) may be found such that the probability distribution \\(p(S(L),t)\\) can be written in the form\n\\[\np(S(L),t)=b(L)\\tilde{p}(b(L)S(L),a(L)t)\n\\tag{7.2}\\] where \\(\\tilde{p}\\) is a function unique to a universality class. The role of the scale factors \\(a\\) and \\(b\\) is to absorb the basic non-universal scales identified in Section 7.2. The critical exponents are implicit in the \\(L\\)-dependence of these scale factors. Specifically one finds:\n\\[\n\\begin{aligned}\na(L) & =a_0L^{1/\\nu} \\\\\nb(L) & =b_0L^{\\beta/\\nu}\n\\end{aligned}\n\\tag{7.3}\\] where the amplitudes \\(a_0\\) and \\(b_0\\) are system specific (non-universal) constants.\nThese results state that the critical exponents (in the form \\(1/\\nu\\) and \\(\\beta/\\nu\\)) characterize the ways in which the configuration spectrum evolves under coarse-graining. Consider, first the exponent ratio \\(\\beta/\\nu\\). Precisely at the critical point, there is only one way in which the coarse-grained configurations change with \\(L\\): the overall scale of the coarse-grained variable (the black-white contrast in our grey scale representation) is eroded with increasing \\(L\\). Thus the configurations of coarse-graining length \\(L_1\\) match those of a larger coarse-graining length \\(L_2\\) only if the variable scale in the latter configurations is amplified. The required amplification follows from Equation 7.2 and and Equation 7.3: it is\n\\[\n\\frac{b(L_2)}{b(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{\\beta/\\nu}\\:.\n\\] The exponent ratio \\(\\beta/\\nu\\) thus controls the rate at which the scale of the ordering variable decays with increasing coarse-graining length.\nConsider now the exponent \\(1/\\nu\\). For small but non-zero reduced temperature (large but finite \\(\\xi\\)) there is second way in which the configuration spectrum evolves with \\(L\\). As noted previously, coarse graining reduces the ratio of correlation length to coarse-graining length, and results in configurations with a less critical appearance. More precisely, we see from Equation 7.2 that increasing the coarse graining length from \\(L_1\\) to \\(L_2\\) while keeping the reduced temperature constant has the same effect on the configuration spectrum as keeping coarse-graining length constant which amplifying the reduced temperature \\(t\\) by a factor\n\\[\n\\frac{a(L_2)}{a(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{1/\\nu}\\:.\n\\] One may think of the combination \\(a(L)t\\) as a measure of the effective reduced temperature of the physical system viewed with resolution length \\(L\\). The exponent \\(1/\\nu\\) controls the rate at which the effective reduced temperature flows with increasing coarse-graining length.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-fss",
    "href": "phase-transitions/rg.html#sec-fss",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.7 Finite-size scaling",
    "text": "7.7 Finite-size scaling\nWe can exploit the fact that the scale factors \\(a(L)\\) and \\(b(L)\\) depend on critical exponents to estimate the values of these exponents using computer simulation. Consider the average of the block variable \\(S(L)\\). Consideration of Equation 7.1 shows that this is non other than the value of the order parameter \\(Q\\), measured over a block of side \\(L\\). Thus from the definition of an average\n\\[\nQ(L,t)=\\bar {S}(L,t)=\\int S(L)p(S(L),t)dS(L)\n\\] where \\(p(S(L))\\) is the probability distribution of \\(S(L)\\).\nMaking use of the representation of Equation 7.2, we then have that\n\\[Q\n(L,t) = \\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)dS(L)\n\\]\nTo integrate this we need to change the integration variable from \\(S(L)\\) to \\(b(L)S(L)\\). We have \\(d(b(L)S(L))=b(L)dS(L)\\) since \\(b(L)\\) does not fluctuate. Thus \\[\n\\begin{aligned}\nQ(L,t)  & =  b^{-1}(L)\\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)d(b(L)S(L))\\nonumber\\\\\n        & =  b^{-1}(L)f(a(L)t)\\nonumber\\\\\n       & =  b_0^{-1}L^{-\\beta/\\nu}f(a_0L^{1/\\nu}t)\n\\end{aligned}\n\\]\nwhere \\(f\\) is a universal function (defined as the first moment of \\(\\tilde{p}(x,y)\\) with respect to \\(y\\)).\nThe above results provide a method for determining the critical exponent ratios \\(\\beta/\\nu\\) and \\(1/\\nu\\) via computer simulations of near critical systems. For instance, at the critical point (\\(t=0\\)) and for finite block size, \\(Q(L,0)\\) will not be zero (the \\(T\\) at which Q vanishes for finite \\(L\\) is above the true \\(T_c\\), cf. Section 6.2. However, we know that its value must vanish in the limit of infinite \\(L\\); it does so like\n\\[Q(L,0)=b_0L^{-\\beta/\\nu}f(0)\\equiv Q_0L^{-\\beta/\\nu}\\]\nThus by studying the critical point \\(L\\) dependence of \\(Q\\) we can estimate \\(\\beta/\\nu\\). A similar approach in which we study two block sizes \\(L\\), and tune \\(t\\) separately in each case so that the results for \\(QL^{\\beta/\\nu}\\) are identical provides information on the value of \\(1/\\nu\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#summary-of-main-points",
    "href": "phase-transitions/rg.html#summary-of-main-points",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.8 Summary of main points",
    "text": "7.8 Summary of main points\nAs this is quite a long chapter let us summarise the main points:\n\nLimitations of conventional theories: Mean field theories and the scaling hypothesis are insufficient in the critical region due to their neglect of correlations across all relevant length scales.\nCritical region characteristics: Near-critical systems exhibit correlated microstructure on all length scales up to the correlation length. The complexity of this structure makes both analytical and computational study challenging.\nRelevant length scales:\n\nCorrelation length (\\(\\xi\\))\nMinimum microscopic scale (\\(L_\\textrm{min}\\))\nMacroscopic system size (\\(L_\\textrm{max}\\))\n\nWindow Condition for criticality: The true critical regime satisfies \\(L_\\textrm{max} \\gg \\xi \\gg L_\\textrm{min}\\), encompassing a broad range of scales where complex, often fractal-like, structures are present.\nRenormalisation Group (RG) methodology: RG involves the stepwise elimination of degrees of freedom by coarse-graining the system over increasing length scales. A fourth scale, \\(L\\), represents the resolution at which the system is described.\nEffect of coarse-graining: Coarse-graining changes the effective reduced temperature, captured by the relation \\(a(L)t\\), where \\(a(L)\\) scales with \\(L\\) as \\(L^{1/\\nu}\\). This helps describe how critical configurations evolve with resolution.\nUniversality:\n\nCoarse-graining reveals that microscopically different systems can exhibit identical critical behavior when observed at large scales.\nThe concept of universality explains why disparate systems, such as magnets and fluids, can show the same critical exponents and scaling laws.\nCritical behavior depends primarily on general features like dimensionality and symmetry, rather than microscopic details.\n\nFinite-Size scaling:\n\nThe average block variable \\(Q(L,t)\\) is related to block size \\(L\\) and reduced temperature \\(t\\) through scaling relations. Computer simulations exploit this to extract scaling functions and the values of critical exponents.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "href": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)",
    "text": "7.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)\n\n\n\n\n\n\nNotes for those interested in a different perspective on RG theory.\n\n\n\n\n\nLet us begin by returning to our fundamental Equation 2.1, which we rewrite as\n\\[p = Z^{-1}e^{-{\\cal H}}\\] where \\({\\cal H}\\equiv E/k_BT\\).\nThe first step is then to imagine that we generate, by a computer simulation procedure for example, a sequence of configurations with relative probability \\(\\exp(-{\\cal H})\\). We next adopt some coarse-graining procedure which produces from these original configurations a set of coarse-grained configurations. We then ask the question: what is the energy function \\({\\cal H}^\\prime\\) of the coarse-grained variables which would produce these coarse-grained configurations with the correct relative probability \\(\\exp(-{\\cal H}^\\prime)\\)? Clearly the form of \\({\\cal H}^\\prime\\) depends on the form of \\({\\cal H}\\) thus we can write symbolically\n\\[{\\cal H}^\\prime=R({\\cal H})\\]\nThe operation \\(R\\), which defines the coarse-grained configurational energy in terms of the microscopic configurational energy function is known as a renormalisation group transformation (RGT). What it does is to replace a hard problem by a less hard problem. Specifically, suppose that our system is near a critical point and that we wish to calculate its large-distance properties. If we address this task by utilizing the configurational energy and appealing to the basic machinery of statistical mechanics set out in Equation 2.1 and Equation 2.2, the problem is hard. It is hard because the system has fluctuations on all the (many) length scales intermediate between the correlation length \\(\\xi\\) and the minimum length scale \\(L_\\textrm{min}\\).\nHowever, the task may instead be addressed by tackling the coarse-grained system described by the energy \\({\\cal H}^\\prime\\). The large-distance properties of this system are the same as the large-distance properties of the physical system, since coarse-graining operation preserves large-scale configurational structure. In this representation the problem is a little easier: while the \\(\\xi\\) associated with \\({\\cal H}^\\prime\\) is the same as the \\(\\xi\\) associated with \\({\\cal H}\\), the minimum length scale of \\({\\cal H}^\\prime\\) is bigger than that of \\({\\cal H}\\). Thus the statistical mechanics of \\({\\cal H}^\\prime\\) poses a not-quite-so-many-length-scale problem, a problem which is effectively a little less critical and is thus a little easier to solve. The benefits accruing from this procedure may be amplified by repeating it. Repeated application of \\(R\\) will eventually result in a coarse- grained energy function describing configurations in which the \\(\\xi\\) is no bigger than the minimum length scale. The associated system is far from criticality and its properties may be reliably computed by any of a wide variety of approximation schemes. These properties are the desired large-distance properties of the physical system. As explicit reference to fluctuations of a given scale is eliminated by coarse-graining, their effects are carried forward implicitly in the parameters of the coarse-grained energy.\nIn order to setup the framework for a simple illustrative example, let is return to the lattice Ising model for which the energy function depended only on the product of nearest neighbour spins. The coefficient of this product in the energy is the exchange coupling, \\(J\\). In principle, however, other kinds of interactions are also allowed; for example, we may have a product of second neighbour spins with strength \\(J_2\\) or, perhaps, a product of four spins (at sites forming a square whose side is the lattice spacing), with strength \\(J_3\\). Such interactions in a real magnet have their origin in the quantum mechanics of the atoms and electrons and clearly depend upon the details of the system. For generality therefore we will allow a family of exchange couplings \\(J_1\\),\\(J_2\\),\\(J_3,\\dots\\), or \\(J_a, a =\n1,2,\\dots\\) In reduced units, the equivalent coupling strengths are \\(K_a =J_a/k_BT\\). Their values determine uniquely the energy for any given configuration.\n\nWe note that it is not only useful to allow for arbitrary kinds of interactions: if we wish to repeat the transformation several (indeed many) times, it is also necessary because even if we start with only the nearest neighbour coupling in \\({\\cal H}\\) the transformation will in general produce others in \\({\\cal H}^\\prime\\).\n\nNow consider the coarse-graining procedure. Let us suppose that this procedure takes the form of a ‘majority rule’ operation in which the new spins are assigned values \\(+1\\) or \\(-1\\) according to the signs of the magnetic moments of the blocks with which they are associated. The new energy function \\({\\cal H}^\\prime\\) will be expressible in terms of some new coupling strengths \\(K^\\prime\\) describing the interactions amongst the new spin variables (and thus, in effect, the interactions between blocks of the original spin variables). The RGT simply states that these new couplings depend on the old couplings: \\(K_1^\\prime\\) is some function \\(f_1\\) of all the original couplings, and generally\n\\[K^\\prime_a=f_a(K_1,K_2,\\dots) =f_a(\\mathbf {K}),\\quad a= 1, 2,\\dots\n\\tag{7.4}\\] where K is shorthand for the set \\(K_1, K_2,\\dots\\)\n\n7.9.1 A simple example\nThis example illustrates how one can perform the RG transformation Equation 7.4 directly, without recourse to a ‘sequence of typical configurations’. The calculation involves a very crude approximation which has the advantage that it simplifies the subsequent analysis.\n\n\n\n\n\n\nFigure 7.5: Coarse graining by decimation. The spins on the original lattice are divided into two sets \\(\\{s^\\prime\\}\\) and \\(\\{\\tilde{s}\\}\\). The \\(\\{s^\\prime\\}\\) spins occupy a lattice whose spacing is twice that of the original. The effective coupling interaction between the \\(\\{s^\\prime\\}\\) spins is obtained by performing the configurational average over the \\(\\{\\tilde{s}\\}\\)\n\n\n\nConsider an Ising model in two dimensions, with only nearest neighbour interactions as shown in Figure 7.5. We have divided the spins into two sets, the spins \\(\\{s^\\prime\\}\\) form a square lattice of spacing \\(2\\), the others being denoted by \\(\\{\\tilde{s}\\}\\). One then defines an effective energy function \\({\\cal H^\\prime}\\) for the \\(s^\\prime\\) spins by performing an average over all the possible arrangements of the \\(\\tilde{s}\\) spins\n\\[\n\\exp(-{\\cal H}^\\prime)=\\sum_{\\{\\tilde {s}\\}} \\exp(-{\\cal H}).\n\\tag{7.5}\\]\nThis particular coarse-graining scheme is called ‘decimation’ because a certain fraction (not necessarily one-tenth!) of spins on the lattice is eliminated. This formulation of a new energy function realizes two basic aims of the RG method: the long-distance physics of the ‘original’ system, described by \\({\\cal H}\\), is contained in that of the ‘new’ system, described by \\({\\cal H}^\\prime\\) (indeed the partition functions are the same as one can see by summing both sides over \\(s^\\prime\\)) and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing (‘minimum length scale’) has been reduced by a factor of \\(1/2\\) (the ratio of the lattice spacings of the two systems). We must now face the question of how to perform the configuration sum in Equation 7.5. This cannot in general be done exactly, so we must resort to some approximation scheme. The particular approximation which we invoke is the high temperature series expansion. In its simplest mathematical form, since \\({\\cal H}\\) contains a factor \\(1/k_BT\\), it involves the expansion of \\(\\exp(-{\\cal H})\\) as a power series:\n\\[\\exp(-{\\cal H}/k_BT)=1-{\\cal H}/k_BT +\\frac{1}{2!}({\\cal H}/k_BT)^2+.....\\]\nWe substitute this expansion into the right hand side of Equation 7.5 and proceed to look for terms which depend on the \\(s^\\prime\\) spins after the sum over the possible arrangements of the \\(\\tilde{s}\\) spins is performed. This sum extends over all the possible (\\(\\pm 1\\)) values of all the \\(\\tilde{s}\\) spins. The first term (the 1) in the expansion of the exponential is clearly independent of the values of the \\(s^\\prime\\) spins. The second term (\\({\\cal H}\\)) is a function of the \\(s^\\prime\\) spins, but gives zero when the sum over the \\(s^\\prime\\) spins is performed because only a single factor of any \\(s^\\prime\\) ever appears, and \\(+ 1 - 1 = 0\\). The third term (\\({\\cal H}^2/2\\)) does contribute. If one writes out explicitly the form of \\({\\cal H}^2/2\\) one finds terms of the form \\(K^2s_1^\\prime\\tilde{s}\\tilde{s}s_2^\\prime=K^2s_1^\\prime s_2^\\prime\\), where \\(s_1^\\prime\\) and \\(s_2^\\prime\\) denote two spins at nearest neighbour sites on the lattice of \\(s^\\prime\\) spins and \\(\\tilde{s}\\) is the spin (in the other set) which lies between them. Now, in the corresponding expansion of the left hand side of Equation 7.5, we find terms of the form \\(K^\\prime s_1^\\prime s_2^\\prime\\), where \\(K^\\prime\\) is the nearest neighbour coupling for the \\(s^\\prime\\) spins. We conclude (with a little more thought than we detail here) that\n\\[\nK^\\prime=K^2\n\\tag{7.6}\\]\nOf course many other terms and couplings are generated by the higher orders of the high temperature expansion and it is necssary to include these if one wishes reliable values for the critical temperature and exponents, However, our aim here is to use this simple calculation to illustrate the RG method. Let us therefore close our eyes, forget about the higher order terms and show how the RGT Equation 7.6 can be used to obtain information on the phase transition.\n\n\n\n\n\n\nFigure 7.6: Coupling flow under the decimation transformation described in the text.\n\n\n\nThe first point to note is that that mathematically Equation 7.6 has the fixed point \\(K^*= 1\\); if \\(K= 1\\) then the new effective coupling \\(K^\\prime\\) has the same value \\(1\\). Further, if \\(K\\) is just larger than \\(1\\), then \\(K^\\prime\\) is larger than \\(K\\), i.e. further away from \\(1\\). Similarly, if \\(K\\) is less than \\(1\\), \\(K^\\prime\\) is less than \\(K\\). We say that the fixed point is unstable: the flow of couplings under repeated iteration of Equation 7.6 is away from the fixed point, as illustrated in Figure 7.6. The physical significance of this is as follows: suppose that the original system is at its critical point so that the ratio of \\(\\xi\\) to lattice spacing is infinite. After one application of the decimation transformation, the effective lattice spacing has increased by a factor of two, but this ratio remains infinite; the new system is therefore also at its critical point. Within the approximations inherent in Equation 7.6, the original system is an Ising model with nearest neighbour coupling \\(K\\) and the new system is an Ising model with nearest neighbour coupling \\(K^\\prime\\). If these two systems are going to be at a common critically, we must identify \\(K^\\prime=\nK\\). The fixed point \\(K^*= 1\\) is therefore a candidate for the critical point \\(K_c\\), where the phase transition occurs. This interpretation is reinforced by considering the case where the original system is close to, but not at, criticality. Then \\(\\xi\\) is finite and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing is reduced by a factor of two. This instability of a fixed point to deviations of \\(K\\) from \\(K^*\\) is a further necessary condition for its interpretation as a critical point of the system. In summary then we make the prediction\n\\[\nK_c=J/k_BT_c=1\n\\tag{7.7}\\]\nWe can obtain further information about the behaviour of the system close to its critical point. In order to do so, we rewrite the transformation (Equation 7.6) in terms of the deviation of the coupling from its fixed point value. A Taylor expansion of the function \\(K^\\prime=K^2\\) yields\n\\[\n\\begin{aligned}\nK^\\prime =& (K^*)^2 +(K-K^*)\\left.\\frac{\\partial K^\\prime}{\\partial K}\\right|_{K=K^*}+\\frac{1}{2}(K-K^*)^2\\left.\\frac{\\partial^2 K^\\prime}{\\partial K^2}\\right|_{K=K^*}+\\ldots\\nonumber\\\\\nK^\\prime - K^* =& 2 (K - K^*)+ (K - K^*)^2\n\\end{aligned}\n\\]\nwhere in the second line we have used the fact that the first derivative evaluates to \\(2K^*=2\\) and \\((K^*)^2=K^*\\).\nFor a system sufficiently close to its critical temperature the final term can be neglected. The deviation of the coupling from its fixed point (critical) value is thus bigger for the new system than it is for the old by a factor of two. This means that the reduced temperature is also bigger by a factor of two:\n\\[t^\\prime= 2t\\]\nBut \\(\\xi\\) (in units of the appropriate lattice spacing) is smaller by a factor of \\(1/2\\):\n\\[\\xi^\\prime= \\xi/2\\]\nThus, when we double \\(t\\), we halve \\(\\xi\\), implying that\n\\[\\xi\\propto t^{-1}\\]\nfor \\(T\\) close to \\(T_c\\). Thus we see that the RGT predicts scaling behaviour with calculable critical exponents. In this simple calculation we estimate the critical exponent \\(\\nu=1\\) for the square lattice Ising model. This prediction is actually in agreement with the exactly established value. The agreement is fortuitous- the prediction in Eq. refeq:Kc for \\(K_c\\), is larger than the exactly established value by a factor of more than two. In order to obtain reliable estimates more sophisticated and systematic methods must be used.\nThe crude approximation in the calculation above produced a transformation, Equation 7.6, involving only the nearest neighbour coupling, with the subsequent advantages of simple algebra. We pay a penalty for this simplicity in two ways: the results obtained for critical properties are in rather poor agreement with accepted values, and we gain no insight into the origin of universality.\n\n\n7.9.2 Universality and scaling\nIn order to expose how universality can arise, we should from the start allow for several different kinds of coupling \\(J_a\\), and show how the systems with different \\(J_a\\) can have the same critical behaviour.\n\n\n\n\n\n\nFigure 7.7: General flow in coupling space\n\n\n\nFigure 7.7 is a representation of the space of all coupling strengths \\(K_a\\) in the energy function \\({\\cal H}/k_BT\\). This is of course actually a space of infinite dimension, but representing three of these, as we have done, enables us to illustrate all the important aspects. First let us be clear what the points in this space represent. Suppose we have some magnetic material which is described by a given set of exchange constants \\(J_1,J_2,J_3.....\\) As the temperature \\(T\\) varies, the coupling strengths \\(K_a=J_a/k_BT\\) trace out a straight line, or ray, from the origin of the space in the direction (\\(J_1,J_2,J_3 ....\\) ). Points on this ray close to the origin represent this magnet at high temperatures, and conversely points far from the origin represent the magnet at low temperatures. The critical point of the magnet is represented by a specific point on this ray, \\(K_a=\nJ_a/k_BT, a= 1,2,\\dots\\) The set of critical points on all of the possible rays forms a surface, the critical surface. Formally, it is defined by the set of all possible models (of the Ising type) which have infinite \\(\\xi\\). It is shown schematically as the shaded surface in Figure 7.7. (In the figure it is a two-dimensional surface; more generally it has one dimension less than the full coupling constant space, dividing all models into high and low temperature phases.)\nOur immediate goal then is to understand how the RGT can explain why different physical systems near this critical surface have the same behaviour. Let us turn now to the schematic representation of the RG flow in Figure 7.7. Suppose we start with a physical system, with coupling strengths \\(K_a,  a= 1,2, \\dots\\). What the RGT does is generate a new point in the figure, at the coupling strengths \\(K_a^{(1)}=f_a(\\mathbf {K})\\); these are the couplings appearing in the effective energy function describing the coarse-grained system. If we repeat the transformation, the new energy function has coupling strengths \\(K_a^{(2)}=f_a(\\mathbf {K})\\). Thus repeated application of the transformation generates a flow of points in the figure: \\(\\mathbf{K}\\to\nK^{(1)}\\to\\dots\\to K^{(n)}\\) where the superscript (\\(n\\)) labels the effective couplings after \\(n\\) coarse-graining steps. if the change in coarse-graining scale is \\(b\\) (\\(&gt; 1\\)) at each step, the total change in coarse-graining scale is \\(b^n\\) after \\(n\\) steps. In the process, therefore, the ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor of \\(b^{-n}\\). The dots in Figure 7.7 identify three lines of RG flow starting from three systems differing only in their temperature. (The flow lines are schematic but display the essential features revealed in detailed calculations.)\nConsider first the red dots which start from the nearest neighbour Ising model at its critical point. The ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor b at each step, but, since it starts infinite, it remains infinite after any finite number of steps. In this case we can in principle generate an unbounded number of dots, \\(\\mathbf{ K^{(1)}, K^{(2)},\\dots,K^{(n)}}\\), all of which lie in the critical surface. The simplest behaviour of such a sequence as \\(n\\) increases is to tend to a limit, \\(K^*\\), say. In such a case\n\\[K^*_a=f_a(K^*)~~~~ a= 1,2 .....\\]\nThis point \\(\\mathbf{K^*} \\equiv K_1^*, K_2^*, \\dots\\) is therefore a fixed point which lies in the critical surface.\nBy contrast, consider the same magnet as before, now at temperature \\(T\\) just greater than \\(T_c\\), its couplings \\(K_a\\), will be close to the first red dot (in fact they will be slightly smaller) and so will the effective couplings \\(K_a^{(1)},K_2^{(2)},\\dots\\) of the corresponding coarse-grained systems. The new flow will therefore appear initially to follow the red dots towards the same fixed point. However, the flow must eventually move away from the fixed point because each coarse-graining now produces a model further from criticality. The resulting flow is represented schematically by one set of black dots. The other set of black dots shows the expected flow starting from the same magnet slightly below its critical temperature.\nWe are now in a position to understand both universality and scaling within this framework. We will suppose that there exists a single fixed point in the critical surface which sucks in all flows starting from a point in that surface. Then any system at its critical point will exhibit large-length scale physics (large-block spin behaviour) described by the single set of fixed point coupling constants. The uniqueness of this limiting set of coupling constants is the essence of critical point universality. It is, of course, the algebraic counterpart of the unique limiting spectrum of coarse-grained configurations, discussed in Section 7.5. Similarly the scale-invariance of the critical point configuration spectrum (viewed on large enough length scales) is expressed in the invariance of the couplings under iteration of the transformation (after a number of iterations large enough to secure convergence to the fixed point).\nTo understand the behaviour of systems near but not precisely at critically we must make a further assumption (again widely justified by explicit studies). The flow line stemming from any such system will, we have argued, be borne towards the fixed point before ultimately deviating from it after a number of iterations large enough to expose the system’s noncritical character. We assume that (as indicated schematically in the streams of red and blue lines in Figure 7.7 the deviations lie along a single line through the fixed point, the direction followed along this line differing according to the sign of the temperature deviation \\(T-T_c\\). Since any two sets of coupling constants on the line (on the same side of the fixed point) are related by a suitable coarse-graining operation, this picture implies that the large-length-scale physics of all near- critical systems differs only in the matter of a length scale. This is the essence of near-critical point universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html",
    "href": "phase-transitions/master-equation-and-diffusion.html",
    "title": "8  Introduction to stochastic processes",
    "section": "",
    "text": "8.1 The Master Equation\nMany natural phenomena are stochastic — they involve randomness in their evolution over time.\nIn classical physics, this randomness may arise from our lack of knowledge about microscopic details. For example, in a gas, we do not know the precise positions and velocities of each particle, so their collisions with the container walls appear random.\nIn quantum mechanics, stochasticity is even more intrinsic. The fundamental objects are probability amplitudes, and outcomes are inherently probabilistic.\nTo describe these systems effectively, we use a coarse-grained probabilistic description, which tracks the likelihood of different outcomes rather than precise trajectories. One of the key tools in this approach is the master equation.\nConsider a system in microstate \\(i\\) with energy \\(E_i\\). It can transition to neighboring microstates \\(j\\), where the energy difference \\(|E_j - E_i|\\) is small (within \\(\\delta E\\)).\nLet \\(\\nu_{ij}\\) be the rate at which the system jumps from state \\(i\\) to state \\(j\\). Over an infinitesimal time interval \\(dt\\), the probability \\(p_i\\) changes as:\n\\[\ndp_i = \\left[ -p_i \\sum_j  \\nu_{ij} + \\sum_j \\nu_{ji} p_j \\right] dt\n\\]\nThis expression contains two terms:\nThe master equation becomes:\n\\[\n\\frac{dp_i}{dt} = -\\sum_j \\nu_{ij} p_i + \\sum_j \\nu_{ji} p_j\n\\]\nThis is a linear first-order differential equation for the vector of probabilities \\(\\{p_i\\}\\).\nAlternatively, in matrix form:\n\\[\n\\frac{d\\mathbf{p}}{dt} = W \\mathbf{p}\n\\]\nwhere \\(W\\) is the rate matrix with entries:\nThis structure ensures probability conservation: the total probability \\(\\sum_i p_i = 1\\) remains constant in time.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "title": "8  Introduction to stochastic processes",
    "section": "",
    "text": "A loss term: the system leaves state \\(i\\) at rate \\(\\nu_{ij}\\),\nA gain term: the system arrives in state \\(i\\) from other states \\(j\\) at rate \\(\\nu_{ji}\\).\n\n\n\n\n\n\n\n\n\\(W_{ij} = \\nu_{ji}\\) for \\(i \\neq j\\),\n\\(W_{ii} = -\\sum_{j \\neq i} \\nu_{ij}\\).\n\n\n\n\n\n\n\n\nAn aside on entropy production\n\n\n\n\n\nThe master equation is first order in time and does not have time reversal symmetry so describes an irreversible process. This irreversibility arises from the coarse-graining process that throws away information about underlying microphysics which is described by Newton’s equations and which are time reversible. Only by doing so is the entropy allowed to increase which is required by the second law of thermodynamics for an irreversible process. Consequently the increase of entropy is linked to our knowledge about the system rather than anything it is doing internally in a manner that may appear dubious. Can it be possible that macroscopic and reproducible phenomena such as heat flow depend on how we handle information? Perhaps yes since the division between work and heat is somewhat arbitrary. Were we able to track all the particle positions there would be no need to talk about heat energy or heat flow.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "title": "8  Introduction to stochastic processes",
    "section": "8.2 From the Master Equation to the Diffusion Equation",
    "text": "8.2 From the Master Equation to the Diffusion Equation\nNow consider the situation where the state index \\(i\\) corresponds to a position in space, \\(x_i = i a\\), ie a one-dimensional lattice with lattice spacing \\(a\\), and transitions only occur between neighboring lattice sites.\nWe assume:\n\nTransition rates are symmetric: \\(\\nu_{i, i+1} = \\nu_{i, i-1} = \\nu\\),\nThe spacing \\(a\\to 0\\) in the continuum limit.\n\nThe master equation becomes a finite-difference equation:\n\\[\n\\begin{aligned}\n\\frac{dp_i}{dt} =  &\\sum_j\\nu_{ij}(p_j-p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i-1}-p_i) + \\nu(p_{i+1} - p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i+1} + p_{i-1} - 2p_i)\n\\end{aligned}\n\\]\nWe now define a continuous variable \\(x = i a\\), and a probability density \\(p(x, t)\\) such that \\(p_i(t) \\approx p(x, t)\\).\nUsing Taylor expansions:\n\\[\np(x \\pm a, t) = p(x, t) \\pm a \\frac{\\partial p}{\\partial x} + \\frac{a^2}{2} \\frac{\\partial^2 p}{\\partial x^2} + \\cdots\n\\]\nSubstituting into the master equation gives:\n\\[\n\\frac{\\partial p}{\\partial t} =\n\\nu a^2 \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nDefining the diffusion constant \\(D = \\nu a^2\\), we obtain the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThe diffusion equation describes the evolution of the probability density of a particle with diffusion constant (sometimes called diffusivity) given by \\(D = \\nu a^2\\). The dimensions of \\(D\\) are \\([\\text{length}]^2/[\\text{time}]\\). Typically, after expansion, we set the lattice spacing \\(a\\) to 1. Additionally, the diffusion equation can describe many non-interacting diffusing particles. In this case, we replace \\(p\\) with \\(\\rho\\), representing the density or concentration of particles, and use the normalization \\(\\int dx \\, \\rho = M\\), where \\(M\\) is the number of particles.\nThe diffusion equation, much like the master equation from which it originates, explicitly violates time-reversal symmetry, thus permitting entropy to increase.\nThe solution of the diffusion equation for an initial condition where the particle is initially localized at the origin (formally, \\(p(x,0) = \\delta(x)\\)) is a Gaussian:\n\\[\np(x,t) = (4 \\pi D t)^{-1/2} \\exp\\left[-\\frac{x^2}{4 D t}\\right]\n\\]\nWe explicitly see the arrow of time by examining this Gaussian solution at various times \\(t\\). As \\(t\\) increases, the Gaussian “bell-shaped” curve spreads out. Its width grows according to \\(\\langle x^2 \\rangle^{1/2} \\sim t^{1/2}\\). This is known as “diffusive scaling,” and it implies that, after time \\(t\\), a particle will typically be found at a distance roughly proportional to \\(t^{1/2}\\) from its starting point. Conversely, exploring a region of size \\(L\\) typically requires a time of order \\(O(L^2)\\).\nThe evolution of the solution to the 1d diffusion equation in a spatial region \\(x=[0,1]\\) as a function of times are shown in the movie below. The diffusion constant is \\(D=0.01\\). The movie corresponds to a particle initialised at \\(x=0.5\\). One sees how the probability density spreads out over the range as time increases. This can be used to model the diffusion of particles down a concentration gradient as you will see in the next part of the course.\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport os\n\n# Parameters\nL = 1.0        # Length of the domain\nT = 1.0        # Total time \nnx = 400       # Number of spatial points\nnt = 2000      # Number of time steps\nD = 0.1        # Diffusion coefficient\n\n# Discretization\ndx = L / (nx - 1)\ndt = T / nt\n\n# Stability condition auto-adjust\nif D * dt / dx**2 &gt; 0.5:\n    print(\"Adjusting dt and nt to satisfy stability condition...\")\n    dt = 0.4 * dx**2 / D\n    nt = int(T / dt)\n    dt = T / nt  # Recalculate dt exactly\n\nnt = int(nt / 50)  # Artificial slowdown for animation\n\nprint(f\"Using dt = {dt:.4e}, nt = {nt}\")\n\n# Initialize x and u\nx = np.linspace(0, L, nx)\n\n# Initial condition: smooth narrow Gaussian\nsigma = 0.01\nu = np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n# Normalize initial condition\nu /= np.sum(u) * dx\n\n# Setup figure\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\nfig, ax = plt.subplots()\nline, = ax.plot(x, u)\n\n# Compute peak height for setting y-axis\npeak_height = 1 / (np.sqrt(2 * np.pi) * sigma)\nax.set_ylim(0, peak_height * 1.05)\n\nax.set_xlabel(r'$x$', fontsize=20)\nax.set_ylabel(r'$p(x,t)$', fontsize=20)\n\nax.tick_params(axis='both', which='major', labelsize=14)\n\n# Tiny time counter text\ntime_text = ax.text(0.85, 0.05, '', transform=ax.transAxes, fontsize=10, verticalalignment='bottom')\n\n\n# Function to update the plot\ndef update(frame):\n    global u\n    unew = np.copy(u)\n    unew[1:-1] = u[1:-1] + D * dt / dx**2 * (u[2:] - 2*u[1:-1] + u[:-2])\n    u = unew\n\n    # Normalize at every step\n    u /= np.sum(u) * dx\n\n    line.set_ydata(u)\n    current_time = frame * dt\n    time_text.set_text(r'$t=%.4f$' % current_time)\n    return line, time_text\n\nani = animation.FuncAnimation(fig, update, frames=nt, interval=100, blit=True)\n\n\n# Save the animation as a movie\nWriter = animation.writers['ffmpeg']\nwriter = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nani.save(\"../Movies/diffusion_evolution.mp4\", writer=writer)\n\n\nplt.show()",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "href": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "title": "8  Introduction to stochastic processes",
    "section": "8.3 Consequences of time reversal symmetry",
    "text": "8.3 Consequences of time reversal symmetry\nAs we have seen by introducing a type of coarse graining, the master equation violates time reversal symmetry of the underlying Newtonian dynamics. Remarkably however, the fact that the underlying microphysics is actually time reversal symmetric has several deep consequences which survive the coarse graining procedure These results are some of the cornerstones of nonequilibrium thermodynamics\n\n8.3.1 Detailed balance\nRecall from year 2 Statistical Mechanics that for a system in equilibrium, the principle of equal a-priori probabilities of microstates holds. Therefore\n\\[\n\\nu_{ij} p_i^{eq} = \\nu_{ji} p_j^{eq}\n\\]\nHence, on average, the actual rate of quantum jumps from \\(i\\) to \\(j\\) (the left-hand side) is the same as from \\(j\\) to \\(i\\). This is a stronger statement than the master equation, which asserts only that there is overall balance between the rate of jumping into and out of state \\(i\\) in equilibrium. The result above is known as the principle of detailed balance.\nThis principle is powerful because it applies not only to individual states but also to any grouping of states.\nExercise: Show that for two groups of states, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\nu_{AB} p_A^{eq} = \\nu_{BA} p_B^{eq}\n\\]\nHence, detailed balance arguments can be extended to subsystems within a large isolated system, and even to systems that are not isolated. However, in such cases, the principle is far from obvious, because once states are grouped together:\n\\[\n\\nu_{AB} \\ne \\nu_{BA}, \\quad p_A \\ne p_B\n\\]\n(This can be easily demonstrated, for example, by considering two groups that contain different numbers of states with similar energies.) Nonetheless, the detailed balance relation holds, in equilibrium, in the general form above.\n\n\n8.3.2 Computer simulation\nIn computer simulation, good results will be obtained if one accurately follows the microscopic equations of motion. This is the molecular dynamics (MD) method which we now outline.\nMolecular dynamics\nMolecular Dynamics (MD) involves a system of classical particles interacting through specified interparticle forces. The motion of these particles is determined by numerically integrating Newton’s equations of motion. In MD simulations, averages of state variables are obtained as time averages over trajectories in phase space. Typically, the forces acting between particles are conservative, ensuring that the total energy \\(E\\) remains constant. This conservation implies that the motion is restricted to a \\((2dN - 1)\\)-dimensional surface in phase space, denoted by \\(\\Gamma(E)\\).\nA central aspect of MD is the averaging of observables. For a given observable \\(A\\), its average is computed as the time average along the trajectory. Mathematically, this is expressed as:\n\\[\n\\left\\langle A(\\{ \\mathbf{p}_i \\}, \\{ \\mathbf{r}_i \\}) \\right\\rangle = \\frac{1}{\\tau} \\int_{t_0}^{t_0+\\tau} dt\\, A(\\{ \\mathbf{p}_i(t) \\}, \\{ \\mathbf{r}_i(t) \\})\n\\]\nThis formula represents the integral of the observable over a time interval \\(\\tau\\), normalized by the length of that interval.\nThe practical steps of an MD simulation start with generating an initial random configuration of particle positions \\(\\{ \\mathbf{r}_i \\}\\) and momenta \\(\\{ \\mathbf{p}_i \\}\\). The system’s equations of motion are then iteratively solved using a suitable algorithm to allow it to reach equilibrium. After equilibration, a production run is performed over many time steps to collect meaningful data. Finally, relevant averages, such as pressure or kinetic energy, are calculated from the collected data.\nMonte Carlo\nHowever, to obtain the equilibrium properties of the system, it may be much faster to use a dynamics which is nothing like the actual equations of motion.\nAt first sight, this looks very dangerous; however, if one can prove that in the required equilibrium distribution, the artificial dynamics obey the principle of detailed balance, then it is (almost) guaranteed that the steady state found by simulation is the true equilibrium state.\nThe best known example is the Monte Carlo method, in which the dynamical algorithm consists of random jumps. The jump rates \\(\\nu_{AB}\\) for all pairs of states \\((A, B)\\) take the form:\n\\[\n\\nu_{AB} = \\nu_0 \\quad \\text{if } E_B \\le E_A\n\\]\n\\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)} \\quad \\text{if } E_B \\ge E_A\n\\]\nwhere \\(\\nu_0\\) is a constant.\nExercise: Show that this gives the canonical distribution in steady state.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo show that this jump rate rule gives the canonical distribution in steady state, we assume the system reaches a steady-state probability distribution \\(P_A\\) for state \\(A\\) and apply the condition of detailed balance.\nDetailed balance requires: \\[\nP_A \\nu_{AB} = P_B \\nu_{BA}\n\\]\nAssume the canonical distribution: \\[\nP_A = \\frac{1}{Z} e^{-\\beta E_A}, \\quad P_B = \\frac{1}{Z} e^{-\\beta E_B}\n\\]\nNow consider two cases for \\(\\nu_{AB}\\) and \\(\\nu_{BA}\\):\nCase 1: \\(E_B \\leq E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0, \\quad \\nu_{BA} = \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nSubstitute into the detailed balance condition: \\[\nP_A \\nu_0 = P_B \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A = P_B e^{-\\beta (E_A - E_B)}\n\\]\nUse canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} = \\frac{1}{Z} e^{-\\beta E_B} e^{-\\beta (E_A - E_B)} = \\frac{1}{Z} e^{-\\beta E_A}\n\\]\nVerified.\nCase 2: \\(E_B &gt; E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)}, \\quad \\nu_{BA} = \\nu_0\n\\]\nSubstitute: \\[\nP_A \\nu_0 e^{-\\beta (E_B - E_A)} = P_B \\nu_0\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A e^{-\\beta (E_B - E_A)} = P_B\n\\]\nAgain using canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} e^{-\\beta (E_B - E_A)} = \\frac{1}{Z} e^{-\\beta E_B} = P_B\n\\]\nVerified.\nHence, in both cases the detailed balance condition is satisfied with the canonical distribution, and the steady state is indeed the canonical distribution.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "title": "9  The Langevin Approach",
    "section": "",
    "text": "9.1 The Random Walk and the Langevin equation\nThe concept of a random walk and its continuum limit – diffusion – introduced in the previous chapter, expresses the time evolution of the probability distribution \\(p(x, t)\\) for a particle’s position \\(x\\) by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2},\n\\]\nwhich is a standard example of a so called Fokker-Planck equation, which is second-order in space and first-order in time.\nIn contrast, the Langevin equation provides a stochastic differential equation for the particle’s trajectory \\(x(t)\\). To understand it, consider the random hopping motion of a particle on a 1d lattice over a small time increment \\(\\Delta t\\):\n\\[\nx(t + \\Delta t) = x(t) + \\Delta x(t)\n\\]\nHere, \\(\\Delta x(t)\\) is a random displacement. If the lattice spacing is \\(a\\), we define the step statistics as:\n\\[\n\\Delta x(t) =\n\\begin{cases}\n+a & \\text{with probability } \\nu \\Delta t \\\\\n-a & \\text{with probability } \\nu \\Delta t \\\\\n0 & \\text{with probability } 1 - 2\\nu \\Delta t\n\\end{cases}\n\\]\nThis defines a discrete-time, discrete-space random walk. The average and variance of the step are easily derived (try as an exercise):\nThe steps \\(\\Delta x(t)\\) are uncorrelated across time.\nTo take the continuum limit, we let both \\(a \\to 0\\) and \\(\\Delta t \\to 0\\), while keeping the diffusion constant \\[\nD = \\nu a^2\n\\] fixed. This requirement implies that we need \\(\\nu\\propto a^{-2}\\), which is satisfied if \\[\na \\propto \\sqrt{\\Delta t}.\n\\]\nHence the step size must shrink as the square root of the time step. Only under this scaling does the random walk converge to a well-defined continuum process with finite diffusion constant, namely Brownian motion or the Langevin equation.\nIn this limit, we obtain the Langevin equation:\n\\[\n\\dot{x}(t) = \\eta(t)\n\\]\nwhere \\(\\eta(t)\\equiv\\frac{\\Delta x(t)}{\\Delta t}\\) is a stochastic noise satisfying:\n\\[\n\\langle \\eta(t) \\rangle = 0\n\\]\n\\[\n\\langle \\eta(t) \\eta(t') \\rangle = \\Gamma \\delta(t - t')\n\\]\nThis \\(\\eta(t)\\) is known as white noise — it has zero mean and is uncorrelated at different times. \\(\\Gamma\\) measures its amplitude.\nThe Langevin equation tells us that the velocity \\(\\dot{x}(t)\\) is purely driven by noise. We can formally integrate it:\n\\[\nx(t) - x_0 = \\int_0^t \\eta(t')\\, dt'\n\\]\nTaking ensemble averages:\nComparing this with the diffusion equation result, we identify:\n\\[\n\\Gamma = 2D\n\\]\nHence, the Langevin description yields the same physical behavior — not just the mean-square displacement but also the full probability distribution \\(p(x, t)\\) — as the diffusion (Fokker-Planck) equation. This equivalence arises from the fact that the integral of many small, independent random steps leads to a Gaussian distribution, in agreement with the solution of the diffusion equation.\nFor more details, see: Stochastic Processes in Physics and Chemistry by N.G. van Kampen (North Holland, 1981).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "title": "9  The Langevin Approach",
    "section": "",
    "text": "Mean: \\(\\langle \\Delta x \\rangle = 0\\)\nVariance: \\(\\langle (\\Delta x)^2 \\rangle = 2 a^2 \\nu \\Delta t = 2D \\Delta t\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe mean displacement is \\[\n\\langle \\Delta x \\rangle = (+a)(\\nu \\Delta t) + (-a)(\\nu \\Delta t) + 0(1 - 2\\nu \\Delta t) = 0.\n\\]\nThe mean square displacement is \\[\n\\langle (\\Delta x)^2 \\rangle = (+a)^2(\\nu \\Delta t) + (-a)^2(\\nu \\Delta t) + 0^2(1 - 2\\nu \\Delta t)\n= 2a^2\\nu \\Delta t.\n\\]\nHence the variance is \\[\n\\mathrm{Var}(\\Delta x) = \\langle (\\Delta x)^2 \\rangle - \\langle \\Delta x \\rangle^2 = 2a^2\\nu \\Delta t.\n\\]\nIdentifying \\(D = a^2\\nu\\), we obtain \\[\n\\langle (\\Delta x)^2 \\rangle = 2D\\Delta t.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean displacement: \\[\n\\langle x(t) - x_0 \\rangle = 0\n\\]\nMean square displacement: \\[\n\\langle [x(t) - x_0]^2 \\rangle = \\int_0^t \\int_0^t \\langle \\eta(t') \\eta(t'') \\rangle\\, dt'\\, dt'' = \\Gamma \\int_0^t dt' = \\Gamma t\n\\]",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "title": "9  The Langevin Approach",
    "section": "9.2 Brownian Motion",
    "text": "9.2 Brownian Motion\nLet us now examine Brownian motion, originally observed as the erratic motion of colloidal particles suspended in a fluid. These particles undergo constant collisions with surrounding (smaller) fluid molecules, which results in seemingly random movement.\nFrom a coarse-grained perspective — where we do not track each individual collision — this appears as motion under random forces. This statistical treatment introduces irreversibility at the macroscopic level, even though the underlying molecular dynamics are reversible.\nThe Langevin equation provides a way to model this behavior. For a particle of mass \\(m\\) in one dimension, Langevin proposed the equation:\n\\[\nm \\ddot{x} = -\\gamma \\dot{x} + f(t)\n\\]\nHere:\n\n\\(-\\gamma \\dot{x}\\) is a frictional damping force, where \\(\\gamma\\) is the damping coefficient.\n\\(f(t)\\) is a random force due to molecular collisions.\n\n\nOften, the mobility is defined as \\(\\mu = 1/\\gamma\\) — note that this is unrelated to chemical potential.\n\n\n9.2.1 Noise Properties\nIn principle the random forces are correlated in time since the molecular collisions which cause them are correlated and have some definite duration.\nLet us assume that there is some correlation time \\(t_c\\) over which \\(\\langle f(t_1) f(t_2) \\rangle = g(t_1 - t_2)\\) decays rapidly as shown in the sketch below:\n\n\n\n\n\n\nFigure 9.1: Sketch of \\(g(t1−t2)\\) against \\(|t1− t2|\\)\n\n\n\nThen as long as we consider timescales \\(\\gg t_c\\) we can safely replace \\(g(t_1 - t_2)\\) by a delta function. Thus we can make the approximation of white noise\n\\[\n\\langle f(t) \\rangle = 0\n\\]\n\\[\n\\langle f(t_1) f(t_2) \\rangle = \\Gamma \\delta(t_1 - t_2)\n\\]\n\n\n9.2.2 Solving the Langevin Equation (velocity)\nLet’s set \\(m = 1\\) for simplicity and solve the equation:\n\\[\n\\dot{v} + \\gamma v = f(t)\n\\]\nWe apply an integrating factor to get an exact differential on the LHS:\n\\[\n\\frac{d}{dt} \\left[ v e^{\\gamma t} \\right] = e^{\\gamma t} f(t)\n\\]\nIntegrating both sides:\n\\[\n\\int_0^t \\frac{d}{dt'}\\!\\left( v e^{\\gamma t'} \\right) dt' = \\int_0^t e^{\\gamma t'} f(t')\\, dt'\n\\]\n\\[\nv(t)e^{\\gamma t} - v_0 = \\int_0^t e^{\\gamma t'} f(t')\\, dt'\n\\]\n\\[\nv(t) = v_0 e^{-\\gamma t} + \\int_0^t e^{-\\gamma (t - t')} f(t')\\, dt'\n\\]\nTaking the average:\n\\[\n\\langle v(t) \\rangle = v_0 e^{-\\gamma t}\n\\]\nwhere we have used the fact that \\(f(t)\\) is random with \\(\\langle f(t)\\rangle=0\\). Thus:\n\nAt short times: (\\(\\gamma t \\ll 1\\)): \\(\\langle v \\rangle \\approx v_0\\) ie. friction is negligible.\nAt long times: (\\(\\gamma t \\gg 1\\)): \\(\\langle v \\rangle \\to 0\\) ie. the system loses memory of the initial velocity.\n\n\n\n9.2.3 Mean-square velocity\nWe now compute (do it as an exercise):\n\\[\n\\langle v(t)^2 \\rangle = v_0^2 e^{-2\\gamma t} + \\Gamma \\int_0^t e^{-2\\gamma (t - t')} dt' = v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma} \\left(1 - e^{-2\\gamma t} \\right)\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{From the solution for }v(t)\\text{ with }m=1:\\\\\nv(t)&=v_0 e^{-\\gamma t}+\\int_0^t e^{-\\gamma (t-t')}f(t')\\,dt'.\\\\\n\\Rightarrow\\; v(t)^2 &= v_0^2 e^{-2\\gamma t}\n+ 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} f(t')\\,dt'\n+ \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')} f(t')f(t'')\\,dt'\\,dt''.\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\Rightarrow\\; \\langle v(t)^2\\rangle &= v_0^2 e^{-2\\gamma t} + 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} \\underbrace{\\langle f(t')\\rangle}_{=\\,0}\\,dt' + \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')}\n\\underbrace{\\langle f(t')f(t'')\\rangle}_{=\\,\\Gamma\\,\\delta(t'-t'')}\\,dt'\\,dt''\\\\\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma \\int_0^t e^{-2\\gamma (t-t')}\\,dt'\\\\\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma\\left[\\,-\\frac{1}{2\\gamma}e^{-2\\gamma (t-t')}\\,\\right]_{t'=0}^{t'=t}\\\\\n&= v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma}\\bigl(1-e^{-2\\gamma t}\\bigr).\n\\end{aligned}\n\\]\n\\[\n\\boxed{\\;\\langle v(t)^2\\rangle\n= v_0^2 e^{-2\\gamma t} + \\dfrac{\\Gamma}{2\\gamma}\\left(1-e^{-2\\gamma t}\\right)\\;}\n\\]\n\n\n\nImplying that at\n\nShort times: \\(\\langle v^2 \\rangle \\approx v_0^2\\)\nLong times: \\(\\langle v^2 \\rangle \\to \\Gamma / (2\\gamma)\\)\n\nAt equilibrium, the equipartition theorem gives:\n\\[\n\\frac{1}{2} m \\langle v^2 \\rangle = \\frac{1}{2} k_B T\n\\]\nUsing this to identify \\(\\Gamma\\):\n\\[\n\\Gamma = 2 \\gamma k_B T\n\\]\nThis important result relates the noise strength to the damping and temperature — they have the same microscopic origin (molecular collisions).\n\n\n9.2.4 Mean-square displacement\nWe now integrate \\(v(t)\\) again to get position \\(x(t)\\) (with \\(m = 1\\)):\nUsing the result above and substituting \\(\\Gamma = 2\\gamma k_B T\\), we find:\n\\[\n\\langle [x(t) - x_0]^2 \\rangle = \\frac{(v_0^2 - k_B T)}{\\gamma^2} (1 - e^{-\\gamma t})^2 + \\frac{2 k_B T}{\\gamma} \\left[ t - \\frac{1 - e^{-\\gamma t}}{\\gamma} \\right]\n\\]\nLimiting behaviours:\n\nShort times: (\\(\\gamma t \\ll 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx v_0^2 t^2\n\\]\n(correspinding to ballistic motion)\nLong time (\\(\\gamma t \\gg 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx \\frac{2 k_B T}{\\gamma} t\n\\]\n(corresponding to diffusive motion)\n\n\nThe effective diffusion constant is:\n\\[\nD = \\frac{k_B T}{\\gamma}\n\\]\nThis is the Einstein relation, connecting the rate of diffusion to temperature and damping. It is useful as it allows an explicit expression for the diffusion constant if one knows \\(\\gamma\\). A famous example is a sphere: the equation for fluid flow past a moving sphere may be solved and yields \\(\\gamma=6\\pi\\eta a\\) where \\(a\\) is the radius of the sphere and here \\(\\eta\\) is the fluid viscosity. This gives\n\\[\nD=\\frac{6\\pi\\eta a}{kT}\n\\] which is the Stokes-Einstein formula for the diffusion constant of a colloidal particle.\n\n\n9.2.5 External Forces and Mobility\nNow consider a charged particle with charge \\(q\\) under an external electric field \\(E\\). The Langevin equation becomes:\n\\[\nm \\dot{v} = -\\gamma v + qE\n\\]\nAt long times, the particle reaches a steady drift velocity:\n\\[\n\\langle v \\rangle = \\frac{qE}{\\gamma} = \\frac{qED}{k_B T}\n\\]\nDefining the mobility \\(\\mu\\) by \\(\\langle v \\rangle = \\mu qE\\), we get the Nernst-Einstein relation:\n\\[\n\\mu = \\frac{D}{k_B T}\n\\]\nThis relation connects the response of a system to an external perturbation (mobility) with its internal fluctuations (diffusivity).\n\n\n9.2.6 Molecular Dynamics simulation of Brownian motion for a colloid particle in a liquid suspension\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom numba import njit\n\n# Parameters\nn_fluid = 300\nbox_size = 20.0\nn_steps = 10000\nsigma_f = 1.0\nsigma_c = 10.0\nepsilon = 0.05\nmass_f = 1.0\nmass_c = 2.0\ndt = 1e-5\ndt_e = 1e-5\n\n# Echo the parameter values\nprint(\"Simulation Parameters:\")\nprint(f\"n_fluid = {n_fluid}\")\nprint(f\"box_size = {box_size}\")\nprint(f\"n_steps = {n_steps}\")\nprint(f\"sigma_f = {sigma_f}\")\nprint(f\"sigma_c = {sigma_c}\")\nprint(f\"epsilon = {epsilon}\")\nprint(f\"mass_f = {mass_f}\")\nprint(f\"mass_c = {mass_c}\")\nprint(f\"dt = {dt}\")\n \n # Derived quantities\n\nsigma_f6 = sigma_f ** 6\nsigma_f12 = sigma_f **12\nsigma_cf = 0.5 * (sigma_f + sigma_c)\nsigma_cf6 = sigma_cf ** 6\nsigma_cf12 = sigma_cf ** 12\n\nnp.random.seed(42)\n\n# Safe initialization to avoid overlaps with colloid and other fluid particles\ndef initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos, min_dist_factor=0.85):\n    min_dist_ff = min_dist_factor * sigma_f\n    min_dist_cf = min_dist_factor * 0.5 * (sigma_f + sigma_c)\n    positions = []\n    max_attempts = 20000\n\n    for _ in range(n_fluid):\n        for attempt in range(max_attempts):\n            trial = np.random.rand(2) * box_size\n            too_close = False\n\n            # Check distance to colloid center\n            if np.linalg.norm(trial - colloid_pos[0]) &lt; min_dist_cf:\n                too_close = True\n\n            # Check distances to already placed fluid particles\n            for existing in positions:\n                if np.linalg.norm(trial - existing) &lt; min_dist_ff:\n                    too_close = True\n                    break\n\n            if not too_close:\n                positions.append(trial)\n                break\n        else:\n            raise RuntimeError(\"Failed to place a fluid particle without overlap after many attempts.\")\n    \n    return np.array(positions)\n\ncolloid_pos = np.array([[box_size / 2, box_size / 2]])\nfluid_pos = initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos)\nfluid_vel = (np.random.rand(n_fluid, 2) - 0.5)\ncolloid_vel = np.zeros((1, 2))\n\n@njit\ndef compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid):\n    forces_f = np.zeros_like(fluid_pos)\n    force_c = np.zeros_like(colloid_pos)\n\n    for i in range(n_fluid):\n        # Fluid-colloid interaction\n        rij = fluid_pos[i] - colloid_pos[0]\n        rij -= box_size * np.round(rij / box_size)\n        dist2 = np.dot(rij, rij)\n        if dist2 &lt; (2.5 ** 2) * ((sigma_cf6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n            r2 = dist2\n            r6 = r2 ** 3\n            r12 = r6 ** 2\n            fmag = 48 * epsilon * ((sigma_cf12 / r12) - 0.5 * (sigma_cf6 / r6)) / r2\n            fvec = fmag * rij\n            forces_f[i] += fvec\n            force_c[0] -= fvec\n\n        for j in range(i + 1, n_fluid):\n            rij = fluid_pos[i] - fluid_pos[j]\n            rij -= box_size * np.round(rij / box_size)\n            dist2 = np.dot(rij, rij)\n            if dist2 &lt; (2.5 ** 2) * ((sigma_f6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n                r2 = dist2\n                r6 = r2 ** 3\n                r12 = r6 ** 2\n                fmag = 48 * epsilon * ((sigma_f12 / r12) - 0.5 * (sigma_f6 / r6)) / r2\n                fvec = fmag * rij\n                forces_f[i] += fvec\n                forces_f[j] -= fvec\n\n    return forces_f, force_c\n\n    fluid_history = []\ncolloid_history = []\nforces_f, force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n\nn_equilibration = 2000  # Number of steps to equilibrate before tracking\n\n# Equilibration phase (no history recorded)\n\nfor step in range(n_equilibration):\n    fluid_pos += fluid_vel * dt_e + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt_e + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt_e\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt_e\n    forces_f = new_forces_f\n    force_c = new_force_c\n    # if step &lt; 10:  # Log only the first few steps\n    #     force_mag = np.linalg.norm(force_c[0])\n    #     print(f\"Step {step:3d} | Colloid Pos: {colloid_pos[0]} | Vel: {colloid_vel[0]} | |F|: {force_mag:.4e}\")\n\n# Production phase (history recorded)\n\nfor step in range(n_steps):\n    fluid_pos += fluid_vel * dt + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt\n    forces_f = new_forces_f\n    force_c = new_force_c\n    if step % 10 == 0:\n        fluid_history.append(fluid_pos.copy())\n        colloid_history.append(colloid_pos.copy())\n\nfig, ax = plt.subplots()\n# Calculate figure and plot scale parameters\nfig_width_inch = fig.get_size_inches()[0]\ndpi = fig.dpi\naxis_length_pt = fig_width_inch * dpi\nmarker_scale = 0.1  # Scale factor for visibility\nfluid_marker_size = (marker_scale * axis_length_pt / box_size) ** 2\ncolloid_marker_size = 0.7*(marker_scale * sigma_c / sigma_f * axis_length_pt / box_size) ** 2\nfluid_scatter = ax.scatter([], [], s=fluid_marker_size, c='blue')\ncolloid_scatter = ax.scatter([], [], s=colloid_marker_size, c='red')\ntrajectory, = ax.plot([], [], 'r--', linewidth=1, alpha=0.5)\nax.set_xlim(0, box_size)\nax.set_ylim(0, box_size)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_aspect('equal')\ncolloid_traj = []\n\ndef init():\n    empty_offsets = np.empty((0, 2))\n    fluid_scatter.set_offsets(empty_offsets)\n    colloid_scatter.set_offsets(empty_offsets)\n    trajectory.set_data([], [])\n    return fluid_scatter, colloid_scatter, trajectory\n\ndef update(frame):\n    fluid_scatter.set_offsets(fluid_history[frame])\n    colloid_scatter.set_offsets(colloid_history[frame])\n    colloid_traj.append(colloid_history[frame][0])\n    traj_array = np.array(colloid_traj)\n    trajectory.set_data(traj_array[:, 0], traj_array[:, 1])\n    return fluid_scatter, colloid_scatter, trajectory\n\nani = animation.FuncAnimation(fig, update, frames=len(fluid_history), init_func=init, blit=True, interval=20)\nani.save(\"brownian_colloid.mp4\", writer=\"ffmpeg\", fps=30)\nprint(\"Simulation complete. Video saved as 'brownian_colloid.mp4'.\")\n\n\nMore about the scientists mentioned in this chapter:\nPaul Langevin\nRobert Brown\nAlbert Einstein\nWalther Nernst\nGeorge Stokes",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html",
    "href": "phase-transitions/precourse-reading.html",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Ensembles and free energies\nComplex disordered systems are composed of an enormous number of interacting components—typically on the order of \\(\\sim 10^{23}\\). These interactions can lead to fascinating emergent behaviour, but they also render the systems analytically intractable; it is clearly impossible to solve Newton’s equations for such vast numbers of particles. To address this difficulty, we turn to Statistical Mechanics, which you first encountered in your second year. Statistical Mechanics provides the essential framework for connecting the microscopic behaviour of individual constituents with the macroscopic thermodynamic and dynamical properties of the system as a whole.\nIn this section, we will revisit and expand upon key concepts relevant to our discussion, with particular emphasis on the free energy—a central quantity that captures the balance between energy minimisation and entropy maximisation in determining the system’s equilibrium state. If any of these ideas feel unfamiliar, you may find it useful to revise the Statistical Mechanics material from your Year 2 Thermal Physics course notes.\nStatistical mechanics can be formulated in a variety of ensembles reflecting the relationship between the system and its environment. In what follows we summarise the formalism, focussing on the case of a particle fluid. Analogous equations apply to lattice spin models (see lectures and the book by Yeomans). Key ensembles are:",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "href": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Microcanonical ensemble\nApplies to a system of \\(N\\) particles (or spins) in a fixed volume \\(V\\) having adiabatic walls so that the internal energy \\(E\\) is constant. Denoted as constant-\\(NVE\\). Let \\(\\Omega\\) be the number of (micro)states having the prescribed energy:\n\\[\n\\Omega=\\sum_\\textrm{all states having energy E}\n\\]\nThermodynamically, the states favored in the canonical ensemble are those that maximise the entropy:\n\\[\nS=k_B\\ln \\Omega\\: .\n\\]\nwhere \\(k_B\\) is Boltzmann’s constant The microcanonical ensemble is useful for defining the entropy, but is little used in practice.\n\n\nCanonical ensemble\nApplies to a system of \\(N\\) particles in a fixed volume \\(V\\) and coupled to a heat bath at temperature \\(T\\). Denoted as constant-\\(NVT\\). A central quantity is the partition function\n\\[\nZ_{NVT}=\\sum_\\textrm{ all states i}e^{-\\beta E_i},~~~~~\\beta=1/(k_BT)\n\\tag{1}\\] which is a weighted sum over the states. The partition function provides the normalisation constant in the probability of finding the system in a given state \\(i\\).\n\\[\nP_i=\\frac{e^{-\\beta E_i}}{Z_{NVT}}.\n\\tag{2}\\]\nThe states favored in the canonical ensemble are those that minimise the free energy:\n\\[\nF_{NVT}=-\\beta^{-1}\\ln Z_{NVT}\\:.\n\\]\n\\(F_{NVT}\\) is known as the Helmholtz free energy. Thermodynamics also supplies a relation for the Helmholtz free energy:\n\\[\nF_{NVT}=E-TS\\:,\n\\] where \\(E\\) is the average internal energy. In minimising the free energy, the system strikes a compromise between low energy and high entropy. The temperature plays the role of arbiter, favouring high entropy at high \\(T\\), and low energy at low \\(T\\). The canonical ensemble is usually used to describe systems such as magnets, or a fluid held at constant volume. It is the ensemble we shall use most in this course.\n\n\nGrand canonical ensemble\nApplies to a system with a variable number of particle in a fixed volume \\(V\\) coupled to both a heat bath at temperature \\(T\\) and a particle reservoir with chemical potential \\(\\mu\\) (which is the field conjugate to \\(N\\)). Denoted as constant-\\(\\mu VT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty e^{\\beta\\mu N}Z_{NVT}\n\\] and a state probability analogous to Equation 2 holds. One can recast this in a form similar to Equation 1:\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty\\:\\sum_\\textrm{all~states~i}e^{-\\beta {\\cal H}_i},\n\\tag{3}\\] where \\({\\cal H}_i=E_i-\\mu N\\) is the form of the Hamiltonian in the grand canonical ensemble.\nStatistically, the states favored in the grand canonical ensemble are those that minimise the free energy:\n\\[\nF_{\\mu VT}=-\\beta^{-1}\\ln Z_{\\mu VT}\n\\] \\(F_{\\mu VT}\\) is known as the grand potential. It can also be derived from thermodynamics, from which one finds\n\\[\nF_{\\mu VT}=E-TS-\\mu N=-pV,\n\\] where \\(p\\) is the pressure.\nThe grand canonical ensemble is usually used to describe systems such as fluid connected to a particle reservoir. Sometimes for a magnet we consider the effects of an applied magnetic field, which is analogous to working in the grand canonical ensemble: the magnetic field (which is conjugate to the magnetisation) plays a similar role to the chemical potential in a fluid.\n\n\nIsothermal-isobaric ensemble\nApplies to a system with a fixed number of particles \\(N\\) that is coupled to a heat bath at temperature \\(T\\) and a reservoir that exerts a constant pressure \\(p\\) which allows the sample volume to fluctuate. Denoted as constant-\\(NpT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}\n\\] or \\[\nZ_{NpT}=\\int_0^\\infty dV\\:\\sum_\\textrm{i}e^{-\\beta {\\cal H}_i},\n\\tag{4}\\] where \\({\\cal H}_i=E_i+pV\\) is the form of the Hamiltonian in the constant-\\(NpT\\) ensemble. Again a state probability analogous to Equation 2 holds.\nStatistically, the states favored in the constant-\\(NpT\\) ensemble are those that minimise the free energy:\n\\[\nF_{NpT}=-\\beta^{-1}\\ln Z_{NpT}\n\\] \\(F_{NpT}\\) is known as the Gibb’s free energy (often denoted \\(G\\)). It can also be derived from thermodynamics, from which one finds\n\\[\nF_{NpT}=E-TS+pV=\\mu N\n\\]\nThe constant-\\(NpT\\) ensemble is usually used to describe systems such as a fluid subject to a variable pressure, or a magnet coupled to a magnetic field \\(H\\). In the latter case the quantity \\(HM\\) plays the role of \\(pV\\) and\n\\[\nF_{NpT}=E-TS-MH\\:,\n\\] with \\(M\\) the total magnetisation.",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "href": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "title": "Tools for understanding complex disordered matter",
    "section": "From free energies to observables",
    "text": "From free energies to observables\nFree energies are not directly observable quantities. However, all physical observables can be expressed in terms of derivatives of the free energy. One can derive the appropriate relations either from Thermodynamics, or the corresponding statistical mechanics (Revise your year-2 Thermal Physics notes on this if necessary). As an example let us consider a fluid in the isothermal-isobaric ensemble for which the appropriate free energy is \\(F_{NpT}=E-TS+pV\\), and where the volume fluctuates in response to the prescribed pressure. We shall seek an expression for the average volume in terms of the free energy. First lets us take the thermodynamic route. Differentiating the free energy and applying the chain rule we have:\n\\[\ndF=dE-TdS-sdT+pdV+VdP\\:.\n\\] But from the first law of thermodynamics, \\(dE=TdS-pdV\\), so\n\\[\ndF=-SdT+Vdp\\:,\n\\] and rearranging yields \\[\nV=\\left(\\frac{\\partial F}{\\partial p}\\right)_T\\:.\n\\]\nWe can now show that this result is consistent with the definition of \\(F_{NpT}\\) in terms of the partition function. Write\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}=\\int_0^\\infty dV\\sum_{all~states~i}e^{-\\beta (p V_i+E_i)}\n\\]\nThen\n\\[\\begin{align}\n\\left(\\frac{\\partial F}{\\partial p}\\right)_T\n&= -\\frac{1}{\\beta} \\left(\\frac{\\partial \\ln Z_{NpT}}{\\partial p}\\right)_T \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\frac{\\partial Z_{NpT}}{\\partial p} \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\int_0^\\infty dV \\int_{\\text{all states}} (-\\beta V) e^{-\\beta (p V + E)} \\\\\n&= \\langle V \\rangle_T \\,.\n\\end{align}\\]\nwhere in the last step we have used the fact that the probability of a state is defined to be \\(e^{-\\beta (p V_i+E_i)}/Z_{NpT}\\).\nExercise. Repeat these manipulations to find an expression for the mean particle number \\(N\\) in the grand canonical ensemble\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn the grand canonical ensemble (GCE), the relevant free energy is\n\\[\nF_{\\mu VT} = E - TS - \\mu N\n\\]\nFrom the first law of thermodynamics changes in the internal energy are given by:\n\\[\ndE = TdS - PdV + \\mu dN=TdS+\\mu dN\n\\] where we have used the fact that \\(V\\) is fixed in the GCE, so \\(dV=0\\).\nDifferentiating \\(F_{\\mu VT}\\):\n\\[\ndF_{\\mu VT} = dE - TdS - SdT - \\mu dN - N d\\mu\n= -S dT - N d\\mu\n\\] where for the last equality we have substitued for \\(dE\\) from above.\nThus \\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V} = -N\n\\quad \\Rightarrow \\quad\n\\langle N \\rangle = -\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V}\n\\]\nNow consider the statistical mechanics route to calculate \\(\\langle N\\rangle\\):\n\\[\nZ_{\\mu V T} = \\sum_{N=0}^{\\infty} \\sum_{\\text{states}} e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nThe grand potential (now written as \\(F_{\\mu VT}\\)) is:\n\\[\nF_{\\mu VT} = -k_B T \\ln Z_{\\mu V T}\n\\]\nWe now differentiate:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\left( \\frac{1}{Z} \\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} \\right)\n\\]\nFrom the partition function\n\\[\n\\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} = \\sum_{N=0}^\\infty \\sum_{\\text{states}} \\left( \\beta N \\right) e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nSubstitute:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\cdot \\beta \\cdot \\frac{1}{Z_{\\mu V T}} \\sum_{N=0}^\\infty \\sum_{\\text{states}} N\\, e^{-\\beta (E_{N,i} - \\mu N)} = - \\langle N \\rangle\n\\]\nwhere in the last step we have used the fact that in the GCE the Boltzmann probability of a microstate is defined to be \\(e^{-\\beta (E_{N,i}-\\mu N)}/Z_{\\mu VT}\\).",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html",
    "href": "phase-transitions/problems.html",
    "title": "Unifying concepts: Problems",
    "section": "",
    "text": "Although you should try all of these questions, some of them are deliberately quite challenging. If you don’t get very far with some, don’t worry. We’ll be going over them in problems classes, so you can just regard them as worked examples.\n\n1. Existence of a phase transition in \\(d=2\\).\nIn lectures it was argued that no long ranged order occurs at finite-temperatures in a one dimensional system because of the presence of domain walls. Were macroscopic domain walls to exist in two dimensions at finite temperature, they would similarly destroy long ranged order and prevent a phase transition. By calculating the free energy of a 2D domain wall for an Ising lattice, show that domain walls do not in fact exist for sufficiently low \\(T\\).\n(Hint: Model the domain wall as a non-reversing \\(N\\)-step random walk on the lattice and find an expression for its energy and -from the number of random walk configurations- its entropy.)\n\n\n\n2. Correlation Length\nFor a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small ((as occurs at low \\(T\\))?\n\n\n\n3. A model fluid\nThe van der Waals (vdW) equation of state is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nWrite a Python script to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of \\(P\\) vs \\(V\\) on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\n4. Mean field theory of the Ising model heat capacity\nUsing results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle E\\rangle/\\partial T\\) has the behaviour\n\\[\n\\begin{aligned}\nC_H=& 0 \\quad T&gt;T_c\\\\\nC_H=& 3Nk_B/2 \\quad T\\le T_c\n\\end{aligned}\n\\] —\n\n\n5. Magnetisation and fluctuations\nA system of spins on a lattice in the presence of an applied field \\(h\\), has a Hamiltonian \\[\n{\\cal H}=E - hM\n\\] where \\(E\\) is the spin-spin interaction energy, \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\n\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\n\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\n6. Spin-1 Ising model\nA set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j - h\\sum_i s_i\\]\nFind the partition function in the mean field approximation and hence show that in the same approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\n7. Transfer Matrix.\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\n8. Landau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\n9. Scaling equation of state\nConsider a Landau expression for the free energy of a magnetic system having magnetisation \\(m\\):\n\\[\nF=F_0+\\tilde{a}_2tm^2+a_4m^4-Hm\\:,\n\\] where \\(t=T-T_c\\) and \\(H\\) is an applied magnetic field; \\(\\tilde{a}_2\\) and \\(a_4\\) are positive constants and \\(F_0\\) is a constant background term.\nShow that the equation of state for the model is\n\\[\nH=2\\tilde{a}_2tm+4a_4m^3\\:.\n\\]\nUse the near-critical power law behaviour of \\(m\\) to show that the equation of state may be written in the scaling form\n\\[\n\\frac{H}{m^\\delta}=g\\left(\\frac{t}{m^{1/\\beta}}\\right)\\:,\n\\] and find the (mean field) values of the critical exponents \\(\\delta\\) and \\(\\beta\\).\nDeduce that \\(g(x)=x+1\\) up to a choice of scale for \\(\\tilde{a}_2\\) and \\(a_4\\).\n\n\n\n10. Scaling laws\nUsing the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\n\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\n\\]\nHence derive the scaling laws among the critical exponents:\n\\[\n\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=& 2 \\\\\n\\alpha+2\\beta+\\gamma =& 2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\n\\] (Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 8: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\n\n\n\n11. Classical nucleation theory\nA supercooled liquid metal is undergoing solidification. According to classical nucleation theory, the Gibbs free energy change \\(\\Delta G\\) for forming a spherical solid nucleus of radius \\(r\\) in the liquid is given by:\n\\[\n\\Delta G(r) = \\frac{4}{3}\\pi r^3 \\Delta G_v + 4\\pi r^2 \\gamma\n\\] where \\(\\Delta G_v &lt; 0\\) is the free energy change per unit volume due to the phase change, and \\(\\gamma &gt; 0\\) is the interfacial energy between the solid and liquid phases.\n(a) Derive the expression for the critical radius \\(r^*\\) at which the nucleus becomes stable and begins to grow.\n(b) Show that the critical energy barrier for nucleation \\(\\Delta G^*\\) is given by:\n\\[\n\\Delta G^* = \\frac{16\\pi \\gamma^3}{3 (\\Delta G_v)^2}\n\\]\n(c) Explain qualitatively how the degree of undercooling \\(\\Delta T\\) affects the rate of nucleation. You may use the fact that \\(\\Delta G_v \\propto \\Delta T\\) to support your answer.\n\n\n\n12. Colloidal diffusion\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + \\frac{e^{-\\gamma t}}{M} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{kT}{M} + \\left( u_0^2 - \\frac{kT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\n\n13. Einstein’s expression for the diffusion coefficient\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle [X(t) - X(0)]^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nUse the fact that \\(X(t) - X(0)\n= \\int_{0}^{t} u(t')\\,\\mathrm{d}t'\\) to show that the Einstein relation may be written as\n\\[\n\\gamma = \\frac{1}{\\mu} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\n\n14. Master equation\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\n\n15. Detailed balance\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\n\n16. Jump processes\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump from state 1 to 2 and 2 to 3 but not directly from 1 to 3. Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the rate matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_partial.html",
    "href": "phase-transitions/Solutions_partial.html",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "",
    "text": "Here we present outline solutions to the problems.\n\n1. Existence of a phase transition in \\(d=2\\).\nConsider the simplest elementary excitation that will destroy long range order in the 2d system: a domain wall of \\(N\\) segments which divides an Ising system of \\(L\\times L\\) spins into a spin up and a spin down part.\n\n\n\n\n\n\nFigure 1: An \\(N\\)-step domain wall in an Ising lattice.\n\n\n\nThe associated energy cost is \\(2JN \\equiv \\Delta E\\).\nTo evaluate the entropy gain due to a domain wall in the system we have to estimate \\(\\Omega\\) the number of possible paths for the domain wall. If we start at the left hand side then there are \\(L\\) starting positions. At each step the domain wall can move to the right, move up or move down. This implies that the number of domain walls is approximately\n\\[\n\\Omega\\approx L3^N\n\\] Hence the entropy gain is:\n\\[\n\\Delta S=Nk_B\\ln 3+k_B\\ln L\\approx Nk_B\\ln 3  \n\\]\nAccordingly, the change in the free energy associated with inserting such a domain wall into an ordered system is\n\\[\n\\Delta F=\\Delta E-T\\Delta S= N(2J-k_BT\\ln 3)\n\\]\nFor small enough \\(T &lt;2J/(k_B\\ln 3)\\), the free energy change is positive. Thus the ordered phase is free energetically stable against formation of a wall. Accordingly there will be a non zero value for \\(T_c\\) in two dimensions.\n\n\n\n2. Correlation Length\nDenote by \\(m\\) the number of domain walls between sites \\(i\\) and \\(j\\). Then \\(s_is_j=1\\) for \\(m\\) even, and \\(s_is_j=-1\\) for \\(m\\) odd.\nHence\n\\[\n\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\n\\] with \\(p_m\\) the probability of finding \\(m\\) domain walls between them.\nNow \\(p_m\\) is given by the binomial distribution, with the probability of a single domain wall at each bond given by\n\\[\np=\\frac{e^{-2J/k_BT}}{1+e^{-2J/k_BT}}\n\\]\nand the probability of no wall is \\(1-p\\). Now, in the regime where \\(T\\) is small, \\(p\\) is very small, and there will be few domain walls between sites \\(i\\) and \\(j\\). If additionally, \\(R_{ij}=|i-j|a\\) is large, it tranpires that the binomial distribution assumes the limiting form of a Poissonian distribution (revise this if necessary). Thus\n\\[\np_m=\\frac{\\overline{m}^me^{-\\overline{m}}}{m!}\n\\]\nwhere \\(\\overline{m}=p|j-i|=pR_{ij}/a\\) . Then\n\\[\n\\begin{aligned}\n\\langle s_i s_j\\rangle =& e^{-\\overline {m}}\\sum_m\\frac{(-1)^m\\overline{m}^m} {m!}\\approx e^{-2\\overline{m}}\\\\\n                       =& e^{-2pR_{ij}/a}\\\\\n                       =& e^{-R_{ij}/\\xi}\n\\end{aligned}\n\\] with \\(\\xi=a/2p\\), the correlation length.\n\n\n\n3. A model fluid\nThe van der Waals (vdW) equation of state (See Sec 4.4.1 of the book by Yeomans) is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\n\\left(P+\\frac{a}{V^2}\\right)(V-b)=Nk_BT\n\\] where \\(a\\) and \\(b\\) are constants chosen to describe a specific substance and \\(N\\) is Avogadro’s number. Hence\n\\[\nP=\\frac{Nk_BT}{V-b}-\\frac{a}{V^2}\n\\tag{1}\\]\n\\[\n\\Rightarrow \\frac{\\partial P}{\\partial V}=\\frac{-Nk_BT}{(V-b)^2}+\\frac{2a}{V^3}\n\\]\n\\[\n\\Rightarrow \\frac{\\partial^2 P}{\\partial V^2}=\\frac{2Nk_BT}{(V-b)^3}-\\frac{6a}{V^4}\n\\]\nNow at criticality (ie. a continuous transition).\n\\[\n\\left(\\frac{\\partial P}{\\partial V}\\right)_T=\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\n\\]\nThus \\[\n\\begin{aligned}\n\\frac{Nk_BT}{(V_c-b)^2} =& \\frac{2a}{V_c^3}\\\\\n\\frac{2Nk_BT}{(V_c-b)^3} =& \\frac{6a}{V_c^4}\n\\end{aligned}\n\\] solving for \\(V_c\\) and \\(Nk_BT_c\\) yields\n\\[\n\\begin{aligned}\nV_c =& 3b\\\\\nNk_BT_c =& \\frac{8a}{27b}\n\\end{aligned}\n\\]\nSubstituting these two results into Equation 1 yields\n\\[\nP_c=\\frac{a}{27b^2}\n\\]\nNow let \\(P=P_c p, V=V_cv, T=T_ct\\) in the vdW eqn. (Note that in this context \\(t\\) is not the reduced temperature).\n\\[\n\\left(P_cp+\\frac{a}{(V_cv)^2}\\right)(V_cv-b)=N_Ak_BT_ct\n\\]\nSubstituting in for \\(V_c, N_Ak_BT_c\\) and \\(P_c\\)\n\\[\n\\begin{aligned}\n\\left(p\\frac{a}{27b^2}+\\frac{a}{9b^2v^2}\\right)\\left(3bv-b\\right)=&\\frac{8a}{27b}t\\\\\n\\Rightarrow\\left(p+\\frac{3}{v^2}\\right)\\left(v-\\frac{1}{3}\\right)=&\\frac{8}{3}t\n\\end{aligned}\n\\]\nThis expression for the equation of state in terms of reduced variables is useful because reference to the system specific parameters \\(a\\) and \\(b\\) has vanished. In this form the equation is therefore universal.\nPlotting \\(P/P_c\\) vs \\(V/V_c\\) for isotherms (values of \\(t\\)) and focussing on the region close to the critical point, one finds\n\n\n\n\n\n\nFigure 2: Isotherms of \\(p\\) versus \\(v\\) for various \\(t\\) spanning the critical temperatures\n\n\n\n\n\n\n\n\n\nFigure 3: (a) \\(\\frac{\\partial p}{\\partial v}\\) for \\(T=T_c\\). (b) \\(\\frac{\\partial^2 p}{\\partial v^2}\\) for \\(T=T_c\\).\n\n\n\nPlotting \\((\\frac{\\partial p}{\\partial v})_{t=1}\\) and \\((\\frac{\\partial^2 p}{\\partial v^2})_{t=1}\\), we see that there is indeed a point of inflexion on the critical isotherm, at \\(v=1\\), this is the critical point (ie. a continuous phase transition), Figure 3 .\nSubcritical isotherms (first order phase transition) exhibit a so called van-der Waals loop.\nTo find the compressibility critical exponent \\(\\gamma\\), we recall that\n\\[\n\\kappa_T=\\frac{-1}{V}\\left(\\frac{\\partial V}{\\partial P}\\right)_T=\\frac{-1}{p_cv}\\left(\\frac{\\partial v}{\\partial p}\\right)_t\\propto \\tilde{t}^{-\\gamma}\n\\] with \\(\\tilde{t}=(T-T_c)/T_c\\) small.\nNow from the reduced equation of state\n\\[\n\\frac{\\partial p}{\\partial v}=\\frac{-8t}{3(v-1/3)^2}+\\frac{6}{v^3}\n\\] setting \\(t=\\tilde{t}+1\\) and \\(v=1\\) gives \\(\\frac{\\partial p}{\\partial v} =-6\\tilde{t}\\), ie the compressibility diverges\n\\[\n\\kappa_T\\propto \\tilde{t}^{-1}\n\\] ie. \\(\\gamma=1\\), which is the same as the mean field result which we derived in another context of the magnetic susceptibility.\n\n\n\n4. Mean field theory of the Ising model heat capacity\nWe insert into the expression for the mean Ising energy\n\\[\n\\langle E \\rangle =-J\\sum_{&lt;i,j&gt;}\\langle s_is_j\\rangle\\:,\n\\] the simplest mean field approximation \\(\\langle s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Recalling the behaviour of the order parameter for small \\(t\\), that the number of bonds \\(=qN/2\\), and the mean field value of \\(T_c=qJ/k_B\\), we have for \\(T&lt;T_c\\)\n\\[\n\\begin{aligned}\n\\langle E \\rangle =& \\frac{-NqJm^2}{2}\\\\\n\\:                =& \\frac{3NqJt}{2}\\\\\n\\:                =& \\frac{3Nk_B(T-T_c)}{2}\n\\end{aligned}\n\\] while \\(\\langle E \\rangle= \\textrm{ constant}\\) for \\(T&gt;T_c\\).\nHence differentiating, we find \\[\n\\begin{aligned}\nC_H= 0; &\\quad T&gt;T_c\\\\\nC_H= 3Nk_B/2; & \\quad T\\le T_c\n\\end{aligned}\n\\]\nThis independence of the heat capacity on \\(t\\) corresponds to a critical exponent \\(\\alpha=0\\)\n\n\n\n5. Magnetisation and fluctuations\nThe free energy is\n\\[\nF=-k_BT\\ln Z\n\\] with the partition function\n\\[\nZ=\\sum_{{s}}\\exp[-(E-hM)/k_BT]\n\\]\nThus \\[\n\\begin{aligned}\n-\\left(\\frac{\\partial F}{\\partial h}\\right)_T =& k_BT\\frac{1}{Z}\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\\\\n\\: =&\\frac{1}{Z}\\sum_{{s}}M \\exp[-(E-hM)/k_BT]\\\\\n   =& \\langle M\\rangle\n\\end{aligned}\\] where we have used the definition of the average of an observable given in lectures.\nNow \\[\n\\begin{aligned}\n\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T =& -k_BT\\left[\\frac{1}{Z}\\left(\\frac{\\partial^2 Z}{\\partial h^2}\\right)_T-\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\frac{1}{Z^2}\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\right]\\\\\n\\: =&\\frac{-1}{k_BT}\\left[\\frac{1}{Z}\\sum_{{s}}M^2 \\exp[-(E-hM)/k_BT]-\\langle M\\rangle^2\\right]\\\\\n\\:  =& \\frac{-1}{k_BT}\\left[\\langle M^2\\rangle-\\langle M\\rangle^2\\right]\n\\end{aligned}\n\\] You should recognise the terms in square brackets as the variance of the magnetisation distribution.\nThus the susceptibility is \\[\n\\chi_H\\equiv\\frac{\\partial \\langle M\\rangle}{\\partial h}=\\frac{1}{k_BT}\\left[\\langle M^2\\rangle-\\langle M\\rangle^2\\right]\n\\]\nIncidently, this is known as the fluctuation-dissipation theorem. It is a neat result, because it allows you to calculate the response to a perturbation from equilibrium, without actually perturbing the system! Instead one merely looks at the form of the equilibrium fluctuations. It is used extensively in computer simulations.\n\n\n\n6. Spin-1 Ising model\nAs in lectures, the mean field Hamiltonian for a single spin is\n\\[\n{\\cal H}(s_0)=-s_0\\left(qJm + h\\right)+NqJm^2/2\n\\] where here \\(h\\) is the magnetic field.\nThe probability of finding this spin with value \\(s_0\\) is \\[\n\\begin{aligned}\np(s_0) =& \\frac{e^{-\\beta{\\cal H}(s_0)}} {\\sum_{s_0=0,\\pm 1}e^{-\\beta{\\cal H}(s_0)}}\\\\\n=&\\frac{e^{\\beta s_0(qJm+h)}}{1+e^{\\beta(qJm+h)}+e^{-\\beta(qJm+h)}}\n\\end{aligned}\n\\]\nNow for consistency \\(\\langle s_0\\rangle=m\\), so \\[\\begin{aligned}\nm =& \\sum_{s_0=0,\\pm 1}s_0p(s_0)\\\\\n\\:=& \\frac{0+e^{\\beta(qJm+h)}-e^{\\beta(qJm+h)}} {e^0+e^{\\beta(qJm+h)}+e^{-\\beta(qJm+h)}}\\\\\n\\:=&  \\frac{2\\sinh[\\beta(Jqm+h)]}{1+2\\cosh[\\beta(Jqm+h)]}\n\\end{aligned}\\] To get the critical temperature, we can solve this graphically. One plots the RHS as a function of \\(m\\), for various \\(\\beta\\). On the same graph one plots the curve \\(y=m\\) (representing the LHS). \\(T_c\\) is the highest \\(T\\) for which the two curves intersect.\nAlternatively to get \\(T_c\\) analytically, set \\(h=0\\) and expand for small \\(m\\) (i.e. small \\(x=\\beta J q m\\)), we have \\[\nm \\approx \\frac{2(\\beta J q m)}{1+2} = \\frac{2}{3}\\,\\beta J q\\, m .\n\\] (I would advise looking up the expansions of \\(\\sinh(x)\\) and \\(\\cosh(x)\\) to see how this is obtained.)\nNow, a nonzero solution appears when the prefactor equals \\(1\\), i.e. when \\(m=m\\). Thus \\[\n1 = \\frac{2}{3}\\,\\beta_c J q\n\\quad \\Rightarrow \\quad\n\\beta_c = \\frac{3}{2Jq}.\n\\]\nHence the critical temperature is \\[\nk_B T_c = \\frac{2}{3}\\, J q .\n\\]\n\n\n\n7. Transfer Matrix\nThe transfer matrix is a list of the possible interactions of a pair of spins with one another and with a magnetic field. For a 1d spin-1/2 system it takes the form: \\[\n\\mathbf{V}(H)=\\begin{bmatrix}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{bmatrix}\n\\] We need to find the eigenvalues, so we solve the characteristic equation det(\\(\\mathbf{V}-\\lambda \\mathbf{I})=0\\), i.e.\n\\[\n\\mathbf{V}(H)=\\begin{vmatrix}\ne^{\\beta(J+H)} -\\lambda & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}-\\lambda\n\\end{vmatrix} =0\n\\]\nThen \\(\\lambda^2-(a+d)\\lambda+(ad-bc)=0\\). So\n\\[\n\\begin{aligned}\n\\lambda_\\pm =& \\frac{a+d\\pm\\sqrt{(a+d)^2-4(ad-bc)}}{2}\\\\\n\\lambda_{\\pm} =& e^{\\beta J}\\cosh(\\beta H) \\pm \\frac{1}{2}\\sqrt{e^{2\\beta J}4\\cosh^2\\beta H-4(e^{2\\beta J}-e^{-2\\beta J})}\\\\\n\\lambda_{\\pm} =& e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\n\\end{aligned}\n\\] (You’ll need the identity \\(\\cosh^2 x-\\sinh^2 x = 1\\)).\nFrom lectures, you should know that the partition function\n\\[\\begin{align}\nZ=\\textrm{Tr}(\\mathbf{V}^N)=&\\lambda_+^N+\\lambda_-^N\\\\\n\\approx & \\lambda_+^N \\hspace{5mm}\\textrm{N~large}\n\\end{align}\\]\nwhere \\(\\lambda_+\\) is the largest of the two evals.\nHence the free energy \\(F=-k_BT\\ln(Z)\\) can be written\n\\[\nF=-Nk_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\n\\]\nNow the magnetisation per site is\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\frac{k_BT}{\\lambda_+}\\frac{\\partial \\lambda_+}{\\partial H}\n\\]\nYou can either be a hero here, or use a symbolic solution program like Maple or Wolfram Alpha. I did the latter to find the stated result.\n\\[\nm=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{(-4\\beta J)}}}\n\\]\nHence at zero \\(H\\), there is no spontaneous magnetisation at any \\(T\\).\n\n\n\n8. Landau theory\nIf this were an Ising model problem (ie a microscopic model) we could write down the partition function, get an explicit expression for the free energy and differentiate once (wrt \\(T\\)) to get the energy and again (wrt \\(T\\)) to get the heat capacity. But the starting point for Landau theory is the free energy itself, so we need another starting point, namely thermodynamics. The appropriate thermodynamic potential for the magnet is \\(F=E-TS-MH\\) with \\(E\\) the internal energy. Then \\[\n\\begin{aligned}\ndF =& dE-TdS-SdT-MdH-HdM\\\\\n\\: =& TdS+HdM-TdS-SdT-MdH-HdM\\\\\n\\: =& -SdT-MdH\n\\end{aligned}\n\\] where we have used the first law for a magnet \\(dE=TdS+HdM\\).\nThus \\[\n\\begin{aligned}\n\\left(\\frac{\\partial F}{\\partial T}\\right)_H =& -S\\\\\n-T\\left(\\frac{\\partial^2 F}{\\partial T^2}\\right)_H =&T\\frac{dS}{dT}=\\frac{dQ}{dT}\\\\ C_H=-T\\left(\\frac{\\partial^2 F}{\\partial T^2}\\right)_H\n\\end{aligned}\\]\nwhere \\(C_H\\) is the specific heat at constant field and we have used the fact that \\(dS=dQ/T\\).\nNow from lectures, the equilibrium magnetisation in the Landau free energy is given by\n\\[\nm^2=\\frac{-a_2}{2a_4}\n\\] for \\(T&lt;T_c\\) and zero otherwise. Substituting this into the Landau free energy \\(F=F_0+a_2m^2+a_4m^4\\) gives\n\\[\n\\begin{aligned}\nF = F_0 & \\quad T&gt;T_c\\\\\nF = -a_2^2/4a_4 & \\quad T &lt; T_c\n\\end{aligned}\n\\] Using the fact that \\(a_2=\\tilde{a_2} t\\), with \\(t=(T-T_c)/T_c\\) and differentiating wrt \\(T\\) twice, to get the heat capacity, we find\n\\[\n\\begin{aligned}\nC_H =  0; & \\quad T\\to T_c^+\\\\\nC_H = \\frac{T\\tilde a_2^2}{2a_4T_c^2}; & \\quad T \\to T_c^-\\:,\n\\end{aligned}\\] The jump discontinuity rather that a divergence in the specific heat at \\(T=T_c\\) formally corresponds to a critical exponent \\(\\alpha=0\\).\nTo get the susceptibility exponent, we add a magnetic field to the free energy\n\\[\nF(m)=F_0+a_2m^2+a_4m^4-Hm\n\\]\nThen the equilibrium magnetisation satisfies \\[\n\\begin{aligned}\n\\frac{dF}{dm} =&2\\tilde{a_2} tm+4a_4m^3-H=0\\\\\n\\Rightarrow H =&2\\tilde{a_2} tm+4a_4m^3\\\\\n\\Rightarrow \\left(\\frac{\\partial H}{\\partial m}\\right )_T =&2\\tilde{a_2} t+12a_4m^2\n\\end{aligned}\\] Now using the results that \\(m^2=0\\) for \\(t&gt;0\\) and \\(m^2=-\\tilde{a_2}t/(2a_4)\\) for \\(t&lt;0\\), we have that in both cases\n\\[\n\\left(\\frac{\\partial H}{\\partial m}\\right )_T\\propto t\n\\]\nHence\n\\[\n\\left(\\frac{\\partial m}{\\partial H}\\right )_T\\propto t^{-1}\n\\] so \\(\\gamma=1\\).\n\n\n\n9. Scaling equation of state\nThe equilibrium state corresponds to the minimum of the free energy \\(\\partial F/\\partial M=0\\). This gives the equation of state\n\\[\nH=2\\tilde{a_2}tm+4a_4m^3\n\\] (Note that we ignore the solution \\(H=m=0\\) which corresponds to a maximum of the free energy.)\nThe near critical power law scaling of the magnetisation is \\(m\\propto t^\\beta\\) and \\(m\\propto H^{1/\\delta}\\). To find a scaling form for the equation of state we need to transform to scaled variables \\(H/m^\\delta\\) and \\(t/m^{1/\\beta}\\). We can get a scaling equation in terms of these variables by dividing through the equation of state by \\(m^3\\), so that\n\\[\n\\frac{H}{m^{3}}=\\frac{2\\tilde{a_2}t}{m^2}+4a_4\n\\]\nHence \\(\\delta=3\\) and \\(\\beta=1/2\\).\nThe choice of scale \\(a_4=1/4\\) and \\(\\tilde{a_2}=1/2\\) yields the given form of the scaling function.\n\n\n\n10. Scaling laws\nFirst of all recall the definition of the critical exponents: \\[\n\\begin{aligned}\nm       \\propto  t^{\\beta};& \\quad (h=0) \\\\\n\\chi_T  \\propto  t^{-\\gamma};&\\quad (h=0) \\\\\nC_H  \\propto  t^{-\\alpha};& \\quad (h=0) \\\\\nm  \\propto  h^{1/\\delta}.& \\quad (t=0) \\\\\n\\end{aligned}\n\\] The free energy in generalised homogeneous form is\n\\[\nF(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h)\n\\]\nThe first of the scaling relations to be derived was covered in lectures: Let \\(\\lambda^a=1/t\\), so that \\(\\lambda=t^{-1/a}\\). Then\n\\[\n\\begin{aligned}\nF(t,h)=&t^{1/a}F(1,t^{-b/a}h)\\\\\nm(t,h)=&-\\left(\\frac{\\partial F}{\\partial h}\\right)_t= -t^{(1-b)/a} \\left.\\frac{\\partial F(1,y)}{\\partial y}\\right|_{ht^{-b/a}}=t^{(1-b)/a}m(1,ht^{-b/a})\n\\end{aligned}\n\\] so when \\(h=0\\), we have \\(m(1,t^{-b/a}h)=m(1,0)=\\textrm{ const}\\) and hence we can identify \\(\\boxed{\\beta=(1-b)/a}\\).\nWe also have for the isothermal susceptibility\n\\[\n\\chi=\\left(\\frac{\\partial m}{\\partial h}\\right)_t=-t^{(1-2b)/a}\\left.\\frac{\\partial^2 F(1,y)}{\\partial y^2}\\right|_{ht^{-b/a}},\n\\] so taking again \\(h=0\\), we find \\(\\boxed{\\gamma=(2b-1)/a}\\).\nFor the specific heat at constant (zero) field, we have the definition: \\[\nC_H = \\left(\\frac{\\partial E}{\\partial T}\\right)_{h=0}=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\:,\n\\] where in the last step have used \\(E=-\\partial (\\beta F)/\\partial\\beta\\), with \\(\\beta=(k_BT)^{-1}\\) (see fig 2.1 in notes). Alternatively one can use the thermodynamic derivation of this relation given in an earlier problem on Landau theory. Transforming from \\(T\\) to \\(t=(T-T_c)/T_c\\) and inserting the generalised homogeneous form for \\(F\\) gives:\n\\[\n\\begin{aligned}\nC_H =& -\\frac{T}{T_c^2}\\frac{\\partial^2}{\\partial t^2}[t^{1/a}F(1,t^{-b/a}h)]\\\\\nC_H \\approx& -\\frac{1}{T_c}\\frac{\\partial^2}{\\partial t^2}[t^{1/a}F(1,t^{-b/a}h)]\\\\\nC_H =& -\\frac{1}{T_c}\\frac{1}{a}(\\frac{1}{a}-1)t^{(1/a-2)}F(1,0)\n\\end{aligned}\\] Here we have neglected all derivatives of \\(F\\) since they are multiplied by at least one power of \\(h\\) which is zero. Hence \\(\\boxed{\\alpha=2-1/a}\\).\nFinally, if we let \\(\\lambda^b=1/h\\), so that \\(\\lambda=h^{-1/b}\\) and consider the critical isotherm \\(t=0\\). Then\n\\[\n\\begin{aligned}\nF(t,h)=&h^{1/b}F(h^{-a/b}t,1)\\\\\n\\Rightarrow m(t,h)=& \\frac{ta}{b}h^{(1-a-b)/b}\\left.\\frac{\\partial F(x,1)}{\\partial x}\\right|_{h^{-a/b}t}-\\frac{1}{b}h^{1/b-1}F(h^{-a/b}t,1).\n\\end{aligned}\\] so when \\(t=0\\), we get \\(m(0,h)\\) and can identify \\(\\boxed{\\delta=b/(1-b)}\\).\nTo derive the relationships (``scaling laws’’) among the critical exponents, we eliminate \\(a\\) and \\(b\\) from the boxed scaling relations. Setting \\(a=(2-\\alpha)^{-1}\\) in the first scaling relation, we find \\(b=1-\\beta/(2-\\alpha)\\). Substituting this into the second scaling relation gives the second of the two scaling laws quoted in the notes. Substituting into the 4th scaling relation gives the first scaling law.",
    "crumbs": [
      "Unifying concepts",
      "Solutions to problems"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html",
    "href": "phase-transitions/assignment_October2025.html",
    "title": "PHYSM0071: First coursework assignment",
    "section": "",
    "text": "1. Introduction and background\nIn this coursework assignment, you will explore the phenomena of spatial correlations and their relationship to phase behaviour in a simple lattice gas model. This model is a crude representation of a fluid in which particles can occupy the sites of a hypercubic lattice. The occupancy of a site \\(i\\) is specified by the variable \\(c_i=1\\) (occupied) or \\(c_i=0\\) vacant. The complete list of these occupancies \\(\\{c\\}\\) specifies a microstate. The instantaneous particle number density (fraction of occupied sites) is given by\n\\[\n\\rho=L^{-d}\\sum_i c_i\n\\]\nwhere \\(L\\) is the linear extent of the lattice (i.e. the number of unit cells along a coordinate axis) and \\(d\\) its dimensionality.\nWithin the Grand Canonical ensemble (see precourse reading) the Hamiltonian of the lattice gas model is\n\\[{\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j - \\mu\\sum_ic_i\\]\nwhere \\(\\epsilon\\) is an attraction energy between a pair of particles on adjacent (nearest neighbouring) sites and \\(\\mu\\) is a field known as the chemical potential which couples to the particle number density \\(\\rho\\). The number density typically fluctuates around a mean value controlled by the prescribed value of \\(\\mu\\).\nOne can also consider the model under conditions in which the particle number is fixed to some prescribed value - then there is no chemical potential and the corresponding set of microstates define the canonical ensemble (see precourse reading) of the lattice gas at that density.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "href": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.1 Mapping between lattice gas and Ising model",
    "text": "1.1 Mapping between lattice gas and Ising model\nThe lattice gas model is interesting because whilst being a plausible model for a fluid, it maps onto the Ising model. We say that they are isomorphic to one another. This isomorphism extends the applicability of the Ising model. To expose the mapping we write the grand partition function of the lattice gas:\n\\[ \\Xi=\\sum_\\textrm{ state}\\exp-\\beta{\\cal H}_{LG}=\\sum_{\\{c\\}}\\exp\\left[\\beta \\epsilon\\sum_{&lt;i,j&gt;}c_ic_j +\\beta\\mu\\sum_ic_i\\right] \\] where the sum runs over all possible combinations of occupancies of the lattice sites.\nWe now change variables to\n\\[c_i=(1+s_i)/2; ~~~~ J=\\frac{\\epsilon}{4}; ~~~~\nh=\\frac{\\epsilon q+2\\mu}{4},\n\\] where \\(q\\) is the lattice coordination number.\nOne finds,\n\\[{\\cal H}_{LG}={\\cal H}_\\textrm{ I} + \\textrm{ constant}\\] Since the last term does not depend on the configuration, it feeds through as an additive constant in the free energy; and since all observables feature as derivatives of the free energy, the constant has no physical implications.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#phase-diagram",
    "href": "phase-transitions/assignment_October2025.html#phase-diagram",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.2 Phase diagram",
    "text": "1.2 Phase diagram\nUsing the isomorphism we can plot the phase diagram of the lattice gas in the plane of average number density and temperature.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of the lattice gas model in the density-temperature plane.\n\n\n\nIn the \\(\\mu-T\\) plane there is a line of first order phase transitions terminating at a critical point which has density \\(\\rho_c=0.5\\). The first order line means that if \\(T&lt;T_c\\) and starting from the gas phase we smoothly increase the chemical potential through the coexistence value of \\(\\mu\\), the average number density of particles on our lattice \\(\\rho=N/L^d\\) jumps discontinuously from a low value \\(\\rho_\\textrm{gas}\\) to a high value \\(\\rho_\\textrm{liquid}\\). This is the gas-liquid phase transition. The values of \\(\\rho_\\textrm{gas}\\) and \\(\\rho_\\textrm{liquid}\\) merge at \\(T_c\\), the gas-liquid critical temperature. At higher temperatures, the distinction between the phases disappears.\n\n\n\n\n\n\n1.3 Aside on Real Fluids\n\n\n\n\n\nYou may wish to compare the phase diagram of the lattice gas mode with the results of (say) van der Waals equation (see recommended textbooks for the required phase diagram). The main difference is that the lattice gas has so-called “particle-hole” symmetry, \\(\\rho\\to 1-\\rho\\) (inherited from the up-down symmetry of the Ising model) which is not present for a real fluid. Accordingly, the phase diagram in a real fluid looks like a lopsided version of the above picture as shown in Figure 1.2. See here for some real experimental data showing the asymmetry of the coexistence curve in liquid metals.\n\n\n\n\n\n\nFigure 1.2: Schematic of the liquid-gas phase diagram in the \\(\\rho-T\\) plane for a realistic fluid .",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc-algorithm",
    "href": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc-algorithm",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.1 Grand Canonical Monte Carlo (GCMC) algorithm",
    "text": "2.1 Grand Canonical Monte Carlo (GCMC) algorithm\nA suitable algorithm for this case is to repeatedly update the system via the following fours steps:\n\nSelect a random lattice site \\((i,j)\\).\nIf the site is empty, propose inserting a particle. If it is occupied, propose removing the particle.\nCompute the change in energy \\(\\Delta E\\) and particle number \\(\\Delta N = \\pm 1\\).\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta {\\cal H}_{LG}&lt; 0, \\\\\ne^{-\\beta \\Delta {\\cal H}_{LG}} & \\text{otherwise},\n\\end{cases}\n\\]\nwhere \\(\\Delta {\\cal H}_{LG} = \\Delta E - \\mu \\Delta N\\) is the change in the grand canonical Hamiltonian.\nThis update scheme allows the system to explore states with varying particle numbers according to grand canonical statistics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "href": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.2 Fixed-particle-number case (Canonical Ensemble)",
    "text": "2.2 Fixed-particle-number case (Canonical Ensemble)\nIn contrast, we may for simplicity want to simulate with the total number of particles \\(N\\) fixed, as in the canonical ensemble. Then particle insertions and deletions are not allowed and the chemical potential term drops out of the Hamiltonian. We can use Kawasaki dynamics, which conserves particle number by moving particles between lattice sites. An update consists of the following four steps.\n\nSelect two sites \\((i,j)\\) and \\((i',j')\\) at random.\nIf one site is occupied and the other is empty, propose exchanging the particle and vacancy.\nCompute the energy change \\(\\Delta E = E(C') - E(C)\\) associated with this proposed particle move.\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta E &lt; 0, \\\\\ne^{-\\beta \\Delta E} & \\text{otherwise}.\n\\end{cases}\n\\]\nThis approach ensures particle number is conserved while still allowing the system to explore equilibrium configurations of the canonical (ie. fixed \\(N\\)) ensemble.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "href": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.3 Observables and measurement",
    "text": "2.3 Observables and measurement\nA Monte Carlo sweep in either ensemble involves performing \\(L^2\\) update attempts on a lattice of size \\(L \\times L\\).\nThe expectation value of a macrovariable \\(O\\) (e.g., energy, particle density, correlation functions) is estimated by averaging over sampled configurations:\n\\[\n\\overline O \\approx \\frac{1}{{\\cal N}} \\sum_{n=0}^{\\cal N} O_n,\n\\]\nwhere \\(O_n\\) is the value measured in configuration \\(C_n\\), and \\({\\cal N}\\) is the total number of configurations use for sampling.\nOther observables (those which are second derivatives of the free energy) such as the specific heat capacity can’t be calculated as simple averages. Instead they are calculated as a variance. The formula for the specific heat (cf. problem set) is\n\\[\nC=\\frac{1}{k_BT^2} \\left[\\overline{E^2} - \\overline{E}^2\\right] = \\frac{(\\Delta E)^2}{k_BT^2}\n\\]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#setup",
    "href": "phase-transitions/assignment_October2025.html#setup",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThis should be completed in the week prior to the release of the assignment to make sure that any technical problems are resolved.\n\nOn the PHYSM0071 Blackboard page open the Unit Information and Resources tab\nScroll down to Notable and open it (if off campus make sure you have the UoB VPN enabled)\nSelect the Jupyter Notebook (Legacy) notebook server option\nWhen the notebook has opened click the +Gitrepo button\nUnder enter Git Repository insert: https://github.com/nbwilding/Lattice-gas-coursework\nPress the “clone” button. This will download a notebook called lattice-gas.ipynb into Jupyter\nCheck that the program runs\nFamiliarise yourself with the main features of the program. Pay attention to how to change the temperature, number of lattice sites \\(N=L^2\\), and the number of equilibration and sampling sweeps. Also check how to toggle live visualisation on and off. Be aware that the program can take several minutes to run depending on the system size, and equilibration/sampling parameters.\n\nThe assignment itself will be released at 12:30pm on Monday 13th October 2025 on Blackboard via the Unit Assessment tab. The submission deadline is Monday 27th October 2025 at 09:30.\nPlease contact me (nigel.wilding@bristol.ac.uk) if you have trouble with the setup described above, detailing the problem you encountered.\nThe various elements of the assignment are set out below. Instructions are given in italics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "href": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.2 Isomorphism of the lattice gas and Ising model",
    "text": "3.2 Isomorphism of the lattice gas and Ising model\nBy referring to section 1.1 above, show that the Hamiltonian of the Lattice Gas model in the grand canonical ensemble\n\\[\n    {\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j -\\mu\\sum_ic_i\n\\] is transformed to that of the Ising model by means of the change of variable\n\\[\n    s_i=2c_i-1;~~~~ J=\\frac{\\epsilon}{4};~~~~\n    h=\\frac{\\epsilon q+2\\mu}{4}.\n\\] where \\(q\\) is the lattice coordination number (\\(q=4\\) in 2-dimensions and \\(q=6\\) in 3d).\n(Hint: Note that when doing sums over bonds \\(\\sum_{\\langle i,j\\rangle}\\) for a lattice of coordination \\(q\\) there are \\(q/2\\) bonds per site since each bond is shared between two sites.) [4 marks]\nGiven that the critical temperature of the 2d Ising model is \\(T_c\\approx 2.269\\: J/k_B\\), use the above mapping to find the value of the critical temperature of the 2d lattice gas model in units of \\(\\epsilon/k_B\\). [2 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "href": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.3 Code modification tasks",
    "text": "3.3 Code modification tasks\nThe program provided to you simulates the 2D lattice gas model in the canonical ensemble (i.e., at a fixed particle number density \\(\\rho\\)), using the Kawasaki swap algorithm as described above. It allows the user to specify the number of lattice sites and the temperature for the simulation. The program computes the pair correlation function (also known as the radial distribution function) \\(g(r)\\) and the structure factor \\(S(\\mathbf{k})\\) (refer to Chapter 2 of the notes for the definitions of these quantities).\nIn this implementation, the particle density is fixed at \\(\\rho = \\rho_c = 0.5\\). This choice ensures that the system approaches the critical point as the temperature is lowered within the supercritical regime. For convenience, the program uses dimensionless units by setting \\(\\epsilon = k_B = 1\\). The linear system size is intially set to \\(L=50\\) lattice units.\n\nReview the program thoroughly and gain a clear understanding of its functionality.\nThe program calculates the average form of the 2d structure factor \\(S(\\mathbf{k})\\). Using this, obtain and plot the radially averaged structure 1d factor \\(S(|k|)\\).  [6 marks]\nAdd a function to calculate and print the dimensionless total energy and specific heat of the system. For the latter you will need the expression given in section 2.2, above.\n[5 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "href": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.4 Computational investigations of correlations on the approach to criticality",
    "text": "3.4 Computational investigations of correlations on the approach to criticality\n\nPerform simulations at several (six or seven) temperatures within the super critical range \\(T_c \\le T \\lesssim 2T_c\\). For each temperature, save the final configuration and describe how the overall structural characteristics evolve with temperature. Consider how you can be sure that your final configuration is equilibrated [4 marks]\nPlot and compare the radial distribution function \\(g(r)\\) and the structure factor \\(S(\\mathbf{k})\\), and its radially averaged form, \\(S(|k|)\\) at each temperature. [4 marks]\nInterpret the peak structure in \\(g(r)\\) and \\(S(|k|)\\) and discuss the relationship to clustering or ordering. [3 marks]\nThe full width at half maximum (FWHM) of the radially averaged structure factor can be used to estimate the correlation length \\(\\xi\\) through \\(\\xi=1/FWHM\\). For each temperature studied, estimate this quantity from your data. Plot the temperature dependence of \\(\\xi\\) and comment on its behaviour. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "href": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.5 Temperature and system size dependence of the specific heat",
    "text": "3.5 Temperature and system size dependence of the specific heat\n*Examine how the specific heat capacity varies with dimensionless temperature over a broad range, including both subcritical and supercritical regimes (e.g \\(T_c/2&lt;T&lt;2T_c\\)). Describe the overall behavior of the specific heat across this temperature range and offer an explanation for its form. Describe how you checked that your sampling was from reasonably well equilibrated configurations. [4 marks]\nRepeat the analysis that you performed for the \\(L=50\\) system, for system sizes \\(L=20\\) and \\(L=35\\). Compare the results for all three system size and highlight any observed differences. Discuss possible explanations for these size-dependent effects. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#your-report",
    "href": "phase-transitions/assignment_October2025.html#your-report",
    "title": "PHYSM0071: First coursework assignment",
    "section": "4.0 Your report",
    "text": "4.0 Your report\nYou should produce a short skeleton report (upto 4 sides of A4 including figures and in no less than 11 pt fontsize) focussing on and summarising the results of the above investigations and your comments/observations on the findings.\nAs well as a title and your name, the report should be laid out using the headings above:\n\nIsomorphism of the lattice gas and Ising model\nCode modifications. Include snippets of your modified code, highlighting the modifications in yellow. Do not include the whole code, just enough to see what you changed and where you did it.\n\nComputational investigations of correlations on the approach to criticality\nTemperature and system size dependence of the specific heat\nInclude a line with following declaration at the end of the report: “I have not made use of Artificial Intelligence tools in completing this assessement beyond those of category 2 in the University’s categorisation scheme.”\n\nThere is no need for any other section headings ie. you don’t need abstract, introduction, references etc. Marks will be deducted for reports that exceed four sides (faces) of A4.\nSubmit your report in pdf format for grading via blackboard. The report will be marked out of 40, and will count for \\(30\\%\\) of the unit mark.\nLike any lab report, the marking focus will be on:\n\nDo your results look physically plausible?\nDid you describe the pertinent features?\nAre your descriptions clear?\nDo you explain them correctly using appropriate concepts?",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "experimental/part1.html",
    "href": "experimental/part1.html",
    "title": "10  Macroscopic measurements",
    "section": "",
    "text": "10.1 Introduction\nIn the course so far you have discussed the theoretical origins of phase transitions and have been given a theoretical introduction to the properties of disordered materials. In this section we will look briefly at some of the experimental techniques and experimental data that have driven and tested the theoretical work. Much of the quantitative experimental work is challenging and in many cases would take many lectures to justify properly. So, in these notes we will concentrate on the ideas underpinning the experiments and I will try to give a justification for the results obtained. The number of techniques discussed is limited and not exhaustive, you may find others as you read along.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#macroscopic-observation-and-experiments",
    "href": "experimental/part1.html#macroscopic-observation-and-experiments",
    "title": "10  Macroscopic measurements",
    "section": "10.2 Macroscopic observation and experiments",
    "text": "10.2 Macroscopic observation and experiments\nBefore discussing experiments in detail it is worth taking a little time to think about our observation of phase transitions and the different types of material we encounter in our everyday life. In the following I will concentrate mainly on water and its states as we know them well. You may consider it as a liquid at room temperature, a solid at cold temperatures and a gas at high temperatures. However, even for this `simple’ material we can observe unusual effects as we change its state. So in the following I will use water as an example even though, hopefully, you can see how the ideas will apply in general to other materials.\n\n10.2.1 1st Order Phase Transitions - ice and water as an example.\nPhase transitions are all around us in our everyday life. Some take place naturally, for example, the freezing of water on a cold winter night, some are exploited (and maybe hidden) in the technology that surrounds us in for example, liquid crystal displays or non-volatile computer memory.\nLet’s take water as our example and consider our everyday experience. In a warm building at a temperature of 20\\(^o\\)C, water in a beaker is a liquid, it freely flows and you can pour it from one container to another. What happens as we lower the temperature slowly (so that we keep, as best we can, a uniform temperature in the liquid). Common experience tells us it remains as a liquid until we reach its freezing point at 0\\(^o\\)C when it starts to solidify and form solid water - ice. But how does happen and does it always happen in the same way? Let’s think about this a little more deeply.\nSuppose we put our beaker of water at 20\\(^o\\)C straight into a freezer at, say, -10\\(^o\\) C. To start with the water in the beaker will lose heat to the surroundings (mainly by conduction and convection) and temperature gradients will appear in the water (the water at the edges will be cooler than the water in the middle). As the outside approaches 0\\(^o\\)C we expect the water at the edges to freeze (we’ll discuss this further later) and a block of solid ice will eventually form in the beaker. (We might break our container as, unusually, the density of ice is less than water so the total volume occupied by the water tries to get larger). During this process we will have an ice-water \\(\\textit{interface}\\) that stays at 0\\(^o\\)C but with large temperature gradients in the freezer and beaker - we are far from having the uniform temperature we mentioned above. As we can see from this short description, the temperature gradients mean that in practise phase transitions never take place uniformly in a material and there will always be some localization of the processes taking place. We can already see that freezing is more complicated than we might first imagine!\nIf we are going to look at the ice-water transition more carefully experimentally and to reduce these non-local effects we try to do things more slowly and try to maintain a uniform temperature by e.g. stirring. Ideally we’d also like to know qualitatively how quickly we are removing the heat and how much heat is removed in the process (the latent heat). What happens in this case? Well, if the outside temperature of the beaker is very slightly below zero then heat will be lost slowly from the (stirred) water and we expect small regions of ice mixed with the water to form. The water in the beaker does not freeze instantaneously but the ratio of the amount of ice to water steadily increases as heat is gradually removed (we should describe this as quasi-equilibrium where we consider we are moving gradually from one thermodynamic equilibrium state to another). As more ice forms we get a slushy mixture. If we carefully measure the temperature of the ice-water mixture (slush) during this process we will see that it remains at 0\\(^o\\)C until all the water has changed to ice at which point the ice will reach equilibrium with its surroundings (that we remember was just below zero). If, at any time during the process we prevent heat entering or leaving the system, the ice-water mixture will maintain the same ratio (although changes in the sizes of the regions may take place). The ice and water are then said to be in co-existence at the \\(\\textit{phase ~ transition ~temperature}\\) of 0\\(^o\\)C.\nCo-existence of two phases at the transition temperature is the typical characteristic of a first order transition.\nYou may wish to remind yourself of this process when we discuss thermal analysis later.\n\n\n10.2.2 First Order transitions - Experimental measurements.\nIn the discussion above I tried to define more carefully how we were cooling the system. This hints at some of the difficulties in actually trying to measure the details of about what happens when the transition is taking place. Fundamentally we are trying to use equlibrium thermodynamics to explain what happens in a system that is not in equilibrium but in the process of equilibrating. The study of non-equilibrium processes is a very active and important area of study. We might note that the ice-water co-existence is an equilibrium state - it is all at 0\\(^o\\)C but can have very different appearances (slushy, an ice cube surrounded by water etc.). The result is that in all careful (accurate calorimetry) experiments care is taken to avoid non-unformity in the system (i.e. temperature gradients, field gradients in magnetic systems, …). The problem of non-uniform temeperature is even more acute for second order phases transitions as we will discuss below.\nThere are many ways in which you might study phase transitions apart from visually, but the most common, quantative and closely related techniques are Differential Thermal Analysis (DTA) and Differential Scanning Calorimetry (DSC). Machines for doing this are commercially available and are ubiquitous in materials science labs.\nWe will look at these techniques in more detail later.\n\n\n10.2.3 First Order Phase transitions - nucleation and growth.\nThe form of the system after the transition has taken place may be of more interest for practical applications. For example, we know we can get ice by cooling water below 0\\(^o\\)C and if you are looking to cool your drink, the water in the form of a large ice cube is ideal (‘Scotch on the rocks’) whereas it wouldn’t be nice to ‘eat’ on a hot summers day. In contrast, if we can make ice crystals as small as possible (10’s of \\(\\mu\\)m), we have the basis for making good ice cream. Indeed if the crystals are small enough the texture is creamy without the need for any ‘cream’ at all. Hence, the way in which macroscopic properties through the transition can be controlled may be as important as understanding the transition itself. For example, one favourite method of making ice cream is to use liquid nitrogen. This has the effect, with rapid stirring, of forming the tiny crystals needed for good ice cream. The processing of materials through phase transitions is ubiquitous. It’s perhaps no surprise to see that major industries including food processing, cosmetics, engineering materials are all heavily dependent on our knowledge of what happens as phase transitions take place.\nTo deepen our knowledge of how the transitions take place we need to think beyond the thermodynamics and ask what happens at the microscopic (atomic) level. So what happens to the atoms/particles as a first order freezing transition takes place? To make progress we need to introduce the ideas and theories of the nucleation and growth of crystals at the phase transition temperature. These concepts have been introduced in the theory part of these lectures but let’s thing about what happens in our water/ice system. If I start with liquid water at 0\\(^o\\)C and remove a small amount of heat from the system I expect, from our discussion above, to start forming an ice/water mixture. But how does the first ice crystal start to form?\nThe first thing to note and you may be surprised to learn is that crystallization doesn’t have to take place at all at 0\\(^o\\)C! It is perfectly possible to have stable liquid water below 0\\(^o\\)C even under standard atmosphere and pressure. This is known as supercooling or undercooling. It can also occur at other transitions, for example you can find superheated water above 100\\(^o\\)C. Although supercooled/superheated materials are not the equilbrium states they can stay in this \\(\\textit{metastable}\\) state for many weeks/months on end unless disturbed.\nHence, if we are careful we can cool water to well below 0\\(^o\\)C without it freezing. So what happens as we cool our water below 0\\(^o\\)C and why, in some cases does it not freeze? What observations measurements could we make? Firstly, if it supercools, no first-order phase transition has taken place and there is no release of latent heat. Hence, in heat capacity measurements (DSC) we would expect to see a steady and smooth decrease in temperature. The physical properties of the supercooled liquid water would stay broadly the same - it will still flow and pour as a liquid but we might observe steady changes in, for example, its volume or viscosity (which we will discuss in relation to the glass transition later) as the temperature decreases. But where has the heat (energy) associated with the latent heat of freezing (if the sample had frozen) gone? The answer is it hasn’t gone anywhere, it remains in the water even though the temperature has decreased. In other words there is more energy in supercooled water at -10 \\(^o\\)C than ice at the same temperature. Hence, the metastable state is not the \\(\\textit{global}\\) minimum energy configuration of the system but is at a \\(\\textit{local}\\) energy minimum (i.e. it will return to this local equilibrium after small deviations from the equilibrium state). In order for the global minimum to be reached from the local minimum there is an energy barrier to overcome. In order to freeze from the supercooled state this energy barrier needs to be overcome. The idea that there may be one or more possible local minima for a configuration is known as the \\(\\textit{energy landscape}\\) of the material. This is illustrated conceptually in the figure below. Any configuration that lies above the landscape (the squiggly line) is a possible state of the system. For a given energy the system is rapidly changing configuration by moving to a neighbouring state on the configuration axis (in practise the energy landscape is multidimensional). At high energies (above the blue dotted line) it is possible to reach any configuration of the system in time. This would be equivalent to the normal liquid state. The lowest minimum (green dotted line) represents the equilibrium (minimum energy state) state e.g. ice. Now we imagine starting in the liquid state to the left of the vertical purple line and remove some energy. Once we are below this line we see that we cannot access any states to its right - the global minimum, ice, has become inaccessible. If we continue to remove energy we may find ourselves in the local minimum in this part of configuration space. This could perhaps be the supercooled liquid state. To reach the global minimum we have to overcome the energy barrier separating the local from the global minimum. Natural fluctuations in energy may be sufficient and explain why under normal cicumstances we get to the global minimum. However, if the local minimum is deep enough we get stuck in the metastable state.\n\n\n\nIn principle we could get ‘stuck’ in any of the local minima if we process our material in a certain way.\nSo what happens as we cool the water further - there are two possibilities: it will suddenly freeze to form ice (we find the global minimum energy), or its viscosity becomes so high that the internal molecular motion of the atoms is slowed down so much that the disordered arrangements of the atoms become ‘frozen’ so that it forms a ‘solid’ disordered structure that we call a glass (i.e. we’ve got stuck in a local minimum). In fact, no one has ever been able to produce ‘glassy’ water in this way \\(^*\\) but the transition to a glass on cooling does a occur in many materials. We’ll leave discussion of the glass transition and how we observe it until later.\nSo, if we observe our water as we cool it, we find it will suddenly and rapidly appear to freeze to form ice. If you seacrh for `supercooled water’ on youtube (or other) you will find many videos of this happening. So what is happening? In our supercooled water there will always be small regions that spontaneously start to form the atomic arrangements reminiscent of the crystal you’d expect to see at that temperature. But, as you’ve seen in the theory lectures, there is an energy barrier to overcome before the crystals become large enough to start growing continuously. In other words our embryonic crystals start to grow but dissolve again before they have the chance to grow further. However, if a crystal gets large enough, there is no energy barrier to stop more atoms/molecules attaching to the nascent crystal and it will continue to grow.\nOnce the crystals are macroscopically large we start to see them and can observe the growth. However, there are a few things we should note. Firstly, this spontaneous crystallization (known as \\(\\textit{homogeneous nucleation}\\)), is a ‘local’ event - it could happen anywhere in the liquid. There are many questions that can then be asked. Is it only one crystal that forms and grows, or does homogeneous nucleation take place simultaneously in different parts of fluid at the same time? This poses some very difficult questions and when you investigate further you realise that the simple theory of nucleation and growth is just that - in reality how and where nucleation physically takes place in different materials is hard to predict! In some materials you might see a small number of large crystal form, in others, lots of smaller crystals start to form at the same time. In many cases you see a `front’ of nucleation advancing rapidly from the original nucleation event. We don’t have time in these lectures to explore this fascinating subject but the consequences for materials science (i.e the strength and physical properties of the materials produced) are important.\nThere are still a few things we should note. We know in our example that there is more energy in the supercooled liquid water than the same amount of water in the form of ice. So what happens when an ice crystal starts to grow spontaneously in our supercooled water? As the crystal starts to grow, the latent heat associated with the transition, that has remained in the liquid, must be released, so locally around the forming crystal the temperature of the liquid and the crystal will start to rise. In other words temperature gradients will start to form around the crystal and heat will only leave via transport processes (conduction, convection, possibly radiation) through the liquid. So, as the nucleation takes place the material is far from equlibrium with large variations in temperature present etc. What is the final state of the system once equlibrium has been reached in an isolated system? Well, as we noted, as nucleation takes place, the temperature goes up. So, do we end up with ice at a temperature below zero or a mixture of ice and water, which we know should be at 0\\(^o\\)C (at stp)? The answer is the latter, you end up with an ice/water mixture at 0\\(^o\\)C with the ratio of ice to water dependent on the level of supercooling. The more deeply supercooled the greater the proportion of ice to water.\nExercise. Look up the latent heat of freezing for water and estimate the temperature at which supercooled water needs to be if it was form only ice (i.e. no water) at 0\\(^o\\)C on nucleation.\nFrom the discussion above you can see that the local and microscopic nature of the nucleation processes make precise experiments difficult. The results will depend, on the size of the sample, the rate of heat transfer through the sample, the randomness, of the nucleation processes, whether the process was at fixed pressure or constant volume (remember ice at stp has lower density than water and think about the implications if the process takes place at constant volume) etc.\nSo how does a first order transition from a supercooled state manifest itself in a calorimetry experiment, for example in a DSC? Well, if the transition takes place at the freezing temperature we’d expect to see the temperature of the sample remain at the transition temperature until the transition is complete, at which point the temperatures starts to drop steadily again at a rate corresponding to the heat capacity of the material. If however the material supercools its temperature will decrease steadily through the the transition temperature (no transition takes place). When the material eventually nucleates and crystallizes you observe a sudden rise in temperature (back to the transtion temperature) where it stays until the transition is complete and the temperature decreases as before. The deeper the supercooling the bigger the temperature jump. As noted above, the precise shape of the reheating peak observed depends on the sample size, thermal conductivity etc.\nThe sudden reheating of a material from a supercooled state is called \\(\\textit{recalescence}\\).\nExercise. Recalescence is not confined to liquid/solid/gas transitions. It may occur for any first order transition, for example, the austenitic transition in stainless steel which is of commerical importance. Can you find other examples.\nFinal comments about 1st order phase transitions.\n\nSupercooling is often difficult to achieve. The reason is that in real situations we need to hold the material in containers in which there might be impurities or specks of dirt etc. These impurities can act very much like embryonic crystals that form in homogeneous nucleation and they lower the free energy barrier for nucleation. Processing materials under very clean conditions in smooth walled containers is very difficult. Nucleation on impurities is known as \\(\\textit{heterogeneous}\\) nucleation and is often the process most observed unless great attention is taken to the preparation method.\nIf there is a known transition and neither the transition or recalescence is observed in for example a DSC scan, then it is possible that the material formed a glass. This is discussed below.\n\n\\(^*\\) Although it is possible to make amorphous ice - a topic great interest but we don’t have time to study it here.\n\n\n10.2.4 Phase Separation in mixtures - spinodal decomposition.\nIn the previous section we saw that a first order transition doesn’t necessarily take place at the transition temperature. That is, rather than observing two phases in coexistence at the transition temperature, the material may remain in a metastable supercooled or superheated single state. The crucial observation is that for the transition to start, a nucleation event needs to take place, from which the second phase will grow. The next question you might ask is what happens if I continue to cool the material? Firstly, as the temperature difference increases the likelihood of homogeneous nucleation (and recalescence) increases. However, there are other possibilities, the material may form a glass (which we will discuss shortly) or we reach the point where spinodal decomposition takes place. You have discussed spinodal decomposition in the theoretical section of this course, but briefly there comes a point where the barrier to the separation disappears and the formation of two distinct phases occurs throughout the material (it does not rely on a nucleation centre). This is especially true if you are below and close to the critical point. This is a dynamic process as the size of the different regions grows over time - eventually two distinct macroscopically large phases appear. Spinodal decomposition, has an important role to play in material processing as it has an effect on the microsctructure of the material produced. If the material is quenched fast the growth of the separate regions that form will be rapidly arrested leading to a fine grained solid. If the quench is slow the resulting structure will be coarse grained. This graining of the microstructure will affect the physical properties of the material produced. This behaviour is most associated with phase transitions that involve the separation of components in composition, such as metallic alloys and polymer blends rather than pure liquid/gas, liquid/solid transitions in which spinodal decomposition is far harder to observe.\n\n\n10.2.5 The Glass Transition.\nIn the section above we discussed how, particularly in the case of compositional phase separation, the microstucture of a material may be related to the speed in which it is quenched. As a consequence the end point is a material that is composed of a mixture of different phases rather than a single phase - the result of a true first order phase transition. So, what happens if we manage to cool without nucleation or spinodal decomposition occuring? i.e. what happens if we rapidly quench the sample to the extent that the dynamics driving for example spinodal decomposition are ‘frozen’ out. If we manage to slow the diffusion of the atoms enough, then the atomic re-arrangements needed to cause crystallization or spinodal decomposition cannot take place and the atoms/molecules appear to be fixed in place, but with a disordered arrangement. However we note that overall the material remains homogeneous and uniform - there is no microstructure. As in the case of supercooled liquids, this is not the equilibrium state, but another form of metastable state known as a glass. An example, is typically the material in our windows, `glass’, that is the result of cooling liquid silicates. Not all materials readily form glasses and this is often expressed loosely as the glass forming ability of the material. A good glass former is a material that can be cooled quite slowly, a poor glass former is one that needs to be quenched fast (there are more complex definitions but this is the gist). A characteristic of the glass is that no transition appears to take place in the material as it is quenched. There is no latent heat released, there is no recalescence and there is no evidence of phase separation (microstructure) in the material produced. In other words we imagine the glass forming from the supercooled liquid. So what do we mean by the glass transition and the glass transition temperature \\(T_g\\)? To answer this question, it is interesting to ask what happens as we heat our glass up? If, all we had was a very very high viscosity (behaving like a solid), but metastable supercooled liquid phase, then we would eventually return to the true liquid phase (above the melting point) without any evidence of a phase transition taking place (unless nucleation and recalescence took place). However, when we heat glass, we do see heat evolved at a temperature below the melting point (we’ll see this later on our summary of thermal analysis). For any given glass this occurs consistently at roughly the same temperature (but notably not well defined) and the material will start to change into a more viscous, `rubbery’ state. This is known as the glass transition. It is not a true thermodynamic transition and there are many ways in which it may defined (all occuring at roughly the same temperature). It depends on cooling/heating rate and can be observed in a change in for example viscosity, thermal expansion coefficients, heat capacity etc. It also depends on the thermal history of the material.\nIn summary, the existence of a glassy state of matter is readily apparent. However, the transformation of a liquid into a glass and how and why the structure changes on heating/cooling at the glass transition are still very poorly understood and the subject of much research. Indeed, there is no universal definition of the glass transition despite many attempts to define it in a rigorous thermodynamic way.\nThe terms glassy and amorphous materials are often used synonymously. Stricter definitions state that a (true) glass is formed by quenching from the liquid state whereas an amorphous solid includes materials that have been formed by for example deposition from a vapour, preciptation from a solution, mechanical processing. Amorphous materials also show a glass transition temperature but the manner in which they form is more complicated to describe theoretically.\n\n\n10.2.6 Second Order Phase transitions\nIn the discussion above we charaterised a first order phase transtion as one in which latent heat is absorbed/emitted. i.e. one in which two phases coexist at the transition temperature and the temperature stays constant until the transition is complete. However, we can also identify other transitions in which no latent heat is associated with the change in phase taking place. Typical examples might be the liquid-vapour transition at the critical point or the ferromagnetic transition. The key point is that the transition between the two states is continuous. For example, in the ferromagnetic transition the magnetization is zero above the transition and increases steadily from zero below the transition. In this case it is the discontinuity in slope of the magnetization vs. temperature that is the signature of the transition taking place. In other cases a discontinuity in the gradient of the temperature dependence of the heat capacity is observed (while the heat capacity itself is continuous across the transition). These transitions have been discussed in the the theory section of this course and their beauty is the observation that the behaviour for a variety of supposedly very different physical transitions may be characterised in terms of universal behaviour. The \\(\\textit{universality class}\\) of the transition is characterised by its critical exponents. You might note that many attempts have been made (unsuccessfully) to describe the glass transition in terms of an underlying second order phase transition.\nExperiments to determine, for example, the critical exponents of the transition (rather than observing the transition itself) are extremely difficult. Precise control and accuracy of the temperature measurements is needed, great care must be taken to make any temperature gradients in the material as small as possible and in order to get accurate values of the critical exponents you need to get very close to the transition temperature.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#examples-of-macroscopic-measurements.",
    "href": "experimental/part1.html#examples-of-macroscopic-measurements.",
    "title": "10  Macroscopic measurements",
    "section": "11.1 Examples of macroscopic measurements.",
    "text": "11.1 Examples of macroscopic measurements.\n\n11.1.1 Differential Thermal analysis.\nDifferential Thermal Analysis (DTA) methods are now a common and routine way of establishing the existenceo of phase transitions in materials in both a qualitative and quantitative way. The methods are all based on the same principle, namely the response to heating/cooling of the material under study is compared to that of a known calibration material that does not have any phase transitions in the region of study. I describe the two most common forms below.\n\n11.1.1.1 Heat flux DSC (Boersma DTA)\nThis is the simplest thermal analysis technqiue. Two materials, a sample and a reference are heated together in a single furnace as shown below.\n\n\n\nThe temperature of the two materials is monitored carefully as the furnace is heated steadily. The reference is known not to show any phase transitions over the temperature range studied and will therefore heat steadily. If there is a phase change in the sample on heating/cooling, it will show as a temperature difference between the sample and reference. If the change is endothermic the sample will be at a lower temperature until the transition is complete after which the sample will catch up to the base temperature of the furnace, if the change is exothermic its temperature will increase until the transition is complete and then return to the furnace temperature. If the mass of the sample and reference is known and the heat capacity of the reference known, quantative values of, for example, latent heat, can be obtained. In quantitative measurements it is sometimes known as a heat flux DSC. A schematic of a heat flux DSC trace is shown below.\n\n\n\n\n\n11.1.1.2 Heat flow DSC\nThis technique is similar to heat flux DSC but the sample and reference are located in their own separate furnaces (it is a double furnace technique). The power needed to steadily heat the sample and reference (and hence the heat added to the system) is controlled in such a way as to keep the temperature of the sample and reference the same.\n\n\n\nThe difference in power supplied to the furnaces indicates when a transition takes place and with the mass of the reference and sample allows the energy absorbed/released in the transition be determined. If on heating there is a first order phase transition then more power is needed to overcome the latent heat to maintain the temperature. If an endothermic transition takes place then the power needed would decrease. The output from the instrument is usually presented as the heat flow to the sample as the temperature is raised. The trace is very similar in form to the heat flux DSC above except that the peaks and troughs observed are the opposite directions. (Increased heat flow into the sample is equivalent to the temperature of the sample being lower in the heat heat flux DSC). With computer control and a carefully designed and insulated chamber accurate measurements of latent heat etc. may be made.\n\n\n11.1.1.3 Practical considerations\nIn use, practical questions involve the rate at which you wish to heat/cool the sample (transitions don’t take place instantaneously) and the sensitivity to small changes you are tryinh to observe. One common extension to the technique is \\(\\textit{Temperature ~ Modulated ~ DSC}\\). In this case the heating/cooling may be modulated (e.g. stopped or reversed periodically. Application of this method allows the kinetics of the transition in the DSC to be observed. In particular, it can distinguish between a reversible second order phase transition and an irreversible transition such as the glass transition.\nA typical DSC analysis would involve monitoring the sample as it is heated to some target temperature, for example, to above its melting point. (The limit would be set by the maximum temperature limit of the apparatus, the material stability (does it breakdown/oxidise) or the onset of a reaction between sample and container). If the sample was initially in a non equilibrium state (glassy, supercooled so you pass through \\(T_g\\) or recalescence takes place) then you do not expect to see transitions at the same place in the heating/cooling cycles. Similarly, observations of the differences in the DSC trace of materials with different thermal histories will be different.\n\n\n\n11.1.2 Other techniques (non exhaustive)\nDifferential thermal analysis is not the only method for studying phase transitions. There are many other ways, quantitative and qualitative that may be used. Here are a few examples.\n\\(\\bf{Thermal ~ mechanical ~ analysis ~ (TMA)}\\). In this case the mechanical properties (modulus) of the material is observed on heating/cooling. For example, on heating, at the glass transition temperature, a glass will start to soften. Note, the glass transition temperature measured in this way is often slightly different to that measured by DSC and is indicative of the vagueness of the definition of \\(T_g\\) itself!\n\\(\\bf{Thermogravimetric ~ analysis}\\). Observation of changes in sample mass with temperature.\n\\(\\bf{Imaging}\\). It is quite possible to observe (and hence video) the phase transition taking place and getting some insight into the mechanism of the transition taking place.\n\\(\\bf{Magnetic ~ susceptibilty}\\). To observe transitions in para to ferro/antiferro magnetic transitions. (e.g. Curie and Néel points),\n\\(\\bf{Electrical ~ Conductivity}\\). In addition to structural changes in a material changes in other properties, for example, the electronic conductivity may also take place. For example, some chalcogenide (S,Se,Te) based materials are good glass formers and have very high resistivity (low conductivity) in the glassy state. By contrast, in the crystalline state they have high conductivity. The associated glass and melting transitions may be observed easily by measuring resistance. Indeed, the glass transition in these materials, is exploited in non-volatile memory devices.\n\\(\\bf{Pyrometric ~ measurements}\\). This is an example from my own research on oxide glasses (that melt in excess of 2000K). In this case, the glass transition, recalescence, and freezing my all be observed in a semi-quantitative way by measuring the temperature of a cooling sample as a function of time.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#keypoints",
    "href": "experimental/part1.html#keypoints",
    "title": "10  Macroscopic measurements",
    "section": "11.2 Keypoints",
    "text": "11.2 Keypoints\nWe have taken some time here to discuss various aspects about phase transitions as macroscopically observable changes in a material. We have introduced the ideas of\n\nfirst and second order phase transitions,\nnucleation and growth,\nsupercooling and superheating,\nspinodal decomposition,\nthe formation of glasses\nandhave given a brief introduction as how to observe and measure them with especial emphasis on differential thermal analysis techniques.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html",
    "href": "experimental/part2.html",
    "title": "11  Measurements in real space",
    "section": "",
    "text": "11.1 A simple description of \\(g(r)\\)\nIn the theory part of the course you have been introduced formally to the radial distribution function \\(g(r)\\) and seen how it is a useful description of the arrangement of particles in a disordered system. In particular, in the absence of the long range periodic order that is associated with crystalline materials it is one of the first things we wish to find for liquid or glassy material. In our examples we assume that we are looking at a uniform and isotropic material, that is, there is no preferred direction in the material and no density variations on length scales much greater than the particle size. That is, macroscopically the material appears is of uniform density. It is also possible to give a more general definition \\(g(\\bf{r})\\) where we recognise that there is some preferred orientation of the atoms around each other in the material but we will not consider it here.\nIn layman’s terms the radial distribution function is often described loosely as the probability of finding another particle at a distance from a particle at the origin. If we think about it this way we must remember that \\(g(r)\\) is a statistical function so what we are really trying to express is the average of this distribution taking each atom in the material in turn as the ‘origin’. Expressed in this way you might worry about what this means for atoms on the ‘edge’. Normally this is not a concern for large samples, but beware about the definition of \\(g(r)\\)if you start approaching ‘nano-sized’ materials.\nA better definition is that \\(g(r)\\) represents the deviation from the mean particle density, normalised to one, as you move outward from a particle at the origin. At large \\(r\\) we expect this to equate to a constant, related to the mean particle density of the material. When properly normalised to the mean density \\(g(r) _{r\\rightarrow \\infty} \\rightarrow 1\\). The figure below shows the key features of a typical radial distribution function.\nPoints to note are",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#a-simple-description-of-gr",
    "href": "experimental/part2.html#a-simple-description-of-gr",
    "title": "11  Measurements in real space",
    "section": "",
    "text": "that \\(g(r) = 0\\) for a distance below a ‘cutoff’ distance. This recognises that our particles have a finite size and when we mean ‘position of particle’ we mean its centre. Hence this cut off represents the diameter of the particles. What we mean by diameter of particle and what this means for \\(g(r)\\) we will discuss shortly.\n\\(g(r)\\) typically rises from 0 to give a ‘first peak’. The position of this peak is referred to as the nearest neighbour distance. (NB. it is not the same as the diameter of the particles).\nAfter the nearest neighbour peak, \\(g(r)\\) typically dips below the mean density and is characterised by damped oscillating peaks and troughs until \\(g(r)\\) reaches the constant mean density.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#what-can-we-learn-from-gr",
    "href": "experimental/part2.html#what-can-we-learn-from-gr",
    "title": "11  Measurements in real space",
    "section": "11.2 What can we learn from \\(g(r)\\)?",
    "text": "11.2 What can we learn from \\(g(r)\\)?\n\n11.2.1 Peak positions.\nThe peaks observed in \\(g(r)\\) correspond to the most common interparticle seperation. For an atomic system, they can typically be associated with chemical bonds and as you move further in \\(r\\) the \\(\\textit{medium}\\) (i.e. next nearest neighbour …) range ordering. However, we must remember that this is only a pair corrrelation function. It does not \\(\\it{per~se}\\) tell us about for example bond angles. These are formally be obtained from higher order correlation functionz (triplet, …) but these are not obtainable directly from \\(g(r)\\) itself.\nNote. If we have a crystalline material the peaks in \\(g(r)\\) will be narrow (they will have an intrinsic width at least due to thermal vibration) and at low values of \\(r\\), \\(g(r)\\) will be zero between them. However, even in a crystalline material these peaks will start to broaden and get smaller at high \\(r\\) until, again, \\(g(r)\\) reaches mean density.\n\n\n11.2.2 Coordination numbers.\n\\(g(r)\\) is a dimensionless function that normalises to a mean density of one at large \\(r\\). For a material with a \\(\\textit{number~density}\\), \\(\\rho_N\\), the mean number density at distance \\(r\\) is hence \\(\\rho_N g(r)\\). A thin spherical shell of thickness \\(\\Delta r\\) at a distance \\(r\\) will have a volume \\(4\\pi r^2 \\Delta r\\) so that the mean number of atoms in the shell will be,\n\\(4\\pi r^2 \\rho_N(r) \\Delta r\\).\nHence the mean number of atoms in a spherical shell of inner radius \\(r_1\\) and outer radius \\(r_2\\) will be given by,\n\\(\\bar{n} = 4\\pi\\rho_N\\int_{r1}^{r2}r^2g(r)dr\\).\nIf we define a peak in \\(g(r)\\) by \\(r_1\\) and \\(r_2\\) we call \\(n\\) the coordination number of the particles in the peak.\n\n\n\nIt is important to note that the coordination number is an average (\\(g(r)\\) is a statistical distribution) and in general will vary particle by particle in the material. It does not have to be integer and unless \\(g(r_1)=g(r_2)=0\\) it is an ill defined quantity. Nevertheless, it is a commonly quoted number in research papers so it is important to read carefully how it has been caclulated to obtain meaningful conclusions that can be compared to figures in other research work.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#how-to-we-measure-gr-directly-for-a-material",
    "href": "experimental/part2.html#how-to-we-measure-gr-directly-for-a-material",
    "title": "11  Measurements in real space",
    "section": "11.3 How to we measure \\(g(r)\\) directly for a material?",
    "text": "11.3 How to we measure \\(g(r)\\) directly for a material?\nIn order to measure \\(g(r)\\) directly, we need to obtain a 3D (unless we are looking at a 2D system) image of the material in which all the particle coordinates \\(r_i(x,y,z)\\) can be obtained. Although some modern techniques can obtain images with atomic resolution (for example scanning tunneling microscopy) they are mostly limited to surface imaging and not suitable for studying liquids (where atoms are moving around). Although some X-ray and electron microscopy techniques (see for example ptychography) can produce atomic scale resolution (using coherent imgaing techniques) these are again largely confined to 2D surfaces and are again not suitable for atomic liquids.\nHistorically (1950-60s) the first attempts to determine \\(g(r)\\) came from measurement of the packing of real particles made by literally constructing disordered structures with balls and sticks, gelatine balls, plasticine etc. and then painstakingly finding the particle coordinates during a careful and systematic disassembly. Needless to say this wasn’t a particularly accurate method but it did reveal, what suprised many at the time, dense packing of particles in liquids was characterised by 5-fold coordination.\nToday there is much literature on the study of disorder and phase changes in colloidal systems (particles of the order of 100s of nanometres in size and greater) and we will take a look at some of the methods used to do these experiments. The spirit of these experiments is the same - we try to measure the positions of the particles in space and ideally also over time (to see how they move).",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#the-structure-of-colloidal-systems.",
    "href": "experimental/part2.html#the-structure-of-colloidal-systems.",
    "title": "11  Measurements in real space",
    "section": "11.4 The structure of colloidal systems.",
    "text": "11.4 The structure of colloidal systems.\nColloids are complex systems composed of mesoscopic particles suspended in a host liquid. They and emulsions (liquid drops in a host liquid) are found everywhere around us. They are used extensively in food processing and paints etc. Much work has been done to stabilize and control their properties but here we will look briefly at how they have been used to study phase transitions, glasses and particle coordination in ‘simple’ systems.\nIn its simplest form a colloid consists of small (mesoscopic) particles suspended in a solvent. However, if you simply mix and stir say small spherical particles in a solvent you will often observe \\(\\textit{flocculation}\\). Fundamentally, there is an attractive force, called the depletion force, that is driven by the excluded volumes (and associated entropy) around hard spheres as they come into contact. That is, the particles tend to clump together into larger aggregates. So, in order to produce a homogeneous phase of uniform particle density, the colloid needs to be \\(\\textit{stabilised}\\). The stabilization of colloids and emulsions is a vast subject, but in short, by adding additional chemicals (e.g. surfactants) to the solvent it is possible to introduce some repulsion between the particles that overcomes the tendency to flocculate. The type and concentration of the added chemicals causes a change in the electrostatic attraction/repulsion between the particles and gives rise to the zeta (\\(\\zeta\\)) potential. To mimic ideal `hard sphere’ behaviour the aim is to have a colloid with a zeta potential of zero. i.e. there is no net attraction or repulsion (except the hard sphere surface itself) between the particles.\nThere are other ways to stabilize colloids for example, steric repulsion. In this method long chain molelcules are incorporated into the particles to make them `hairy’. The presence of these hairs reduces the excluded volume around the particles and hence reduces the depletion force between them.\nFrom here on we will assume that we are able to create such an ideal hard sphere colloid! For a comphrensive and detailed review of studies of colloidal hard sphere systems see the review of Royall et. al. 2024",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#how-to-we-obtain-gr-and-information-about-coordination-from-colloid-systems.",
    "href": "experimental/part2.html#how-to-we-obtain-gr-and-information-about-coordination-from-colloid-systems.",
    "title": "11  Measurements in real space",
    "section": "11.5 How to we obtain \\(g(r)\\) and information about coordination from colloid systems.",
    "text": "11.5 How to we obtain \\(g(r)\\) and information about coordination from colloid systems.\nIf we have our ideal colloid there are several things we may wish to control. For example, what is the size of the particles (there will be a tendency for large particles to sediment), are the particles all the same size (polydispersivity), can we make systems with different shapes (e.g. ovoid), can we make systems with mixed particle sizes, can we introduce directional interactions (Janus particles) and how do you control the number density of the particles (the density of the particles is the crucial parameter for describing the hard sphere phase diagram)? These all pose theoretical and experimental difficulties of their own. Finally, once we’ve prepared our colloid with a controlled density … how do we study its structure (and dynamics)?",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#confocal-microscopy.",
    "href": "experimental/part2.html#confocal-microscopy.",
    "title": "11  Measurements in real space",
    "section": "11.6 Confocal microscopy.",
    "text": "11.6 Confocal microscopy.\nThere are a few further steps we need to achieve to be able to measure and track the positions of the particles in our hard sphere colloid. Firstly, it is important that the particles are transparent, otherwise, we will not be able to see into the depth of the sample and secondly we need to match, as closely as possible, the refractive index of the solvent. If we don’t do this the difference in refractive index will mean light is quickly scattered in all directions meaning we will just have an opaque material. However, by making the particles all but invisible how to we determine their positions? The trick is to include a fluorescent dye their centre. When illuminated with the correct wavelength of light (using a laser) the particles will glow. This works well but how do we measure the positions of the particles in 3D. The answer is to use a confocal microscope as illustrated below (figure take from Royall et. al.2024).\n\n\n\nIn the confocal microscope laser light is passed through a small pinhole and then brought to focus at a point in the sample by a lens, known as the condenser lens. The fluorescent light coming back from the illuminated samples is then focused by the objective lens (which in this case is the same lens as the condenser) and the microscopy image is obtained by the camera. However, although the laser light is brought to a tight focus in the sample there are nevertheless still regions that are illuminated by the laser that, under normal circumstances, would also bring unwanted light into the camera. The key point of the confocal system is that a second pinhole is placed precisely at the focus (hence confocal) of the objective lens for the light coming from the sample. The effect of this pinhole is to reduce the volume of the sample that is illuminated and then seen by the camera. The method is good enough that the fluorescence from individual particles can be observed clearly and the ultimate ability to resolve the individual particles is given by the diffraction limit imposed by the wavelength of the laser light used. (There are also ways in which this has been overcome to an extent in practical systems). The figures below, also taken from Royall et. al. 2024 show examples of confocal images used to map out the phase diagram of the hard sphere system.\n\n\n\nNote, moving the sample up and down changes the ‘z’ direction and a three dimensional image may be obtained by ‘stacking’ vertically scanned 2D images.\nOnce the particles positions have been determined it is possible to plot \\(g(r)\\) for the different packing densities to show how it varies over the phase diagram as shown below\n\n\n\nAs mentioned previously, the early attempts at understanding the packing of particles in disordered materials came to te conclusion that 5 fold coordination (inconsistent with the long range order required for crystals) was the characteristic of simple dense packed structures. In the same spirit as these early experiments the results from confocal imaging have allowed the prevalence of different types of local ordering to be investigated. This leads to the mapping out the \\(\\textit{topological cluster distribution}\\) (i.e. the different types of local structure) that occur in the system. As noted before, these higher order correlations may be calculated from the particle positions (once we have them) but cannot be determined unambiguously from measurements of \\(g(r)\\) alone.\n\n\n\nThese are just a few examples of the areas studied using colloids and confocal microscopy techqniues.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#calculation-of-gr-directly-from-simulation",
    "href": "experimental/part2.html#calculation-of-gr-directly-from-simulation",
    "title": "11  Measurements in real space",
    "section": "11.7 Calculation of \\(g(r)\\) directly from simulation",
    "text": "11.7 Calculation of \\(g(r)\\) directly from simulation\nAlthough simulation is not strictly an experimental method, it is neither a truly theoretical method either. This said, from the experimentalist point of view, especially in the case of the colloind systems studied above, they provide a direct comparison in real space with the data. As such the experiemntal results are an important check of the veracity of the simulation methods especially with regard to the application of for example theoretical potentials. The simulation methods naturally exploit the interaction of the particles in real space (we know the positions of the particles and we calculate how they move under the influence of the interparticle forces. Hence, it is quite straight forward to compare the real space configurations measured in experiment with the particle coordinates in the simulation. The comparison between the experiment and simulation and the level of agreement that can be achieved can be strikingly seen in the comparison of \\(g(r)\\) for hard spheres shown above. Simulations however are not the answer to everything. They get more and more computer intensive as larger numbers of particles are included and the time scales over which they are carried out are extremely short compared to those in experiments. The latter is a particular issue with studies of the glass transition (as always) where the physical processes taking place may occur over a very wide range of timescales.\nThe comparison of simulation results with experimental diffraction data is more difficult. Diffraction experiments are a reciprocal space measurement and to compare to simulation we need to work out the corresponding real space (\\(g(r)\\) ) distributions. Hence when comparing experiment with simulation we have to choose whether to transform our reciprocal space data to real space (with associated problems) or convert the real space data from our simulations to reciprocal space for comparison (that also has problems). This is the subject of the next part of this course.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#dynamics-in-real-space",
    "href": "experimental/part2.html#dynamics-in-real-space",
    "title": "11  Measurements in real space",
    "section": "11.8 Dynamics in real space",
    "text": "11.8 Dynamics in real space\nConfocal microscopy experiments are not only useful for static measurements of \\(g(r)\\) but also open up the possibility for studying the dynamics of the system (how the particles move over time), that is, measurement of the time dependent pair correlation function \\(G(r,t)\\) I’ll not say much about dynamics at the present time but will leave it to the last part of this section of the course. Needless to say the process involves taking confocal images of the particles over time scales in which they will move only slightly from their initial position. With sophisticated software it is then possible to label the particles and track their position and hence coordinates in succesive image frames. This work well if you are in the sweet spot for the measurements but is a challenge, for example in glass transition experiments, where the experimental timescales become longer and longer as you approach the conditions for the glass transition to take place. This is still a very active field.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#summary-and-keypoints",
    "href": "experimental/part2.html#summary-and-keypoints",
    "title": "11  Measurements in real space",
    "section": "11.9 Summary and keypoints",
    "text": "11.9 Summary and keypoints\nIn this section we have considered the experimental determination of \\(g(r)\\).\n\nWe have seen that this is not realistically possible for atmonic systems,\nwe have seen how colloid systems are ideal systems for studying the properties of hard-sphere and closely related systems on the 100nm scale,\nwe have seen how \\(g(r)\\) for colloids may be obtained from the coordinates obtained from confocal microscopy,\nwe have seen how \\(g(r)\\) and higher order correlations may be obtained from these coordinates,\nwe have seen how we can extend the confocal microscopy method to study particle dynamics.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  }
]