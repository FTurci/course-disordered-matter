[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "",
    "text": "Welcome to the Complex Disordered Matter Course!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Overview",
    "text": "Overview\nThis course introduces your to the theoretical, computational and experimental aspects of the physics of complex disordered matter.\n\nComplex disordered matter is a central theme in soft matter physics, encompassing systems such as polymers, colloids, glasses, gels, and emulsions that lack long-range order but display intricate and often tunable behavior. Colloids—suspensions of microscopic particles in a fluid—serve as versatile model systems for studying disordered structures and phase behavior because their particle-scale dynamics are directly observable. Similarly, polymer systems and colloidal suspensions can form amorphous solids, or glasses, when densely packed or cooled, exhibiting solid-like rigidity despite their disordered, liquid-like microscopic structure. These materials frequently undergo equilibrium and non-equilibrium phase transitions such as demixing, crystallisation, gelation, vitrification, or jamming, and near these transitions, they may show critical phenomena where fluctuations and correlations span many length scales.\nIn soft matter systems, the interplay between disorder, softness, and phase behavior leads to rich physical phenomena, particularly near critical points where even small changes in external conditions can trigger large-scale reorganizations and universal behaviour. Glasses, for instance, exhibit slow relaxation and memory effects, while colloidal systems may crystallize, phase separate, or become jammed depending on particle interactions and concentration. Understanding such behaviors involves studying how microscopic interactions and thermal fluctuations influence macroscopic properties, especially in non-equilibrium conditions. Through techniques like scattering, microscopy, rheology, and simulation, one can explore how disordered soft materials respond to stress, age, or undergo transitions—insights that are vital for applications in materials design, biotechnology, and beyond.\nThis course is organized into three interconnected parts, each offering a distinct perspective on the study of complex disordered matter.\n\nPart 1: Unifying theoretical concepts (Nigel Wilding) introduces the theoretical framework for rationalising complex disordered matter which is grounded in statistical mechanics and thermodynamics. We emphasize the theory of phase transitions, critical phenomena, and stochastic dynamics—providing the essential tools needed to describe and predict the behavior of soft and disordered systems.\n\nPart 2: Complex disordered matter (Francesco Turci) explores the phenomenology of key examples of complex disordered soft matter systems, including colloids, polymers, liquid crystals, glasses, gels, and active matter. These systems will be analyzed using the theoretical concepts introduced in Part 1, highlighting how disorder, interactions, and fluctuations shape their macroscopic behavior.\n\nPart 3: Experimental techniques (Adrian Barnes) focuses on the methods of microscopy, and scattering via x-rays, neutrons and light that are used to study complex disordered matter, offering insight into how their properties are measured and understood in real-world contexts.\n\nIn addition to theory and experiment, computer simulation plays a central role in soft matter research. This course includes a substantial coursework component consisting of two computational projects. These exercises will allow you to apply state-of-the-art simulation techniques to investigate the complex behavior of disordered systems, bridging theory and observation through hands-on exploration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#delivery-and-format",
    "href": "index.html#delivery-and-format",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Delivery and format",
    "text": "Delivery and format\n\nDetailed e-notes (accessible via Blackboard) can be viewed on a variety of devices. Pdf is also available.\nWe will give ‘traditional’ lectures (XXday, YYYday) in which we use slides to summarise and explain the lecture content. Questions are welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to the explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are problem classes (XXXday) where you can try problem sheets and seek help. Lecturers will go over some problems with the class.\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Intended learning outcomes",
    "text": "Intended learning outcomes\nThe course will\n\nIntroduce you to the qualitative features of a range of complex and disordered systems and the experimental techniques used to study them.\nProvide you with elementary computational tools to model complex disordered systems and predict their properties\nAllow you to apply your physics background to understand a variety of inter-disciplinary subjects\nConnect with the most recent advances in the research on complex disordered matter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Contact details",
    "text": "Contact details\nThe course will be taught by\n\nProf Nigel B. Wilding (unit director): nigel.wilding@bristol.ac.uk\nDr Francesco Turci: F.Turci@bristol.ac.uk\nDr Adrian Barnes: a.c.barnes@bristol.ac.uk",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#questions-and-comments",
    "href": "index.html#questions-and-comments",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Questions and comments",
    "text": "Questions and comments\nIf you have any questions about the course, please don’t hesitate to contact the relevant lecturer, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025/26. If you find any errors or mistakes or something which isn’t clear, please let us know by email, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "phase-transitions/literature.html",
    "href": "phase-transitions/literature.html",
    "title": "Literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course is the absence of a wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking. To this end perhaps the most appropriate textbooks are:\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\nFor revision on thermodynamics and statistical mechanics\n\nF. Mandl: Statistical Physics.\n\nFor Stochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry",
    "crumbs": [
      "Unifying concepts",
      "Recommended texts"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html",
    "href": "phase-transitions/precourse-reading.html",
    "title": "1  Ensembles and free energies",
    "section": "",
    "text": "1.1 Microcanonical ensemble\nApplies to a system of \\(N\\) particles (or spins) in a fixed volume \\(V\\) having adiabatic walls so that the internal energy \\(E\\) is constant. Denoted as constant-\\(NVE\\). Let \\(\\Omega\\) be the number of (micro)states having the prescribed energy:\n\\[\n\\Omega=\\sum_\\textrm{all states having energy E}\n\\]\nThermodynamically, the states favored in the canonical ensemble are those that maximise the entropy:\n\\[\nS=k_B\\ln \\Omega\\:.\n\\]\nwhere \\(k_B\\) is Boltzmann’s constant The microcanonical ensemble is useful for defining the entropy, but is little used in practice.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Precourse reading and revision</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#canonical-ensemble",
    "href": "phase-transitions/precourse-reading.html#canonical-ensemble",
    "title": "1  Ensembles and free energies",
    "section": "1.2 Canonical ensemble",
    "text": "1.2 Canonical ensemble\nApplies to a system of \\(N\\) particles in a fixed volume \\(V\\) and coupled to a heat bath at temperature \\(T\\). Denoted as constant-\\(NVT\\). A central quantity is the partition function\n\\[\nZ_{NVT}=\\sum_\\textrm{ all states i}e^{-\\beta E_i},~~~~~\\beta=1/(k_BT)\n\\tag{1.1}\\] which is a weighted sum over the states. The partition function provides the normalisation constant in the probability of finding the system in a given state \\(i\\).\n\\[\nP_i=\\frac{e^{-\\beta E_i}}{Z_{NVT}}.\n\\tag{1.2}\\]\nThe states favored in the canonical ensemble are those that minimise the free energy:\n\\[\nF_{NVT}=-\\beta^{-1}\\ln Z_{NVT}\\:.\n\\] \\(F_{NVT}\\) is known as the Helmholtz free energy. Thermodynamics also supplies a relation for the Helmholtz free energy:\n\\[\nF_{NVT}=E-TS\\:,\n\\] where \\(E\\) is the average internal energy. In minimising the free energy, the system strikes a compromise between low energy and high entropy. The temperature plays the role of arbiter, favouring high entropy at high \\(T\\), and low energy at low \\(T\\). The canonical ensemble is usually used to describe systems such as magnets, or a fluid held at constant volume. It is the ensemble we shall use most in this course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Precourse reading and revision</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#grand-canonical-ensemble",
    "href": "phase-transitions/precourse-reading.html#grand-canonical-ensemble",
    "title": "1  Ensembles and free energies",
    "section": "1.3 Grand canonical ensemble",
    "text": "1.3 Grand canonical ensemble\nApplies to a system with a variable number of particle in a fixed volume \\(V\\) coupled to both a heat bath at temperature \\(T\\) and a particle reservoir with chemical potential \\(\\mu\\) (which is the field conjugate to \\(N\\)). Denoted as constant-\\(\\mu VT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty e^{\\beta\\mu N}Z_{NVT}\n\\] and a state probability analogous to Equation 1.2 holds. One can recast this in a form similar to Equation 1.1:\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty\\:\\sum_{\\rm i}e^{-\\beta {\\cal H}_i},\n\\tag{1.3}\\] where \\({\\cal H}_i=E_i-\\mu N\\) is the form of the Hamiltonian in the grand canonical ensemble.\nStatistically, the states favored in the grand canonical ensemble are those that minimise the free energy:\n\\[\nF_{\\mu VT}=-\\beta^{-1}\\ln Z_{\\mu VT}\n\\] \\(F_{\\mu VT}\\) is known as the grand potential. It can also be derived from thermodynamics, from which one finds\n\\[\nF_{\\mu VT}=E-TS-\\mu N=-pV\n\\]\nThe grand canonical ensemble is usually used to describe systems such as fluid connected to a particle reservoir. Sometimes for a magnet we consider the effects of an applied magnetic field, which is analogous to working in the grand canonical ensemble: the magnetic field (which is conjugate to the magnetisation) plays a similar role to the chemical potential in a fluid.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Precourse reading and revision</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#isothermal-isobaric-ensemble",
    "href": "phase-transitions/precourse-reading.html#isothermal-isobaric-ensemble",
    "title": "1  Ensembles and free energies",
    "section": "1.4 Isothermal-isobaric ensemble",
    "text": "1.4 Isothermal-isobaric ensemble\nApplies to a system with a fixed number of particles \\(N\\) that is coupled to a heat bath at temperature \\(T\\) and a reservoir that exerts a constant pressure \\(p\\) which allows the sample volume to fluctuate. Denoted as constant-\\(NpT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}\n\\] or \\[\nZ_{NVT}=\\int_0^\\infty dV\\:\\sum_{\\rm i}e^{-\\beta {\\cal H}_i},\n\\tag{1.4}\\] where \\({\\cal H}_i=E_i+pV\\) is the form of the Hamiltonian in the constant-\\(NpT\\) ensemble. Again a state probability analogous to Equation 1.2 holds.\nStatistically, the states favored in the costant-NpT ensemble are those that minimise the free energy:\n\\[\nF_{NpT}=-\\beta^{-1}\\ln Z_{NpT}\n\\] \\(F_{NpT}\\) is known as the Gibb’s free energy (often denoted \\(G\\)). It can also be derived from thermodynamics, from which one finds\n\\[\nF_{NpT}=E-TS+pV=\\mu N\n\\]\nThe constant-\\(NpT\\) ensemble is usually used to describe systems such as a fluid subject to a variable pressure, or a magnet coupled to a magnetic field \\(H\\). In the latter case the quantity \\(HM\\) plays the role of \\(pV\\) and\n\\[\nF_{NpT}=E-TS-MH\\:,\n\\] with \\(M\\) the total magnetisation.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Precourse reading and revision</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "href": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "title": "1  Ensembles and free energies",
    "section": "1.5 From free energies to observables",
    "text": "1.5 From free energies to observables\nFree energies are not directly observable quantities. However, all physical observables can be expressed in terms of derivatives of the free energy. One can derive the appropriate relations either from Thermodynamics, or the corresponding statistical mechanics (Revise your year-2 Thermal Physics notes on this if necessary). As an example let us consider a fluid in the isothermal-isobaric ensemble for which the appropriate free energy is \\(F_{NpT}=E-TS+pV\\), and where the volume fluctuates in response to the prescribed pressure. We shall seek an expression for the average volume in terms of the free energy. First lets us take the thermodynamic route. Differentiating the free energy and applying the chain rule we have:\n\\[\ndF=dE-TdS-sdT+pdV+VdP\\:.\n\\] But from the first law of thermodynamics, \\(dE=TdS-pdV\\), so\n\\[\ndF=-SdT+Vdp\\:,\n\\] and rearranging yields \\[\nV=\\left(\\frac{\\partial F}{\\partial p}\\right)_T\\:.\n\\]\nWe can now show that this result is consistent with the definition of \\(F_{NpT}\\) in terms of the partition function. Write\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}=\\int_{all~states}e^{-\\beta (p V_i+E_i)}\n\\]\nThen \\[\\begin{eqnarray}\n\\left(\\frac{\\partial F}{\\partial p}\\right)_T&=&-\\frac{1}{\\beta}\\left(\\frac{\\partial \\ln Z_{NpT}}{\\partial p}\\right)_T\\nonumber \\\\\n\\:&=&-\\frac{1}{\\beta}\\left(\\frac{1}{Z_{NPT}}\\frac{\\partial Z_{NpT}}{\\partial p}\\right)_T\\nonumber\\\\\n\\:&=&-\\frac{1}{\\beta}\\left(\\frac{1}{Z_{NPT}} \\int_{all~states}(-\\beta V)e^{-\\beta (p V_i+E_i)}  \\right)_T\\nonumber\\\\\n\\:&=&\\langle V\\rangle_T\\nonumber\\:.\n\\end{eqnarray}\\] where in the last step we have used the fact that the probability of a state is defined to be \\(e^{-\\beta (p V_i+E_i)}/Z_{NpT}\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Precourse reading and revision</span>"
    ]
  },
  {
    "objectID": "phase-transitions/introduction.html",
    "href": "phase-transitions/introduction.html",
    "title": "2  1. Introduction to phase behaviour",
    "section": "",
    "text": "A wide variety of physical systems undergo rearrangements of their internal constituents in response to the thermodynamic conditions to which they are subject. Two classic examples of systems displaying such phase transitions are the ferromagnet and fluid systems. As the temperature of a ferromagnet is increased, its magnetic moment is observed to decrease smoothly, until at a certain temperature known as the critical temperature, it vanishes altogether. We define the magnetisation to be the order parameter of this phase transition.\n\n\n\n\n\n\nFigure 2.1: Phase diagram of a simple magnet (schematic)\n\n\n\nSimilarly, a change of state from liquid to gas can be induced in a fluid system (though not in an ideal gas) simply by raising the temperature. Typically the liquid-vapour transition is abrupt, reflecting the large number density difference between the states either side of the transition. However the abruptness of this transition can be reduced by applying pressure. At one particular pressure and temperature the discontinuity in the density difference between the two states vanishes. These conditions of pressure and temperature serve to locate the critical point for the fluid. We define the density difference \\(\\rho_{liq}-\\rho_{vap}\\) to be the order parameter for the liquid-gas phase transition.\n\n\n\n\n\n\nFigure 2.2: Phase diagram of a simple fluid (schematic)\n\n\n\nIn the vicinity of a critical point, a system displays a host of remarkable behaviors known as critical phenomena. Chief among these is the divergence of thermal response functions—such as specific heat, compressibility, or magnetic susceptibility—which signal an enhanced sensitivity to external perturbations. These singularities arise from the emergence of large-scale cooperative interactions among the system’s microscopic constituents, leading to a diverging correlation length. One visually striking manifestation of this is critical opalescence, particularly observed in fluids like CO\\(_2\\). As carbon dioxide nears its critical temperature and pressure, the distinction between its liquid and gas phases vanishes, giving rise to huge fluctuations in density. These fluctuations scatter visible light, rendering the fluid milky or opalescent. This scattering effect directly reflects the long-range correlations developing within the fluid. The movie below illustrates the effect as the critical temperature of CO\\(_2\\) is approached from above. Note the appearence of a liquid-vapour interface (meniscus) as the system enters the two-phase region.\n\nThe recalcitrant problem posed by the critical region is how best to incorporate such collective effects within the framework of a rigorous mathematical theory that affords both physical insight and quantitative explanation of the observed phenomena. This matter has been (and still is!) the subject of intense theoretical activity.\nThe importance of the critical point stems largely from the fact that many of the phenomena observed in its vicinity are believed to be common to a whole range of apparently quite disparate physical systems. Systems such as liquid mixtures, superconductors, liquid crystals, ferromagnets, antiferromagnets and molecular crystals may display identical behaviour near criticality. This observation implies a profound underlying similarity among physical systems at criticality, regardless of many aspects of their distinctive microscopic nature. These ideas have found formal expression in the so-called ‘universality hypothesis’ which, since its inception some 35 years ago, has enjoyed considerable success.\nIn the next few lectures, principal aspects of the contemporary theoretical viewpoint of phase transitions and critical phenomena will be reviewed. Mean field theories of phase transitions will be discussed and their inadequacies in the critical region will be exposed. The phenomenology of the critical region will we described including power laws, critical exponents and their relationship to scaling phenomena. These will be set within the context of the powerful renormalisation group technique. The notion of universality as a phenomenological hypothesis will be introduced and its implications for real and model systems will be explored. Finally, the utility of finite-size scaling methods for computer studies of critical phenomena will be discussed, culminating in the introduction of a specific technique suitable for exposing universality in model systems.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to phase behaviour</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html",
    "href": "phase-transitions/background.html",
    "title": "2  Background concepts",
    "section": "",
    "text": "In seeking to describe near-critical phenomena, it is useful to have a quantitative measure of the difference between the phases coalescing at the critical point: this is the role of the order parameter, \\(Q\\). In the case of the fluid, the order parameter is taken as the difference between the densities of the liquid and vapour phases. In the ferromagnet it is taken as the magnetisation. As its name suggest, the order parameter serves as a measure of the kind of orderliness that sets in when the temperature is cooled below a critical temperature.\nOur first task is to give some feeling for the principles which underlie the ordering process. The probability \\(p_a\\) that a physical system at temperature \\(T\\) will have a particular microscopic arrangement (configuration), labelled \\(a\\), of energy \\(E_a\\) is\n\\[\np_a=\\frac{1}{Z}e^{-E_a/k_BT}\n\\tag{2.1}\\]\nThe prefactor \\(Z^{-1}\\) is the partition function: since the system must always have some specific arrangement, the sum of the probabilities \\(p_a\\) must be unity, implying that\n\\[\nZ=\\sum_ae^{-E_a/k_BT}\n\\tag{2.2}\\] where the sum extends over all possible microscopic arrangements.\nThese equations assume that physical system evolves rapidly (on the timescale of typical observations) amongst all its allowed arrangements, sampling them with the probabilities Equation 2.1 the expectation value of any physical observable \\(O\\) will thus be given by averaging \\(O\\) over all the arrangements \\(a\\), weighting each contribution by the appropriate probability:\n\\[\\bar {O}=\\frac{1}{Z}\\sum_a O_a e^{-E_a/k_BT}\n\\tag{2.3}\\]\nSums like Equation 2.3 are not easily evaluated. Nevertheless, some important insights follow painlessly. Consider the case where the observable of interest is the order parameter, or more specifically the magnetisation of a ferromagnet.\n\\[\nQ=\\frac{1}{Z}\\sum_a Q_a e^{-E_a/k_BT}\n\\tag{2.4}\\]\nIt is clear from Equation 2.1 that at very low temperature the system will be overwhelmingly likely to be found in its minimum energy arrangements (ground states). For the ferromagnet, these are the fully ordered spin arrangements having magnetisation \\(+1\\), or \\(-1\\).\nNow consider the high temperature limit. The enhanced weight that the fully ordered arrangement carries in the sum of Equation 2.4 by virtue of its low energy, is now no longer sufficient to offset the fact that arrangements in which \\(Q_a\\) has some intermediate value, though each carry a smaller weight, are vastly greater in number. A little thought shows that the arrangements which have essentially zero magnetisation (equal populations of up and down spins) are by far the most numerous. At high temperature, these disordered arrangements dominate the sum in Equation 2.4 and the order parameter is zero.\nThe competition between energy-of-arrangements weighting (or simply ‘energy’) and the ‘number of arrangements’ weighting (or ‘entropy’) is then the key principle at work here. The distinctive feature of a system with a critical point is that, in the course of this competition, the system is forced to choose amongst a number of macroscopically different sets of microscopic arrangements.\nFinally in this section, we note that the probabilistic (statistical mechanics) approach to thermal systems outlined above is completely compatible with classical thermodynamics. Specifically, the bridge between the two disciplines is provided by the following equation\n\\[\nF=-k_BT \\ln Z\n\\tag{2.5}\\]\nwhere \\(F\\) is the “Helmholtz free energy”. All thermodynamic observables, for example the order parameter \\(Q\\), and response functions such as the specific heat or magnetic susceptibility are obtainable as appropriate derivatives of the free energy. For instance, utilizing Equation 2.2, one can readily verify (try it as an exercise!) that the average internal energy is given by\n\\[\\bar{E}=-\\frac{\\partial \\ln Z}{\\partial \\beta},\\]\nwhere \\(\\beta=(k_BT)^{-1}\\).\nThe relationship between other thermodynamic quantities and derivatives of the free energy are given in fig. Figure 2.1\n\n\n\n\n\n\nFigure 2.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#observables-and-expectation-values",
    "href": "phase-transitions/background.html#observables-and-expectation-values",
    "title": "3  Background concepts",
    "section": "",
    "text": "Figure 3.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#correlations",
    "href": "phase-transitions/background.html#correlations",
    "title": "3  Background concepts",
    "section": "3.2 Correlations",
    "text": "3.2 Correlations\nThe two-point correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector.\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/approach-to-criticality.html",
    "href": "phase-transitions/approach-to-criticality.html",
    "title": "3  The Approach to Criticality",
    "section": "",
    "text": "It is a matter of experimental fact that the approach to criticality in a given system is characterized by the divergence of various thermodynamic observables. Let us remain with the archetypal example of a critical system, the ferromagnet, whose critical temperature will be denoted as \\(T_c\\). For temperatures close to \\(T_c\\), the magnetic response functions (the magnetic susceptibility \\(\\chi\\) and the specific heat) are found to be singular functions, diverging as a power of the reduced (dimensionless) temperature \\(t \\equiv\n(T-T_c)/T_c\\):-\n\\[\n\\chi \\equiv \\frac{\\partial M}{\\partial H}\\propto t^{-\\gamma} ~~~~ (H=0)\n\\tag{3.1}\\]\n(where \\(M=mN\\)), \\[\nC_H \\equiv \\frac{\\partial E}{\\partial T}\\propto t^{-\\alpha} ~~~~ (H=\\textrm{ constant})\n\\tag{3.2}\\]\nAnother key quantity is the correlation length \\(\\xi\\), which measures the distance over which fluctuations of the magnetic moments are correlated. This is observed to diverge near the critical point with an exponent \\(\\nu\\).\n\\[\n\\xi \\propto t^{-\\nu} ~~~~ (T &gt; T_c,\\: H=0)\n\\tag{3.3}\\]\n\n\n\n\n\n\nMore on correlations\n\n\n\n\n\nThe two-point correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector.\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.\n\n\n\nSimilar power law behaviour is found for the order parameter \\(Q\\) (in this case the magnetisation) which vanishes in a singular fashion (it has infinite gradient) as the critical point is is approached as a function of temperature:\n\\[\nm \\propto t^{\\beta} ~~~~ (T &lt; T_c,\\: H=0)\n\\tag{3.4}\\] (here the symbol \\(\\beta\\), is not to be confused with \\(\\beta=1/k_BT\\)– this unfortunately is the standard notation.)\nFinally, as a function of magnetic field:\n\\[m \\propto h^{1/\\delta} ~~~~ (T = T_c,\\: H&gt;0) . \\tag{3.5}\\] with \\(h=(H-H_c)/H_c\\), the reduced magnetic field.\nAs examples, the behaviour of the magnetisation and correlation length are plotted in Figure 3.1 as a function of \\(t\\).\n\n\n\n\n\n\nFigure 3.1: Singular behaviour of the correlation length and order parameter in the vicinity of the critical point as a function of the reduced temperature \\(t\\).\n\n\n\nThe quantities \\(\\gamma, \\alpha, \\nu, \\beta\\) in the above equations are known as critical exponents. They serve to control the rate at which the various thermodynamic quantities change on the approach to criticality.\nRemarkably, the form of singular behaviour observed at criticality for the example ferromagnet also occurs in qualitatively quite different systems such as the fluid. All that is required to obtain the corresponding power law relationships for the fluid is to substitute the analogous thermodynamic quantities in to the above equations. Accordingly the magnetisation order parameter is replaced by the density difference \\(\\rho_{liq}-\\rho_{gas}\\) while the susceptibility is replaced by the isothermal compressibility and the specific heat capacity at constant field is replaced by the specific heat capacity at constant volume. The approach to criticality in a variety of qualitatively quite different systems can therefore be expressed in terms of a set of critical exponents describing the power law behaviour for that system (see the book by Yeomans for examples).\nEven more remarkable is the experimental observation that the values of the critical exponents for a whole range of fluids and magnets (and indeed many other systems with critical points) are identical. This is the phenomenon of universality. It implies a deep underlying physical similarity between ostensibly disparate critical systems. The principal aim of theories of critical point phenomena is to provide a sound theoretical basis for the existence of power law behaviour, the factors governing the observed values of critical exponents and the universality phenomenon. Ultimately this basis is provided by the Renormalisation Group (RG) theory, for which K.G. Wilson was awarded the Nobel Prize in Physics in 1982.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The approach to criticality</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html",
    "href": "phase-transitions/Ising-model.html",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "4.1 The 2D Ising model\nIn order to probe the properties of the critical region, it is common to appeal to simplified model systems whose behaviour parallels that of real materials. The sophistication of any particular model depends on the properties of the system it is supposed to represent. The simplest model to exhibit critical phenomena is the two-dimensional Ising model of a ferromagnet. Actual physical realizations of 2-d magnetic systems do exist in the form of layered ferromagnets such as K\\(_2\\)CoF\\(_4\\), so the 2-d Ising model is of more than just technical relevance.\nThe 2-d spin-\\(\\frac{1}{2}\\) Ising model envisages a regular arrangement of magnetic moments or ‘spins’ on an infinite plane. Each spin can take two values, \\(+1\\) (‘up’ spins) or \\(-1\\) (‘down’ spins) and is assumed to interact with its nearest neighbours according to the Hamiltonian\n\\[\n{\\cal H}_I=-J\\sum_{&lt;ij&gt;}s_is_j - H\\sum_i s_i\n\\tag{4.1}\\]\nwhere \\(J&gt;0\\) measures the strength of the coupling between spins and the sum extends over nearest neighbour spins \\(s_i\\) and \\(s_j\\), i.e it is a sum of the bonds of the lattice. \\(H\\) is a magnetic field term which can be positive or negative (although for the time being we will set it equal to zero). The order parameter is simply the average magnetisation:\n\\[m=\\frac{1}{N} \\langle \\sum_i s_i \\rangle\\:,\\] where \\(\\langle\\cdot\\rangle\\) means an average over configurations.\nThe fact that the Ising model displays a phase transition was argued in Chapter 2. Thus at low temperatures for which there is little thermal disorder, there is a preponderance of aligned spins and hence a net spontaneous magnetic moment (ie. the system is ferromagnetic). As the temperature is raised, thermal disorder increases until at a certain temperature \\(T_c\\), entropy drives the system through a continuous phase transition to a disordered spin arrangement with zero net magnetisation (ie. the system is paramagnetic). These trends are visible in configurational snapshots from computer simulations of the 2D Ising model (see Figure 4.1). Although each spin interacts only with its nearest neighbours, the phase transition occurs due to cooperative effects among a large number of spins. In the neighbourhood of the transition temperature these cooperative effects engender fluctuations that can extend over all length-scales from the lattice spacing up to the correlation length.\nAn interactive Monte Carlo simulation of the Ising model demonstrates the phenonomenology, By altering the temperature you will be able to observe for yourself how the spin arrangements change as one traverses the critical region. Pay particular attention to the configurations near the critical point. They have very interesting properties. We will return to them later!\nAlthough the 2-d Ising model may appear at first sight to be an excessively simplistic portrayal of a real magnetic system, critical point universality implies that many physical observables such as critical exponents are not materially influenced by the actual nature of the microscopic interactions. The Ising model therefore provides a simple, yet quantitatively accurate representation of the critical properties of a whole range of real magnetic (and indeed fluid) systems. This universal feature of the model is largely responsible for its ubiquity in the field of critical phenomena. We shall explore these ideas in more detail later in the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "href": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "\\(T=1.2T_c\\)\n\n\n\n\n\n\n\n\\(T=T_c\\)\n\n\n\n\n\n\n\n\\(T=0.95T_c\\)\n\n\n\n\n\n\nFigure 4.1: Configurations of the 2d Ising model. The patterns depict typical arrangements of the spins (white=+1, black=−1) generated in a computer simulation of the Ising model on a square lattice of \\(N=512\\) sites, at temperatures (from left to right) of \\(T= 1.2T_c\\), \\(T=T_c\\), and \\(T=0.95T_c\\). In each case only a portion of the system containing \\(128\\) sites in shown. The typical island size is a measure of the correlation length \\(\\xi\\): the excess of black over white (below \\(T_c\\) is a measure of the order parameter.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "href": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "4.2 Exact solutions: the one dimensional Ising chain",
    "text": "4.2 Exact solutions: the one dimensional Ising chain\nOne might well ask why the 2D Ising model is the simplest model to exhibit a phase transition. What about the one-dimensional Ising model (ie. spins on a line)? In fact in one dimension, the Ising model can be solved exactly. It turns out that the system is paramagnetic for all \\(T&gt;0\\), so there is no phase transition at any finite temperature. To see this, consider the ground state of the system in zero external field. This will have all spins aligned the same way (say up), and hence be ferromagnetic. Now consider a configuration with a various “domain walls” dividing spin up and spin down regions:\n\n\n\n\n\n\nFigure 4.2: (a) Schematic of an Ising chain at \\(T=0\\). (b) At a small finite temperature the chain is split into domains of spins ordered in the same direction. Domains are separated by notional domain “walls”, which cost energy \\(\\Delta=2J\\). Periodic boundary conditions are assumed.\n\n\n\nInstead of considering the underlying spin configurations, we shall describe the system in terms of the statistics of its domain walls. The energy cost of a wall is \\(\\Delta = 2J\\), independent of position. Domain walls can occupy the bonds of the lattice, of which there are \\(N-1\\). Moreover, the walls are noninteracting, except that you cannot have two of them on the same bond. (Check through these ideas if you are unsure.)\nIn this representation, the partition function involves a count over all possible domain wall arrangements. Since the domain walls are non interacting (eg it doesn’t cost energy for one to move along the chain) we can calculate \\(Z\\) by considering the partition function associated with a single domain wall being present or absent on some given bond, and then simply raise to the power of the number of bonds:\n\\[Z=Z_1^{N-1}\\]\nwhere\n\\[Z_1=e^{\\beta J} + e^{\\beta (J-\\Delta)}=e^{\\beta J}(1+e^{-\\beta\\Delta})\\] is the domain wall partition function for a single bond and represent the sum over the two possible states: domain wall absent or present. Then the free energy per bond of the system is\n\\[\\beta f\\equiv \\beta F/(N-1)=-\\ln Z_1=-\\beta J-\\ln(1+e^{-\\beta\\Delta})\\]\nThe first term on the RHS is simply the energy per spin of the ferromagnetic (ordered) phase, while the second term arises from the free energy of domain walls. Clearly for any finite temperature (ie. for \\(\\beta&lt;\\infty\\)), this second term is finite and negative. Hence the free energy will always be lowered by having a finite concentration of domain walls in the system. Since these domain walls disorder the system, leading to a zero average magnetisation, the 1D system is paramagnetic for all finite temperatures. Exercise: Explain why this argument works only in 1D.\n\n4.2.1 More general 1D spins systems: transfer matrix method\nGenerally speaking one-dimensional systems lend themselves to a degree of analytic tractability not found in most higher dimensional models. Indeed for the case of a 1-d assembly of \\(N\\) spins each having \\(m\\) discrete energy states, and in the presence of a magnetic field, it is possible to reduce the evaluation of the partition function to the calculation of the eigenvalues of a matrix–the so called transfer matrix.\nLet us start by assuming that the assembly has cyclic boundary conditions, then the total energy of configuration \\(\\{s\\}\\) is\n\\[\\begin{aligned}\nH(\\{s\\})=&-\\sum_{i=1}^N (Js_is_{i+1}+Hs_i)\\\\\n\\:=&-\\sum_{i=1}^N (Js_is_{i+1}+H(s_i+s_{i+1})/2)\\\\\n\\:=&\\sum_{i=1}^N E(s_i,s_{i+1})\n\\end{aligned}\\]\nwhere we have defined \\(E(s_i,s_{i+1})=-Js_is_{i+1}-H(s_i+s_{i+1})/2\\).\nNow the partition function may be written\n\\[\\begin{aligned}\nZ_N =& \\sum_{\\{s\\}}\\exp\\left(-\\beta H(\\{s\\})\\right)\\nonumber \\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta[E(s_1,s_2)+E(s_2,s_3)+....E(s_N,s_1)]\\right) \\nonumber\\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta E(s_1,s_2)\\right)\\exp\\left(-\\beta E(s_2,s_3)\\right)....\\exp\\left(-\\beta E(s_N,s_1)\\right) \\nonumber\\\\\n=&\\sum_{i,j,...,l=1}^m V_{ij}V_{jk}...V_{li}\n\\end{aligned} \\tag{4.2}\\]\nwhere the \\(V_{ij}=\\exp(-\\beta E_{ij})\\) are elements of an \\(m \\times m\\) matrix \\({\\bf V}\\), known as the transfer matrix (\\(i,j,k\\) etc are dummy indices that run over the matrix elements). You should see that the sum over the product of matrix elements picks up all the terms in the partition function and therefore Equation 4.2 is an alternative way of writing the partition function.\nThe reason it is useful to transform to a matrix representation is that it transpires that the sum over the product of matrix elements in equation ([eq-Vs]) is simply just the trace of \\({\\bf V}^N\\) (check this yourself for a short periodic chain), given by the sum of its eigenvalues:-\n\\[Z_N=\\lambda_1^N+\\lambda_2^N+...\\lambda_m^N\\] For very large \\(N\\), this expression simplifies further because the largest eigenvalue \\(\\lambda_1\\) dominates the behaviour since \\((\\lambda_2/\\lambda_1)^N\\) vanishes as \\(N\\rightarrow \\infty\\). Consequently in the thermodynamic limit one may put \\(Z_N=\\lambda_1^N\\) and the problem reduces to identifying the largest eigenvalue of the transfer matrix.\nSpecializing to the case of the simple Ising model in the presence of an applied field \\(H\\), the transfer matrix takes the form\n\\[{\\bf V}(H)=\\left(\n\\begin{array}{cc}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{array} \\right)\\]\nThis matrix has two eigenvalues which can be readily calculated in the usual fashion as the roots of the characteristic polynomial \\(|{\\bf V}-\\lambda{\\bf I}|\\). They are\n\\[\\lambda_{\\pm}=e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\\]\nHence the free energy per spin \\(f=-k_BT\\ln \\lambda_+\\) is\n\\[f=-k_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\\]\nThe Ising model in 2D can also be solved exactly, as was done by Lars Onsager in 1940. The solution is extremely complicated and is regarded as one of the pinnacles of statistical mechanics. In 3D no exact solution is known.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html",
    "href": "phase-transitions/mean-field-theory.html",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "",
    "text": "6.1 Mean field solution of the Ising model\nLet us look for a mean field expression for the free energy of the Ising model whose Hamiltonian is given in Equation 4.1 . Write\n\\[s_i=\\langle s_i\\rangle+(s_i-\\langle s_i\\rangle)=m+(s_i-m)=m+\\delta s_i\\]\nThen \\[\\begin{aligned}\n{\\cal H}_I=&-J\\sum_{&lt;i,j&gt;}[m+(s_i-m)][m+(s_j-m)]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{&lt;i,j&gt;}[m^2+m(s_i-m)+m(s_j-m)+\\delta s_i\\delta s_j]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{i}(qms_i-qm^2/2)-H\\sum_i s_i-J\\sum_{&lt;i,j&gt;}\\delta s_i\\delta s_j\n\\end{aligned} \\tag{6.1}\\] where in the last line we have used the fact that when for each site \\(i\\) we perform the sum \\(\\sum_{&lt;i,j&gt;}\\) over bonds of a quantity which is independent of \\(s_j\\), then the result is just the number of bonds per site times that quantity. Since the number of bonds on a lattice of \\(N\\) sites of coordination \\(q\\) is \\(Nq/2\\) (because each bond is shared between two sites), there are therefore \\(q/2\\) bonds per site.\nNow the mean field approximation is to ignore the last term in the last line of Equation 6.1 giving the configurational energy as\n\\[\n{\\cal H}_{mf}=-\\sum_{i}H_{mf}s_i+NqJm^2/2\n\\] with \\(H_{mf}\\equiv qJm+ H\\) the “mean field” seen by spin \\(s_i\\). As all the spins are decoupled (independent) in this approximation we can write down the partition function, which follows by taking the partition function for a single spin (by summing the Boltzmann factor for \\(s_i=\\pm 1\\)) and raising to the power \\(N\\) to find\n\\[\nZ=e^{-\\beta qJm^2N/2}[2\\cosh(\\beta(qJm+H))]^N\n\\]\nThe free energy follows as\n\\[F(m)=NJqm^2/2-Nk_BT\\ln[2\\cosh(\\beta (qJm+H)]\\:.\\]\nand the magnetisation as\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\tanh(\\beta(qJm+H))\n\\]\nTo find \\(m(H,T)\\), we must numerically solve this last equation self consistently.\nNote that we can obtain \\(m\\) in a different way. Consider some arbitary spin, \\(s_i\\) say. Then this spin has an energy \\({\\cal H}_{mf}(s_i)\\). Considering this energy for both cases \\(s_i=\\pm 1\\) and the probability \\(p(s_i)=e^{-\\beta{\\cal H}_{mf}(s_i)}/Z\\) of each, we have that\n\\[\\langle s_i\\rangle=\\sum_{s_i=\\pm 1}s_ip(s_i)\\] but for consistancy, \\(\\langle s_i\\rangle=m\\). Thus\n\\[\n\\begin{aligned}\nm & = \\sum_{s_i=\\pm 1}s_ip(s_i)\\nonumber\\\\\n\\: & = \\frac{e^{\\beta(qJm+H)}-e^{\\beta(qJm+H)}} {e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\\nonumber\\\\\n\\: & = \\tanh(\\beta(qJm+H))\n\\end{aligned} \\tag{6.2}\\] as before.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-breaking",
    "href": "phase-transitions/mean-field-theory.html#sec-breaking",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.2 Spontaneous symmetry breaking",
    "text": "6.2 Spontaneous symmetry breaking\n\n\n\n\n\n\nFigure 6.1: Schematic of the form of the free energy for a critical, subcritical and supercritical temperature\n\n\n\nThis mean field analysis reveals what is happening in the Ising model near the critical temperature \\(T_c\\). Figure 6.1 shows sketches for \\(\\beta F(m)/N\\) as a function of temperature, where for f simplicity we restrict attention to \\(H=0\\). In this case \\(F(m)\\) is symmetric in \\(m\\), Moreover, at high \\(T\\), the entropy dominates and there is a single minimum in \\(F(m)\\) at \\(m=0\\). As \\(T\\) is lowered, there comes a point (\\(T=T_c=qJ/k_B\\)) where the curvature of \\(F(m)\\) at the origin changes sign; precisely at this point\n\\[\\frac{\\partial^2 F}{\\partial m^2}=0.\\] At lower temperature, there are instead two minima at nonzero \\(m=\\pm m^\\star\\), where the equilibrium magnetisation \\(m^\\star\\) is the positive root (calculated explicitly below) of\n\\[m^\\star=\\tanh(\\beta Jqm^\\star)= \\tanh(\\frac{m^\\star T_c}{T})\\] The point \\(m=0\\) which remains a root of this equation, is clearly an unstable point for \\(T&lt;T_c\\) (since \\(F\\) has a maximum there).\nThis is an example of spontaneous symmetry breaking. In the absence of an external field, the Hamiltonian (and therefore the free energy) is symmetric under \\(m\\to -m\\). Accordingly, one might expect the actual state of the system to also show this symmetry. This is true at high temperature, but spontaneously breaks down at low ones. Instead there are a pair of ferromagnetic states (spins mostly up, or spins mostly down) which – by symmetry– have the same free energy, lower than the unmagnetized state.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#phase-diagram",
    "href": "phase-transitions/mean-field-theory.html#phase-diagram",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.3 Phase diagram",
    "text": "6.3 Phase diagram\nThe resulting zero-field magnetisation curve \\(m(T,H=0)\\) looks like Figure 6.2.\n\n\n\n\n\n\nFigure 6.2: Phase diagram of a simple magnet in the \\(m\\)-\\(T\\) plane.\n\n\n\nThis shows the sudden change of behaviour at \\(T_c\\) (phase transition). For \\(T&lt;T_c\\) it is arbitrary which of the two roots \\(\\pm m^\\star\\) is chosen; typically it will be different in different parts of the sample (giving macroscopic “magnetic domains”). But this behaviour with temperature is qualitatively modified by the presence of a field \\(H\\), however small. In that case, there is always a slight magnetization, even far above \\(T_c\\) and the curves becomes smoothed out, as shown. There is no doubt which root will be chosen, and no sudden change of the behaviour (no phase transition). Spontaneous symmetry breaking does not occur, because the symmetry is already broken by \\(H\\). (The curve \\(F(m)\\) is lopsided, rather than symmetrical about \\(m=0\\).)\nOn the other hand, if we sit below \\(T_c\\) in a positive field (say) and gradually reduce \\(H\\) through zero so that it becomes negative, there is a very sudden change of behaviour at \\(h=0\\): the equilibrium state jumps discontinuously from \\(m=m^\\star\\) to \\(m=-m^\\star\\).\n\n\n\n\n\n\nFigure 6.3: Phase diagram of a simple magnet in the \\(H\\)-\\(T\\) plane.\n\n\n\nThis is called a first order phase transition as opposed to the “second order” or continuous transition that occurs at \\(T_c\\) in zero field. The definitions are:\nFirst order transition: magnetisation (or similar order parameter) depends discontinuously on a field variable (such as \\(h\\) or \\(T\\)).\nContinuous transition: Change of functional form, but no discontinuity in \\(m\\); typically, however, \\((\\partial m/\\partial T)_h\\) (or similar) is either discontinuous, or diverges with an integrable singularity.\nIn this terminology, we can say that the phase diagram of the magnet in the \\(H,T\\) plane shows a line of first order phase transitions, terminating at a continuous transition, which is the critical point.\n\n\n\n\n\n\nAside on Quantum Criticality\n\n\n\n\n\nIn some magnetic systems such as \\(CePd_2Si_2\\), one can, by applying pressure or altering the chemical composition, depress the critical temperature all the way to abolute zero! This may seem counterintuitive, after all at \\(T=0\\) one should expect perfect ordering, not the large fluctuations that accompany criticality. It turns out that the source of the fluctuations that drive the system critical is zero point motion associated with the Heisenberg uncertainty principle. Quantum criticality is a matter of ongoing active research, and open questions concern the nature of the phase diagrams and the relationship to superconductivity. Although the subject goes beyond the scope of this course, there is an accessible article here if you want to learn more.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "href": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.4 A closer look: critical exponents",
    "text": "6.4 A closer look: critical exponents\nLet us now see how we can calculate critical exponents within the mean field approximation.\n\n6.4.1 Zero H solution and the order parameter exponent\nIn zero field\n\\[m=\\tanh(\\frac{mT_c}{T})\\] where \\(T_c=qJ/k_B\\) is the critical temperature at which \\(m\\) first goes to zero.\nWe look for a solution where \\(m\\) is small (\\(\\ll 1\\)). Expanding the tanh function and replacing \\(\\beta=(k_BT)^{-1}\\) yields\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +O(m^5)\\:.\\] Then \\(m=0\\) is one solution. The other solution is given by\n\\[m^2=3\\left(\\frac{T}{T_c} \\right)^3\\left(\\frac{T_c}{T} -1\\right)\\]\nNow, considering temperatures close to \\(T_c\\) to guarantee small \\(m\\), and employing the reduced temperature \\(t=(T-T_c)/T_c\\), one finds\n\\[m^2\\simeq -3t\\]\nHence\n\\[\\begin{aligned}\nm= 0  &    ~~~\\textrm{for } T&gt;T_c \\:\\:\\:  \\textrm{ since~otherwise~{\\it m}~imaginary}\\nonumber\\\\\nm= \\pm\\sqrt{-3t} & ~~\\textrm{ for}  \\:\\:\\: T&lt;T_c ~~\\textrm{ real}\n\\end{aligned}  \\tag{6.3}\\] This result implies that (within the mean field approximation) the critical exponent \\(\\beta=1/2\\).\n\n\n6.4.2 Finite (but small) field solution: the susceptibility exponent\nIn a finite, but small field we can expand Equation 6.2 thus:\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +\\frac{H}{kT}\\]\nConsider now the isothermal susceptibility\n\\[\\begin{aligned}\n\\chi  \\equiv & \\left(\\frac{\\partial m}{\\partial H}\\right)_T\\\\\n      =     & \\frac{T_c}{T}\\chi - \\left(\\frac{T_c}{T}\\right)^3 \\chi m^2 + \\frac{1}{k_BT}  \n\\end{aligned}\\]\nThen\n\\[\\chi \\left[ 1-\\frac{T_c}{T} +\\left(\\frac{T_c}{T}\\right)^3m^2  \\right]=\\frac{1}{k_BT}\\]\nHence near \\(T_c\\)\n\\[\\chi=\\frac{1}{k_BT_c}\\left(\\frac{1}{t+m^2}\\right)\\]\nThen using the results of Equation 6.3\n\\[\\begin{aligned}\n\\chi= (k_BT_ct)^{-1} & \\textrm{ for} ~~~ T&gt; T_c \\\\\n\\chi= (-2k_BT_ct)^{-1} & \\textrm{ for}  ~~~T \\le T_c\n\\end{aligned}\\]\nwhere one has to take the non-zero value for \\(m\\) below \\(T_c\\) to ensure +ve \\(\\chi\\), i.e. thermodynamic stability. This result implies that (within the mean field approximation) the critical exponent \\(\\gamma=1\\).\nThe schematic behaviour of the Ising order parameter and susceptibility are shown in Figure 6.5 (a) and (b)\n\n\n\n\n\n\n\n\n\n\n\n(a) Mean field behaviour of the Ising magnetisation (schematic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Mean field behaviour of the Ising susceptibility (schematic)\n\n\n\n\n\n\n\nFigure 6.4",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#landau-theory",
    "href": "phase-transitions/mean-field-theory.html#landau-theory",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.5 Landau theory",
    "text": "6.5 Landau theory\nLandau theory is a slightly more general type of mean field theory than that discussed in the previous subsection because it is not based on a particular microscopic model. Its starting point is the Helmholtz free energy, which Landau asserted can be written in terms of power series expansion of the order parameter \\(\\phi\\):\n\\[\nF_(\\phi)=\\sum_{i=0}^{\\infty}a_i\\phi^i\n\\] The equilibrium value of \\(\\rho\\) is that which minimises the Landau free energy.\n\n\n\n\n\n\nA note on order parameters\n\n\n\n\n\nWe have already seen examples of these in earlier sections, e.g., for the liquid-gas transition this was \\[\n\\rho_{liq} - \\rho_{gas}: \\quad \\textrm{difference in density of two coexisting phases},\n\\] while for the Ising magnet it is the magnetisation \\(m\\). Both quantities vanish at the critical point. These are examples of scalar order parameters – a single number is required to represent the degree of order (\\(n = 1\\)).\nIn the absence of a symmetry-breaking field, the Landau free-energy density \\(f_L\\) must have symmetry \\(f_L(-\\phi) = f_L(\\phi)\\) (Ising case).\nFor some other systems, \\(n\\) component vectors are required in order to represent the order:\n\\[\n\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_n)\n\\]\nThen \\(f_L(\\boldsymbol{\\phi})\\) should be symmetric under \\(O(n)\\) rotations in \\(n\\)-component \\(\\phi\\)-space.\nThe table below lists examples of order parameters for various physical systems.\n\n\n\n\n\n\n\n\nPhysical System\nOrder Parameter \\(\\varphi\\)\nSymmetry Group\n\n\n\n\nUniaxial (Ising) ferromagnet\nMagnetisation per spin, \\(m\\)\n\\(O(1)\\)\n\n\nFluid (liquid-gas)\nDensity difference, \\(\\rho - \\rho_c\\)\n\\(O(1)\\)\n\n\nLiquid mixtures\nConcentration difference, \\(c - c_c\\)\n\\(O(1)\\)\n\n\nBinary (AB) alloy (e.g., \\(\\beta\\)-brass)\nConcentration of one of the species, \\(c\\)\n\\(O(1)\\)\n\n\nIsotropic (vector) ferromagnet\n\\(n\\)-component magnetisation, \\(\\mathbf{m} = (m_1, m_2, \\dots, m_n)\\)\n\\(O(n)\\)\n\n\n\n\\(n = 2\\): xy model\n\\(O(2)\\)\n\n\n\n\\(n = 3\\): Heisenberg model\n\\(O(3)\\)\n\n\nSuperfluid He\\(^4\\)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nSuperconductor (s-wave)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nNematic liquid crystal\nOrientational order, \\(\\langle P_2(\\cos \\theta)\\rangle\\)\n\n\n\nSmectic A liquid crystal\n1-dimensional periodic density\n\n\n\nCrystal\n3-dimensional periodic density\n\n\n\n\nNotes:\n\nIn superfluid \\(^4He\\) the order parameter is \\[\n\\Psi = |\\Psi| e^{i\\theta},\n\\] the complex wavefunction of the macroscopic condensate.\n\nBoth the amplitude \\(|\\Psi|\\) and phase \\(\\theta\\) must be specified, so this corresponds to \\(n = 2\\).\nSuperconductors also correspond to \\(n = 2\\).\n\nIn a nematic liquid crystal, the orientational order parameter is \\[\n\\langle P_2(\\cos \\theta) \\rangle \\equiv \\frac{1}{2}\\langle 3\\cos^2 \\theta - 1\\rangle,\n\\] where \\(\\theta\\) is the angle a molecule makes with the average direction of the long axes of the molecules (known as the director \\(\\hat{n}\\)). Rotational symmetry is broken. For the case of an \\(n\\) component vector, the free energy should be a function of:\n\n\\[\n\\phi^2 \\equiv |\\boldsymbol{\\phi}|^2 = \\phi_1^2 + \\phi_2^2 + \\dots + \\phi_n^2 = \\sum_{i=1}^n \\phi_i^2\n\\]\nin the absence of a symmetry breaking field. Rotational symmetry is incorporated into the theory.\n\n\n\n\n\n\n\n\n\n\n\n(a) Schematic of the isotropic liquid phase of a system of elongated molcules\n\n\n\n\n\n\n\n\n\n\n\n(b) Schematic of the nematic liquid phase of a system elongated molcules. This phase has uniaxial ordering\n\n\n\n\n\n\n\nFigure 6.5: Isotropic and uniaxially ordered (nematic) phases of liquid crystal molecules.\n\n\n\n\n\n\nTo exemplify the approach, let us specialise to the case of a ferromagnet where \\(\\phi=m\\), the magnetisation and write the Landau free energy as\n\\[\nF(m)=F_0+a_2m^2+a_4m^4\n\\tag{6.4}\\]\nHere only the terms compatible with the order parameter symmetry are included in the expansion and we truncat the series at the 4th power because this is all that is necessary to yield the essential phenomenology. On symmetry grounds, the free energy of a ferromagnet should be invariant under a reversal of the sign of the magnetisation. Terms linear and cubic in \\(m\\) are not invariant under \\(m\\to -m\\), and so do not feature.\nOne can understand how the Landau free energy can give rise to a critical point and coexistence values of the magnetisation, by plotting \\(F(m)\\) for various values of \\(a_2\\) with \\(a_4\\) assumed positive (which ensures that the magnetisation remains bounded). This is shown in the following movie:\n\n\nThe situation is qualitatively similar to that discussed in Section 6.2. Thermodynamics tells us that the system adopts the state of lowest free energy. From the movie, we see that for \\(a_2&gt;0\\), the system will have \\(m=0\\), i.e. will be in the disordered (or paramagnetic) phase. For \\(a_2&lt;0\\), the minimum in the free energy occurs at a finite value of \\(m\\), indicating that the ordered (ferromagnetic) phase is the stable one. In fact, the physical (up-down) spin symmetry built into \\(F\\) indicates that there are two equivalent stable states at \\(m=\\pm m^\\star\\). \\(a_2=0\\) corresponds to the critical point which marks the border between the ordered and disordered phases. Note that it is an inflexion point, so has \\(\\frac{d^2F}{dm^2}=0\\).\nClearly \\(a_2\\) controls the deviation from the critical temperature, and accordingly we may write\n\\[a_2=\\tilde{a_2} t\\] where \\(t\\) is the reduced temperature. Thus we see that the trajectory of the minima as a function of \\(a_2&lt;0\\) in the above movie effective traces out the coexistence curve in the \\(m-T\\) plane.\nWe can now attempt to calculate critical exponents. Restricting ourselves first to the magnetisation exponent \\(\\beta\\) defined by \\(m=t^\\beta\\), we first find the equilibrium magnetisation, corresponding to the minimum of the Landau free energy:\n\\[\n\\frac{dF}{dm}=2\\tilde{a_2} tm+4a_4m^3=0\n\\tag{6.5}\\]\nwhich implies\n\\[m\\propto (-t)^{1/2},\\] so \\(\\beta=1/2\\), which is again a mean field result.\nLikewise we can calculate the effect of a small field \\(H\\) if we sit at the critical temperature \\(T_c\\). Since \\(a_2=0\\), we have\n\\[F(m)=F_0+a_4m^4-Hm\\]\n\\[\\frac{\\partial F}{\\partial m}=0 \\Rightarrow m(H,T_c)=\\left(\\frac{H}{4a_4}\\right)^{1/3}\\]\nor\n\\[H \\sim m^\\delta ~~~~~ \\delta=3\\] which defines a second critical exponent.\nNote that at the critical point, a small applied field causes a very big increase in magnetisation; formally, \\((\\partial m/\\partial H)_T\\) is infinite at \\(T=T_c\\).\nA third critical exponent can be defined from the magnetic susceptibility at zero field\n\\[\\chi=\\left(\\frac{\\partial m}{\\partial H}\\right)_{T,V} \\sim |T-T_c|^{-\\gamma}\\]\nExercise: Show that the Landau expansion predicts \\(\\gamma=1\\).\nFinally we define a fourth critical exponent via the variation of the heat capacity (per site or per unit volume) \\(C_H\\), in fixed external field \\(H=0\\):\n\\[C_H \\sim |T-T_c|^{-\\alpha}\\]\nBy convention, \\(\\alpha\\) is defined to be positive for systems where there is a divergence of the heat capacity at the critical point (very often the case). The heat capacity can be calculated from\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\]\nFrom the minimization over \\(m\\) @#eq-minimize one finds (exercise: check this)\n\\[\\begin{aligned}\nF = & 0 ~~~~T&gt;T_c\\nonumber\\\\\nF = & -a_2^2/4a_4 ~~~~ T &lt; T_c\n\\end{aligned}\\]\nUsing the fact that \\(a_2\\) varies linearly with \\(T\\), we have\n\\[\\begin{aligned}\nC_H =& 0 ~~~~ T\\to T_c^+\\nonumber\\\\\nC_H =& \\frac{T\\tilde a_2^2}{2a_4} ~~~~ T \\to T_c^-\\:,\n\\end{aligned}\\]\nwhich is actually a step discontinuity in specific heat. Since for positive \\(\\alpha\\) the heat capacity is divergent, and for negative \\(\\alpha\\) it is continuous, this behaviour formally corresponds to \\(\\alpha=0\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "href": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.6 Shortcomings of mean field theory",
    "text": "6.6 Shortcomings of mean field theory\nWhile mean field theories provide a useful route to understanding qualitatively the phenomenology of phase transitions, in real ferromagnets, as well as in more sophisticated theories, the critical exponents are not the simple fraction and integers found here. This failure of mean field theory to predict the correct exponents is of course traceable to their neglect of correlations. In later sections we shall start to take the first steps to including the effects of long range correlations.\n\n\n\nComparison of true Ising critical exponents with their mean field theory predictions in a number of dimensions.\n\n\n\\(\\:\\)\nMean Field\n\\(d=1\\)\n\\(d=2\\)\n\\(d=3\\)\n\n\nCritical temperature \\(k_BT/qJ\\)\n\\(1\\)\n\\(0\\)\n\\(0.5673\\)\n\\(0.754\\)\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\frac{1}{2}\\)\n-\n\\(\\frac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(1\\)\n\\(\\infty\\)\n\\(\\frac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(\\frac{1}{2}\\)\n\\(\\infty\\)\n\\(1\\)\n\\(0.63\\pm 0.001\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html",
    "href": "phase-transitions/lattice-gas.html",
    "title": "7  The Lattice Gas model",
    "section": "",
    "text": "7.1 Mapping between Ising model and lattice gas\nThe lattice gas model is interesting because whilst being a plausible model for a fluid, it maps onto the Ising model. This extends the applicability of the Ising model. To expose the mapping we write the grand partition function of the lattice gas:\n\\[ \\Xi=\\sum_\\textrm{ state}\\exp-\\beta{\\cal H}_{LG}=\\sum_{\\{c\\}}\\exp\\left[\\beta \\epsilon\\sum_{&lt;i,j&gt;}c_ic_j +\\beta\\mu\\sum_ic_i\\right] \\] where the sum is an unrestricted sum over the occupancies of the lattice sites. We now change variables to\n\\[c_i=(1+s_i)/2; ~~~~ J=\\frac{\\epsilon}{4} ~~~~\nh=\\frac{\\epsilon q+2\\mu}{4}\\] Hence\n\\[{\\cal H}_{LG}={\\cal H}_\\textrm{ I} + \\textrm{ constant}\\] Since the last term does not depend on the configuration, it feeds through as an additive constant in the free energy; and since all observables feature as derivatives of the free energy, the constant has no physical implications.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html#phase-diagram",
    "href": "phase-transitions/lattice-gas.html#phase-diagram",
    "title": "7  The Lattice Gas model",
    "section": "7.2 Phase diagram",
    "text": "7.2 Phase diagram\nUsing these translation rules we can plot the phase diagram of the lattice gas in the number density-temperature plane.\n\n\n\n\n\n\nFigure 7.1: Phase diagram of the lattice gas model in the density-temperature plane.\n\n\n\nIn the \\(\\mu-T\\) plane there is a line of first order phase transitions terminating at a critical point. The first order line means that if \\(T&lt;T_c\\) we smoothly increase the chemical potential through the coexistence value of \\(\\mu\\), the density of particles on our lattice \\(\\rho=N/L^d\\) jumps discontinuously from a low to a high value.\n\\[\\rho_\\textrm{ gas}=\\frac{1-m^\\star}{2} \\to \\rho_\\textrm{ liquid}=\\frac{1+m^\\star}{2}\n\\] These values merge at \\(T_c\\), the gas-liquid critical point. At higher temperatures, the distinction between the phases disappears.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html#real-fluids",
    "href": "phase-transitions/lattice-gas.html#real-fluids",
    "title": "7  The Lattice Gas model",
    "section": "7.3 Real Fluids",
    "text": "7.3 Real Fluids\nYou may wish to compare this with the results of (say) van der Waals equation (see recommended textbooks for the required phase diagram). The main difference is that the lattice gas has so-called “particle-hole” symmetry, \\(\\rho\\to 1-\\rho\\) (inherited from the up-down symmetry of the Ising model) which is not present for a real fluid. Accordingly, the phase diagram in a real fluid looks like a lopsided version of the above picture as shown in Figure 7.2. See here for some real experimental data showing the asymmetry of the coexistence curve in liquid metals.\n\n\n\n\n\n\nFigure 7.2: Schematic of the liquid-gas phase diagram in the \\(\\rho-T\\) plane for a realistic fluid .",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html",
    "href": "phase-transitions/scaling.html",
    "title": "7  The Static Scaling Hypothesis",
    "section": "",
    "text": "7.1 Experimental Verification of Scaling\nHistorically, the first step towards properly elucidating near-critical behaviour was taken with the static scaling hypothesis. This is essentially a plausible conjecture concerning the origin of power law behaviour which appears to be consistent with observed phenomena. According to the hypothesis, the basis for power law behaviour (and associated scale invariance or “scaling”) in near-critical systems is expressed in the claim that: in the neighbourhood of a critical point, the basic thermodynamic functions (most notably the free energy) are generalized homogeneous functions of their variables. For such functions one can always deduce a scaling law such that by an appropriate change of scale, the dependence on two variables (e.g. the temperature and applied field) can be reduced to dependence on one new variable. This claim may be warranted by the following general argument.\nA function of two variables \\(g(u,v)\\) is called a generalized homogeneous function if it has the property\n\\[g(\\lambda^au,\\lambda^bv)=\\lambda g(u,v)\\] for all \\(\\lambda\\), where the parameters \\(a\\) and \\(b\\) (known as scaling parameters) are constants. An example of such a function is \\(g(u,v)=u^3+v^2\\) with \\(a=1/3, b=1/2\\).\nNow, the arbitrary scale factor \\(\\lambda\\) can be redefined without loss of generality as \\(\\lambda^a=u^{-1}\\) giving\n\\[g(u,v)=u^{1/a}g(1,\\frac{v}{u^{b/a}})\\] A corresponding relation is obtained by choosing the rescaling to be \\(\\lambda^b=v^{-1}\\).\n\\[\\label{eq:sca2}\ng(u,v)=v^{1/b}g(\\frac{u}{v^{a/b}},1)\\]\nThis equation demonstrates that \\(g(u,v)\\) indeed satisfies a simple power law in \\(\\it{one}\\) variable, subject to the constraint that \\(u/v^{a/b}\\) is a constant. It should be stressed, however, that such a scaling relation specifies neither the function \\(g\\) nor the parameters \\(a\\) and \\(b\\).\nNow, the static scaling hypothesis asserts that in the critical region, the free energy \\(F\\) is a generalized homogeneous function of the (reduced) thermodynamic fields \\(t=(T-T_c)/T_c\\) and \\(h=(H-H_c)\\). Remaining with the example ferromagnet, the following scaling assumption can then be made:\n\\[F(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h) \\:.\n\\label{eq:scagibbs}\\]\nWithout loss of generality, we can set \\(\\lambda^a=t^{-1}\\), implying \\(\\lambda=t^{-1/a}\\) and \\(\\lambda^b=t^{-b/a}\\).\nThen \\[F(t,h)=t^{1/a}F(1,t^{-b/a}h)\\] where our choice of \\(\\lambda\\) ensures that \\(F\\) on the rhs is now a function of a single variable \\(t^{-b/a}h\\).\nNow, as stated in @sec:background, the free energy provides the route to all thermodynamic functions of interest. An expression for the magnetisation can be obtained simply by taking the field derivative of \\(F\\) (cf. Figure 2.1)\n\\[m(t,h)=-t^{(1-b)/a}m(1,t^{-b/a}h)\n\\tag{7.1}\\]\nIn zero applied field \\(h=0\\), this reduces to\n\\[m(t,0)=(-t)^{(1-b)/a}m(1,0)\\] where the r.h.s. is a power law in \\(t\\). Equation 4.4 then allows identification of the exponent \\(\\beta\\) in terms of the scaling parameters \\(a\\) and \\(b\\).\n\\[\\beta=\\frac{1-b}{a}\\]\nBy taking further appropriate derivatives of the free energy, other relations between scaling parameters and critical exponents may be deduced. Such calculations (Exercise: try to derive them) yield the results \\(\\delta =\nb/(1-b)\\),\\(\\gamma = (2b-1)/a\\), and \\(\\alpha =(2a-1)/a\\) . Relationships between the critical exponents themselves can be obtained trivially by eliminating the scaling parameters from these equations. The principal results (known as “scaling laws”) are:-\nThus, provided all critical exponents can be expressed in terms of the scaling parameters \\(a\\) and \\(b\\), then only two critical exponents need be specified, for all others to be deduced. Of course these scaling laws are also expected to hold for the appropriate thermodynamic functions of analogous systems such as the liquid-gas critical point.\nThe validity of the scaling hypothesis finds startling verification in experiment. To facilitate contact with experimental data for real systems, consider again Equation 7.1. Eliminating the scaling parameters \\(a\\) and \\(b\\) in favour of the exponents \\(\\beta\\) and \\(\\delta\\) gives\n\\[\n\\frac{m(t,h)}{t^{\\beta}}=m(1,\\frac{h}{t^{\\beta\\delta}})\n\\] where the RHS of this last equation can be regarded as a function of the single scaled variable \\(\\tilde{H} \\equiv t^{-\\beta\\delta} h(t,M)\\).\nFor some particular magnetic system, one can perform an experiment in which one measures \\(m\\) vs \\(h\\) for various fixed temperatures. This allows one to draw a set of isotherms, i.e. \\(m-h\\) curves of constant \\(t\\). These can be used to demonstrate scaling by plotting the data against the scaling variables \\(M=t^{-\\beta}m(t,h)\\) and \\(\\tilde{H}=t^{-\\beta\\delta}h(t,M)\\). Under this scale transformation, it is found that all isotherms (for \\(t\\) close to zero) coincide to within experimental error. Reassuringly, similar results are found using the scaled equation of state of simple fluid systems such as He\\(^3\\) or Xe.\nIn summary, the static scaling hypothesis is remarkably successful in providing a foundation for the observation of power laws and scaling phenomena. However, it furnishes little or no guidance regarding the role of co-operative phenomena at the critical point. In particular it provides no means for calculating the values of the critical exponents appropriate to given model systems.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "href": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "title": "7  The Static Scaling Hypothesis",
    "section": "",
    "text": "Figure 7.1: Magnetisation of CrBr\\(_3\\) in the critical region plotted in scaled form (see text). From Ho and Lister (1969).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#sec-compsim",
    "href": "phase-transitions/scaling.html#sec-compsim",
    "title": "7  The Static Scaling Hypothesis",
    "section": "7.2 Computer simulation",
    "text": "7.2 Computer simulation\nIn seeking to employ simulation to obtain estimates of bulk critical point properties (such as the location of a critical point and the values of its associated exponents), one is immediately confronted with a difficulty. The problem is that simulations are necessarily restricted to dealing with systems of finite-size and cannot therefore accommodate the truly long ranged fluctuations that characterize the near-critical regime. As a consequence, the critical singularities in \\(C_v\\), order parameter, etc. appear rounded and shifted in a simulation study. Figure 7.2 shows a schematic example for the susceptibility of a magnet.\n\n\n\n\n\n\nFigure 7.2: Schematic of the near-critical temperature dependence of the magnet susceptibility in a finite-sized system.\n\n\n\nThus the position of the peak in a response function (such as \\(C_v\\)) measured for a finite-sized system does not provide an accurate estimate of the critical temperature. Although the degree of rounding and shifting reduces with system size, it is often the case, that computational constraints prevent access to the largest system sizes which would provide accurate estimates of critical parameters. To help deal with this difficulty, finite-size scaling (FSS) methods have been developed to allow extraction of bulk critical properties from simulations of finite size. FSS will be discussed in Section 9.7",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html",
    "href": "phase-transitions/rg.html",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "9.1 The critical point: A many length scale problem\nA near critical system can be characterized by three important length scales, namely\nThe authentic critical region is defined by a window condition:\n\\[L_\\textrm{ max} \\gg \\xi \\gg L_\\textrm{ min}\\]\nThe physics of this regime is hard to tackle by analytic theory because it is characterized by configurational structure on all scales between \\(L_\\textrm{ min}\\) and \\(\\xi\\) (in fact it turns out that the near critical configurational patterns are fractal-like, cf. fig. 5(b)). Moreover different length scales are correlated with one another, giving rise to a profusion of coupled variables in any theoretical description. The window regime is also not easily accessed by computer simulation because it entails studying very large system sizes \\(L_\\textrm{\nmax}\\), often requiring considerable computing resources.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "href": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "The correlation length, \\(\\xi\\), ie the size of correlated microstructure.\nMinimum length scale \\(L_\\textrm{ min}\\), i.e. the smallest length in the microscopics of the problem, e.g. lattice spacing of a magnet or the particle size in a fluid.\nMacroscopic size \\(L_{max}\\) eg. size of the system.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-rgmethod",
    "href": "phase-transitions/rg.html#sec-rgmethod",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.2 Methodology of the RG",
    "text": "9.2 Methodology of the RG\nThe central idea of the renormalisation group (RG) method is a stepwise elimination of the degrees of freedom of the system on successively larger length-scales. To achieve this one introduces a fourth length scale \\(L\\). In contrast to the other three, which characterize the system itself, \\(L\\) characterises the description of the system. It may be thought of as typifying the size of the smallest resolvable detail in a description of the system’s microstructure.\nConsider the Ising model arrangements displayed in Figure 5.1 (a)-(c). These pictures contain all the details of each configuration shown: the resolution length \\(L\\) in this case has its smallest possible value, coinciding with the lattice spacing i.e. \\(L=L_{\\min}\\). In the present context, the most detailed description is not the most useful: the essential signals with which we are concerned are hidden in a noise of relevant detail. A clue to eliminating this noise lies in the nature of the correlation length, i.e. the size of the largest droplets. The explicit form of the small scale microstructure is irrelevant to the behaviour of \\(\\xi\\). The small scale microstructure is the noise. To eliminate it, we simply select a larger value of the resolution length (or ‘coarse-graining’ length) \\(L\\).\nThere are many ways of implementing this coarse-graining procedure. We adopt a simple strategy in which we divide our sample into blocks of side \\(L\\), each of which contains \\(L^d\\) sites, with \\(d\\) the space dimensions . The centres of the blocks define a lattice of points indexed by \\(I=1,2,N/L^d\\). We associate with each block lattice point centre, \\(I\\), a coarse-grained or block variable \\(S_I(L)\\) defined as the spatial average of the local variables it contains:\n\\[\nS_I(L)=L^{-d}\\sum_i^Is_i\n\\tag{9.1}\\] where the sum extends over the \\(L^d\\) sites in the block \\(I\\). The set of coarse grained coordinates \\(\\{S(L)\\}\\) are the basic ingredients of a picture of the system having spatial resolution of order \\(L\\).\nThe coarse graining operation is easily implemented on a computer. In so doing one is faced with the fact that while the underlying Ising spins can only take two possible values, the block variables \\(S_I(L)\\) have \\(L^d+1\\) possible values. Accordingly in displaying the consequences of the blocking procedure, we need a more elaborate colour convention than that used in Figure 5.1. We will associate with each block a shade of grey drawn from a spectrum ranging from black to white.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(aiv)\n\n\n\n\n\n\n\n(biv)\n\n\n\n\n\n\nFigure 9.1: See text for details\n\n\n\nThe results of coarse-graining configurations typical of three different temperatures are shown in Figure 9.1 and Figure 9.2. Two auxiliary operations are implicit in these results. The first operation is a length scaling: the lattice spacing on each blocked lattice has been scaled to the same size as that of the original lattice, making possible the display of correspondingly larger portions of the physical system. The second operation is a variable scaling: loosely speaking, we have adjusted the scale (‘contrast’) of the block variable so as to match the spectrum of block variable values to the spectrum of shades at our disposal.\nConsider first a system marginally above its critical point at a temperature \\(T\\) chosen so that the correlation length \\(\\xi\\) is approximately 6 lattice spacing units. A typical arrangement (without coarse-graining) is shown in Figure 9.1(ai). The succeeding figures, Figure 9.1(aii) and Figure 9.1(aiii), show the result of coarse-graining with block sizes \\(L=4\\) and \\(L=8\\), respectively. A clear trend is apparent. The coarse-graining amplifies the consequences of the small deviation of \\(T\\) from \\(T_c\\). As \\(L\\) is increased, the ratio of the size of the largest configurational features (\\(\\xi\\)) to the size of the smallest (\\(L\\)) is reduced. The ratio \\(\\xi/L\\) provides a natural measure of how ‘critical’ is a configuration. Thus the coarse-graining operation generates a representation of the system that is effectively less critical the larger the coarse-graining length. The limit point of this trend is the effectively fully disordered arrangement shown in Figure 9.1(aiii) and in an alternative form in Figure 9.1(aiv), which shows the limiting distribution of the coarse grained variables, averaged over many realizations of the underlying configurations: the distribution is a Gaussian which is narrow (even more so the larger the \\(L\\) value) and centred on zero. This limit is easily understood. When the system is viewed on a scaled \\(L\\) larger than \\(\\xi\\), the correlated microstructure is no longer explicitly apparent; each coarse-grained variable is essentially independent of the others.\nA similar trend is apparent below the critical point. Figure 9.1(bi) show a typical arrangement at a temperature \\(T&lt;T_c\\) such that again \\(\\xi\\) is approximately \\(6\\) lattice spacings. Coarse-graining with \\(L=4\\) and \\(L=8\\) again generates representations which are effectively less critical (Figure 9.1(bii) and (biii)). This time the coarse-graining smoothes out the microstructure which makes the order incomplete. The limit point of this procedure is a homogeneously ordered arrangement in which the block variables have a random (Gaussian) distribution centred on the order parameter (Figure 9.1(biv)).\nConsider now the situation at the critical point. Figure 9.2(ai) shows a typical arrangement; Figure 9.2(aii) and (aiii) show the results of coarse-graining with \\(L=4\\) and \\(L=8\\) respectively. Since the \\(\\xi\\) is as large as the system itself the coarse graining does not produce less critical representations of the physical system: each of the figures displays structure over all length scales between the lower limit set by \\(L\\) and the upper limit set by the size of the display itself. A limiting trend is nevertheless apparent. Although the \\(L=4\\) pattern is qualitatively quite different from the pattern of the local variables, the \\(L=4\\) and \\(L=8\\) patterns display qualitatively similar features. These similarities are more profound than is immediately apparent. A statistical analysis of the spectrum of \\(L=4\\) configurations (generated as the local variables evolve in time) show that it is almost identical to that of the \\(L=8\\) configurations (given the block variable scaling). The implication of this limiting behaviour is clear: the patterns formed by the ordering variable at criticality look the same (in a statistical sense) when viewed on all sufficiently large length scales.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(iv)\n\n\n\n\n\n\nFigure 9.2: See text for details\n\n\n\nLet us summarize. Under the coarse-graining operation there is an evolution or flow of the system’s configuration spectrum. The flow tends to a limit, or fixed point, such that the pattern spectrum does not change under further coarse-graining. These scale-invariant limits have a trivial character for \\(T&gt;T_c\\), (a perfectly disordered arrangement) and \\(T&lt; T_c\\), (a perfectly ordered arrangement). The hallmark of the critical point is the existence of a scale-invariant limit which is neither fully ordered nor fully disordered but which possesses structure on all length scales. A nice illustration of critical point scale invariance in the Ising model can be viewed here.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#universality-and-scaling",
    "href": "phase-transitions/rg.html#universality-and-scaling",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.3 Universality and Scaling",
    "text": "9.3 Universality and Scaling\nEquipped with the coarse-graining technique, we now address the universality phenomenon. We aim to understand how it is that systems that are different microscopically can nevertheless display critical point behaviour which (in certain respects) is quantitatively identical.\nTo obtain initial insight we introduce a spin-1 Ising model in which the spins take on three values (\\(s_i=1,0,-1\\)), in contrast to the two values (\\(s_i=1,-1\\)) of the spin-1/2 Ising model. The two models have properties which are different: for example, \\(T_c\\) for the three-state model is some \\(30\\%\\) lower than that of the two-state model (for the same coupling \\(J\\)). However, there is abundant evidence that the two models have the same universal properties.\nLet us explore what is the same and what is different in the configurations of the two models at criticality. The configurations of the local variables \\(s_i\\) are clearly qualitatively different for the two models. Now consider the coarse-grained configurations (with \\(L=4\\) and \\(L=8\\) respectively) for the three-state model at the critical point. We have already seen that the coarse-graining operation bears the configuration spectrum of the critical two-state Ising model to a non-trivial scale-invariant limit. It is scarcely surprising that the same is true for the three-state model. What is remarkable is that the two limits are the same! This is expressed in Figure 9.2(iv), which shows the near coincidence of the distribution of block variables (grey-levels) for the two different coarse-graining lengths. Thus the coarse-graining erases the physical differences apparent in configurations where the local behaviour is resolvable, and exposes a profound configurational similarity.\n\n9.3.1 Fluid-magnet universality\nLet us now turn to fluid-magnet universality. In a magnet, the relevant configurations are those formed by the coarse-grained magnetisation (the magnetic moment averaged over a block of side \\(L\\)). In a fluid, the relevant configurations are those of the coarse-grained density (the mass averaged over a block if side \\(L\\)) or more precisely, its fluctuation from its macroscopic average (Figure 9.3). The patterns in the latter (bubbles of liquid or vapour) may be matched to pattern in the former (microdomains of the magnetisation), given appropriate scaling operations to camouflage the differences between the length scales and the differences between the variable scales.\n\n\n\n\n\n\nFigure 9.3: Schematic representation of the coarse graining operation via which the universal properties of fluids and magnets may be exposed.\n\n\n\nThe results is illustrated in Figure 9.4.\n\n\n\n\n\n\n\n\n2D critical Ising model and 2d critical Lennard-Jones fluid at small lengthscales\n\n\n\n\n\n\n\n\n\nSame models as above, but viewed at large lengthscales\n\n\n\n\n\n\nFigure 9.4: Snapshot configurations of the 2D critical Ising model (left) and the 2D critical Lennard-Jones fluid (right). When viewed on sufficiently large length scales the configurational patterns appear universal and self similar.\n\n\n\nA movie in which we progressively zoom out shows how the loss of microscopic details reveals the large lengthscale universal features.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#near-critical-scaling",
    "href": "phase-transitions/rg.html#near-critical-scaling",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.4 Near critical scaling",
    "text": "9.4 Near critical scaling\nThe similarity of coarse-grained configurations of different systems is not restricted to the critical temperature itself. Suppose we have a two state spin model and a three state spin model each somewhat above their critical points at reduced temperature \\(t\\). The two systems will have somewhat different correlation lengths, \\(\\xi_1\\) and \\(\\xi_2\\) say. Suppose however, we choose coarse-graining lengths \\(L_1\\) for \\(L_2\\) for the two models such that \\(\\xi_1/L_1=\\xi_2/L_2\\). We adjust the scales of the block variables (our grey level control) so that the typical variable value is the same for the two systems. We adjust the length scale of the systems (stretch or shrink our snapshots) so that the sizes of the minimum-length-scale structure (set by \\(L_1\\) and \\(L_2\\)) looks the same for each system. Precisely what they look like depends upon our choice of \\(\\xi/L\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-unipics",
    "href": "phase-transitions/rg.html#sec-unipics",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.5 Universality classes",
    "text": "9.5 Universality classes\nCoarse graining does not erase all differences between the physical properties of critical systems. Differences in the space dimension \\(d\\) of two critical systems will lead to different universal properties such as critical exponents. Thus, for instance, the critical exponents of the 2D magnet, match those of the 2d fluid, but they are different to those of 3d magnets and fluids.\nIn fact the space dimension is one of a small set of qualitative features of a critical system which are sufficiently deep-seated to survive coarse graining and which together serve to define the system’s universal behaviour, or universality class. The constituents of this set are not all identifiable a priori. They include the number of components \\(n\\) of the order parameter. Up to now, we have only considered order parameters which are scalar (for a fluid the density, for a magnet the magnetisation), for which \\(n=1\\). In some ferromagnets, the order parameter may have components along two axes, or three axes, implying a vector order parameter, with \\(n=2\\) or \\(n=3\\), respectively. It is clear that the order-parameter \\(n\\)-value will be reflected in the nature of the coarse-grained configurations, and thus in the universal observables they imply.\nA third important feature which can change the universality class of a critical system is the range of the interaction potential between its constituent particles. Clearly for the Ising model, interactions between spins are inherently nearest neighbour in character. Most fluids interact via dispersion forces (such as the Lennard-Jones potential) which is also short ranged owing to the \\(r^{-6}\\) attractive interaction. However some systems have much longer ranged interactions. Notable here are systems of charged particles which interact via a Coulomb potential. The long ranged nature of the Coulomb potential (which decays like \\(r^{-1}\\)) means that charged systems often do not have the same critical exponents as the Ising model and fluid.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#critical-exponents",
    "href": "phase-transitions/rg.html#critical-exponents",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.6 Critical exponents",
    "text": "9.6 Critical exponents\nWe consider now how the critical exponents, may be computed via the coarse-graining procedure. In what follows we will refer only to the behaviour of a single typical coarse grained variable, which we shall denote \\(S(L)\\). We suppose that \\(t\\) is sufficiently small that \\(\\xi \\gg\nL_\\textrm{ min}\\). Universality and scaling may be expressed in the claim that, for any \\(L\\) and \\(t\\), scale factors \\(a(L)\\) and \\(b(L)\\) may be found such that the probability distribution \\(p(S(L),t)\\) can be written in the form\n\\[\np(S(L),t)=b(L)\\tilde{p}(b(L)S(L),a(L)t)\n\\tag{9.2}\\] where \\(\\tilde{p}\\) is a function unique to a universality class. The role of the scale factors \\(a\\) and \\(b\\) is to absorb the basic non-universal scales identified in Section 9.2. The critical exponents are implicit in the \\(L\\)-dependence of these scale factors. Specifically one finds:\n\\[\n\\begin{aligned}\na(L) & =a_0L^{1/\\nu} \\\\\nb(L) & =b_0L^{\\beta/\\nu}\n\\end{aligned}\n\\tag{9.3}\\] where the amplitudes \\(a_0\\) and \\(b_0\\) are system specific (non-universal) constants.\nThese results state that the critical exponents (in the form \\(1/\\nu\\) and \\(\\beta/\\nu\\)) characterize the ways in which the configuration spectrum evolves under coarse-graining. Consider, first the exponent ratio \\(\\beta/\\nu\\). Precisely at the critical point, there is only one way in which the coarse-grained configurations change with \\(L\\): the overall scale of the coarse-grained variable (the black-white contrast in our grey scale representation) is eroded with increasing \\(L\\). Thus the configurations of coarse-graining length \\(L_1\\) match those of a larger coarse-graining length \\(L_2\\) only if the variable scale in the latter configurations is amplified. The required amplification follows from Equation 9.2 and and Equation 9.3: it is\n\\[\n\\frac{b(L_2)}{b(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{\\beta/\\nu}\\:.\n\\] The exponent ratio \\(\\beta/\\nu\\) thus controls the rate at which the scale of the ordering variable decays with increasing coarse-graining length.\nConsider now the exponent \\(1/\\nu\\). For small but non-zero reduced temperature (large but finite \\(\\xi\\)) there is second way in which the configuration spectrum evolves with \\(L\\). As noted previously, coarse graining reduces the ratio of correlation length to coarse-graining length, and results in configurations with a less critical appearance. More precisely, we see from Equation 9.2 that increasing the coarse graining length from \\(L_1\\) to \\(L_2\\) while keeping the reduced temperature constant has the same effect on the configuration spectrum as keeping coarse-graining length constant which amplifying the reduced temperature \\(t\\) by a factor\n\\[\n\\frac{a(L_2)}{a(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{1/\\nu}\\:.\n\\] One may think of the combination \\(a(L)t\\) as a measure of the effective reduced temperature of the physical system viewed with resolution length \\(L\\). The exponent \\(1/\\nu\\) controls the rate at which the effective reduced temperature flows with increasing coarse-graining length.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-fss",
    "href": "phase-transitions/rg.html#sec-fss",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.7 Finite-size scaling",
    "text": "9.7 Finite-size scaling\nWe can exploit the fact that the scale factors \\(a(L)\\) and \\(b(L)\\) depend on critical exponents to estimate the values of these exponents using computer simulation. Consider the average of the block variable \\(S(L)\\). Consideration of Equation 9.1 shows that this is non other than the value of the order parameter \\(Q\\), measured over a block of side \\(L\\). Thus from the definition of an average\n\\[\nQ(L,t)=\\bar {S}(L,t)=\\int S(L)p(S(L),t)dS(L)\n\\] where \\(p(S(L))\\) is the probability distribution of \\(S(L)\\).\nMaking use of the representation of Equation 9.2, we then have that\n\\[Q\n(L,t) = \\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)dS(L)\n\\]\nTo integrate this we need to change the integration variable from \\(S(L)\\) to \\(b(L)S(L)\\). We have \\(d(b(L)S(L))=b(L)dS(L)\\) since \\(b(L)\\) does not fluctuate. Thus\n\\[\\begin{aligned}\nQ(L,t)  & =  b^{-1}(L)\\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)d(b(L)S(L))\\nonumber\\\\\n        & =  b^{-1}(L)f(a(L)t)\\nonumber\\\\\n       & =  b_0L^{-\\beta/\\nu}f(a_0L^{1/\\nu}t)\n\\end{aligned}\\]\nwhere \\(f\\) is a universal function (defined as the first moment of \\(\\tilde{p}(x,y)\\) with respect to \\(y\\)).\nThe above results provide a method for determining the critical exponent ratios \\(\\beta/\\nu\\) and \\(1/\\nu\\) via computer simulations of near critical systems. For instance, at the critical point (\\(t=0\\)) and for finite block size, \\(Q(L,0)\\) will not be zero (the \\(T\\) at which Q vanishes for finite \\(L\\) is above the true \\(T_c\\), cf. Section 8.2. However, we know that its value must vanish in the limit of infinite \\(L\\); it does so like\n\\[Q(L,0)=b_0L^{-\\beta/\\nu}f(0)\\equiv Q_0L^{-\\beta/\\nu}\\]\nThus by studying the critical point \\(L\\) dependence of \\(Q\\) we can estimate \\(\\beta/\\nu\\). A similar approach in which we study two block sizes \\(L\\), and tune \\(t\\) separately in each case so that the results for \\(QL^{\\beta/\\nu}\\) are identical provides information on the value of \\(1/\\nu\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#summary-of-main-points",
    "href": "phase-transitions/rg.html#summary-of-main-points",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.8 Summary of main points",
    "text": "9.8 Summary of main points\nAs this is quite a long chapter let us summarise the main points:\n\nLimitations of conventional theories: Mean field theories and the scaling hypothesis are insufficient in the critical region due to their neglect of correlations across all relevant length scales.\nCritical region characteristics: Near-critical systems exhibit correlated microstructure on all length scales up to the correlation length. The complexity of this structure makes both analytical and computational study challenging.\nRelevant length scales:\n\nCorrelation length (\\(\\xi\\))\nMinimum microscopic scale (\\(L_\\textrm{min}\\))\nMacroscopic system size (\\(L_\\textrm{max}\\))\n\nWindow Condition for criticality: The true critical regime satisfies \\(L_\\textrm{max} \\gg \\xi \\gg L_\\textrm{min}\\), encompassing a broad range of scales where complex, often fractal-like, structures are present.\nRenormalisation Group (RG) methodology: RG involves the stepwise elimination of degrees of freedom by coarse-graining the system over increasing length scales. A fourth scale, \\(L\\), represents the resolution at which the system is described.\nEffect of coarse-graining: Coarse-graining changes the effective reduced temperature, captured by the relation \\(a(L)t\\), where \\(a(L)\\) scales with \\(L\\) as \\(L^{1/\\nu}\\). This helps describe how critical configurations evolve with resolution.\nUniversality:\n\nCoarse-graining reveals that microscopically different systems can exhibit identical critical behavior when observed at large scales.\nThe concept of universality explains why disparate systems, such as magnets and fluids, can show the same critical exponents and scaling laws.\nCritical behavior depends primarily on general features like dimensionality and symmetry, rather than microscopic details.\n\nFinite-Size scaling:\n\nThe average block variable \\(Q(L,t)\\) is related to block size \\(L\\) and reduced temperature \\(t\\) through scaling relations. Computer simulations exploit this to extract scaling functions and the values of critical exponents.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "href": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "title": "9  The Renormalisation Group Theory of Critical Phenomena",
    "section": "9.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)",
    "text": "9.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)\n\n\n\n\n\n\nNotes for those interested in a different perspective on RG theory.\n\n\n\n\n\nLet us begin by returning to our fundamental Equation 3.1, which we rewrite as\n\\[p = Z^{-1}e^{-{\\cal H}}\\] where \\({\\cal H}\\equiv E/k_BT\\).\nThe first step is then to imagine that we generate, by a computer simulation procedure for example, a sequence of configurations with relative probability \\(\\exp(-{\\cal H})\\). We next adopt some coarse-graining procedure which produces from these original configurations a set of coarse-grained configurations. We then ask the question: what is the energy function \\({\\cal H}^\\prime\\) of the coarse-grained variables which would produce these coarse-grained configurations with the correct relative probability \\(\\exp(-{\\cal H}^\\prime)\\)? Clearly the form of \\({\\cal H}^\\prime\\) depends on the form of \\({\\cal H}\\) thus we can write symbolically\n\\[{\\cal H}^\\prime=R({\\cal H})\\]\nThe operation \\(R\\), which defines the coarse-grained configurational energy in terms of the microscopic configurational energy function is known as a renormalisation group transformation (RGT). What it does is to replace a hard problem by a less hard problem. Specifically, suppose that our system is near a critical point and that we wish to calculate its large-distance properties. If we address this task by utilizing the configurational energy and appealing to the basic machinery of statistical mechanics set out in Equation 3.1 and Equation 3.2, the problem is hard. It is hard because the system has fluctuations on all the (many) length scales intermediate between the correlation length \\(\\xi\\) and the minimum length scale \\(L_\\textrm{min}\\).\nHowever, the task may instead be addressed by tackling the coarse-grained system described by the energy \\({\\cal H}^\\prime\\). The large-distance properties of this system are the same as the large-distance properties of the physical system, since coarse-graining operation preserves large-scale configurational structure. In this representation the problem is a little easier: while the \\(\\xi\\) associated with \\({\\cal H}^\\prime\\) is the same as the \\(\\xi\\) associated with \\({\\cal H}\\), the minimum length scale of \\({\\cal H}^\\prime\\) is bigger than that of \\({\\cal H}\\). Thus the statistical mechanics of \\({\\cal H}^\\prime\\) poses a not-quite-so-many-length-scale problem, a problem which is effectively a little less critical and is thus a little easier to solve. The benefits accruing from this procedure may be amplified by repeating it. Repeated application of \\(R\\) will eventually result in a coarse- grained energy function describing configurations in which the \\(\\xi\\) is no bigger than the minimum length scale. The associated system is far from criticality and its properties may be reliably computed by any of a wide variety of approximation schemes. These properties are the desired large-distance properties of the physical system. As explicit reference to fluctuations of a given scale is eliminated by coarse-graining, their effects are carried forward implicitly in the parameters of the coarse-grained energy.\nIn order to setup the framework for a simple illustrative example, let is return to the lattice Ising model for which the energy function depended only on the product of nearest neighbour spins. The coefficient of this product in the energy is the exchange coupling, \\(J\\). In principle, however, other kinds of interactions are also allowed; for example, we may have a product of second neighbour spins with strength \\(J_2\\) or, perhaps, a product of four spins (at sites forming a square whose side is the lattice spacing), with strength \\(J_3\\). Such interactions in a real magnet have their origin in the quantum mechanics of the atoms and electrons and clearly depend upon the details of the system. For generality therefore we will allow a family of exchange couplings \\(J_1\\),\\(J_2\\),\\(J_3,\\dots\\), or \\(J_a, a =\n1,2,\\dots\\) In reduced units, the equivalent coupling strengths are \\(K_a =J_a/k_BT\\). Their values determine uniquely the energy for any given configuration.\n\nWe note that it is not only useful to allow for arbitrary kinds of interactions: if we wish to repeat the transformation several (indeed many) times, it is also necessary because even if we start with only the nearest neighbour coupling in \\({\\cal H}\\) the transformation will in general produce others in \\({\\cal H}^\\prime\\).\n\nNow consider the coarse-graining procedure. Let us suppose that this procedure takes the form of a ‘majority rule’ operation in which the new spins are assigned values \\(+1\\) or \\(-1\\) according to the signs of the magnetic moments of the blocks with which they are associated. The new energy function \\({\\cal H}^\\prime\\) will be expressible in terms of some new coupling strengths \\(K^\\prime\\) describing the interactions amongst the new spin variables (and thus, in effect, the interactions between blocks of the original spin variables). The RGT simply states that these new couplings depend on the old couplings: \\(K_1^\\prime\\) is some function \\(f_1\\) of all the original couplings, and generally\n\\[K^\\prime_a=f_a(K_1,K_2,\\dots) =f_a({\\bf K}),~~~~ a= 1, 2,\\dots\n\\tag{9.4}\\] where K is shorthand for the set \\(K_1, K_2,\\dots\\)\n\n9.9.1 A simple example\nThis example illustrates how one can perform the RG transformation Equation 9.4 directly, without recourse to a ‘sequence of typical configurations’. The calculation involves a very crude approximation which has the advantage that it simplifies the subsequent analysis.\n\n\n\n\n\n\nFigure 9.5: Coarse graining by decimation. The spins on the original lattice are divided into two sets \\(\\{s^\\prime\\}\\) and \\(\\{\\tilde{s}\\}\\). The \\(\\{s^\\prime\\}\\) spins occupy a lattice whose spacing is twice that of the original. The effective coupling interaction between the \\(\\{s^\\prime\\}\\) spins is obtained by performing the configurational average over the \\(\\{\\tilde{s}\\}\\)\n\n\n\nConsider an Ising model in two dimensions, with only nearest neighbour interactions as shown in Figure 9.5. We have divided the spins into two sets, the spins \\(\\{s^\\prime\\}\\) form a square lattice of spacing \\(2\\), the others being denoted by \\(\\{\\tilde{s}\\}\\). One then defines an effective energy function \\({\\cal H^\\prime}\\) for the \\(s^\\prime\\) spins by performing an average over all the possible arrangements of the \\(\\tilde{s}\\) spins\n\\[\n\\exp(-{\\cal H}^\\prime)=\\sum_{\\{\\tilde {s}\\}} \\exp(-{\\cal H}).\n\\tag{9.5}\\]\nThis particular coarse-graining scheme is called ‘decimation’ because a certain fraction (not necessarily one-tenth!) of spins on the lattice is eliminated. This formulation of a new energy function realizes two basic aims of the RG method: the long-distance physics of the ‘original’ system, described by \\({\\cal H}\\), is contained in that of the ‘new’ system, described by \\({\\cal H}^\\prime\\) (indeed the partition functions are the same as one can see by summing both sides over \\(s^\\prime\\)) and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing (‘minimum length scale’) has been reduced by a factor of \\(1/2\\) (the ratio of the lattice spacings of the two systems). We must now face the question of how to perform the configuration sum in Equation 9.5. This cannot in general be done exactly, so we must resort to some approximation scheme. The particular approximation which we invoke is the high temperature series expansion. In its simplest mathematical form, since \\({\\cal H}\\) contains a factor \\(1/k_BT\\), it involves the expansion of \\(\\exp(-{\\cal H})\\) as a power series:\n\\[\\exp(-{\\cal H}/k_BT)=1-{\\cal H}/k_BT +\\frac{1}{2!}({\\cal H}/k_BT)^2+.....\\]\nWe substitute this expansion into the right hand side of Equation 9.5 and proceed to look for terms which depend on the \\(s^\\prime\\) spins after the sum over the possible arrangements of the \\(\\tilde{s}\\) spins is performed. This sum extends over all the possible (\\(\\pm 1\\)) values of all the \\(\\tilde{s}\\) spins. The first term (the 1) in the expansion of the exponential is clearly independent of the values of the \\(s^\\prime\\) spins. The second term (\\({\\cal H}\\)) is a function of the \\(s^\\prime\\) spins, but gives zero when the sum over the \\(s^\\prime\\) spins is performed because only a single factor of any \\(s^\\prime\\) ever appears, and \\(+ 1 - 1 = 0\\). The third term (\\({\\cal H}^2/2\\)) does contribute. If one writes out explicitly the form of \\({\\cal H}^2/2\\) one finds terms of the form \\(K^2s_1^\\prime\\tilde{s}\\tilde{s}s_2^\\prime=K^2s_1^\\prime s_2^\\prime\\), where \\(s_1^\\prime\\) and \\(s_2^\\prime\\) denote two spins at nearest neighbour sites on the lattice of \\(s^\\prime\\) spins and \\(\\tilde{s}\\) is the spin (in the other set) which lies between them. Now, in the corresponding expansion of the left hand side of Equation 9.5, we find terms of the form \\(K^\\prime s_1^\\prime s_2^\\prime\\), where \\(K^\\prime\\) is the nearest neighbour coupling for the \\(s^\\prime\\) spins. We conclude (with a little more thought than we detail here) that\n\\[\nK^\\prime=K^2\n\\tag{9.6}\\]\nOf course many other terms and couplings are generated by the higher orders of the high temperature expansion and it is necssary to include these if one wishes reliable values for the critical temperature and exponents, However, our aim here is to use this simple calculation to illustrate the RG method. Let us therefore close our eyes, forget about the higher order terms and show how the RGT Equation 9.6 can be used to obtain information on the phase transition.\n\n\n\n\n\n\nFigure 9.6: Coupling flow under the decimation transformation described in the text.\n\n\n\nThe first point to note is that that mathematically Equation 9.6 has the fixed point \\(K^*= 1\\); if \\(K= 1\\) then the new effective coupling \\(K^\\prime\\) has the same value \\(1\\). Further, if \\(K\\) is just larger than \\(1\\), then \\(K^\\prime\\) is larger than \\(K\\), i.e. further away from \\(1\\). Similarly, if \\(K\\) is less than \\(1\\), \\(K^\\prime\\) is less than \\(K\\). We say that the fixed point is unstable: the flow of couplings under repeated iteration of Equation 9.6 is away from the fixed point, as illustrated in Figure 9.6. The physical significance of this is as follows: suppose that the original system is at its critical point so that the ratio of \\(\\xi\\) to lattice spacing is infinite. After one application of the decimation transformation, the effective lattice spacing has increased by a factor of two, but this ratio remains infinite; the new system is therefore also at its critical point. Within the approximations inherent in Equation 9.6, the original system is an Ising model with nearest neighbour coupling \\(K\\) and the new system is an Ising model with nearest neighbour coupling \\(K^\\prime\\). If these two systems are going to be at a common critically, we must identify \\(K^\\prime=\nK\\). The fixed point \\(K^*= 1\\) is therefore a candidate for the critical point \\(K_c\\), where the phase transition occurs. This interpretation is reinforced by considering the case where the original system is close to, but not at, criticality. Then \\(\\xi\\) is finite and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing is reduced by a factor of two. This instability of a fixed point to deviations of \\(K\\) from \\(K^*\\) is a further necessary condition for its interpretation as a critical point of the system. In summary then we make the prediction\n\\[\nK_c=J/k_BT_c=1\n\\tag{9.7}\\]\nWe can obtain further information about the behaviour of the system close to its critical point. In order to do so, we rewrite the transformation (Equation 9.6) in terms of the deviation of the coupling from its fixed point value. A Taylor expansion of the function \\(K^\\prime=K^2\\) yields\n\\[\\begin{aligned}\nK^\\prime =& (K^*)^2 +(K-K^*)\\left.\\frac{\\partial K^\\prime}{\\partial K}\\right|_{K=K^*}+\\frac{1}{2}(K-K^*)^2\\left.\\frac{\\partial^2 K^\\prime}{\\partial K^2}\\right|_{K=K^*}+\\ldots\\nonumber\\\\\nK^\\prime - K^* =& 2 (K - K^*)+ (K - K^*)^2\n\\end{aligned}\\]\nwhere in the second line we have used the fact that the first derivative evaluates to \\(2K^*=2\\) and \\((K^*)^2=K^*\\).\nFor a system sufficiently close to its critical temperature the final term can be neglected. The deviation of the coupling from its fixed point (critical) value is thus bigger for the new system than it is for the old by a factor of two. This means that the reduced temperature is also bigger by a factor of two:\n\\[t^\\prime= 2t\\]\nBut \\(\\xi\\) (in units of the appropriate lattice spacing) is smaller by a factor of \\(1/2\\):\n\\[\\xi^\\prime= \\xi/2\\]\nThus, when we double \\(t\\), we halve \\(\\xi\\), implying that\n\\[\\xi\\propto t^{-1}\\]\nfor \\(T\\) close to \\(T_c\\). Thus we see that the RGT predicts scaling behaviour with calculable critical exponents. In this simple calculation we estimate the critical exponent \\(\\nu=1\\) for the square lattice Ising model. This prediction is actually in agreement with the exactly established value. The agreement is fortuitous- the prediction in Eq. refeq:Kc for \\(K_c\\), is larger than the exactly established value by a factor of more than two. In order to obtain reliable estimates more sophisticated and systematic methods must be used.\nThe crude approximation in the calculation above produced a transformation, Equation 9.6, involving only the nearest neighbour coupling, with the subsequent advantages of simple algebra. We pay a penalty for this simplicity in two ways: the results obtained for critical properties are in rather poor agreement with accepted values, and we gain no insight into the origin of universality.\n\n\n9.9.2 Universality and scaling\nIn order to expose how universality can arise, we should from the start allow for several different kinds of coupling \\(J_a\\), and show how the systems with different \\(J_a\\) can have the same critical behaviour.\n\n\n\n\n\n\nFigure 9.7: General flow in coupling space\n\n\n\nFigure 9.7 is a representation of the space of all coupling strengths \\(K_a\\) in the energy function \\({\\cal H}/k_BT\\). This is of course actually a space of infinite dimension, but representing three of these, as we have done, enables us to illustrate all the important aspects. First let us be clear what the points in this space represent. Suppose we have some magnetic material which is described by a given set of exchange constants \\(J_1,J_2,J_3.....\\) As the temperature \\(T\\) varies, the coupling strengths \\(K_a=J_a/k_BT\\) trace out a straight line, or ray, from the origin of the space in the direction (\\(J_1,J_2,J_3 ....\\) ). Points on this ray close to the origin represent this magnet at high temperatures, and conversely points far from the origin represent the magnet at low temperatures. The critical point of the magnet is represented by a specific point on this ray, \\(K_a=\nJ_a/k_BT, a= 1,2,\\dots\\) The set of critical points on all of the possible rays forms a surface, the critical surface. Formally, it is defined by the set of all possible models (of the Ising type) which have infinite \\(\\xi\\). It is shown schematically as the shaded surface in Figure 9.7. (In the figure it is a two-dimensional surface; more generally it has one dimension less than the full coupling constant space, dividing all models into high and low temperature phases.)\nOur immediate goal then is to understand how the RGT can explain why different physical systems near this critical surface have the same behaviour. Let us turn now to the schematic representation of the RG flow in Figure 9.7. Suppose we start with a physical system, with coupling strengths \\(K_a,  a= 1,2, \\dots\\). What the RGT does is generate a new point in the figure, at the coupling strengths \\(K_a^{(1)}=f_a({\\bf K})\\); these are the couplings appearing in the effective energy function describing the coarse-grained system. If we repeat the transformation, the new energy function has coupling strengths \\(K_a^{(2)}=f_a({\\bf K})\\). Thus repeated application of the transformation generates a flow of points in the figure: \\({\\bf K\\to\nK^{(1)}\\to\\dots\\to K^{(n)}}\\) where the superscript (\\(n\\)) labels the effective couplings after \\(n\\) coarse-graining steps. if the change in coarse-graining scale is \\(b\\) (\\(&gt; 1\\)) at each step, the total change in coarse-graining scale is \\(b^n\\) after \\(n\\) steps. In the process, therefore, the ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor of \\(b^{-n}\\). The dots in Figure 9.7 identify three lines of RG flow starting from three systems differing only in their temperature. (The flow lines are schematic but display the essential features revealed in detailed calculations.)\nConsider first the red dots which start from the nearest neighbour Ising model at its critical point. The ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor b at each step, but, since it starts infinite, it remains infinite after any finite number of steps. In this case we can in principle generate an unbounded number of dots, \\({\\bf K^{(1)}, K^{(2)},\\dots,K^{(n)}}\\), all of which lie in the critical surface. The simplest behaviour of such a sequence as \\(n\\) increases is to tend to a limit, \\(K^*\\), say. In such a case\n\\[K^*_a=f_a(K^*)~~~~ a= 1,2 .....\\]\nThis point \\({\\bf K^*} \\equiv K_1^*, K_2^*, \\dots\\) is therefore a fixed point which lies in the critical surface.\nBy contrast, consider the same magnet as before, now at temperature \\(T\\) just greater than \\(T_c\\), its couplings \\(K_a\\), will be close to the first red dot (in fact they will be slightly smaller) and so will the effective couplings \\(K_a^{(1)},K_2^{(2)},\\dots\\) of the corresponding coarse-grained systems. The new flow will therefore appear initially to follow the red dots towards the same fixed point. However, the flow must eventually move away from the fixed point because each coarse-graining now produces a model further from criticality. The resulting flow is represented schematically by one set of black dots. The other set of black dots shows the expected flow starting from the same magnet slightly below its critical temperature.\nWe are now in a position to understand both universality and scaling within this framework. We will suppose that there exists a single fixed point in the critical surface which sucks in all flows starting from a point in that surface. Then any system at its critical point will exhibit large-length scale physics (large-block spin behaviour) described by the single set of fixed point coupling constants. The uniqueness of this limiting set of coupling constants is the essence of critical point universality. It is, of course, the algebraic counterpart of the unique limiting spectrum of coarse-grained configurations, discussed in Section 9.5. Similarly the scale-invariance of the critical point configuration spectrum (viewed on large enough length scales) is expressed in the invariance of the couplings under iteration of the transformation (after a number of iterations large enough to secure convergence to the fixed point).\nTo understand the behaviour of systems near but not precisely at critically we must make a further assumption (again widely justified by explicit studies). The flow line stemming from any such system will, we have argued, be borne towards the fixed point before ultimately deviating from it after a number of iterations large enough to expose the system’s noncritical character. We assume that (as indicated schematically in the streams of red and blue lines in Figure 9.7 the deviations lie along a single line through the fixed point, the direction followed along this line differing according to the sign of the temperature deviation \\(T-T_c\\). Since any two sets of coupling constants on the line (on the same side of the fixed point) are related by a suitable coarse-graining operation, this picture implies that the large-length-scale physics of all near- critical systems differs only in the matter of a length scale. This is the essence of near-critical point universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html",
    "href": "phase-transitions/nucleation-and-growth.html",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "",
    "text": "10.1 Introduction to nucleation\nIn previous discussions, we considered first-order phase transitions but deferred a detailed analysis of the dynamical mechanism by which a system evolves from one phase to another. We now address this question explicitly.\nConsider again the Ising model at a temperature \\(T &lt; T_c\\), where the system is initially prepared in the majority spin-up phase at zero external field, \\(H = 0\\). We now examine the system’s response to the application of a small negative external field \\(H &lt; 0\\), which lowers the free energy of the spin-down phase relative to the spin-up phase.\nDespite the global free energy favoring the spin-down phase, the system does not undergo an instantaneous transition. This delay is a consequence of the free energy barrier associated with nucleating a region of the stable phase within the metastable one, as introduced in Chapter 5. The dynamical pathway of the phase transition proceeds via nucleation of localized regions—referred to as droplets—of the stable (spin-down) phase embedded within the metastable (spin-up) background. Once nucleated, these droplets may grow over time and ultimately coalesce to transform the system to the stable phase.\nThe nucleation of a droplet of the stable phase of size \\(n\\) spins entails a competition between bulk and interfacial contributions to the free energy. The bulk free energy gain is linear in \\(n\\), given by \\(-nH\\), due to the alignment of spins with the external field. However, this gain is offset by an interfacial free energy cost arising from broken bonds at the boundary between phases. For the Ising model, each broken bond contributes an energy cost of \\(+2J\\), so the total interfacial energy scales with the perimeter (in 2D) or surface area (in 3D) of the droplet. This interfacial contribution is referred to as the surface tension, and constitutes a true free energy cost: it includes not only the energetic penalty from broken bonds but also an entropic contribution due to the configurational degrees of freedom associated with the droplet shape.\nThe resulting competition between the extensive free energy gain and the sub-extensive interfacial cost leads to a free energy barrier for droplet formation. Only fluctuations that produce a droplet larger than a critical size \\(n_c\\) will grow; smaller droplets will shrink. This framework is formalized in classical nucleation theory, which provides a quantitative description of the nucleation rate, critical droplet size, and the associated activation energy barrier.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation and domain growth</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#classical-nucleation-theory-homogeneous-nucleation",
    "href": "phase-transitions/nucleation-and-growth.html#classical-nucleation-theory-homogeneous-nucleation",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.2 Classical Nucleation Theory: Homogeneous Nucleation",
    "text": "10.2 Classical Nucleation Theory: Homogeneous Nucleation\nWe now present the framework of classical nucleation theory (CNT) for the case of homogeneous nucleation, in which the nucleation of the stable phase occurs spontaneously and uniformly throughout the bulk of the metastable phase, without the aid of impurities, defects, or surfaces.\nLet us consider a droplet of the stable (spin-down) phase of radius \\(R\\) embedded within the metastable (spin-up) background. The total change in free energy \\(\\Delta F(R)\\) associated with forming such a droplet consists of two competing contributions:\n\nBulk free energy gain: The interior of the droplet consists of \\(V \\sim R^d\\) spins aligned with the external field \\(H &lt; 0\\), leading to a volume free energy change\n\\[\n\\Delta F_{\\text{bulk}}(R) = -|\\Delta f| \\, R^d,\n\\]\nwhere \\(|\\Delta f| \\propto |H|\\) is the free energy density difference between the metastable and stable phases, and \\(d\\) is the spatial dimensionality of the system.\nInterfacial free energy cost: The boundary between the two phases has a surface area scaling as \\(R^{d-1}\\), and incurs a free energy cost proportional to the surface tension \\(\\sigma\\):\n\\[\n\\Delta F_{\\text{surface}}(R) = \\sigma \\, S_d \\, R^{d-1},\n\\]\nwhere \\(S_d\\) is a geometrical factor (e.g., \\(S_2 = 2\\pi\\) in 2D and \\(S_3 = 4\\pi\\) in 3D).\n\nThe total free energy change is therefore given by\n\\[\n\\Delta F(R) = \\sigma \\, S_d \\, R^{d-1} - |\\Delta f| \\, V_d \\, R^d,\n\\]\nwhere \\(V_d\\) is another dimension-dependent constant. This expression exhibits a characteristic maximum at a critical droplet radius \\(R_c\\), obtained by extremizing \\(\\Delta F(R)\\) with respect to \\(R\\):\n\\[\n\\frac{d \\Delta F}{dR} = 0 \\quad \\Rightarrow \\quad R_c = \\frac{(d-1)\\sigma S_d}{d |\\Delta f| V_d}.\n\\]\n\n\n\n\n\n\nFigure 10.1: Free energy barrier \\(\\Delta F(r)\\) for nucleation of a spherical droplet as a function of radius \\(R\\) (schematic)\n\n\n\nThe corresponding free energy barrier for nucleation is\n\\[\n\\Delta F_c = \\Delta F(R_c) = \\frac{(d-1)^{d-1}}{d^d} \\cdot \\frac{(S_d)^d \\, \\sigma^d}{(|\\Delta f|)^{d-1} \\, (V_d)^{d-1}}.\n\\]\nThis barrier must be surmounted by thermal fluctuations in order for a critical nucleus to form and grow. The nucleation rate per unit volume is given (in the Arrhenius approximation) by\n\\[\nI \\sim I_0 \\exp\\left( -\\frac{\\Delta F_c}{k_B T} \\right),\n\\]\nwhere \\(I_0\\) is a prefactor determined by microscopic kinetics, and \\(k_B\\) is Boltzmann’s constant.\n\n10.2.1 Interpretation and Scaling Behavior\nSeveral key features emerge from this analysis:\n\nBarrier scaling: The nucleation barrier \\(\\Delta F_c \\sim \\sigma^d / |\\Delta f|^{d-1}\\) diverges as \\(H \\to 0\\), reflecting the increasing stability of the metastable phase near the coexistence point.\nCritical radius: The critical droplet size \\(R_c \\sim \\sigma / |\\Delta f|\\) also diverges as \\(|\\Delta f| \\to 0\\), indicating that larger fluctuations are required to initiate nucleation close to the coexistence line.\nDimensional dependence: Both \\(\\Delta F_c\\) and \\(R_c\\) exhibit strong dependence on the spatial dimension \\(d\\), with nucleation becoming increasingly suppressed in higher dimensions due to the dominance of interfacial cost.\n\nIn summary, homogeneous nucleation in a first-order transition is governed by a delicate balance between surface tension and bulk free energy gain. Only droplets exceeding a critical size can overcome the barrier and initiate a transition. This sets an intrinsic timescale for the dynamics of phase transformation, which can become extremely long near coexistence due to the exponentially small nucleation rate.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation and domain growth</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#late-time-dynamics-domain-growth-and-coarsening",
    "href": "phase-transitions/nucleation-and-growth.html#late-time-dynamics-domain-growth-and-coarsening",
    "title": "9  Dynamics of first order phase transitions: nucleation and growth",
    "section": "9.3 Late-Time Dynamics: Domain Growth and Coarsening",
    "text": "9.3 Late-Time Dynamics: Domain Growth and Coarsening\nFollowing successful nucleation of a supercritical droplet, the system enters a regime where the global transformation is driven by the deterministic growth of domains of the stable phase. This phase of the dynamics is often referred to as coarsening or phase ordering dynamics.\nAt late times, the dynamics are controlled not by rare fluctuations, but by the energetically-driven evolution of domain structures. The nature of domain growth depends sensitively on whether the order parameter is conserved or not.\n\n\n9.3.1 Non-Conserved Order Parameter Dynamics (Model A)\nIn systems with a non-conserved order parameter, such as the Ising model with single spin flip (so called Glauber) dynamics, the order parameter can relax locally without constraint. This leads to curvature-driven motion of interfaces.\nThe typical domain size \\(L(t)\\) grows algebraically with time:\n\\[\nL(t) \\sim t^{1/2},\n\\]\ncorresponding to a dynamic exponent \\(z = 2\\). The growth is driven by reduction of the total interfacial area—regions of high curvature (small domains) shrink and are absorbed by larger, flatter ones.\nLet is aassume that domain walls move with velocity proportional to curvature: \\[\n  v \\sim \\frac{1}{L}\n  \\] Then with the typical domain size being \\(L(t)\\) it follows that\n\\[\n  \\frac{dL}{dt} \\sim \\frac{1}{L}\n  \\]\nIntegrating both sides yields the \\(t^{1/2}\\) domain growth law.\nIt turns out that the detailed evolution of the coarse-grained order parameter field \\(\\phi(\\mathbf{r}, t)\\) is governed by the Allen–Cahn equation:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = - \\frac{\\delta F[\\phi]}{\\delta \\phi},\n\\]\nwhere \\(F[\\phi]\\) is a coarse-grained Ginzburg–Landau free energy functional:\n\\[\nF[\\phi] = \\int d^d x \\left[ \\frac{1}{2} (\\nabla \\phi)^2 + V(\\phi) \\right],\n\\]\nwith \\(V(\\phi)\\) typically a double-well potential such as \\(V(\\phi) = \\frac{1}{4}(\\phi^2 - 1)^2\\).\n\n\n\n9.3.2 Conserved Order Parameter Dynamics (Model B)\nFor systems with a conserved order parameter, such as phase separation in binary alloys or the Ising model with spin-swap (so called ‘Kawasaki’) dynamics, the order parameter (e.g., composition or particle number) must be conserved locally. This imposes a diffusive constraint on the dynamics.\nThe domain size again grows algebraically, but with a different exponent:\n\\[\nL(t) \\sim t^{1/3},\n\\]\ncorresponding to a dynamic exponent \\(z = 3\\).\nA chemical potential difference drives diffusion and is given by \\[\n\\Delta \\mu \\sim \\frac{\\sigma}{L}\n\\] (again due to curvature, where \\(\\sigma\\) is surface tension)\nNow the flux is proportional to the chemical potential gradient (Fick’s law): \\[\n\\text{Flux} \\sim -\\nabla \\mu \\sim \\frac{\\Delta \\mu}{L} \\sim \\frac{\\sigma}{L^2}\n\\]\nand the rate of change of domain size is proportional to this flux: \\[\n\\frac{dL}{dt} \\sim \\frac{1}{L^2}\n\\quad\\Rightarrow\\quad\nL(t) \\sim t^{1/3}\n\\]\nIt turns out that the detailed dynamics are described by the Cahn–Hilliard equation, a continuity equation of the form:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = \\nabla^2 \\left( \\frac{\\delta F[\\phi]}{\\delta \\phi} \\right),\n\\]\nreflecting that the order parameter can only evolve via diffusion of its conjugate chemical potential. This leads to the slow transport of material across domains and a more sluggish coarsening process compared to the non-conserved case.\n\n\n9.3.3 Schematic of Domain Growth in 2D Ising model\nHere are schematic illustrations of domain growth for:\n\nNon-conserved dynamics (Model A): Domains coarsen rapidly, with smoother and larger regions due to free relaxation of the order parameter.\nConserved dynamics (Model B): Coarsening is slower and domains are more intricate, reflecting the constraint of local conservation.\n\n\n\n\n\n\n\n\n\nNon-Conserved Dynamics (Model A)\n\n\n\n\n\n\n\nConserved Dynamics (Model B)\n\n\n\n\n\n\nFigure 9.2: Schematic illustrations of domain morphology resulting from non-conserved and conserved dynamics.\n\n\n\n\n\n9.3.4 Dynamics Scaling Hypothesis\nAt late times, both conserved and non-conserved systems exhibit dynamic scaling: the statistical properties of the domain morphology become self-similar under rescaling of lengths by \\(L(t)\\).\nFor example, the equal-time two-point correlation function satisfies\n\\[\nC(r, t) = f\\left(\\frac{r}{L(t)}\\right),\n\\]\nwhere \\(f(x)\\) is a time-independent scaling function. Plots of \\(C(r, t)\\) collapse when plotted as a function of \\(r/L(t)\\).\nThe structure factor \\(S(k, t)\\), which is the Fourier transform of the correlation function, is experimentally accessible eg via X-ray or neutron scattering, and also obeys dynamic scaling:\n\\[\nS(k, t) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r, t).\n\\]\nSubstituting the scaling form of \\(C(r, t)\\) into this expression, and changing variables to \\(\\vec{u} = \\vec{r}/L(t)\\), gives:\n\\[\nS(k, t) = L(t)^d \\int d^d u \\, e^{-i \\vec{k} \\cdot L(t) \\vec{u}} \\, f(u) = L(t)^d \\, g(kL(t)),\n\\]\nwith \\(g(x)\\) a universal scaling function dependent on the dynamical class and dimensionality.\n\n\n9.3.5 Summary of Growth Laws\n\n\n\n\n\n\n\n\n\n\nDynamics Type\nConservation\nEquation Type\nGrowth Law\nDynamic Exponent\n\n\n\n\nModel A (e.g. Glauber)\nNo\nAllen–Cahn\n\\(L(t) \\sim t^{1/2}\\)\n\\(z = 2\\)\n\n\nModel B (e.g. Kawasaki)\nYes\nCahn–Hilliard\n\\(L(t) \\sim t^{1/3}\\)\n\\(z = 3\\)\n\n\n\n\n\nRemarks: The domain growth exponents \\(1/z\\) are robust under many conditions, but can be modified in the presence of disorder, long-range interactions, or hydrodynamic effects.\n\n\nIn both the model A and model B cases, the system coarsens until it reaches equilibrium, characterized by a uniform macroscopic phase and the complete elimination of interfaces.\n\n\nThe approach to equilibrium is algebraically slow (described by power laws) due to the scale-free nature of domain dynamics.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nucleation and domain growth</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html",
    "href": "phase-transitions/master-equation-and-diffusion.html",
    "title": "10  Introduction to Stochastic Processes",
    "section": "",
    "text": "10.1 The Master Equation\nMany natural phenomena are stochastic — they involve randomness in their evolution over time.\nIn classical physics, this randomness may arise from our lack of knowledge about microscopic details. For example, in a gas, we do not know the precise positions and velocities of each particle, so their collisions with the container walls appear random.\nIn quantum mechanics, stochasticity is even more intrinsic. The fundamental objects are probability amplitudes, and outcomes are inherently probabilistic.\nTo describe these systems effectively, we use a coarse-grained probabilistic description, which tracks the likelihood of different outcomes rather than precise trajectories. One of the key tools in this approach is the master equation.\nConsider a system in microstate \\(i\\) with energy \\(E_i\\). It can transition to neighboring microstates \\(j\\), where the energy difference \\(|E_j - E_i|\\) is small (within \\(\\delta E\\)).\nLet \\(\\nu_{ij}\\) be the rate at which the system jumps from state \\(i\\) to state \\(j\\). Over an infinitesimal time interval \\(dt\\), the probability \\(p_i\\) changes as:\n\\[\ndp_i = \\left[ -p_i \\sum_j  \\nu_{ij} + \\sum_j \\nu_{ji} p_j \\right] dt\n\\]\nThis expression contains two terms:\nThe master equation becomes:\n\\[\n\\frac{dp_i}{dt} = -\\sum_j \\nu_{ij} p_i + \\sum_j \\nu_{ji} p_j\n\\]\nThis is a linear first-order differential equation for the vector of probabilities \\(\\{p_i\\}\\).\nAlternatively, in matrix form:\n\\[\n\\frac{d\\mathbf{p}}{dt} = W \\mathbf{p}\n\\]\nwhere \\(W\\) is the rate matrix with entries:\nThis structure ensures probability conservation: the total probability \\(\\sum_i p_i = 1\\) remains constant in time.\nThe master equation is first order in time and does not have time reversal symmetry so describes an irreversible process. This irreversibility arises from the coarse-graining process that throws away information about underlying microphysics which is described by Newton’s equations and which are time reversible. Only by doing so is the entropy allowed to increase which is required by the second law of thermodynamics for an irreversible process. Consequently the increase of entropy is linked to our knowledge about the system rather than anything it is doing internally in a manner that may appear dubious. Can it be possible that macroscopic and reproducible phenomena such as heat flow depend on how we handle information? Perhaps yes since the division between work and heat is somewhat arbitrary. Were we able to track all the particle positions there would be no need to talk about heat energy or heat flow.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "title": "10  Introduction to Stochastic Processes",
    "section": "",
    "text": "A loss term: the system leaves state \\(i\\) at rate \\(\\nu_{ij}\\),\nA gain term: the system arrives in state \\(i\\) from other states \\(j\\) at rate \\(\\nu_{ji}\\).\n\n\n\n\n\n\n\n\n\\(W_{ij} = \\nu_{ji}\\) for \\(i \\neq j\\),\n\\(W_{ii} = -\\sum_{j \\neq i} \\nu_{ij}\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "title": "10  Introduction to Stochastic Processes",
    "section": "10.2 From the Master Equation to the Diffusion Equation",
    "text": "10.2 From the Master Equation to the Diffusion Equation\nNow consider the situation where the state index \\(i\\) corresponds to a position in space, \\(x_i = i a\\), ie a one-dimensional lattice with lattice spacing \\(a\\), and transitions only occur between neighboring lattice sites.\nWe assume:\n\nTransition rates are symmetric: \\(\\nu_{i, i+1} = \\nu_{i, i-1} = \\nu\\),\nThe spacing \\(a\\to 0\\) in the continuum limit.\n\nThe master equation becomes a finite-difference equation:\n\\[\\begin{aligned}\n\\frac{dp_i}{dt} =  &\\sum_j\\nu_{ij}(p_j-p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i-1}-p_i) + \\nu(p_{i+1} - p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i+1} + p_{i-1} - 2p_i)\n\\end{aligned}\\]\nWe now define a continuous variable \\(x = i a\\), and a probability density \\(p(x, t)\\) such that \\(p_i(t) \\approx p(x, t)\\).\nUsing Taylor expansions:\n\\[\np(x \\pm a, t) = p(x, t) \\pm a \\frac{\\partial p}{\\partial x} + \\frac{a^2}{2} \\frac{\\partial^2 p}{\\partial x^2} + \\cdots\n\\]\nSubstituting into the master equation gives:\n\\[\n\\frac{\\partial p}{\\partial t} =\n\\nu a^2 \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nDefining the diffusion constant \\(D = \\nu a^2\\), we obtain the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThe diffusion equation describes the evolution of the probability density of a particle with diffusion constant (sometimes called diffusivity) given by \\(D = \\nu a^2\\). The dimensions of \\(D\\) are \\([\\text{length}]^2/[\\text{time}]\\). Typically, after expansion, we set the lattice spacing \\(a\\) to 1. Additionally, the diffusion equation can describe many non-interacting diffusing particles. In this case, we replace \\(p\\) with \\(\\rho\\), representing the density or concentration of particles, and use the normalization \\(\\int dx \\, \\rho = M\\), where \\(M\\) is the number of particles.\nThe diffusion equation, much like the master equation from which it originates, explicitly violates time-reversal symmetry, thus permitting entropy to increase.\nThe solution of the diffusion equation for an initial condition where the particle is initially localized at the origin (formally, \\(p(x,0) = \\delta(x)\\)) is a Gaussian:\n\\[\np(x,t) = (4 \\pi D t)^{-1/2} \\exp\\left[-\\frac{x^2}{4 D t}\\right]\n\\]\nWe explicitly see the arrow of time by examining this Gaussian solution at various times \\(t\\). As \\(t\\) increases, the Gaussian “bell-shaped” curve spreads out. Its width grows according to \\(\\langle x^2 \\rangle^{1/2} \\sim t^{1/2}\\). This is known as “diffusive scaling,” and it implies that, after time \\(t\\), a particle will typically be found at a distance roughly proportional to \\(t^{1/2}\\) from its starting point. Conversely, exploring a region of size \\(L\\) typically requires a time of order \\(O(L^2)\\).\nThe evolution of the solution to the 1d diffusion equation in a spatial region \\(x=[0,1]\\) as a function of times are shown in the movie below. The diffusion constant is \\(D=0.01\\). The movie corresponds to a particle initialised at \\(x=0.5\\). One sees how the probability density spreads out over the range as time increases. This can be used to model the diffusion of particles down a concentration gradient as you will see in the next part of the course.\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport os\n\n# Parameters\nL = 1.0        # Length of the domain\nT = 1.0        # Total time \nnx = 400       # Number of spatial points\nnt = 2000      # Number of time steps\nD = 0.1        # Diffusion coefficient\n\n# Discretization\ndx = L / (nx - 1)\ndt = T / nt\n\n# Stability condition auto-adjust\nif D * dt / dx**2 &gt; 0.5:\n    print(\"Adjusting dt and nt to satisfy stability condition...\")\n    dt = 0.4 * dx**2 / D\n    nt = int(T / dt)\n    dt = T / nt  # Recalculate dt exactly\n\nnt = int(nt / 50)  # Artificial slowdown for animation\n\nprint(f\"Using dt = {dt:.4e}, nt = {nt}\")\n\n# Initialize x and u\nx = np.linspace(0, L, nx)\n\n# Initial condition: smooth narrow Gaussian\nsigma = 0.01\nu = np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n# Normalize initial condition\nu /= np.sum(u) * dx\n\n# Setup figure\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\nfig, ax = plt.subplots()\nline, = ax.plot(x, u)\n\n# Compute peak height for setting y-axis\npeak_height = 1 / (np.sqrt(2 * np.pi) * sigma)\nax.set_ylim(0, peak_height * 1.05)\n\nax.set_xlabel(r'$x$', fontsize=20)\nax.set_ylabel(r'$p(x,t)$', fontsize=20)\n\nax.tick_params(axis='both', which='major', labelsize=14)\n\n# Tiny time counter text\ntime_text = ax.text(0.85, 0.05, '', transform=ax.transAxes, fontsize=10, verticalalignment='bottom')\n\n\n# Function to update the plot\ndef update(frame):\n    global u\n    unew = np.copy(u)\n    unew[1:-1] = u[1:-1] + D * dt / dx**2 * (u[2:] - 2*u[1:-1] + u[:-2])\n    u = unew\n\n    # Normalize at every step\n    u /= np.sum(u) * dx\n\n    line.set_ydata(u)\n    current_time = frame * dt\n    time_text.set_text(r'$t=%.4f$' % current_time)\n    return line, time_text\n\nani = animation.FuncAnimation(fig, update, frames=nt, interval=100, blit=True)\n\n\n# Save the animation as a movie\nWriter = animation.writers['ffmpeg']\nwriter = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nani.save(\"../Movies/diffusion_evolution.mp4\", writer=writer)\n\n\nplt.show()",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "href": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "title": "10  Introduction to Stochastic Processes",
    "section": "10.3 Consequences of time reversal symmetry",
    "text": "10.3 Consequences of time reversal symmetry\nAs we have seen by introducing a type of coarse graining, the master equation violates time reversal symmetry of the underlying Newtonian dynamics. Remarkably however, the fact that the underlying microphysics is actually time reversal symmetric has several deep consequences which survive the coarse graining procedure These results are some of the cornerstones of nonequilibrium thermodynamics\n\n10.3.1 Detailed balance\nRecall from year 2 Statistical Mechanics that for a system in equilibrium, the principle of equal a-priori probabilities of microstates holds. Therefore\n\\[\n\\nu_{ij} p_i^{eq} = \\nu_{ji} p_j^{eq}\n\\]\nHence, on average, the actual rate of quantum jumps from \\(i\\) to \\(j\\) (the left-hand side) is the same as from \\(j\\) to \\(i\\). This is a stronger statement than the master equation, which asserts only that there is overall balance between the rate of jumping into and out of state \\(i\\) in equilibrium. The result above is known as the principle of detailed balance.\nThis principle is powerful because it applies not only to individual states but also to any grouping of states.\nExercise: Show that for two groups of states, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\nu_{AB} p_A^{eq} = \\nu_{BA} p_B^{eq}\n\\]\nHence, detailed balance arguments can be extended to subsystems within a large isolated system, and even to systems that are not isolated. However, in such cases, the principle is far from obvious, because once states are grouped together:\n\\[\n\\nu_{AB} \\ne \\nu_{BA}, \\quad p_A \\ne p_B\n\\]\n(This can be easily demonstrated, for example, by considering two groups that contain different numbers of states with similar energies.) Nonetheless, the detailed balance relation holds, in equilibrium, in the general form above.\n\n\n10.3.2 Computer simulation\nIn computer simulation, good results will be obtained if one accurately follows the microscopic equations of motion. This is the molecular dynamics (MD) method which we now outline.\nMolecular dynamics\nMolecular Dynamics (MD) involves a system of classical particles interacting through specified interparticle forces. The motion of these particles is determined by numerically integrating Newton’s equations of motion. In MD simulations, averages of state variables are obtained as time averages over trajectories in phase space. Typically, the forces acting between particles are conservative, ensuring that the total energy \\(E\\) remains constant. This conservation implies that the motion is restricted to a \\((2dN - 1)\\)-dimensional surface in phase space, denoted by \\(\\Gamma(E)\\).\nA central aspect of MD is the averaging of observables. For a given observable \\(A\\), its average is computed as the time average along the trajectory. Mathematically, this is expressed as:\n\\[\n\\left\\langle A(\\{ \\mathbf{p}_i \\}, \\{ \\mathbf{r}_i \\}) \\right\\rangle = \\frac{1}{\\tau} \\int_{t_0}^{t_0+\\tau} dt\\, A(\\{ \\mathbf{p}_i(t) \\}, \\{ \\mathbf{r}_i(t) \\})\n\\]\nThis formula represents the integral of the observable over a time interval \\(\\tau\\), normalized by the length of that interval.\nThe practical steps of an MD simulation start with generating an initial random configuration of particle positions \\(\\{ \\mathbf{r}_i \\}\\) and momenta \\(\\{ \\mathbf{p}_i \\}\\). The system’s equations of motion are then iteratively solved using a suitable algorithm to allow it to reach equilibrium. After equilibration, a production run is performed over many time steps to collect meaningful data. Finally, relevant averages, such as pressure or kinetic energy, are calculated from the collected data.\nMonte Carlo\nHowever, to obtain the equilibrium properties of the system, it may be much faster to use a dynamics which is nothing like the actual equations of motion.\nAt first sight, this looks very dangerous; however, if one can prove that in the required equilibrium distribution, the artificial dynamics obey the principle of detailed balance, then it is (almost) guaranteed that the steady state found by simulation is the true equilibrium state.\nThe best known example is the Monte Carlo method, in which the dynamical algorithm consists of random jumps. The jump rates \\(\\nu_{AB}\\) for all pairs of states \\((A, B)\\) take the form:\n\\[\n\\nu_{AB} = \\nu_0 \\quad \\text{if } E_B \\le E_A\n\\]\n\\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)} \\quad \\text{if } E_B \\ge E_A\n\\]\nwhere \\(\\nu_0\\) is a constant.\nExercise: Show that this gives the canonical distribution in steady state.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "title": "12  The Langevin Approach",
    "section": "",
    "text": "12.1 The Random Walk and the Langevin equation\nThe concept of a random walk and its continuum limit — diffusion — introduced in the previous chapter, expresses the time evolution of the probability distribution \\(p(x, t)\\) for a particle’s position \\(x\\) is described by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2},\n\\]\nwhich is a standard example of a so called Fokker-Planck equation, which is second-order in space and first-order in time.\nIn contrast, the Langevin equation provides a stochastic differential equation for the particle’s trajectory \\(x(t)\\). To understand it, consider the motion over a small time increment \\(\\Delta t\\):\n\\[\nx(t + \\Delta t) = x(t) + \\Delta x(t)\n\\]\nHere, \\(\\Delta x(t)\\) is a random displacement. If the lattice spacing is \\(a\\), we define the step statistics as:\n\\[\n\\Delta x(t) =\n\\begin{cases}\n+a & \\text{with probability } \\nu \\Delta t \\\\\n-a & \\text{with probability } \\nu \\Delta t \\\\\n0 & \\text{with probability } 1 - 2\\nu \\Delta t\n\\end{cases}\n\\]\nThis defines a discrete-time, discrete-space random walk. The average and variance of the step are:\nThe steps \\(\\Delta x(t)\\) are uncorrelated across time.\nTo take the continuum limit, we let both \\(a \\to 0\\) and \\(\\Delta t \\to 0\\) in such a way that:\n\\[\na \\propto \\sqrt{\\Delta t}\n\\]\nIn this limit, we obtain the Langevin equation:\n\\[\n\\dot{x}(t) = \\eta(t)\n\\]\nwhere \\(\\eta(t)\\) is a stochastic force (noise) satisfying:\n\\[\n\\langle \\eta(t) \\rangle = 0\n\\]\n\\[\n\\langle \\eta(t) \\eta(t') \\rangle = \\Gamma \\delta(t - t')\n\\]\nThis \\(\\eta(t)\\) is known as white noise — it has zero mean and is uncorrelated at different times.\nThe Langevin equation tells us that the velocity \\(\\dot{x}(t)\\) is purely driven by noise. We can formally integrate it:\n\\[\nx(t) - x_0 = \\int_0^t \\eta(t')\\, dt'\n\\]\nTaking ensemble averages:\nComparing this with the diffusion equation result, we identify:\n\\[\n\\Gamma = 2D\n\\]\nHence, the Langevin description yields the same physical behavior — not just the mean-square displacement but also the full probability distribution \\(p(x, t)\\) — as the diffusion (Fokker-Planck) equation. This equivalence arises from the fact that the integral of many small, independent random steps leads to a Gaussian distribution, in agreement with the solution of the diffusion equation.\nFor more details, see: Stochastic Processes in Physics and Chemistry by N.G. van Kampen (North Holland, 1981).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "title": "12  The Langevin Approach",
    "section": "",
    "text": "Mean: \\(\\langle \\Delta x \\rangle = 0\\)\nVariance: \\(\\langle (\\Delta x)^2 \\rangle = 2 a^2 \\nu \\Delta t = 2D \\Delta t\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean displacement: \\[\n\\langle x(t) - x_0 \\rangle = 0\n\\]\nMean square displacement: \\[\n\\langle [x(t) - x_0]^2 \\rangle = \\int_0^t \\int_0^t \\langle \\eta(t') \\eta(t'') \\rangle\\, dt'\\, dt'' = \\Gamma \\int_0^t dt' = \\Gamma t\n\\]",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "title": "12  The Langevin Approach",
    "section": "12.2 Brownian Motion",
    "text": "12.2 Brownian Motion\nLet us now examine Brownian motion, originally observed as the erratic motion of colloidal particles suspended in a fluid. These particles undergo constant collisions with surrounding (smaller) fluid molecules, which results in seemingly random movement.\nFrom a coarse-grained perspective — where we do not track each individual collision — this appears as motion under random forces. This statistical treatment introduces irreversibility at the macroscopic level, even though the underlying molecular dynamics are reversible.\nThe Langevin equation provides a way to model this behavior. For a particle of mass \\(m\\) in one dimension, Langevin proposed the equation:\n\\[\nm \\ddot{x} = -\\gamma \\dot{x} + f(t)\n\\]\nHere:\n\n\\(-\\gamma \\dot{x}\\) is a frictional damping force, where \\(\\gamma\\) is the damping coefficient.\n\\(f(t)\\) is a random force due to molecular collisions.\n\n\nOften, the mobility is defined as \\(\\mu = 1/\\gamma\\) — note that this is unrelated to chemical potential.\n\n\n12.2.1 Noise Properties\nIn principle the random forces are in time since the molecular collisions which cause them are correlated and have some definite duration.\nLet us assume that there is some correlation time \\(t_c\\) over which \\(\\langle f(t_1) f(t_2) \\rangle = g(t_1 - t_2)\\) decays rapidly as shown in the sketch:\n\n\n\n\n\n\nFigure 12.1: Sketch of \\(g(t1−t2)\\) against \\(|t1− t2|\\)\n\n\n\nThen as long as we consider timescales \\(\\gg t_c\\) we can safely replace \\(g(t_1 - t_2)\\) by a delta function. Thus we can approximate the noise by\n\\[\n\\langle f(t) \\rangle = 0\n\\]\n\\[\n\\langle f(t_1) f(t_2) \\rangle = \\Gamma \\delta(t_1 - t_2)\n\\]\nSuch uncorrelated noise is known as “white noise”.\n\n\n12.2.2 Solving the Langevin Equation (Velocity)\nLet’s set \\(m = 1\\) for simplicity and solve the equation:\n\\[\n\\dot{v} + \\gamma v = f(t)\n\\]\nWe apply an integrating factor:\n\\[\n\\frac{d}{dt} \\left[ v e^{\\gamma t} \\right] = e^{\\gamma t} f(t)\n\\]\nIntegrating both sides:\n\\[\nv(t) = v_0 e^{-\\gamma t} + \\int_0^t e^{-\\gamma (t - t')} f(t')\\, dt'\n\\]\nTaking the average:\n\\[\n\\langle v(t) \\rangle = v_0 e^{-\\gamma t}\n\\]\nThus:\n\nAt short times: (\\(\\gamma t \\ll 1\\)): \\(\\langle v \\rangle \\approx v_0\\) ie. friction is negligible.\nAt long times: (\\(\\gamma t \\gg 1\\)): \\(\\langle v \\rangle \\to 0\\) ie. the system loses memory of the initial velocity.\n\n\n\n12.2.3 Mean-Square Velocity\nWe now compute:\n\\[\n\\langle v(t)^2 \\rangle = v_0^2 e^{-2\\gamma t} + \\Gamma \\int_0^t e^{-2\\gamma (t - t')} dt' = v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma} \\left(1 - e^{-2\\gamma t} \\right)\n\\]\nImplying that at\n\nShort times: \\(\\langle v^2 \\rangle \\approx v_0^2\\)\nLong times: \\(\\langle v^2 \\rangle \\to \\Gamma / (2\\gamma)\\)\n\nAt equilibrium, the equipartition theorem gives:\n\\[\n\\frac{1}{2} m \\langle v^2 \\rangle = \\frac{1}{2} k_B T\n\\]\nUsing this to identify \\(\\Gamma\\):\n\\[\n\\Gamma = 2 \\gamma k_B T\n\\]\nThis important result relates the noise strength to the damping and temperature — they have the same microscopic origin (molecular collisions).\n\n\n12.2.4 Mean-Square Displacement\nWe now integrate \\(v(t)\\) again to get position \\(x(t)\\) (with \\(m = 1\\)):\nUsing the result above and substituting \\(\\Gamma = 2\\gamma k_B T\\), we find:\n\\[\n\\langle [x(t) - x_0]^2 \\rangle = \\frac{(v_0^2 - k_B T)}{\\gamma^2} (1 - e^{-\\gamma t})^2 + \\frac{2 k_B T}{\\gamma} \\left[ t - \\frac{1 - e^{-\\gamma t}}{\\gamma} \\right]\n\\]\nLimiting behaviors:\n\nShort times: (\\(\\gamma t \\ll 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx v_0^2 t^2\n\\]\n(correspinding to ballistic motion)\nLong time (\\(\\gamma t \\gg 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx \\frac{2 k_B T}{\\gamma} t\n\\]\n(corresponding to diffusive motion)\n\nThe effective diffusion constant is:\n\\[\nD = \\frac{k_B T}{\\gamma}\n\\]\nThis is the Einstein relation, connecting the rate of diffusion to temperature and damping. It is useful as it allows an explicit expression for the diffusion constant if one knows \\(\\gamma\\). A famous example is a sphere: the equation for fluid flow past a moving sphere may be solved and yields \\(\\gamma=6\\pi\\eta a\\) where \\(a\\) is the radius of the sphere and here \\(\\eta\\) is the fluid viscosity. This gives\n\\[\nD=\\frac{6\\pi\\eta a}{kT}\n\\] which is the Stokes-Einstein formula for the diffusion constant of a colloidal particle.\n\n\n12.2.5 External Forces and Mobility\nNow consider a charged particle with charge \\(q\\) under an external electric field \\(E\\). The Langevin equation becomes:\n\\[\nm \\dot{v} = -\\gamma v + qE\n\\]\nAt long times, the particle reaches a steady drift velocity:\n\\[\n\\langle v \\rangle = \\frac{qE}{\\gamma} = \\frac{qED}{k_B T}\n\\]\nDefining the mobility \\(\\mu\\) by \\(\\langle v \\rangle = \\mu qE\\), we get the Nernst-Einstein relation:\n\\[\n\\mu = \\frac{D}{k_B T}\n\\]\nThis relation connects the response of a system to an external perturbation (mobility) with its internal fluctuations (diffusivity).\n\n\n12.2.6 Molecular Dynamics simulation of Brownian motion for a colloid particle in a liquid suspension\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom numba import njit\n\n# Parameters\nn_fluid = 300\nbox_size = 20.0\nn_steps = 10000\nsigma_f = 1.0\nsigma_c = 10.0\nepsilon = 0.05\nmass_f = 1.0\nmass_c = 2.0\ndt = 1e-5\ndt_e = 1e-5\n\n# Echo the parameter values\nprint(\"Simulation Parameters:\")\nprint(f\"n_fluid = {n_fluid}\")\nprint(f\"box_size = {box_size}\")\nprint(f\"n_steps = {n_steps}\")\nprint(f\"sigma_f = {sigma_f}\")\nprint(f\"sigma_c = {sigma_c}\")\nprint(f\"epsilon = {epsilon}\")\nprint(f\"mass_f = {mass_f}\")\nprint(f\"mass_c = {mass_c}\")\nprint(f\"dt = {dt}\")\n \n # Derived quantities\n\nsigma_f6 = sigma_f ** 6\nsigma_f12 = sigma_f **12\nsigma_cf = 0.5 * (sigma_f + sigma_c)\nsigma_cf6 = sigma_cf ** 6\nsigma_cf12 = sigma_cf ** 12\n\nnp.random.seed(42)\n\n# Safe initialization to avoid overlaps with colloid and other fluid particles\ndef initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos, min_dist_factor=0.85):\n    min_dist_ff = min_dist_factor * sigma_f\n    min_dist_cf = min_dist_factor * 0.5 * (sigma_f + sigma_c)\n    positions = []\n    max_attempts = 20000\n\n    for _ in range(n_fluid):\n        for attempt in range(max_attempts):\n            trial = np.random.rand(2) * box_size\n            too_close = False\n\n            # Check distance to colloid center\n            if np.linalg.norm(trial - colloid_pos[0]) &lt; min_dist_cf:\n                too_close = True\n\n            # Check distances to already placed fluid particles\n            for existing in positions:\n                if np.linalg.norm(trial - existing) &lt; min_dist_ff:\n                    too_close = True\n                    break\n\n            if not too_close:\n                positions.append(trial)\n                break\n        else:\n            raise RuntimeError(\"Failed to place a fluid particle without overlap after many attempts.\")\n    \n    return np.array(positions)\n\ncolloid_pos = np.array([[box_size / 2, box_size / 2]])\nfluid_pos = initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos)\nfluid_vel = (np.random.rand(n_fluid, 2) - 0.5)\ncolloid_vel = np.zeros((1, 2))\n\n@njit\ndef compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid):\n    forces_f = np.zeros_like(fluid_pos)\n    force_c = np.zeros_like(colloid_pos)\n\n    for i in range(n_fluid):\n        # Fluid-colloid interaction\n        rij = fluid_pos[i] - colloid_pos[0]\n        rij -= box_size * np.round(rij / box_size)\n        dist2 = np.dot(rij, rij)\n        if dist2 &lt; (2.5 ** 2) * ((sigma_cf6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n            r2 = dist2\n            r6 = r2 ** 3\n            r12 = r6 ** 2\n            fmag = 48 * epsilon * ((sigma_cf12 / r12) - 0.5 * (sigma_cf6 / r6)) / r2\n            fvec = fmag * rij\n            forces_f[i] += fvec\n            force_c[0] -= fvec\n\n        for j in range(i + 1, n_fluid):\n            rij = fluid_pos[i] - fluid_pos[j]\n            rij -= box_size * np.round(rij / box_size)\n            dist2 = np.dot(rij, rij)\n            if dist2 &lt; (2.5 ** 2) * ((sigma_f6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n                r2 = dist2\n                r6 = r2 ** 3\n                r12 = r6 ** 2\n                fmag = 48 * epsilon * ((sigma_f12 / r12) - 0.5 * (sigma_f6 / r6)) / r2\n                fvec = fmag * rij\n                forces_f[i] += fvec\n                forces_f[j] -= fvec\n\n    return forces_f, force_c\n\n    fluid_history = []\ncolloid_history = []\nforces_f, force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n\nn_equilibration = 2000  # Number of steps to equilibrate before tracking\n\n# Equilibration phase (no history recorded)\n\nfor step in range(n_equilibration):\n    fluid_pos += fluid_vel * dt_e + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt_e + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt_e\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt_e\n    forces_f = new_forces_f\n    force_c = new_force_c\n    # if step &lt; 10:  # Log only the first few steps\n    #     force_mag = np.linalg.norm(force_c[0])\n    #     print(f\"Step {step:3d} | Colloid Pos: {colloid_pos[0]} | Vel: {colloid_vel[0]} | |F|: {force_mag:.4e}\")\n\n# Production phase (history recorded)\n\nfor step in range(n_steps):\n    fluid_pos += fluid_vel * dt + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt\n    forces_f = new_forces_f\n    force_c = new_force_c\n    if step % 10 == 0:\n        fluid_history.append(fluid_pos.copy())\n        colloid_history.append(colloid_pos.copy())\n\nfig, ax = plt.subplots()\n# Calculate figure and plot scale parameters\nfig_width_inch = fig.get_size_inches()[0]\ndpi = fig.dpi\naxis_length_pt = fig_width_inch * dpi\nmarker_scale = 0.1  # Scale factor for visibility\nfluid_marker_size = (marker_scale * axis_length_pt / box_size) ** 2\ncolloid_marker_size = 0.7*(marker_scale * sigma_c / sigma_f * axis_length_pt / box_size) ** 2\nfluid_scatter = ax.scatter([], [], s=fluid_marker_size, c='blue')\ncolloid_scatter = ax.scatter([], [], s=colloid_marker_size, c='red')\ntrajectory, = ax.plot([], [], 'r--', linewidth=1, alpha=0.5)\nax.set_xlim(0, box_size)\nax.set_ylim(0, box_size)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_aspect('equal')\ncolloid_traj = []\n\ndef init():\n    empty_offsets = np.empty((0, 2))\n    fluid_scatter.set_offsets(empty_offsets)\n    colloid_scatter.set_offsets(empty_offsets)\n    trajectory.set_data([], [])\n    return fluid_scatter, colloid_scatter, trajectory\n\ndef update(frame):\n    fluid_scatter.set_offsets(fluid_history[frame])\n    colloid_scatter.set_offsets(colloid_history[frame])\n    colloid_traj.append(colloid_history[frame][0])\n    traj_array = np.array(colloid_traj)\n    trajectory.set_data(traj_array[:, 0], traj_array[:, 1])\n    return fluid_scatter, colloid_scatter, trajectory\n\nani = animation.FuncAnimation(fig, update, frames=len(fluid_history), init_func=init, blit=True, interval=20)\nani.save(\"brownian_colloid.mp4\", writer=\"ffmpeg\", fps=30)\nprint(\"Simulation complete. Video saved as 'brownian_colloid.mp4'.\")",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html",
    "href": "phase-transitions/dynamics-of-fluctuations.html",
    "title": "13  Dynamics of Fluctuations",
    "section": "",
    "text": "13.1 Linear Response Theory and the Fluctuation-Dissipation Theorem\nNow suppose we gently perturb the system. For example, we might apply a small thermodynamic force \\(f_x\\) that couples to a fluctuating variable \\(x\\) — like a weak magnetic field \\(h\\) acting on a local magnetization.\n(Formally, this means adding a perturbation \\(-f_x x\\) to the Hamiltonian.)\nA typical experimental protocol applies the perturbation from \\(t = -\\infty\\) and switches it off at \\(t = 0\\). For \\(t &gt; 0\\), the average response of another variable \\(y\\) is observed to decay:\n\\[\n\\langle y(t) \\rangle_f = R_{yx}(t) \\, f_x\n\\]\nHere, \\(R_{yx}(t)\\) is the response function describing how \\(y\\) responds to a small force applied to \\(x\\) at earlier times (for \\(t \\geq 0\\)).\nThe key idea is this: If the perturbation is small enough, its effects are indistinguishable from those of a spontaneous fluctuation. So, the decay of the response function should mirror the decay of the correlation function of naturally occurring fluctuations.\nThis is the essence of the fluctuation-dissipation theorem:\n\\[\nk_B T \\, R_{yx}(t) = M_{yx}(t)\n\\]\nThis powerful result says that the system’s response to a small disturbance is directly related to the correlation of fluctuations in thermal equilibrium.\nThe factor \\(k_B T\\) ensures both sides of the equation have the same physical dimensions.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Dynamics of fluctuations</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html#linear-response-theory-and-the-fluctuation-dissipation-theorem",
    "href": "phase-transitions/dynamics-of-fluctuations.html#linear-response-theory-and-the-fluctuation-dissipation-theorem",
    "title": "13  Dynamics of Fluctuations",
    "section": "",
    "text": "Figure 13.2: Eﬀect of perturbation on a quantity which is zero in equilibrium\n\n\n\n\n\n\n\n\n\nNote: We skip the full proof, which requires formal machinery from classical mechanics (e.g., Poisson brackets) or quantum mechanics (density matrices). For more, see the final chapter of Chandler.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Dynamics of fluctuations</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html#onsagers-theorem",
    "href": "phase-transitions/dynamics-of-fluctuations.html#onsagers-theorem",
    "title": "13  Dynamics of Fluctuations",
    "section": "13.2 Onsager’s Theorem",
    "text": "13.2 Onsager’s Theorem\nNow, recall from the previous section that the correlation matrix is symmetric:\n\\[\nM_{xy}(t) = M_{yx}(t)\n\\]\nCombining this with the fluctuation-dissipation theorem gives:\n\\[\nR_{xy}(t) = R_{yx}(t)\n\\]\nThis result is known as Onsager’s reciprocal relation. It states that the response of variable \\(x\\) to a force acting on \\(y\\) is the same as the response of \\(y\\) to a force acting on \\(x\\) — provided the system is in equilibrium.\nThis is a deep and subtle consequence of microscopic reversibility, and Onsager’s real contribution was realizing such a connection could exist. (Onsager also solved the 2D Ising model and won the Nobel Prize in Chemistry in 1968.)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Dynamics of fluctuations</span>"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html",
    "href": "phase-transitions/problems.html",
    "title": "Unifying theoretical concepts: Problems",
    "section": "",
    "text": "Although you should try all of these questions, some of them are deliberately quite challenging. If you don’t get very far with some, don’t worry. We’ll be going over them in problems classes, so you can just regard them as worked examples.\n\nExistence of a phase transition in \\(d=2\\).\n(Straightforward) In lectures it was argued that no long ranged order occurs at finite-temperatures in a one dimensional system because of the presence of domain walls. Were macroscopic domain walls to exist in two dimensions at finite temperature, they would similarly destroy long ranged order and prevent a phase transition. By calculating the free energy of a 2D domain wall for an Ising lattice, show that domain walls do not in fact exist for sufficiently low \\(T\\).\n(Hint: Model the domain wall as a non-reversing \\(N\\)-step random walk on the lattice and find an expression for its energy and -from the number of random walk configurations- its entropy.)\n\n\n\nCorrelation Length\n(Challenging) For a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small (as occurs at low \\(T\\))?\n\n\n\nA model fluid\n(Straightforward) The van der Waals (vdW) equation of state (recall PH10002) is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nUse a graph-plotting program such as “Excel” to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of P vs V on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\nMean field theory of the Ising model heat capacity\n(Straightforward) Using results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle\n  E\\rangle/\\partial T\\) has the behaviour\n\\[\\begin{aligned}\nC_H=& 0 ~~~~ T&gt;T_c\\\\\nC_H=& 3Nk_B/2 ~~~~ T\\le T_c\n\\end{aligned}\\]\n\n\n\nMagnetisation and fluctuations\n(Slightly tricky) A system of spins on a lattice, has, in the absence of an applied field, a Hamiltonian \\({\\cal H}\\). In the presence of a field \\(h\\) the Hamiltonian becomes \\[\\tilde {\\cal H}={\\cal H}-hM\\] where \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\nSpin-1 Ising model\n(Straightforward) A set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j + h\\sum_i s_i\\]\nFind the partition function and hence show that in the mean field approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\nTransfer Matrix.\n(Straightforward strategy but some lengthy algebra required)\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\nLattice gas model (Straightforward). Check the claim, made in lectures that the “Hamiltonian” of the Lattice Gas model in the grand canonical ensemble\n\\[{\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j -\\mu\\sum_ic_i\\]\nis transformed to that of the Ising model by means of the change of variable\n\\[s_i=2c_i-1;~~~~ J=\\frac{\\epsilon}{4}~~~~\nh=\\frac{\\epsilon q+2\\mu}{4}\\]\n(Hint: Note that when doing sums over bonds \\(\\sum_{\\langle\n    i,j\\rangle}\\) for a lattice of coordination \\(q\\) there are \\(q/2\\) bonds per site since each bond is shared between two sites.)\n\n\n\nLandau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\nScaling laws\n(Straightforward.)Using the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\\]\nHence derive the scaling laws among the critical exponents:\n\\[\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\\]\n(Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 9: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\n\n\n\nRenormalization Group transformation for the 1D Ising model\n(Challenging.) Consider a 1D Ising model of \\(N\\) spins in zero field given by the Hamiltonian\n\\[{\\cal H}_I/k_BT=-K\\sum_{&lt;ij&gt;}s_is_j -\\sum_iC\\] where a background term is included because even if set zero initially, it will be generated by an RG transformation.\n\nA renormalisation is to be implemented with a scale factor \\(b=2\\). By partially performing the sum in the partition function over the spins on the even numbered lattice sites, show that the partially summed partition function can be written\n\\[Z=\\sum_{s_1,s_3,s_5...}\\prod_{i=2,4,6...} [\\exp\n[K(s_{i-1}+s_{i+1})+2C] + \\exp[-K(s_{i-1}+s_{i+1})+2C]\\]\nNext, relabel the remaining (odd numbered) spins so that they are numbered consecutively eg by an index \\(j\\) (this is a matter of convenience only). Then cast this partially summed partition function in a form that makes it look the same as that for an Ising model with \\(N/2\\) spins. i.e. require that \\(Z^\\prime\\) has the form\n\\[Z=\\sum_{\\{s\\}}\\prod_{j=1}^{N/2}\\exp[K^\\prime s_js_{j+1}+C^\\prime]\\:,\\] for all \\(s_j,s_{j+1}=\\pm 1\\).\nHence show that the effective coupling \\(K^\\prime\\), and background constant \\(C^\\prime\\) of the renormalised system are related to those of the underlying Hamiltonian by\n\\[\\begin{aligned}\n\\exp(K^\\prime+C^\\prime) &=& \\exp(2C)[\\exp(2K)+\\exp(-2K)] \\\\\n\\exp(-K^\\prime+C^\\prime) &=& 2\\exp(2C)\n\\end{aligned}\\]\nUse these results to show that the effective coupling flows under the renormalization according to:\n\\[K^\\prime = (1/2)\\ln (\\cosh 2K)\\]\nDoes this flow drive the effective coupling to the low or high temperature fixed point?\nFinally show that the partition function transforms under the RG as\n\\[Z(K,N)=2^{N/2}\\exp(NK^\\prime/2)Z(K^\\prime,N/2)\\]\n\nNote. The above is a hard question, which we’ll go over this in detail in a problems class. So if you don’t make much progress, don’t worry, just treat it as a worked example.\n\n\n\nRG flow of the free energy\n(Straightforward) An alternative RG equation for the effective coupling in the 1D Ising model is\n\\[K = (1/2)\\cosh^{-1}(\\exp 2K^\\prime)\\] which is the inverse of the transformation found in the previous question. This transformation thus drives the system from small effective coupling (high temperature) to high effective coupling (low temperature).\nUnder the transformation, the free energy density transforms like\n\\[\\beta f(K)=(1/2)\\beta f(K^\\prime)-(1/2)\\ln 2-K^\\prime/2\\:.\\]\n\nUse these findings to iteratively calculate the free energy density of the Ising model at low temperatures, starting from the high temperature limit.\nHint: Start with a very low coupling eg. \\(K^\\prime=0.01\\), and show that because interactions between spins are negligible, \\(\\beta f(K^\\prime)\\approx\n-\\ln 2\\). Then iterate the RG equations above \\(8\\) or so times to generate a sequence of \\(K\\) and \\(\\beta f(K)\\) values.\nCompare your results for each iteration with the exact results for the free energy density of the 1D Ising model obtained in lectures.\n\n\n\n\nColloidal diffusion\n\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + e^{-\\gamma t} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{kT}{M} + \\left( u_0^2 - \\frac{kT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\nEinstein’s expression for the diffusion coefficient\n\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle \\{ X(t) - X(0) \\}^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nShow that the Einstein relation may be written as\n\\[\n\\gamma = \\frac{1}{\\mu} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\nLife in one dimension\n\nA particle lives on the sites of a one-dimensional lattice. At any instant it has probability \\(\\alpha\\) per unit time that it will hop to the site on its right and probability \\(\\alpha\\) per unit time of hopping to the site on its left.\nWrite down the master equation for the set of probabilities \\(p_n(t)\\) of finding the particle at the \\(n^{\\text{th}}\\) site, where \\(-\\infty &lt; n &lt; \\infty\\).\nSolve the master equation for the \\(p_n\\), subject to the initial condition that the particle was at the site \\(n = 0\\) at time \\(t = 0\\). Hence obtain the mean position \\(\\langle n \\rangle\\) and root mean square deviation from the mean, both as functions of time.\nHint: The second part of the question is most easily done by introducing the generating function\n\\[\nF(z, t) = \\sum_{n=-\\infty}^{\\infty} p_n(t) z^n.\n\\]\n\n\nMaster equation\n\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\nDetailed balance\n\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\nJump processes\n\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump from state 1 to 2 and 2 to 3 but not directly from 1 to 3. Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the transition matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "phase-transitions/live-test.html",
    "href": "phase-transitions/live-test.html",
    "title": "14  Ising model example",
    "section": "",
    "text": "We provide you with a simple package able to run an Ising model simulation.\n\n\n\n\n\n\nYou can access directly the comfigurations as numpy arrays\n\n\n\n\n\n\nQuestion. Can you write a function to calculate the energy of a configuration?",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Coding</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_plan.html",
    "href": "soft-matter/soft-matter_plan.html",
    "title": "15  Plan",
    "section": "",
    "text": "INTRO: Entropy matters!\n\n\n\nPart 1: COLLOIDS\n\nBrownian motion\nInteracting colloidal suspensions\n\n\n\n\nPart 2: POLYMERS\n\nChemical structure\nPolymer conformations\nConcentrated polymer solutions\n\n\n\n\nPart 3: SURFACTANTS\n\nChemical structure\nSelf-assembled structures\n\n\n\n\nPart 4: GLASSES\n\nGlass forming systems\nRelaxation time and viscosity",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Plan</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_intro.html",
    "href": "soft-matter/soft-matter_intro.html",
    "title": "16  Entropy matters",
    "section": "",
    "text": "Most of the matter around us does not simply fit within the idealised pictures of crystallines solids or simple liquids: examples include colloids, polymers, surfactants, liquid crystals, foams, gels, and biological materials such as proteins, DNA, and cell membranes.\nThis means that cellular life itself (the very constituents that make us) obeys to principles that go beyond the standard patters of conventional solid-state physics.\nThis branch of physics is called soft condensed matter physics, or macromolecular physics, or the physics of complex fluids.\nWhile we often think about problems in physics as a matter of energy minimisation, in soft-matter physics a key role is played by the fluctuations. Typically (but not exclusively) these are thermal fluctuations. This means that entropy and not only the energy from the interactions plays a key role.\nThis is because soft matter systems are typically composed of many microscopic constituents in contact with an environment. The appropriate description of the macroscopic state of such systems is therefore statistical and uses the language of statistical mechanics. The relevant energy, therefore, is the free energy of the statistical ensemble representative of the system under consideration. For example, in the canonical ensemble, this is the Helmholtz free energy\n\\[F = U-TS\\]\nwhere \\(U\\) is the internal energy, \\(T\\) the temperature and \\(S\\) is the entropy of the system. Therefore, in a broader sense, soft matter is the physics of those systems for which the internal energy and the entropy are on comparable scales.\nIn other words, fluctuations of the internal energy are on the same scale as thermal fluctuations:\n\\[\\Delta U \\sim k_BT \\]\nwhere \\(k_B\\) is the Bolztmann constant and \\(\\Delta U\\) indicates standard deviations from the average internal energy.\n\n\n\n\n\n\nThe Physics of Entropy\n\n\n\n\n\nSoft matter physics is fundamentally the physics of entropy. Unlike traditional systems where energy minimization dominates, in soft matter, entropy plays a crucial role in determining the structure, dynamics, and behavior of the system. The interplay between entropy and energy leads to the rich and diverse phenomena observed in soft matter systems.\n\n\n\n\nAll these systems contain structural units in the colloidal length scale, \\(1 \\mathrm{~nm} \\lesssim \\mathrm{R} \\lesssim 1 \\mu \\mathrm{~m}\\).\n\n16.0.1 Systems and definitions\n\nA colloid consists of a disperse phase (solid, liquid or gas ‘particles’) distributed in a finely divided state in a dispersion medium (solid, liquid or gas ‘solvent’).\n\n\n\n\n\n\n\n\n\n\ndisp. phase  disp. medium\nsolid\nliquid\ngas\n\n\n\n\nsolid\nsolid suspension: pigmented plastics, stained glass, ruby glass, opal, pearl\nsol, coll. susp.: metal sol, toothpaste, paint, ink, clay slurries, mud\nsolid aerosol: smoke, dust\n\n\nliquid\nsolid emulsion: bituminous road paving, ice cream\nemulsion: milk, mayonnaise, butter, pharmaceutical creams, digestion\nliquid aerosol: fog, mist, tobacco smoke, hair spray\n\n\ngas\nsolid foam: zeolites, expanded polystyrene, ‘silica gel’\nfoam: froths, soap foam, fire-extinguisher\n-\n\n\n\n\nA polymer is a large molecule made up of many (hundreds to millions) repeats of a single subunit (‘monomer’). It is kept together by covalent chemical bonds. Different architectures exist: linear-chain homopolymers, (block) copolymers, starpolymers, dendrimers etc. We concentrate on linear chains, which also represents the main building block of the other structures. Examples: polyethylene, cellulose, DNA, proteins.  \nA surfactant (SURFace ACTive AgeNT) is a special kind of molecule. In the simplest case, it has a water-loving (hydrophilic) ‘head group’, and a water-hating (hydrophobic) ‘tail’. Above a certain concentration they spontaneously selfassemble into a wide variety of structures, the simplest being a spherical micelle. They associate by (rather weak) physical interactions (‘association colloids’). Examples:  Sodium Dodecylsulfate (SDS), lipids.\n\nNote: In general, when in bulk form, the substance forming colloidal particles is quite insoluble in the dispersion medium. However, when the bulk material is broken down into very small pieces it can be dispersed. Such a system is called a lyophobic (‘solvent-avoiding’) colloid. Some colloidal particles themselves consist of polymers or surfactant assemblies which can easily be dispersed and are thus labelled lyophilic (‘solvent-loving’). Mostly when we talk about “colloids”, we do not care about the ‘internal’ structure of the particles, while we are in general interested in the structure of polymers or surfactant assemblies.\n\nMany liquid-state soft matter system, particular colloids and polymers can be cooled to very low temperatures without crystallizing. At these low temperatures, the typical relaxation times of the particles becomes very high, even on experimental time scales. Thus the liquid-like disorder is “frozen-in” and we refer to the system as a glass.\n\n\n\n16.0.2 Macromolecules: principal distinguishing features\n\n16.0.2.1 Size Range ( \\(1 \\mathrm{~nm} \\lesssim R \\lesssim 1 \\mu \\mathrm{~m}\\) )\nLower limit (about 1 nm ): This ensures that the size of the dispersed particles is much larger than the size of the molecules forming the dispersion medium. This not only distinguishes colloids from ordinary (true) solutions, but also results in a ‘welldefined’ interface between the particles and the surrounding medium. Furthermore, (most of the time) this allows us both, to treat the dispersion medium as a continuum and to avoid quantum mechanics.\n\n\n16.0.2.2 Upper limit (about \\(1 \\mu \\mathrm{~m}\\) ):\n\nBalance of thermal and potential energy:\n\nEvery particle has thermal energy \\(\\mathrm{E}_{\\text {therm }}=(3 / 2) k_{B} T\\) and potential energy \\(E_{\\mathrm{pot}}=m g z=(4 \\pi / 3) R^{3} \\Delta \\rho g z\\), where \\(z\\) is a characteristic length, e.g. \\(R\\), and \\(\\Delta \\rho\\) is the density difference between the particle and the dispersion medium. For a significant influence of the thermal energy we need \\(E_{\\mathrm{pot}}&lt;E_{\\text {therm }}\\) and thus for a typical system \\(\\left(\\Delta \\rho \\approx 0.3 \\mathrm{~g} \\mathrm{~cm}^{-3}\\right): \\quad R&lt;\\left(9 k_{B} T / 8 \\pi \\Delta \\rho g\\right)^{1 / 4} \\approx 1 \\mu \\mathrm{~m}\\).\n\n16.0.2.2.1 Sedimentation:\nGravity will lead to sedimentation of the suspended particles (or creaming, if the density of the particles is lower than the density of the dispersion medium) and to the formation of a dense sediment.\n\nThe sedimentation velocity \\(v_{\\text {sed }}\\) results from the balance of gravity and hydrodynamic drag:\n\\[\n\\begin{aligned}\n& \\quad \\text { gravity: } \\\\\nF_{g}= & m_{\\text {part }} g-m_{\\text {liq }} g \\\\\n= & V\\left(\\rho_{\\text {part }}-\\rho_{\\text {liq }}\\right) g \\\\\n& =V \\Delta \\rho g \\\\\n\\downarrow & \\text { sphere } \\\\\n& F_{g}=\\frac{4 \\pi}{3} R^{3} \\Delta \\rho g\n\\end{aligned}\n\\]\n\nfriction (Stokes law): where \\(m\\) is the mass and \\(\\rho\\) the density of the particle (part) and liquid (liq), respectively, \\(\\Delta \\rho\\) the density difference, \\(V\\) the volume and \\(R\\) the radius of the particle, \\(\\eta\\) the viscosity, \\(v\\) the sedimentation velocity and \\(g\\) the acceleration due to gravity.\n\n\n\n\n16.0.3 Brownian motion and random walks\nRandom collisions with the liquid molecules cause the particle to change its direction of motion rapidly (resulting in ‘mixing’ the sample). This can be modelled as a random (drunk!) walk of \\(N\\) steps of length \\(b\\) with a random angle between the individual steps. How far from the origin does the particle get in N steps? \nAverage over many tries (or many particles):\n\\[\n\\left\\langle\\underline{R}&gt;=\\sum_{j=1}^{N}\\left\\langle\\underline{r}_{j}\\right\\rangle=0 \\quad \\text { since } \\underline{r}_{j}\\right. \\text { can be in any direction }\n\\]\nMean square displacement:\n\\[\n\\begin{aligned}\n&&lt;\\underline{R}^{2}&gt;=&lt;\\underline{R} \\cdot \\underline{R}&gt;=&lt;\\sum_{j=1}^{N} \\underline{r}_{j} \\cdot \\sum_{k=1}^{N} \\underline{r}_{k}&gt;=\\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left\\langle\\underline{r}_{j} \\cdot \\underline{r}_{k}&gt;\\right. \\\\\n& j=k:&lt;\\underline{r}_{j} \\cdot \\underline{r}_{k}&gt;=\\left\\langle\\underline{r}_{j}^{2}&gt;=b^{2}\\right. \\\\\n& j \\neq k:&lt;\\underline{r}_{j} \\cdot \\underline{r}_{k}&gt;=b^{2}&lt;\\cos \\theta&gt;=0 \\quad \\text { since any } \\theta \\text { in } 0 \\leqq \\theta \\leqq 2 \\pi \\\\\n&&lt;\\underline{R}^{2}&gt;=\\sum_{j=1}^{N} b^{2}=N b^{2} \\propto t \\quad \\text { since time } t \\propto N\n\\end{aligned}\n\\]\nIn fact a more careful treatment shows that the root mean-square displacement \\(&lt;\\underline{R}^{2}&gt;1 / 2\\) \\(=\\sqrt{2 D t} \\propto \\sqrt{t}\\) where for spherical particles the diffusion constant \\(D=k_{B} T / 6 \\pi \\eta R\\), where \\(R\\) is the particle radius.\nExample: Consider uncharged protein spheres in water ( \\(\\rho\\) protein \\(=1.35 \\mathrm{~g} \\mathrm{~cm}^{-3}\\) ) at \\(20^{\\circ} \\mathrm{C}(\\eta\\) \\(=1 \\mathrm{Ps})\\). We can calculate the displacement due to Brownian motion and sedimentation respectively as a function of the particle radius \\(R\\).\nFor Brownian motion, we have \\(\\left\\langle\\underline{R}^{2}\\right\\rangle^{1 / 2}=\\sqrt{2 D t}\\) with \\(D=k_{B} T / 6 \\pi \\eta R\\) For sedimentation, we have \\(v_{\\text {sed }}=2 R^{2} \\Delta \\rho g / 9 \\eta\\) Hence we can make a plot of displacement versus particle radius.\n\n\n\n\n\n\n\n\n\n\nR\n\\(\\mathrm{D}\\left(20^{\\circ} \\mathrm{C}\\right)\\)\n\\(\\left(\\left\\langle\\underline{R}^{2}\\right\\rangle\\right)^{1 / 2}\\) after 1 h\n\n\\(\\mathrm{v}_{\\text {sed }} \\times 1 \\mathrm{~h}\\)\n\n\n\n\n1 nm\n\\(2.1 \\cdot 10^{-10} \\mathrm{~m}^{2} / \\mathrm{s}\\)\n\\(1230 \\mu \\mathrm{~m}\\)\n\\(&gt;\\)\n2.8 nm\n\n\n10 nm\n\\(2.1 \\cdot 10^{-11} \\mathrm{~m}^{2} / \\mathrm{s}\\)\n\\(390 \\mu \\mathrm{~m}\\)\n\\(&gt;\\)\n\\(0.3 \\mu \\mathrm{~m}\\)\n\n\n100 nm\n\\(2.1 \\cdot 10^{-12} \\mathrm{~m}^{2} / \\mathrm{s}\\)\n\\(123 \\mu \\mathrm{~m}\\)\n\\(&gt;\\)\n\\(30 \\mu \\mathrm{~m}\\)\n\n\n\\(1 \\mu \\mathrm{~m}\\)\n\\(2.1 \\cdot 10^{-13} \\mathrm{~m}^{2} / \\mathrm{s}\\)\n\\(39 \\mu \\mathrm{~m}\\)\n\\(&lt;\\)\n3 mm\n\n\n\\(10 \\mu \\mathrm{~m}\\)\n\\(2.1 \\cdot 10^{-14} \\mathrm{~m}^{2} / \\mathrm{s}\\)\n\\(12 \\mu \\mathrm{~m}\\)\n\\(&lt;\\)\n0.3 m\n\n\n\n\nMore generally we can write down equations which express the probability of finding a particle at a given vector displacement from its starting point. In the limit of a large number of random steps, one finds that in 1 dimension this probability is given by\n\\[\nP_{N}(x) d x=\\frac{1}{\\sqrt{2 \\pi&lt;(\\Delta x)^{2}&gt;}} e^{-\\frac{(x-\\langle x\\rangle)^{2}}{\\left.2&lt;(\\Delta x)^{2}\\right\\rangle}} d x \\quad \\text { A 1d Gaussian distribution }\n\\]\nThis has mean \\(\\langle x\\rangle=0\\) and variance \\(\\left\\langle(\\Delta x)^{2}\\right\\rangle \\equiv \\sigma^{2}&gt;0\\).\nIn 3 dimensions, the corresponding relation is\n\\[\nP(\\underline{r}) d \\underline{r}=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{3 / 2}} e^{-r^{2} / 2 \\sigma^{2}} d \\underline{r}\n\\]\nThe distribution has its first moment at the origin and second moment\n\\[\n\\left\\langle\\underline{r}^{2}\\right\\rangle=\\int_{-\\infty}^{\\infty} \\underline{r}^{2} P(\\underline{r}) d \\underline{r}=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{3 / 2}} \\int_{0}^{\\pi} \\sin \\theta d \\theta \\int_{0}^{2 \\pi} d \\phi \\int_{0}^{\\infty} r^{4} e^{-r^{2} / 2 \\sigma^{2}} d r=3 \\sigma^{2}\n\\]\nThis latter result can also be obtained in a direct calculation, considering \\(N\\) random steps in x direction, \\(N\\) in y direction and \\(N\\) in z direction with \\(\\left\\langle x^{2}\\right\\rangle=\\left\\langle y^{2}\\right\\rangle=\\left\\langle z^{2}\\right\\rangle=\\sigma^{2}\\) and thus \\(\\left\\langle\\underline{r}^{2}\\right\\rangle=\\left\\langle x^{2}\\right\\rangle+\\left\\langle y^{2}\\right\\rangle+\\left\\langle z^{2}\\right\\rangle=3 \\sigma^{2}=N b^{2}\\)\n\n\n16.0.4 Large interface between dispersed phase and dispersion medium\nIn commercial terms it is often highly desirable to produce soft matter systems whose (chemical) properties lie somewhere between those of bulk and molecularly dispersed systems, i.e. whose properties are not just the sum of the contributions from the molecules in the bulk phases. To achieve this, a significant proportion of the molecules should lie within or close to the particle-medium interface, which is equivalent to a large surface-to-volume ratio (‘microheterogeneous’ systems, link to surface science). \nOften colloidal particles have a spherical shape, which allows for a relatively small surface-to-volume ratio, but they may also be ellipsoids (prolate or oblate), discs, rods, fibres, random coils etc.\n\n\n16.0.5 Making measurements: Scattering experiments\n\nThe most commonly used experimental tool for studying soft matter systems is scattering. The experiment may utilize light, neutrons or X-rays as the scattering probe. Due to the random arrangement of particles, a random diffraction pattern is observed. This fluctuates as particles move in Brownian motion. The scattered intensity as measured under a certain scattering angle will thus also fluctuate with time (given the detector area is small enough).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Entropy matters</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html",
    "href": "soft-matter/soft-matter_colloids.html",
    "title": "17  Colloids",
    "section": "",
    "text": "17.0.1 Brownian Motion\n\n\n17.0.2 Brief history:\n1828 Scottish botanist Robert Brown observed irregular motion of pollen grains suspended in water. ~1880 Many accepted that Brownian motion was associated with the nature of heat, but no quantitative theory 1905 Brownian motion explained by Albert Einstein 1926 Nobel Prize in Physics: Jean Baptist Perrin for investigations on Brownian motion (sedimentation equilibrium and mean square displacement) in connection with reality of molecules, linking Gas constant and Avogadro’s number which led to the acceptance of the ‘molecular hypothesis’.\n\n\n17.0.3 Diffusion\nIdea: So far we considered a single particle and calculated the probability for finding it after N steps in the volume element \\(d \\underline{r}\\) centred at \\(\\underline{r}\\). We found that \\(\\left\\langle r^{2}\\right\\rangle=N b^{2}\\), which suggests \\(\\left\\langle\\underline{r}^{2}\\right\\rangle \\propto t\\).\nNow we are interested in many particles, where each undergoes a random walk and is not influenced by the others. We then ask: What fraction will be in a certain volume element after time \\(t\\) ? This is the process of diffusion, e.g. spread of a drop of colour ink in water.\nMathematics: Suppose that the distribution of particles \\(n(x, t)\\) is not uniform. This nonequilibrium situation will lead to a motion of particles (diffusion) tending to increase entropy and restore a homogeneous concentration. We are interested in the time-dependent concentration (number per unit volume) profile of particles \\(n(x, t)\\). In the case of an initially very narrow spatial distribution of particles this will correspond to the probability of any particle having moved to \\(x\\). The mean number of particles crossing unit area of a plane (perpendicular to the \\(x\\)-axis) per unit time (i.e. the ‘flux’), is denoted by \\(J_{x}\\). This flux is expected to be (to a good approximation) proportional to the concentration gradient:\n\\[\nJ(x, t)=-D \\frac{\\partial n(x, t)}{\\partial x} \\quad \\text { Fick's law }\n\\]\nwhere the diffusion constant (or coefficient) \\(D\\) has been defined as the constant of proportionality\nNow, conservation of particle number demands\n\\[\n\\begin{aligned}\n& \\frac{\\partial}{\\partial t}(n(x, t) A d x)=A J_{x}(x, t)-A J_{x}(x+d x, t) \\\\\n& \\text { Flux in ................. Flux out } \\\\\n& =A\\left(J_{x}(x, t)-\\left(J_{x}(x, t)+\\frac{\\partial J_{x}(x, t)}{\\partial x} d x\\right)\\right)\n\\end{aligned}\n\\]\nand thus\n\\[\n\\frac{\\partial n(x, t)}{\\partial t}=-\\frac{\\partial J_{x}(x, t)}{\\partial x} \\quad \\text { Particle conservation }\n\\]\nFick’s law and particle conservation together give (assuming \\(D \\neq D(x)\\) ):\n\\[\n\\frac{\\partial n(x, t)}{\\partial t}=D \\frac{\\partial^{2} n(x, t)}{\\partial x^{2}} \\quad \\text { Diffusion equation }\n\\]\nThe solution, up to a constant \\(C\\), which satisfies the initial condition \\((\\mathrm{t}=0)\\) of having all the particles close together at \\(x=0\\) is\n\\[\nn(x, t)=C \\frac{1}{\\sqrt{4 \\pi D t}} \\exp \\left(-x^{2} / 4 D t\\right) \\quad \\text { Gaussian Distribution }\n\\]\nThe constant \\(C\\) can be determined by \\(N_{\\text {total }}=\\int n d V=A \\int n d x=A C\\). Hence\n\\[\np(x, t) d x=\\frac{A d x n(x, t)}{N_{\\text {tot }}}=\\frac{1}{\\sqrt{4 \\pi D t}} \\exp \\left(-x^{2} / 4 D t\\right) d x\n\\]\n \nBut the spreading of the particle distribution with time is due to the random walk of individual particles, for which we already have an equation (1-D):\n\\[\np_{N}(x)=\\frac{1}{\\sqrt{2 \\pi&lt;(\\Delta x)^{2}&gt;}} \\exp \\left(-\\frac{(x-&lt;x&gt;)^{2}}{2&lt;(\\Delta x)^{2}&gt;}\\right)\n\\]\nComparing the result from the two points of view and setting \\(\\langle x\\rangle=0\\), we get \\(&lt;\\Delta x^{2}&gt;=2 D t\\) As expected the mean square displacement increases linearly with time, but we have now found the constant of proportionality: \\(2 D\\). Generalisation to three dimensions yields:\n\\[\n\\begin{array}{ll}\nJ=-D \\nabla n(\\underline{r}, t) & \\text { Fick's law } \\\\\n\\frac{\\partial n}{\\partial t}=-\\nabla \\cdot \\underline{J} & \\text { Particle conservation } \\\\\n\\frac{\\partial n}{\\partial t}=D \\nabla^{2} n & \\text { Diffusion equation } \\\\\n\\left\\langle\\underline{r}^{2}\\right\\rangle=6 D t & \\text { Mean square displacement }\n\\end{array}\n\\]\nThis last relation could constitute an alternative definition of the diffusion coefficient, instead of Fick’s law.\n\n\n17.0.4 Stokes-Einstein relation\nSuppose that the particles are subjected to an external force F in the x direction, e.g. gravity. In thermal equilibrium the Maxwell-Boltzmann distribution is valid, i.e. the particle density \\(n(x)\\) is given by\n\\[\nn(x)=n_{0} \\exp \\left(-U(x) / k_{B} T\\right)=n_{0} \\exp \\left(-F x / k_{B} T\\right)\n\\]\nwhere we assumed a constant force \\(F\\) in the last equation. In the case of gravity \\(F=m_{B} g\\) with \\(m_{B}\\) the buoyant mass. \\(n(x)\\) results from the balance between the motion of the particles due to the external force setting up a concentration gradient, and the resultant diffusion given by Fick’s law. \nIn the case of gravity, this leads to a sedimentation equilibrium. (a) Flux due to external force, \\(J_{F}\\)\nThe velocity of a particle under an applied force \\(F\\) in a viscous fluid can be written as \\(v=\\) \\(F / \\xi\\) which defines the friction coefficient \\(\\xi\\). Hence\n\\[\nJ_{F}=n(x) v=\\frac{n(x) F}{\\xi}\n\\]\n\nDiffusive flux, \\(J_{D}\\) \\(J_{D}\\) is given by Fick’s Law (see above):\n\n\\[\nJ_{D}(x)=-D \\frac{\\partial n(x)}{\\partial x}\n\\]\nEquating the two fluxes \\(J_{F}=J_{D}\\) we get\n\\[\n\\frac{n(x) F}{\\xi}=-D \\frac{\\partial n(x)}{d x}=+D \\frac{F}{k_{B} T} n(x)\n\\]\nThe second equation is obtained by differentiating the Maxwell-Boltzmann distribution. This gives the relation between the diffusion and friction coefficients:\n\\[\nD=\\frac{k_{B} T}{\\xi}=\\frac{k_{B} T}{6 \\pi \\eta R}\n\\]\nThe last equation applies to a spherical particle of radius \\(R\\) in a fluid of viscosity \\(\\eta\\), for which Stokes’s Law gives \\(\\xi=6 \\pi \\eta R\\) (which applies only at low Reynold’s number, \\(\\rho R v / \\eta\\) \\(\\ll 1\\) ) resulting in the Stokes-Einstein relation.\n\nThe Stokes-Einstein relation is a very deep result. It relates equilibrium fluctuations in a system to the energy dissipation when the system is driven off equilibrium. Here, the fluctuations in the fluid give rise to the diffusive motion of the suspended particle and \\(D\\) is therefore the ‘fluctuation’ part. A sheared fluid will dissipate energy because of its finite viscosity and thus \\(\\eta\\) represents the dissipative part.\nMore generally, Brownian motion sets a natural limit to the precision of physical measurements. Example: A mirror suspended on a torsion fibre reflects a spot of light onto a scale. The spot will jiggle due to the random impact of air molecules and the random motion of atoms in the quartz fibre. To reduce the jiggle, the apparatus has to be cooled. The relation between fluctuation and dissipation tells us where to cool. ‘This depends upon where [the mirror] is getting its ’kicks’ from. If it is through the fibre, we cool it … if the mirror is surrounded by a gas and is getting hit mostly by collisions in the gas, it is better to cool the gas. As a matter of fact, if we know where the damping of the oscillations comes from, it turns out that that is always the source of the fluctuations.’ (Feynman, Chapter 41)\n\n\n\n17.0.5 Interacting colloidal particles\nSo far we have dealt with colloidal particles as independent spheres suspended in a medium. Now we will study concentrated suspensions in which interparticle interactions cannot be ignored.\n\n17.0.5.1 Samples: model hard-sphere colloids\nTo be specific let us discuss a monodisperse colloid made up of polymethyl- methacrylate (PMMA; perspex in bulk form) spheres, which are covered with polymeric hairs (Poly-hydroxystearic acid) to stop them sticking to each other. They show to a very good approximation hard sphere behaviour. \nThe radius of the colloid is typically \\(R \\leq 0.5 \\mu m\\) with a very small polydispersity (size variation) of \\(\\Delta R / R&lt;0.05\\). The usual way of referring to particle densities is the ‘volume fraction’ \\(\\phi\\), defined to be the fraction of the suspension occupied by particles. If we have \\(N\\) monodisperse spheres with radius \\(R\\) in the total volume \\(V\\), then\n\\[\n\\phi=\\frac{V_{\\mathrm{part}}}{V}=\\frac{N}{V} \\frac{4}{3} \\pi R^{3}\n\\]\n\n\n17.0.5.2 Phase Behaviour\nAs a reminder we first look at the interaction potential and phase behaviour of a simple atomic substance, such as argon. A generic inter-particle potential \\(U(r)\\) exhibits the infinite hard sphere repulsion as well as an attractive part of depth \\(\\varepsilon\\). The   corresponding generic phase diagram in the temperature-density plane shows the regions where gas, liquid, fluid and crystal exist. A rough estimate of the critical temperature \\(T_{C}\\) is given by \\(k_{B} T_{C} \\sim \\varepsilon\\). If \\(\\varepsilon \\rightarrow 0\\), as in the case of hard spheres, we expect that there will be no critical point and therefore no liquid phase. This is indeed the case.\n\n\n17.0.5.3 Phase behaviour of hard-sphere colloids:\nAt very low densities, the structure of a hard-sphere suspension is like that of a perfect gas; the positions of the colloidal particles are practically uncorrelated and the chance of finding a particle in any particular vicinity is almost independent of the presence of all the other particles.\nUpon an increase in particle density, their positions begin to become correlated as the particles are mutually excluding. If we look inside a suspension of volume fraction \\(\\phi \\geq 0.3\\), then we will find that, at any one instant, each particle is surrounded by a `cage’ of neighbours (short-ranged order). The cage tightens as \\(\\phi\\) increases. The cage size  represents a dominant length scale in the system. Despite the appearance of correlations between particle positions, a hard-sphere suspension below \\(\\phi \\lesssim 0.5\\) is still in what one might call a ‘colloid fluid’ phase, i.e. the arrangement of particles in such a suspension is analogous to the arrangement of atoms in an atomic fluid (e.g. fluid argon) - on average the particle centres are more or  less randomly arranged, and each particle can, given time, wander through the available volume. The structure is described by the radial distribution function \\(g(r)\\). Given a particle centred at \\(\\underline{r}=0\\), \\((N / V) g(\\underline{r}) d \\underline{r}\\) is the number of particles in \\(d \\underline{r}\\) at a distance \\(\\underline{r}\\).\nIf we increase the volume fraction above \\(\\phi \\approx 0.55\\), however, a phase transition occurs. The particles will spontaneously pack into a crystalline arrangement. Particles now can only move round about their average lattice site, but will not wander throughout the available volume. This phase is called a ‘colloidal crystal’. Due to more local freedom the crystal has a higher (!) entropy than the metastable fluid from which it grows. It is an entropically-driven phase transition. Between \\(\\phi=0.5\\) and \\(\\phi=0.55\\), we find coexistence between colloidal fluid and crystal. \nNote that no colloidal liquid is formed ( \\(T_{\\mathcal{C}}=0\\) ), as expected for an interaction potential without attraction. At the highest particle concentrations (above \\(\\phi \\approx 0.58\\) ) crystallization may be avoided if the temperature is quenched very rapidly, and a colloidal glass is formed. We will return to study the behaviour of glasses in Part 4.\n\n\n\n17.0.6 Particle dynamics\n\n17.0.6.1 Hard-spheres\nWe are interested in the mean-square displacement \\(\\left\\langle\\underline{r}^{2}(t)\\right\\rangle\\) as a function of time for different volume fractions. At low volume fractions, the particles undergo Brownian motion (random-walk diffusion) due to collisions with liquid molecules. The meansquare displacement \\(\\left\\langle\\underline{r}^{2}(t)\\right\\rangle=6 D_{0} t\\) as already seen earlier. Above \\(\\phi \\sim 0.3\\), however, different regimes are observed. At short times the particles diffuse with the short time (self) diffusion constant \\(D_{s}\\). This is determined from the short time limit and is smaller than the \\(D_{0}\\) measured for \\(\\phi \\rightarrow 0\\). The motion of the particles (self diffusion) is still driven by collisions with the liquid molecules, but in addition the interactions between particles become significant. While the particles are diffusing in their cages formed by their neighbours, the hydrodynamic interaction  with the neighbours, transmitted through flows in the liquid, causes slowing down relative to the free diffusion at low concentrations. At intermediate times the particles encounter the neighbours and the interactions slow the motion down. To make further progress, the particle has to break out of the cage formed by its neighbours. Now the particles experience a further interaction, direct interactions (hard-sphere interactions), in addition to the hydrodynamic interactions. The long-time and long-ranged movement is also diffusive, i.e. we still have \\(\\left\\langle\\underline{r}^{2}(t)&gt;\\propto t\\right.\\), when the particles undergo large-scale random-walk diffusion through many cages. However, the motion is further slowed and a smaller diffusion constant relative to the motion in the short time limit is observed, the long time (self) diffusion constant \\(D_{L}\\).  short time: \\(&lt;r^{2}(t)&gt;=6 D_{S} t\\) short-time self diff. coeff. \\(D_{S}&lt;D_{0}\\) purely because of hydrodyn. interactions theoretical predictions exist intermediate time: some theory long time: \\(&lt;r^{2}(t)&gt;=6 D_{L} t\\) long-time self diff. coeff. \\(D_{L}&lt;D_{S}\\left(&lt;D_{0}\\right)\\) because of hydrodyn. and direct interactions theory very complicated, but some predictions exist",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html",
    "href": "soft-matter/soft-matter_polymers.html",
    "title": "18  Polymers",
    "section": "",
    "text": "18.0.1 Chemical structure\nA polymer is a large molecule made up of many small, simple chemical units, joined together by chemical bonds. The basic unit of this sequence is called a ‘monomer’, and the number of units in the sequence is called the ‘degree of polymerisation’. It is possible to have polymers containing over \\(10^{5}\\) units and there are naturally occurring polymers with a degree of polymerisation exceeding \\(10^{9}\\). For example, polystyrene with a degree of polymerisation of \\(10^{5}\\) has a molecular weight of about \\(10^{7} \\mathrm{~g} / \\mathrm{mol}\\) and, if fully stretched out, would be about 25 \\(\\mu \\mathrm{m}\\) long. Polymers play a central role in many fields, ranging from technology to biology. This is reflected in a huge number of different chemical structures. Given this manifold and the complexity of polymer molecules, the theories are astonishingly simple. This is possible because of the characteristic feature of polymers: The molecule itself is very large and the macroscopic behaviour is dominated by this large-scale property of the molecule. A description of the ‘fine structure’ of the behaviour of real polymers, however, is not attempted by (most of) these theories. Polymers exist in very different architectures, such as linear, branched, star or cross-linked (see Chapter 1). Furthermore, a large variation in the chemical structure may be achieved by combining different monomers (copolymerisation). Then for the coupling of two monomeric units in the chain two limiting cases exist; statistical or block structure. We will focus on linear homopolymers, i.e. no branch points and all subunits are the same.\n\n\n18.0.2 Examples:\n\n\n\n\n\n\n\nPolyethylene (PE)\nPoly(methyl methacrylate)  (PMMA)\n\n\n\n\n- \\(\\left[\\mathrm{CH}_{2}\\right]\\) -\nCOC(=O)C(C)([14CH3])[SiH3]\n\n\n\nPolystyrene (PS) Natural rubber methacrylate) (PMMA) \n\n\\(\\left[\\mathrm{CH}_{2}-\\mathrm{CH}\\right]-\\)  \n\n\n\n18.0.3 Polymer conformations\n\n18.0.3.1 Models for the conformation of polymers\nIn order to understand the properties of most substances, we must consider a large assembly of molecules. In the case of polymers, however, the molecules themselves are very large and due to their flexibility they can take up an enormous number of configurations by rotation of chemical bonds. The shape of the polymer can therefore only be usefully described statistically and one need to use statistical mechanics to calculate the characteristics of even an isolated polymer. To be able to investigate the properties of a single polymer and to neglect interactions between polymers, the polymer is placed in a very dilute solution. In this chapter, we will theoretically investigate the properties of an isolated, single polymer chain in solution (which in addition is linear and consists of only one kind of monomers).\n\n\n18.0.3.2 Freely-jointed chain\nMany polymers are highly flexible and are coiled up in solution. In a simple model we thus describe a polymer as consisting of a large number of segments freely joined up, where all angles between segments are assumed to be equally likely.\nThe momomers are located at positions \\(\\underline{R}_{\\mathrm{j}}\\) and connected by bonds \\(\\underline{r}_{j}=\\underline{R}_{j}-\\underline{R}_{j-1}\\) of length \\(\\left|\\underline{r}_{j}\\right|=b_{0}\\). The end-to-end vector is \\(\\left|\\underline{R}_{N}-\\underline{R}_{0}\\right|=\\sum_{j=1}^{N} r_{j}\\). At any instant, the configuration (arrangement) of the polymer is one realisation of an N -step random walk in three dimensions. In time, the various segments undergo Brownian motion and the  polymer fluctuates between all possible configurations of the random walk \nSize and shape of a freely-jointed chain: Based on the end-to-end vector \\(\\underline{R}\\), the size and shape of the chain can thus be described probabilistically:\n\\[\n\\begin{aligned}\n& &lt;\\underline{R}&gt;=\\sum_{j=1}^{N}\\left\\langle\\underline{r}_{j}\\right\\rangle \\\\\n& &lt;\\underline{R}^{2}&gt;=\\sum_{j=1}^{N} \\sum_{k=1}^{N}&lt;\\underline{r}_{j} \\underline{\\underline{L}}_{k}&gt;=N b_{0}^{2} \\\\\n& P(\\underline{R}) d \\underline{R}=\\left(\\frac{3}{2 \\pi&lt;\\underline{R}^{2}&gt;}\\right)^{3 / 2} e^{-\\frac{3 \\underline{R}^{2}}{2&lt;\\underline{R}^{2}&gt;}} d R \\quad \\text { for large } \\mathrm{N} \\text { (Gaussian distribution) }\n\\end{aligned}\n\\]\nWhile the end-to-end distance represents a well-defined quantity for a linear chain, we need a more versatile measure of the size for more complicated architectures, such as branched or star-shaped polymers. This is provided by the mean square distance from the centre of mass \\(\\underline{R}_{\\mathrm{G}}\\), the so-called radius of gyration \\(R_{g}\\), which is also a measure of the extension of a (random) chain:\n\\[\n\\begin{aligned}\n& \\underline{R}_{G}=\\frac{1}{N} \\sum_{j=1}^{N} \\underline{R}_{j} \\\\\n& R_{g}^{2}=\\frac{1}{N} \\sum_{j=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{G}\\right)^{2}=\\frac{1}{2 N^{2}} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{k}\\right)^{2} \\quad \\quad \\text { (see problem) } \\\\\n& \\therefore\\left\\langle R_{g}^{2}\\right\\rangle=\\frac{1}{2 N^{2}} \\sum_{j=1}^{N} \\sum_{k=1}^{N}&lt;\\left(\\underline{R}_{j}-\\underline{R}_{k}\\right)^{2}&gt;=\\frac{b_{0^{2}}}{2 N^{2}} \\sum_{j=1}^{N} \\sum_{k=1}^{N}|j-k|\n\\end{aligned}\n\\]\nNow transform to continuous variables \nThus we find the simple result that the mean squared gyration radius is proportional to the mean squared end-to-end distance.\n\n\n18.0.3.3 Freely-rotating chain\nIn a polymer molecule the bond angles are usually restricted, which leads to a limited flexibility of the molecule. Let us consider the case of \\(n\\)-butane:\n\\[\n\\mathrm{H}_{3} \\mathrm{C}-\\mathrm{CH}_{2}-\\mathrm{CH}_{2}-\\mathrm{CH}_{3}\n\\]\nWhile the \\(\\mathrm{C}-\\mathrm{C}\\) bond angle is fixed at about \\(120^{\\circ}\\), rotation about the bond is possible. \nNevertheless, the potential energy of a configuration depends on the valence angle: \nAt low temperatures ( \\(k_{B} T\\) &lt;config. energy) the configuration will thus be predominantly trans. As the temperature is increased ( \\(k_{B} T \\sim\\) config. energy), there will also be gauche configurations and at high temperatures ( \\(k_{B} T \\gg\\) config. energy), any angle will be possible. This suggests that a model should be based on fixed angles between bonds, but free rotation about the bonds. This model is called the freely-rotating chain.\nWe start with a fixed configuration of \\(\\underline{r}_{l}\\), \\(\\underline{r}_{2}, \\ldots, \\underline{r}_{j-1}\\) and then add the next segment \\(\\underline{r}_{j}\\). While the bond angle \\(\\Theta\\) is given by the chemistry of the molecule, the segment can still freely rotate about the axis defined by \\(r_{j-1}\\), i.e. \\(\\varphi\\) can take any value \\(0 \\leq \\varphi \\leq 2 \\pi\\). If we average \\(\\underline{r}_{j}\\) over \\(\\varphi\\), while keeping \\(\\underline{r}_{1}, \\underline{r}_{2}, \\ldots, \\underline{r}_{j-1}\\) fixed, only the component in \\(\\underline{r}_{j}\\) direction remains: \\(\\left.&lt;\\underline{r}_{j}\\right\\rangle_{\\underline{r}}, \\underline{r}_{2}, \\ldots, \\underline{r}_{j-1}\\) fixed \\(=\\cos \\Theta \\underline{r}_{j-1}\\)\nFor the calculation of \\(\\left\\langle R^{2}\\right\\rangle\\) we also need \\(\\left\\langle\\underline{r}_{j} \\cdot \\underline{r}_{k}\\right\\rangle\\),  which we obtain by multiplying both sides with \\(\\underline{r}_{k}\\) and taking the average: \\(\\left\\langle r_{j} \\underline{x}_{k}\\right\\rangle=\\cos \\Theta\\left\\langle r_{j-1} \\underline{x}_{k}\\right\\rangle=\\cos ^{2} \\Theta\\left\\langle r_{j-2} \\underline{\\Sigma}_{k}\\right\\rangle=\\ldots=\\left(\\cos ^{|j-k|} \\Theta\\right)\\left\\langle r_{k}{\\underline{x_{k}}}\\right\\rangle=\\left(\\cos ^{|j-k|} \\Theta\\right) b_{0}^{2}\\) Since \\(\\cos \\Theta&lt;1\\), correlations between \\(\\underline{r}_{j}\\) and \\(\\underline{r}_{k}\\) decrease with increasing distance \\(|j-k|\\) between the links and the orientations of distant links become uncorrelated. The end-to-end distance \\(\\left\\langle R^{2}\\right\\rangle\\) of a freely-rotating chain is hence\n\\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =\\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left\\langle\\underline{r}_{j} \\cdot \\underline{r}_{k}\\right\\rangle=b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\cos ^{(j-k \\mid} \\Theta\\right)=N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{j-1} \\cos ^{(j-k)} \\Theta \\\\\n& =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\Theta \\sum_{k=1}^{j-1} \\cos ^{-k} \\Theta\n\\end{aligned}\n\\]\nwhere we used the same argument as above to deal with \\(|j-k|\\). To calculate these two sums we consider the geometric progression:\n\\[\n\\begin{aligned}\n& S=\\sum_{m=1}^{M} x^{m}=x+x^{2}+\\ldots .+x^{M} \\quad \\therefore x S=x^{2}+x^{3}+\\ldots .+x^{M+1} \\quad \\therefore S-x S=x-x^{M+1} \\\\\n& \\therefore S=\\frac{x-x^{M+1}}{1-x}\n\\end{aligned}\n\\]\nWith the help of this formula we get\n\\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\Theta \\frac{\\frac{1}{\\cos \\Theta}-\\frac{1}{\\cos ^{j} \\Theta}}{1-\\frac{1}{\\cos \\Theta}}=N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\Theta-1}\\left(\\sum_{j=1}^{N} \\cos ^{j} \\Theta-\\sum_{j=1}^{N} \\cos \\Theta\\right) \\\\\n& =N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\Theta-1}\\left(\\frac{\\cos \\Theta-\\cos ^{N+1} \\Theta}{1-\\cos \\Theta}-N \\cos \\Theta\\right)\n\\end{aligned}\n\\]\nFor large N this can be simplified \\(\\left\\langle R^{2}\\right\\rangle \\approx N b_{0}^{2}+\\frac{2 b_{0}^{2}}{1-\\cos \\Theta} N \\cos \\Theta=N b_{0}^{2}\\left(\\frac{1+\\cos \\Theta}{1-\\cos \\Theta}\\right)=C N b_{0}^{2}\\) with \\(C=(1+\\cos \\Theta) /(1-\\cos \\Theta)\\). To get a better idea of the effect of a fixed angle \\(\\Theta\\), i.e. going from a freely-jointed to a freely-rotating chain, we look at a few special (but not necessarily very realistic) cases: (1) \\(\\Theta \\rightarrow 0 \\Rightarrow \\cos \\Theta \\rightarrow 1-\\frac{\\Theta^{2}}{2} \\Rightarrow C=\\frac{2-\\Theta^{2} / 2}{\\Theta^{2} / 2} \\approx \\frac{4}{\\Theta^{2}} \\quad\\left(\\mathrm{C} \\approx 500\\right.\\) for \\(\\left.\\Theta=5^{0}\\right)\\)\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle \\gg N b_{0}^{2}\n\\]\n (2)\n\\[\n\\begin{aligned}\n\\Theta \\rightarrow \\pi-\\delta \\quad & \\Rightarrow \\cos \\Theta \\rightarrow-1-\\frac{\\delta^{2}}{2} \\Rightarrow C=\\frac{\\delta^{2} / 2}{2-\\delta^{2} / 2} \\approx \\frac{\\delta^{2}}{4} \\quad\\left(\\mathrm{C} \\approx 2 \\times 10^{-3} \\text { for } \\Theta=175^{0}\\right) \\\\\n& \\therefore\\left\\langle R^{2}\\right\\rangle \\ll N b_{0}^{2}\n\\end{aligned}\n\\]\n (3) \\(\\Theta \\rightarrow \\pi / 2 \\Rightarrow \\cos \\Theta \\rightarrow 0 \\Rightarrow C=1\\)\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle=N b_{0}^{2}\n\\]\n\nAgain, we get \\(\\left\\langle R^{2}\\right\\rangle \\propto N\\), which suggests that a long freely-rotating chain can be represented by an equivalent freely-jointed chain with \\(N\\) ’segments of length \\(b\\). Real and effective chain must have the same actual length ( \\(N b_{0}=N^{\\prime} b\\) ) and the same end-to-end distance \\(&lt;R^{2}&gt;\\) i.e. \\(\\left(C N b_{0}^{2}=C N^{\\prime} b^{2}\\right)\\). These constraints result in \\(b=C b_{0}\\) and \\(N^{\\prime}=N / C\\). This has important consequences:\n\nAll sufficiently long flexible chains have identical • \\(b\\) is the so called “Kuhn” behaviour as regards their dimensions: the chemical details are hidden in \\(N^{\\prime}\\) and \\(b\\).\nWhile individual monomer pairs are not totally statistical segment length (and twice the “persistence length” \\(l_{p}\\) ) flexible, groups of monomers are\n\\(C\\) represents the number of monomers over which the orientational correlation is lost \n\n\n\n18.0.3.4 Excluded volume effects\nThe fact that two monomers cannot occupy the same space has consequences on different length scales. On a local length scale this prevents neighbouring monomers from coming too close together. This effect is taken into account in terms of a restricted bond-angles, which prevents them from overlaping. Non-overlap, i.e. excluded volume, of distant monomers along the chain has also to be taken into account and can have surprisingly large effects.\nTo estimate the importance of this effect, we consider the fraction of coil volume actually occupied by monomers: volume actually occupied by monomers: \\(\\quad V_{N}=N V_{1} \\sim N b^{3}\\) (where \\(\\mathrm{V}_{1}\\) is the volume of a monomer) volume occupied by the whole coil:\n\\[\nV_{\\text {coil }}=\\frac{4 \\pi}{3}&lt;R_{g}^{2}&gt;^{3 / 2} \\sim \\frac{4 \\pi}{3} N^{3 / 2} b^{3}\n\\]\n\\(\\therefore \\frac{V_{N}}{V_{\\text {coil }}}=\\frac{N b^{3}}{(4 \\pi / 3) N^{3 / 2} b^{3}} \\sim N^{-1 / 2}\\) e.g. \\(N=10^{4}\\) monomers occupy only about \\(1 \\%\\) of the whole coil volume.\nThe overall chain size \\(&lt;R_{g}{ }^{2&gt;1 / 2}\\) is determined by the competition of two effects. Entropy (and chain connectivity) favour a compact chain and avoid the more unlikely stretched configurations, while repulsive excluded volume interactions want to expand the chain to avoid overlap. Based on this balance we will ‘calculate’ the effect of excluded volume in a very hand-waving way. (Due to a fortuitous cancellation of errors introduced by various approximations, the result is practically identical to more rigorous treatments, which are very involved.) We consider the Helmholtz function of a single chain, which is regarded as an assembly of particles with constant volume \\(\\mathrm{NV}_{1}\\) at constant temperature T :\n\\[\nF=U-T S\n\\]\nThe entropy S is given by\n\\[\nS=k_{B} \\ln (\\mathrm{no} \\text {. of configurations })\n\\]\nwhere for a given \\(\\underline{R}\\) the number of configurations is expected to be proportional to\n\\[\nP(\\underline{R})=\\left(\\frac{3}{2 \\pi&lt;R^{2}&gt;}\\right)^{3 / 2} e^{-\\frac{3 R^{2}}{2&lt;R^{2}&gt;}}\n\\]\nand hence\n\\[\nS \\sim \\frac{-3 k_{B} R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nThe internal energy \\(U\\) includes the kinetic and potential energy. However, the kinetic energy is independent of the configuration and thus of \\(\\underline{R}\\) and we only have to consider the potential energy. To estimate the potential energy, we disregard the connectivity of the chain and calculate the interaction energy of a ‘segment gas’ confined in a volume \\(R^{3}\\). The probability of a monomer to lie in this volume is given by the fraction of total coil volume occupied by monomers, which we estimated above to be \\(N V_{1} / R^{3}\\)\nThus the probability of monomer-monomer contacts is \\(N^{2} V_{1} / R^{3} \\sim N^{1 / 2}\\). With an energy \\(\\varepsilon\\) of a monomer-monomer contact, the potential energy \\(U \\sim \\varepsilon N^{2} V_{1} / R^{3}\\). We thus obtain\n\\[\nF=\\frac{\\varepsilon N^{2} V_{1}}{R^{3}}+\\frac{3 k_{B} T R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nwhich can be minimized with respect to \\(R\\), i.e. \\(d F / d R=0\\), yielding\n\\[\nR^{5}=\\frac{\\varepsilon V_{1} b^{2}}{k_{B} T} N^{3} \\sim \\frac{\\varepsilon}{k_{B} T} N^{3} b^{5} \\quad \\therefore R \\sim N^{3 / 5} b\n\\]\nSimulations give a very similar scaling, \\(\\mathrm{R} \\sim N^{0.588}\\). The chain can no longer be modelled by a random walk, but has to be described by a self-avoiding random walk. The distribution of end-to-end distances is also not Gaussian. Although the difference between an exponent of 0.5 (as is characteristic for the freelyjointed and freely-rotating chains, i.e. a random walk) and 0.6 (excluded volume chain, i.e. self-avoiding random walk) seems small, it has a large effect at large N . For example, for \\(N=10^{4}, R=N^{0.5} b=100 b\\), while \\(R=N^{0.6} b=251 b\\), which corresponds to a swelling of the chain by a factor of 2.5 .\n\n\n\n18.0.4 Good, poor and theta solvents\nSo far we only considered monomer-monomer interactions, which we assumed are purely repulsive, and neglected the influence of the solvent. However, the type of solvent has a great effect on the polymer size. If there is a high affinity with the solvent (‘good solvent’) the polymer swells, while it will shrink in a ‘poor solvent’. We consider a lattice model, where each lattice site has \\(z\\) nearest neighbours and there are \\(N_{s}\\) solvent molecules, \\(N_{p}\\) monomers energies of interaction are \\(\\varepsilon_{s s}\\) for and \\(\\mathrm{N}_{\\mathrm{sp}}\\) solvent-monomer contacts. The solvent-solvent, \\(\\varepsilon_{\\mathrm{pp}}\\) for monomer- monomer and \\(\\varepsilon_{s p}\\) for solvent-monomer interactions. Then the energy of mixing \\(\\Delta U_{\\operatorname{mix}}\\) is given by\n\\[\n\\Delta U_{\\operatorname{mix}}=U-\\left(U_{S}+U_{p}\\right)\n\\]\nwhere energy of pure solvent \\(U_{s}=\\frac{z N_{s} \\varepsilon_{s s}}{2}\\) energy of pure polymer \\(U_{p}=\\frac{z N_{p} \\varepsilon_{p p}}{2}\\)  energy of solution \\(U=N_{s p} \\varepsilon_{s p}+\\frac{\\left(z N_{s}-N_{s p}\\right) \\varepsilon_{s s}}{2}+\\frac{\\left(z N_{p}-N_{s p}\\right) \\varepsilon_{p p}}{2}\\) Hence we obtain for the energy of mixing\n\\[\n\\Delta U_{\\mathrm{mix}}=N_{s p}\\left[\\varepsilon_{s p}-\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right)\\right]\n\\]\nwhich can be either positive or negative:\n\n\\(\\varepsilon_{s p}&lt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&lt;0\\) \n\nThis is the case of a ‘good solvent’, because the monomers prefer to be near the solvent molecules. Excluded volume effects then expand the chain.\n\n\\(\\varepsilon_{s p}&gt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&gt;0\\)\n\nThis is the case of a ‘poor solvent’, because the monomers prefer to be near to each other (and similarly for the solvent molecules). The attraction between the different monomers offset the excluded volume effect. The importance of the attractions generally depends on temperature. At very high temperatures the coil is expanded and the solvent quality is good. In contrast, at very low temperatures, the solvent quality is poor, attraction dominates, the coil collapses and phase separation is observed. In between these two limits, there is a temperature, the so-called theta temperature \\(\\Theta\\), where the coil has ideal dimensions and the effects of excluded volume and attraction cancel each other. The solvent at \\(T=\\Theta\\) is called a ‘theta solvent’. The stronger the attractions the higher \\(\\Theta\\) will be, while for weak attractions \\(\\Theta\\) is low. A full treatment of the coil expansion is rather involved and has to take into account excluded volume, attractions, configurational entropy and entropy of mixing.\n\n\n18.0.5 Summary\nA polymer has characteristics on different length scales. On a very global length scale, it has a molar mass \\(M\\) and an overall size which can be characterised by the root mean square end-to-end distance \\(&lt;R^{2}&gt;1 / 2\\) or radius of gyration \\(&lt;R g^{2}&gt;1 / 2 \\propto N^{v} \\propto M^{v}\\), where \\(v=1 / 2\\) for a freely-jointed or freely-rotating chain (random walk) and \\(v=3 / 5\\) for an excluded volume chain (self-avoiding random walk). On a smaller length scale the behaviour will be dominated by the finite flexibility or ‘persistence’ of the chain, which is characterised by the Kuhn length b. The chain will essentially behave like a stiff rod on this length scale. This rod typically has a constant mass per length, \\(M / L\\), and thus \\(M \\alpha L\\). Finally, the local cross-sectional structure will be observed on an even smaller length scale.\n\n\n18.0.6 Concentrated polymer solutions\nPreviously we considered a single polymer in a very dilute solution. Now we increase the concentration in steps until we reach bulk polymers. A special aspect of bulk polymers will then be discussed in more detail in the following chapter. The most important regimes of concentration are: (A) Dilute: \nThe polymer coils are well-separated on average. ‘Dilute’ means:  (B) Overlap concentration c*: \nOverlap occurs when the volume fraction of coils reaches unity and thus\n\\[\n\\frac{c^{*}}{M} N_{A} \\frac{4 \\pi}{3} R_{g}^{3} \\sim 1 \\quad \\therefore c^{*}=\\frac{3 M}{4 \\pi N_{A} R_{g}^{3}}\n\\]\nusing \\(R g=&lt;R g^{2&gt;^{1 / 2}}=B M^{\\nu}\\) gives\n\\[\nc^{*}=\\frac{3}{4 \\pi N_{A} B^{3}} M^{1-3 v}\n\\]\nExample: Polystyrene with \\(M=10^{6} \\mathrm{~g} \\mathrm{~mol}^{-1}\\) in a good solvent ( \\(v=0.6\\) ) and \\(B=0.028 \\mathrm{~nm}\\left(\\mathrm{~g} \\mathrm{~mol}^{-1}\\right)^{-0.6}\\) leads to \\(\\mathrm{c}^{*}=0.29 \\mathrm{~kg} \\mathrm{~m}^{-3}=0.29 \\mathrm{mg} / \\mathrm{ml}\\). With the density of polystyrene \\(\\rho=1050 \\mathrm{~kg} \\mathrm{~m}^{-3}\\), the volume fraction of monomers is \\(\\mathrm{c}^{*} / \\rho=0.28 \\times 10^{-3} . \\mathrm{c}^{*}\\) can be very small for large polymers. (C) Semi-dilute: \n\n18.0.6.1 Abstract\nThe concentration is larger than the overlap concentration \\(\\mathrm{c}^{*}\\), but still much smaller than the bulk density. The coils interpenetrate and entangle, but the solution is still mostly solvent\n\n\n18.0.6.2 Concentrated:\nIn this case the concentration is very close to the bulk density and the polymer monomers occupy a significant fraction of the total volume.\n\n\n18.0.6.3 Bulk polymers:\nBulk polymers are divided into two main classes, characterised by whether they are cross-linked or not. There are elastomers or rubbers with a low degree of cross-linking and thermosets with a high degree of cross-linking. We will investigate the behaviour of rubbers in the next chapter. The second class are thermoplastics, which are not cross-linked. Most everyday plastic products are thermoplastics. We will briefly discuss their behaviour upon cooling, which shows similarities to the behaviour of colloids. At high temperature the free energy is dominated by the entropic terms. The melt resembles a random assembly of mobile, intertwined, flexible coils with a density similar to the density of the corresponding monomer liquid. Upon cooling the potential energy takes over and the bonds are restricted in their rotation leading to configurations which are more straightened out. Below the melting temperature \\(\\mathrm{T}_{\\mathrm{m}}\\), a crystal is the lowest free energy state. Crystallisation, however, requires significant ordering of the initially random melt and is only possible if cooling occurs slow enough. If the melt is rapidly cooled below the glass transition temperature \\(T_{g}\\left(&lt;T_{m}\\right)\\), then instead of a crystal a glass is formed, which represents an amorphous metastable, but long-lived state. Although the polymers can still vibrate, they can no longer move. Solid thermoplastics are frequently a mixture of crystalline and amorphous structures.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html",
    "href": "soft-matter/soft-matter_surfactants.html",
    "title": "19  Surfactants",
    "section": "",
    "text": "19.0.1 Structure and examples\nIn this chapter we discuss the behaviour of a special class of molecules. In these molecules one end contains a ‘hydrophilic’ (water-loving) part, while the other end is ‘hydrophobic’ (water-hating). They are called ‘amphiphilic’ (loving both) molecules, which reflects their structure, or ‘surfactants’ (from SURFace ACTive AgeNT), which refers to their behaviour in solution. \nA few chemical structures are shown below. They illustrates that the hydrophobic tail usually consits of hydrocarbon chains of different lengths and that the hydrophilic head might either be positively or negatively charged, zwitterionic or uncharged. \nThe hydrocarbon chains are insoluble in water. The molecules are thus preferentially located at the surface, which allows the hydrophilic head to be surrounded by water and the hydrophobic chains to avoid contact with water. There is always an equilibrium between surfactants at the surface and in the bulk of the solution. The coverage of the surface leads to a reduction of the surface tension with increasing surfactant concentration.\n\n\n19.0.2 Self-assembled structures\n\n19.0.2.1 Introduction\nAbove a so-called critical micellar concentration (cmc) surfactants self-assemble in solution spontaneously into larger structures. (In the following we will consider aqueous solutions, although the arguments also apply to other polar or non-polar (organic) solvents.) This allows the hydrophobic parts to crowd together while  being ‘shielded’ by the hydrophilic heads. The density of the hydrophobic cores is very similar to the density of fluid hydro-carbons and the random arrangements of the chains resemble closely a fluid structure.\nThe surfactant assemblies are not held together by chemical bonds, but only by weak interactions ( \\(\\lesssim k B T\\) ). Their existence and properties are thus determined by a delicate balance between different effects, such as the transfer of hydrophobic chains into the core, interactions between the head group and the entropy of mixing. Small changes in control parameters, for example temperature, salt concentration or pH , thus have large effects on the characteristics of the surfactant aggregates. Nevertheless, for given conditions, they have very well-defined properties (shape, size etc.).\n\n\n19.0.2.2 Shape of surfactant assemblies\nSurfactants spontaneously self-assemble into a variety of different structures. We use geometric or ‘packing’ considerations to understand and predict the shape of surfactant aggregates. In this model the geometry of a surfactant molecule is described using the following parameters:\n\noptimal headgroup area \\(a_{0}\\) : As discussed in the previous section, this depends on a delicate balance of forces and is thus not only controlled by the ‘chemistry’ of the surfactant molecule, but also depends on different control parameters of the solution, such as salt concentration, pH or temperature.\nvolume v of the hydrophobic part: The hydrophobic part usually consists of hydrocarbon chains and for saturated hydrocarbons the volume \\(v\\) can be approximated by \\(v \\approx(27.4+26.9 \\mathrm{n}) \\times 10^{-3} \\mathrm{~nm}^{3}\\) where n is the number of carbon atoms.\ncritical chain length \\(\\boldsymbol{l}_{\\boldsymbol{c}}\\) : The maximum effective length of the hydrophobic chains is called the critical chain length \\(l_{c}\\), which has to be shorter than the fully extended molecular length of the chain \\(l_{\\max }\\). For saturated hydrocarbons the critical length can be estimated using \\(l_{c} \\leq l_{\\max } \\approx(0.154+0.1265 \\mathrm{n}) \\mathrm{nm}\\) The critical chain length heavily depends on the ‘chemistry’ of the molecule, for example on the presence of double bonds or branching, as well as the temperature.\n\nThe structure which will be adopted is determined by a balance between entropy, which favours small aggregates, and energy considerations: A certain shape or size might only be possible by imposing a headgroup area \\(a&gt;a_{0}\\), which is energetically not favourable. We will now establish the criteria for the different shapes. (A) Spherical micelle: \nFor a spherical micelle with aggregation number N , the total volume and surface area are given by\n\\[\n\\begin{gathered}\nN v=\\frac{4 \\pi}{3} R^{3} \\\\\nN a_{0}=4 \\pi R^{2} \\\\\n\\therefore \\frac{v}{a_{0}}=\\frac{R}{3}&lt;\\frac{l_{c}}{3}\n\\end{gathered}\n\\]\nwhere we used the fact that the radius \\(R\\) cannot be larger than the critical chain length \\(\\mathrm{I}_{\\mathrm{c}}\\). We thus obtain for the critical packing parameter \\(P\\)\n\\[\nP=\\frac{v}{a_{0} l_{c}}&lt;\\frac{1}{3}\n\\]\n\nCylindrical micelles: \n\nFor a cylindrical micelle the total volume and surface area are given by \\(N v=\\pi R^{2} L\\) \\(N a_{0}=2 \\pi R L\\) \\(\\therefore \\frac{v}{a_{0}}=\\frac{R}{2}&lt;\\frac{l_{c}}{2}\\) again using \\(R&lt;I_{C}\\).\nWe thus obtain for the critical packing parameter \\(P\\) \\(\\frac{1}{3}&lt;\\frac{v}{a_{0} l_{c}}&lt;\\frac{1}{2}\\)\nBelow the lower limit spherical micelles are formed.\n\n\n\n19.0.3 Bilayers:\n\nFor a bilayer the total volume and surface area are given by\n\\[\n\\begin{aligned}\n& N v=A D \\\\\n& N a_{0}=2 A \\\\\n& \\frac{v}{a_{0}}=\\frac{D}{2}&lt;l_{c} \\quad\\left(\\text { using } D&lt;2 l_{c}\\right)\n\\end{aligned}\n\\]\nWe thus obtain for the critical packing parameter \\(P\\) \\(\\frac{1}{2}&lt;\\frac{v}{a_{0} l_{c}}&lt;1\\) Below the lower limit cylindrical micelles are formed.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html",
    "href": "soft-matter/soft-matter_glasses.html",
    "title": "20  Glasses",
    "section": "",
    "text": "Many fluids, whether simple liquids like argon or complex liquids such as colloids and polymers, can be quenched to temperatures well below their equilibrium freezing point without crystallization occurring on experimental timescales. In thermodynamic terms this is interpreted as a failure of the system to reach its true equilibrium (minimum free energy) state, namely the ordered crystalline phase. Such out-ofequilibrium systems are known as glasses. In terms of their structure (eg as measured by a radial distribution function, \\(g(r)\\), they are practically indistinguishable from a liquids, i.e. they exhibit some short ranged order, but no long ranged order. However, their dynamic behaviour is quite different from that of liquids; glasses exhibit very slow relaxation because molecules cannot easily diffuse. As a result, glasses do not flow (on experimental timescales) and therefore their mechanical properties, i.e. their response to stress and shearing, is more akin to that of solids.\n\n20.0.1 Glass forming systems\nGlasses may be formed if the cooling rate is so fast that the liquid does not have time to crystallize , or alternatively because the molecules have some feature to their structure or bonding that inhibits the formation of a crystalline phase. Many glass forming materials are found in nature. Below we describe just a few.\nElements: Sulphur, Selenium and Phosporous readily form glasses. Metallic alloys: liquid metals can form glasses if quenched very rapidly ( \\(10^{6} \\mathrm{~K} \\mathrm{~s}^{-1}\\) ). Such glasses find applications in recording heads and electrical transformers. Polymer glasses: Due to entanglement effects, polymers form glasses very easily. Examples from everyday life are polycarbonates (eyeglasses, shatterproof windows) and polymethacrylate (plastic pipes and tubes). Oxide glasses: Familiar examples are \\(\\mathrm{SiO}_{2}, \\mathrm{Na}_{2} \\mathrm{O}\\) and CaO (all components of window glass). Organic glasses: Sucrose solution forms a glass used in boiled sweets. Toluene and methanol also readily form glasses.\n\n\n20.0.2 Relaxation time and viscosity\nLet us first consider relaxation in liquids. We have already seen that at high volume fractions a particle is effectively trapped by its nearest neighbours, within a ‘cage’. To escape the cage, a particle must jump. Let us now consider the characteristic time \\(\\tau\\) between such jumps and its dependence on temperature. Empirically, one finds that\n\\[\n\\tau^{-1} \\sim v \\exp \\left(-\\frac{\\varepsilon}{k_{B} T}\\right)\n\\]\nHere \\(v\\) is the typical vibration frequency for a particle rattling around in its cage. This can also be regarded as the frequency with which the particle attempts to escape its cage. The probability of an escape attempt succeeding depends on a Boltzmann factor, i.e. the exponential of the ratio of the energy cost associated with climbing the cage “walls”, \\(\\varepsilon\\), to the typical thermal energy \\(k_{B} T\\).\nIt can be shown (cf. PH10002), that the relaxation time \\(\\tau\\) is proportional to another quantity, the viscosity \\(\\eta\\), which is a measure of the response of a material to a shear stress, or more loosely speaking a measure of the ability of a liquid to flow. Thus \\(\\tau \\propto \\eta\\) and it follows that\n\\[\n\\eta=\\frac{G_{0}}{v} \\exp \\left(\\frac{\\varepsilon}{k_{B} T}\\right)\n\\]\nwhere \\(G_{0}\\) is a proportionality constant. This characteristic exponential temperature dependence is known as Arrhenius behaviour. Loosely speaking the viscosity provides a measure of the ability of a liquid to flow.\nWhile Arrhenius behaviour is observed for the viscosity of liquids at high temperature, things are very different at low temperature. The experimentally measured viscosity and the associated configurational relaxation time exhibits a temperature dependence that strongly departs from that implied by the characteristic frequency of vibrations \\(v\\)\n\\[\n\\eta=\\eta_{0} \\exp \\left(\\frac{B}{T-T_{0}}\\right)\n\\]\nThe empirical finding, known as the Vogel-Fulcher law implies that the viscosity diverges exponentially at the finite temperature \\(T_{0}\\). In practical terms, this means that as \\(T_{0}\\) is approached from above, one reaches some temperature \\(T_{g}\\) (the glass transition temperature) at which the time for a configuration to relax becomes comparable to the timescale of the experiment itself \\(\\tau_{\\exp }\\). At this point, the system falls out of equilibrium with respect to rearrangements of the particle configurations.\n\n\n20.0.3 Characterising the glass transition\nThe glass transition temperature \\(T_{g}\\) is that temperature at which the system falls out of equilibrium on the experimental timescale \\(\\tau_{\\text {exp. }}\\). Clearly therefore, if one does a long experiment, in which the temperature is lowered slowly, the system will have a greater time to relax at each successive temperature and will stay in equilibrium to a lower temperature. Thus the experimental glass transition temperature depends on the rate at which we do the experiment. To illustrate this consider a measurement of some structural quantity (such as the sample volume) as a function of temperature. Various scenarios for the behaviour of the volume as the temperature are shown in the following figure \nAt the glass transition temperature, the curve exhibits a well defined discontinuity in it slope, rather like what happens at an equilibrium phase transition such as freezing or boiling. The reason why the glass transition is not a true thermodynamic phase transition is that the temperature at which the discontinuity occurs depends on the history of the sample, i.e. how rapidly it is cooled. We say that the glass transition is a kinetic phase transition brought on by the divergence of the structural relaxation time.\n\n\n20.0.4 A simple picture of the glass transition\nTheories of the glass transition are a matter of intense current research and it is probably fair to say that the detailed physics is still not well understood in a comprehensive fashion. Nevertheless simple models offer some insight, in qualitative if not quantitative agreement with experiment. One such theory is based on the idea of cooperativity. In a liquid at high temperatures, a molecule can diffuse simply by moving to occupy the space made available by the random local motions of its neighbours. At low temperatures and high volume fractions, the local motion of neighbours is insufficent to allow diffusion. Instead a number of neighbours must move cooperatively in order to make space for a molecule to move. The minimum number of molecules that have to move in unison in order for diffusion of a molecule to take place gives rise to the concept of a cooperative rearranging region. As the temperature is lowered the size of this region increases, diverging (with the viscosity) at the Vogel-Fulcher temperature \\(T_{0}\\)  \nIf we suppose that the energy barrier for a single molecule to move is \\(\\mu\\) and there are \\(z\\) molecules in the cooperatively rearranging region, then the thermal activation barrier for motion is\n\\[\n\\tau^{-1} \\sim v \\exp \\left(-\\frac{z \\mu}{k_{B} T}\\right)\n\\]\nwith \\(v\\) the microscopic vibration frequency. Within this model, the non Arrhenius behaviour of the relaxation time (see above) derives from the increase in the number of molecules that have to move cooperatively as temperature decreases, i.e. the \\(T\\) dependence of \\(z\\).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Glasses</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_readings.html",
    "href": "soft-matter/soft-matter_readings.html",
    "title": "21  Readings",
    "section": "",
    "text": "The best overall text is: R.A.L Jones, Soft Condensed Matter, Oxford University Press. Shelfmark [539.2 Jon].\nAdditionally, the following more specialised texts should also be useful. They can be found in the University Library under the stated shelfmark.\n\n21.0.1 Colloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet. VCH Publishers (1994). [541.18 Eva]\nR.J.Hunter: Introduction to Modern Colloid Science. Oxford University Press (1993). 541.18 Hun]\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions Cambridge University Press (1989).[541.18 Rus]\nD.H.Everett: Basic Principles of Colloid Science.\n\nRoyal Society of Chemistry Paperbacks (1988) [ 541.18 Eve]\n\n\n21.0.2 Polymers and surfactants\n\nR.J. Young and P.A. Lovell: Introduction to polymers [678 You].\nM. Doi: Introduction to polymer physics [541.64 Doi]\nJ.Israelachvili, Intermolecular and Surface Forces, Academic Press (1992), Chs. 16 and 17\n\n\n\n21.0.3 Glasses\n\nJ. Zarzycki; Glasses and the vitreous state. Cambridge University Press (1991).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Readings</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html",
    "href": "soft-matter/soft-matter_problems.html",
    "title": "22  Problems",
    "section": "",
    "text": "22.0.1 Relaxation time in atomic fluids\nSpherical colloidal particles of radius 30 nm and density \\(1.35 \\mathrm{~g} \\mathrm{~cm}^{-3}\\) are suspended in water (temperature \\(25^{\\circ} \\mathrm{C}\\), density \\(1.0 \\mathrm{~g} \\mathrm{~cm}^{-3}\\), viscosity \\(1.0 \\times 10^{-3}\\) Pa s ). Calculate the average distance from the origin along a given axis travelled by a particle in 1 minute due to Brownian motion and sedimentation, respectively. What are the displacements after 1 hour?\n\n\n22.0.2 Equipartition\nThe equipartitioning theorem states that in thermal equilibrium each generalised co-ordinate which occurs in the total system energy only as a quadratic term contributes \\((1 / 2) \\mathrm{k}_{\\mathrm{B}} \\mathrm{T}\\) to the mean energy of the system. Use the theorem to estimate the typical time it rakes atoms (or small molecules) in an atomic liquid to move distances of the order of their own size, i.e. estimate the relaxation time.\n\n\n22.0.3 Gravitational length\n\nFind the typical length scale (‘gravitational length’) of the distribution of polystyrene particles (density \\(\\rho_{\\mathrm{p}}=1.05 \\times 10^{3} \\mathrm{~kg} \\mathrm{~m}^{-3}\\) ) of radius 50 nm and \\(1 \\mu \\mathrm{~m}\\) respectively, suspended in water (density \\(\\rho_{\\mathrm{w}}=1.00 \\times 10^{3} \\mathrm{~kg} \\mathrm{~m}^{-3}\\), viscosity \\(1.0 \\times 10^{-3} \\mathrm{~Pa} \\mathrm{~s}\\) ) at room temperature. Hint: The gravitational length is the height at which the concentration falls to \\(1 / \\mathrm{e}\\) of its greatest value.\nIf a container of height \\(\\mathrm{H}=4 \\mathrm{~cm}\\) contains a dilute suspension of these particles at an average concentration of \\(n_{a}\\) particles per unit volume, what are the actual concentrations (in terms of \\(n_{a}\\) ) at the top and bottom of the sample when sedimentation equilibrium has been reached.\nIf the above sample is shaken up to give an initially uniform concentration and then left undisturbed, estimate how long it will take the concentration profile to reach its equilibrium state (A surprisingly long time!)\n\n\n\n22.0.4 Peclet number\nA definition of a colloid is a particle small enough that its Brownian motion is not dominated by gravity. Thus if the particle is raised a distance equal to its radius, the increase in its gravitational potential energy must be less than the thermal energy \\((3 / 2) \\mathrm{k}_{\\mathrm{B}} T\\). Another measure of the relative importance of the effects of Brownian motion and gravity, is the Peclet number \\(\\mathrm{Pe}=\\tau_{R} / \\tau_{\\text {sed }}\\). Here \\(\\tau_{\\mathrm{R}}\\) is the time taken by a particle to diffuse a distance equal to its radius, and \\(\\tau_{\\text {sed }}\\) is the time taken by it to sediment the same distance. When \\(\\mathrm{Pe} \\ll 1\\), Brownian motion dominates; when \\(\\mathrm{Pe} \\gg 1\\), gravity dominates. Show that the condition \\(\\mathrm{Pe}&lt;1\\) is more or less equivalent to the definition of a colloid given above.\n\n\n22.0.5 Diffusion equation\nProve by direct substitution that \\(n(x, t)=C(4 \\pi D t)^{-1 / 2} \\exp \\left(-x^{2} / 4 D t\\right)\\) is indeed a solution to the 1 D diffusion equation \\(\\partial n / \\partial t=D^{\\partial^{2} n} / \\partial x^{2}\\). Discuss the meaning of the constant C .\n\n\n22.0.6 Mean square displacement\nStarting from the diffusion equation, deduce the mean square displacement, \\(\\left\\langle x^{2}\\right\\rangle=2 D t\\), without using the full solution to the diffusion equation. Note that \\(n(x, t)\\) is the number density, from which the probability density of finding a particle in the region \\([\\mathrm{x}, \\mathrm{x}+\\mathrm{dx}]\\) during the time interval \\([\\mathrm{t}, \\mathrm{t}+\\mathrm{dt}]\\), can be obtained by normalising with \\(\\int n(x, t) d x=N\\) (the total number of particles). Having this in mind, the mean square displacement is \\(\\left\\langle x^{2}(t)\\right\\rangle=\\frac{\\int_{-\\infty}^{\\infty} x^{2} n(x, t) d x}{\\int_{-\\infty}^{\\infty} n(x, t) d x}=\\frac{1}{N} \\int_{-\\infty}^{\\infty} x^{2} n(x, t) d x\\) Hint: Multiply both sides of the 1D diffusion equation by \\(x^{2}\\) and integrate by parts over \\(x\\) to get an o.d.e for \\(\\left\\langle x^{2}\\right\\rangle\\) with t as the independent variable.\n\n\n22.0.7 Relaxation time in colloidal suspensions\nUse \\(\\left\\langle\\underline{r}^{2}\\right\\rangle=6 D t\\) and the Stokes-Einstein relation to show that the time \\(\\tau_{\\mathrm{R}}\\) for a particle to diffuse its own radius \\(R\\), scales as \\(R^{3}\\). The relaxation time \\(\\tau_{R}\\) is a good estimate for the time taken to return to equilibrium after a disturbance. Estimate \\(\\tau_{\\mathrm{R}}\\) for a suspension of spheres of radius \\(\\mathrm{R} \\approx 1 \\mu \\mathrm{~m}\\), i.e. at the upper limit of the colloidal size range.\n\n\n22.0.8 Colloidal crystals\nColloidal crystals are observed for volume fractions \\(\\phi\\) between 0.545 and 0.74 . Show that the upper limit of \\(\\phi=0.74\\) corresponds to the closest packing in a fcc crystal. What is the average distance between the colloidal particles for the lower limit \\(\\phi=0.545\\) ? Hint: to get the average consider a simple cubic lattice of the same volume fraction\n\n\n22.0.9 Lagrange’s expression for the radius of gyration\nFor N equal point masses at positions \\(\\left\\{\\underline{R}_{j}\\right\\}\\), the radius of gyration is defined by \\(R_{g}^{2}=\\frac{1}{N} \\sum_{j=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{G}\\right)^{2}\\) where \\(\\underline{R}_{G}=\\frac{1}{N} \\sum_{j=1}^{N} \\underline{R}_{j}\\) is the position of the centre of mass. Show that\n\\[\nR_{g}^{2}=\\frac{1}{2 N^{2}} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{k}\\right)^{2}\n\\]\nHint: Note that \\(\\frac{1}{N} \\sum_{j=1}^{N} 1=1\\)\n\n\n22.0.10 Mean length\nCalculate the mean end-to end length of a freely jointed polymer consisting of \\(10^{5}\\) monomers of length \\(4.322 \\AA\\). How does the end-to-end length change if the valence angle of the chain is fixed at \\(108^{\\circ}\\) ?\n\n\n22.0.11 Real data\nLight scattering measurements on dilute solutions of polystyrene, of different molar masses M , at the theta temperature give the following results for the mean square radius of gyration \\(\\left\\langle R_{g}^{2}\\right\\rangle\\) :\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{M}\\left(10^{6} \\mathrm{~g} \\mathrm{~mol}^{-1}\\right)\\)\n4.04\n1.56\n1.20\n1.06\n0.626\n0.394\n\n\n\n\n\\(&lt;R_{g}^{2}&gt;\\left(\\mathrm{nm}^{2}\\right)\\)\n3260\n1210\n928\n770\n484\n305\n\n\n\n\nShow that these results are consistent with \\(\\left\\langle R_{g}^{2}\\right\\rangle^{1 / 2}=B M^{1 / 2} \\quad\\left(M=N M_{M}\\right.\\) where \\(N\\) is the number of monomers in the chain and \\(M_{M}\\) is the molar mass of one monomer; \\(M_{M}=100 \\mathrm{~g} \\mathrm{~mol}^{-1}\\) ) and estimate the value of \\(B\\).\nGiven that the ‘size’ of a styrene monomer is about \\(\\mathrm{b}_{0}=0.23 \\mathrm{~nm}\\), calculate the factor \\(C\\) which represents the effect of restricted bond angles in \\(\\left\\langle R^{2}\\right\\rangle=C N b_{0}^{2}\\). From \\(C\\), calculate the bond angle \\(\\theta\\) which would apply if polystyrene can be represented by a freely-rotating chain. [Remember \\(\\left.\\left\\langle R_{g}^{2}\\right\\rangle=\\frac{1}{6}&lt;R^{2}\\right\\rangle\\) ]\nCalculate \\(\\left.&lt;R_{g}^{2}\\right\\rangle^{1 / 2}\\) for a polystyrene chain of molar mass \\(1 \\times 10^{7} \\mathrm{~g} \\mathrm{~mol}^{-1}\\) at the theta temperature.\nFor the chain of (c), estimate the fraction of the volume \\(\\left.(4 \\pi / 3)&lt;R_{g}^{2}\\right\\rangle^{3 / 2}\\) which is actually occupied by styrene monomers (assume that the density of styrene when polymerised is about \\(1 \\mathrm{~g} \\mathrm{~cm}^{-3}\\) ). What is the overlap concentration \\(c^{*}\\) (in mass per volume)? How do you expect \\(c^{*}\\) to change upon an increase (decrease) in temperature?\n\n\n\n22.0.12 Micelles\nThe volume of a linear hydrocarbon chain with \\(n\\) carbon atoms is given by \\(v=(27.4+26.9 n) \\times 10^{-3} \\mathrm{~nm}^{3}\\), and its critical chain length is \\(l_{c}=(0.154+0.1265 n)\\) \\(n m\\). An amphiphile has an anionic head group with an optimum head group area in aqueous solution of \\(a_{0}=0.65 \\mathrm{~nm}^{2}\\). (i) What shape micelles are formed by amphiphiles with linear hydrocarbon chains having \\(\\mathrm{n}=10\\) ? (ii) What is the average size and aggregation number of each micelle?\n\n\n22.0.13 Viscosity\nFor polystyrene, the variation of viscosity with temperature follows the Vogel-Fulcher law \\(B=710\\) and \\(T_{0}=50^{\\circ} \\mathrm{C}\\). Plot the function \\(\\eta / \\eta_{0}\\) in the temperature range \\(80-150^{\\circ} \\mathrm{C}\\). By what factor does the viscosity and relaxation time vary between the temperatures of \\(100-140^{\\circ} \\mathrm{C}\\) ?\n\n\n22.0.14 The glass transition\nFor polystyrene, a relaxation time associated with configurational rearrangements, \\(\\tau_{\\text {config, }}\\), follows a Vogel-Fulcher law,\n\\[\n\\tau_{\\text {confg }}=\\tau_{0} \\exp \\left(\\frac{B}{T-T_{0}}\\right)\n\\]\nWhere \\(\\tau_{0}, \\mathrm{~B}=710\\) and \\(T_{0}=50^{\\circ} \\mathrm{C}\\) are constants. A value of the experimental glass transition temperature is measured with an experiment carried out at an effective timescale \\(\\tau_{\\text {exp }}=1000\\) s and found to be \\(101.4^{\\circ} \\mathrm{C}\\). (a) Another experiment is carried out at an effective timescale of \\(10^{5} \\mathrm{~s}\\). What is the value of the glass transition temperature obtained from this experiment? (b) On what timescale must an experiment be carried out if it is to measure a glass transition temperature within \\(10^{\\circ} \\mathrm{C}\\) of the Vogel-Fulcher temperature \\(T_{0}\\). Is this practically possible?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "revision_guide.html",
    "href": "revision_guide.html",
    "title": "Appendix A — Revision guide",
    "section": "",
    "text": "In addition to having worked through and being familiar with all the questions on the problem sheets, you are expected to:\n\nKnow the relationship between the free energy of a system and its partition function. Be able to differentiate the free energy to obtain thermodynamic observables and fluctuation relations for the heat capacity and magnetic susceptibility.\nKnow the forms of the singularities exhibited by key observables on the approach to criticality and be able to define the exponents \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\) and \\(\\nu\\).\nBe able to draw the phase diagrams of magnetic and fluid systems and define the critical exponents. You should know the key differences between first and second order phase transitions.\nUnderstand the isomorphism between the Ising model and lattice gas.\nIsing model: Write down the Hamiltonian of the Ising model and be familiar with its properties in \\(d = 1, 2, 3\\). Derive the Ising model free energy in \\(d = 1\\) both in zero magnetic field (kink method) and in a finite field (Transfer matrix method).\nMean field theory: Show that the effective Hamiltonian \\[ H_{\\text{eff}} = H + qJm \\] and derive the equation \\[ m = \\tanh(\\beta H_{\\text{eff}}). \\] From this, find the behaviour of \\(m\\) and \\(\\chi\\) near \\(T_c\\). Know the form of the spin-spin correlation function in mean field theory and use it to calculate the behaviour of the heat capacity near criticality. Describe the shortcomings of mean field theory and explain the underlying reasons.\nLandau theory: Given the Landau free energy, sketch and interpret the form of the free energy as a function of magnetisation for a range of temperatures around \\(T_c\\). Derive the values of the critical exponents \\(\\beta\\), \\(\\delta\\), \\(\\gamma\\), \\(\\alpha\\). Understand the genesis of first order phase transitions and metastability within Landau theory and the importance of symmetry considerations.\nScaling theory: Recast a generalised homogeneous function as a power law in one variable. Applying this to the free energy, deduce scaling forms for key observables and relationships between the scaling variables and the critical exponents. Deduce scaling laws relating the critical exponents. Describe experimental tests of scaling and its utility in the context of computer simulations of critical phenomena.\nRenormalization Group: Describe the main purpose and features of the RG formalism in the real-space (block variable) approach. Describe the relevance of the RG to scaling phenomena and universality. Know which essential qualitative features of a system delineate universality classes.\nClassical nucleation theory: Understand that in first-order phase transitions, like in the Ising model under a small external field, or a slightly oversaturated gas, a free energy barrier must be overcome for the stable phase to nucleate within the metastable phase. Describe in qualitative and mathematical terms how nucleation involves the formation of a critical-sized region of the new phase, which can then grow to complete the transition, with the balance between bulk energy gain and surface energy cost determining the critical size. Describe mathematically how growth dynamics after nucleation depend on factors like interface motion and external driving forces, shaping the overall evolution from the metastable to the stable phase.\nStochastic processes: Understand that many natural processes are stochastic, meaning their evolution over time involves inherent randomness, either from lack of microscopic detail (in classical physics) or intrinsic probabilistic nature (in quantum mechanics). Describe such systems, using coarse-grained probabilistic methods, focusing on the likelihood of different outcomes instead of exact trajectories, ie. the master equation. Derive macroscopic behaviors like diffusion by analyzing the balance of transition rates between states. Understand that the Langevin approach models particle motion using a stochastic differential equation, offering a trajectory-based view complementary to the probability-focused diffusion (Fokker-Planck) equation. Describe mathematically how in the Langevin equation, random forces and friction combine to determine particle displacements over small time intervals, capturing the randomness inherent in Brownian motion. Derive key statistical quantities such as the mean square displacement and the form of the diffusion constant and dampling constant. By analyzing the statistics of these random steps, one can derive the diffusion equation as the continuum limit of the random walk.\nDynamics of fluctuations: Understand and describe mathematically how fluctuations of thermodynamic variables, like local magnetization or density, can be analyzed through their time correlations when the system is in thermal equilibrium. Describe the role of the two-time correlation function \\(M_{xx}(t)\\), which measures how a fluctuation at one time is related to a fluctuation at a later time. Describe how in equilibrium, this correlation depends only on the time difference and typically decays exponentially with a characteristic correlation time \\(t_c\\).\n\nNote that some of the above items were mainly covered in the problem sheets.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Revision guide</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "href": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.3 Domain growth",
    "text": "10.3 Domain growth\nFollowing successful nucleation of a supercritical droplet, the system enters a regime where the global transformation is driven by the deterministic growth of domains of the stable phase. Such domain growth involves more atoms or molecules attaching to these nuclei, causing them to expand into larger structures (e.g., growing crystals or droplets).\nGrowth rate depends on factors like temperature, concentration, and the availability of building blocks.\nThe shape and structure of the final phase often depend on how growth occurs (e.g., slow growth may form perfect crystals; rapid growth may be irregular).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation and domain growth</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "href": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.4 Spinodal decomposition",
    "text": "10.4 Spinodal decomposition\nPreviously we have considered a scenario in which we move our system to a statepoint just inside the coexistence region so that the original phase remains metastable. We then wait for a fluctuation that yields a critical nucleus and subsequent growth.\nNow imagine that rather than positioning the system just inside the coexistence region we move immediately to a state point well inside the coexistence region such that there is no metastable minimum in the free energy. Then there is no nucleation and growth, rather the system is unstable and immediately starts to phase separate at all points in the system. This so called spinodal decomposition.\nThe nature of spinodal decomposition and late-time domain growth depends sensitively on whether the order parameter is conserved or not.\n\n\n10.4.1 Non-Conserved Order Parameter Dynamics (Model A)\nIn systems with a non-conserved order parameter, such as the Ising model with single spin flip (so called Glauber) dynamics, the order parameter can relax locally without constraint. This leads to curvature-driven motion of interfaces.\nThe typical domain size \\(L(t)\\) grows algebraically with time:\n\\[\nL(t) \\sim t^{1/2},\n\\]\ncorresponding to a dynamic exponent \\(z = 2\\). The growth is driven by reduction of the total interfacial area—regions of high curvature (small domains) shrink and are absorbed by larger, flatter ones.\nLet is aassume that domain walls move with velocity proportional to curvature: \\[\n  v \\sim \\frac{1}{L}\n  \\] Then with the typical domain size being \\(L(t)\\) it follows that\n\\[\n  \\frac{dL}{dt} \\sim \\frac{1}{L}\n  \\]\nIntegrating both sides yields the \\(t^{1/2}\\) domain growth law.\nIt turns out that the detailed evolution of the coarse-grained order parameter field \\(\\phi(\\mathbf{r}, t)\\) is governed by the Allen–Cahn equation:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = - \\frac{\\delta F[\\phi]}{\\delta \\phi},\n\\]\nwhere \\(F[\\phi]\\) is a coarse-grained Ginzburg–Landau free energy functional:\n\\[\nF[\\phi] = \\int d^d x \\left[ \\frac{1}{2} (\\nabla \\phi)^2 + V(\\phi) \\right],\n\\]\nwith \\(V(\\phi)\\) typically a double-well potential such as \\(V(\\phi) = \\frac{1}{4}(\\phi^2 - 1)^2\\).\n\n\n\n10.4.2 Conserved Order Parameter Dynamics (Model B)\nFor systems with a conserved order parameter, such as phase separation in binary alloys or the Ising model with spin-swap (so called ‘Kawasaki’) dynamics, the order parameter (e.g., composition or particle number) must be conserved locally. This imposes a diffusive constraint on the dynamics.\nThe domain size again grows algebraically, but with a different exponent:\n\\[\nL(t) \\sim t^{1/3},\n\\]\ncorresponding to a dynamic exponent \\(z = 3\\).\nA chemical potential difference drives diffusion and is given by \\[\n\\Delta \\mu \\sim \\frac{\\sigma}{L}\n\\] (again due to curvature, where \\(\\sigma\\) is surface tension)\nNow the flux is proportional to the chemical potential gradient (Fick’s law): \\[\n\\text{Flux} \\sim -\\nabla \\mu \\sim \\frac{\\Delta \\mu}{L} \\sim \\frac{\\sigma}{L^2}\n\\]\nand the rate of change of domain size is proportional to this flux: \\[\n\\frac{dL}{dt} \\sim \\frac{1}{L^2}\n\\quad\\Rightarrow\\quad\nL(t) \\sim t^{1/3}\n\\]\nIt turns out that the detailed dynamics are described by the Cahn–Hilliard equation, a continuity equation of the form:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = \\nabla^2 \\left( \\frac{\\delta F[\\phi]}{\\delta \\phi} \\right),\n\\]\nreflecting that the order parameter can only evolve via diffusion of its conjugate chemical potential. This leads to the slow transport of material across domains and a more sluggish coarsening process compared to the non-conserved case.\n\n\n10.4.3 Schematic of Domain Growth in 2D Ising model\nHere are schematic illustrations of domain growth for:\n\nNon-conserved dynamics (Model A): Domains coarsen rapidly, with smoother and larger regions due to free relaxation of the order parameter.\nConserved dynamics (Model B): Coarsening is slower and domains are more intricate, reflecting the constraint of local conservation.\n\n\n\n\n\n\n\n\n\nNon-Conserved Dynamics (Model A)\n\n\n\n\n\n\n\nConserved Dynamics (Model B)\n\n\n\n\n\n\nFigure 10.2: Schematic illustrations of domain morphology resulting from spinodal decomposition or late-time domain growth for non-conserved and conserved dynamics.\n\n\n\n\n\n10.4.4 Dynamics Scaling Hypothesis\nAt late times, both conserved and non-conserved systems exhibit dynamic scaling: the statistical properties of the domain morphology become self-similar under rescaling of lengths by \\(L(t)\\).\nFor example, the equal-time two-point correlation function satisfies\n\\[\nC(r, t) = f\\left(\\frac{r}{L(t)}\\right),\n\\]\nwhere \\(f(x)\\) is a time-independent scaling function. Plots of \\(C(r, t)\\) collapse when plotted as a function of \\(r/L(t)\\).\nThe structure factor \\(S(k, t)\\), which is the Fourier transform of the correlation function, is experimentally accessible eg via X-ray or neutron scattering, and also obeys dynamic scaling:\n\\[\nS(k, t) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r, t).\n\\]\nSubstituting the scaling form of \\(C(r, t)\\) into this expression, and changing variables to \\(\\vec{u} = \\vec{r}/L(t)\\), gives:\n\\[\nS(k, t) = L(t)^d \\int d^d u \\, e^{-i \\vec{k} \\cdot L(t) \\vec{u}} \\, f(u) = L(t)^d \\, g(kL(t)),\n\\]\nwith \\(g(x)\\) a universal scaling function dependent on the dynamical class and dimensionality.\n\n\n10.4.5 Summary of Growth Laws\n\n\n\n\n\n\n\n\n\n\nDynamics Type\nConservation\nEquation Type\nGrowth Law\nDynamic Exponent\n\n\n\n\nModel A (e.g. Glauber)\nNo\nAllen–Cahn\n\\(L(t) \\sim t^{1/2}\\)\n\\(z = 2\\)\n\n\nModel B (e.g. Kawasaki)\nYes\nCahn–Hilliard\n\\(L(t) \\sim t^{1/3}\\)\n\\(z = 3\\)\n\n\n\n\n\nRemarks: The domain growth exponents \\(1/z\\) are robust under many conditions, but can be modified in the presence of disorder, long-range interactions, or hydrodynamic effects.\n\n\nIn both the model A and model B cases, the system coarsens until it reaches equilibrium, characterized by a uniform macroscopic phase and the complete elimination of interfaces.\n\n\nThe approach to equilibrium is algebraically slow (described by power laws) due to the scale-free nature of domain dynamics.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation and domain growth</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#examples-of-order-parameters",
    "href": "phase-transitions/mean-field-theory.html#examples-of-order-parameters",
    "title": "6  Mean field theory and perturbation schemes",
    "section": "6.6 Examples of Order Parameters",
    "text": "6.6 Examples of Order Parameters\n\n\n\n\n\n\n\n\nPhysical System\nOrder Parameter \\(\\varphi\\)\nSymmetry Group\n\n\n\n\nUniaxial (Ising) ferromagnet\nMagnetisation per spin, \\(m\\)\n\\(O(1)\\)\n\n\nFluid (liquid-gas)\nDensity difference, \\(\\rho - \\rho_c\\)\n\\(O(1)\\)\n\n\nLiquid mixtures\nConcentration difference, \\(c - c_c\\)\n\\(O(1)\\)\n\n\nBinary (AB) alloy (e.g., \\(\\beta\\)-brass)\nConcentration of one of the species, \\(c\\)\n\\(O(1)\\)\n\n\nIsotropic (vector) ferromagnet\n\\(n\\)-component magnetisation, \\(\\mathbf{m} = (m_1, m_2, \\dots, m_n)\\)\n\\(O(n)\\)\n\n\n\n\\(n = 2\\): xy model\n\\(O(2)\\)\n\n\n\n\\(n = 3\\): Heisenberg model\n\\(O(3)\\)\n\n\nSuperfluid He\\(^4\\)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nSuperconductor (s-wave)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nNematic liquid crystal\nOrientational order, \\(\\langle P_2(\\cos \\theta)\\rangle\\)\n\n\n\nSmectic A liquid crystal\n1-dimensional periodic density\n\n\n\nCrystal\n3-dimensional periodic density",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  }
]