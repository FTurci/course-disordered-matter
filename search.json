[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "",
    "text": "Welcome to the Complex Disordered Matter course!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Overview",
    "text": "Overview\nThis course introduces your to the theoretical, computational and experimental aspects of the physics of complex disordered matter.\n\nComplex disordered matter is the study of wide range of systems like polymers, colloids, glasses, gels, and emulsions, which lack long-range order but exhibit intricate behaviour. Colloids, suspensions of microscopic particles in a fluid, are useful for studying disordered structures due to their observable dynamics. Similarly, polymer systems can form amorphous solids or glasses when densely packed or cooled, showing solid-like rigidity despite their disordered structure. These materials often undergo phase transitions, such as demixing and crystallisation, and near these transitions, they can display critical phenomena with extensive fluctuations and correlations.\nThese various systems are examples of soft matter systems. In such systems, the interplay between disorder, softness, and phase behavior leads to rich physical phenomena, particularly near critical points where even small changes in external conditions can trigger large-scale reorganisations and universal behaviour. Glasses, for instance, exhibit slow relaxation and memory effects, while colloidal systems may crystallize, phase separate, or become jammed depending on particle interactions and concentration. Understanding such behaviors involves studying how microscopic interactions and thermal fluctuations influence macroscopic properties, especially in non-equilibrium conditions. Through techniques like scattering, microscopy, rheology, and simulation, one can explore how disordered soft materials respond to stress, age, or undergo transitions—insights that are vital for applications in materials design, biotechnology, and beyond.\nThis course is organized into three interconnected parts, each offering a distinct perspective on the study of complex disordered matter.\n\nPart 1: Unifying concepts (Nigel Wilding) introduces the theoretical framework for rationalising complex disordered matter which is grounded in statistical mechanics and thermodynamics. We emphasize the theory of phase transitions, thermal fluctuations, critical phenomena, and stochastic dynamics—providing the essential theoretical tools needed to describe and predict the behavior of soft and disordered systems.\n\nPart 2: Complex disordered matter (Francesco Turci) explores the phenomenology of key examples of complex disordered soft matter systems, including colloids, polymers, liquid crystals, glasses, gels, and active matter. These systems will be analyzed using the theoretical concepts introduced in Part 1, highlighting how disorder, interactions, and fluctuations shape their macroscopic behavior.\n\nPart 3: Experimental techniques (Adrian Barnes) focuses on the methods of microscopy, and scattering via x-rays, neutrons and light that are used to study complex disordered matter, offering insight into how their properties are measured and understood in real-world contexts.\n\nIn addition to theory and experiment, computer simulation plays a central role in soft matter research. This course includes a substantial coursework component consisting of a computational project. This exercise will allow you to apply state-of-the-art simulation techniques to investigate the complex behavior of disordered systems, bridging theory and observation through hands-on exploration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#delivery-and-format",
    "href": "index.html#delivery-and-format",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Delivery and format",
    "text": "Delivery and format\n\nDetailed e-notes (accessible via Blackboard) can be viewed on a variety of devices. Pdf is also available.\nWe will give ‘traditional’ lectures (Tuesdays, Wednesdays, Fridays) in which we use slides to summarise and explain the lecture content. Questions are welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to the explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are problem classes (Thursdays) where you can try problem sheets and seek help. Lecturers may go over some problems with the class, treating them as worked examples.\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Intended learning outcomes",
    "text": "Intended learning outcomes\nThe course will\n\nIntroduce you to the qualitative features of a range of complex and disordered systems and the experimental techniques used to study them.\nIntroduce you to a range of model systems and theoretical techniques used to elucidate the physics of complex disordered matter.\nProvide you with elementary computational tools to model complex disordered systems numerically and predict their properties.\nAllow you to apply your physics background to understand a variety of systems of inter-disciplinary relevance.\nConnect with the most recent advances in the research on complex disordered matter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Contact details",
    "text": "Contact details\nThe course will be taught by\n\nProf Nigel B. Wilding (unit director): nigel.wilding@bristol.ac.uk\nDr Francesco Turci: F.Turci@bristol.ac.uk\nDr Adrian Barnes: a.c.barnes@bristol.ac.uk",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#questions-and-comments",
    "href": "index.html#questions-and-comments",
    "title": "Welcome to the Complex Disordered Matter course!",
    "section": "Questions and comments",
    "text": "Questions and comments\nIf you have any questions about the course, please don’t hesitate to contact the relevant lecturer, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025/26. If you find any errors or mistakes or something which isn’t clear, please let us know by email, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Recommended texts and literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course course is the absence of a single wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking.\n\nRevision on thermodynamics and statistical mechanics\nSee your year two Thermal Physics notes. Also\n\nF. Mandl: Statistical Physics\n\n\n\nPhase transitions and critical phenomena\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\n\n\nStochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry\n\n\n\nSoft matter and glasses\nThe best overall text for part 2 of the course is:\n\nR.A.L Jones, Soft Condensed Matter.\n\nAdditionally, the following more specialised texts (which include information on experimental techniques) might be useful.\n\nColloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet\nR.J.Hunter: Introduction to Modern Colloid Science\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions\nD.H.Everett: Basic Principles of Colloid Science\n\n\n\nPolymers and surfactants\n\nR.J. Young and P.A. Lovell: Introduction to polymers\nM. Doi: Introduction to polymer physics\nJ.Israelachvili, Intermolecular and Surface Forces\n\n\n\nGlasses\n\nJ. Zarzycki; Glasses and the vitreous state",
    "crumbs": [
      "Recommended texts"
    ]
  },
  {
    "objectID": "phase-transitions/introduction.html",
    "href": "phase-transitions/introduction.html",
    "title": "1  Introduction to phase behaviour and enhanced fluctuations",
    "section": "",
    "text": "A phase transition can be defined as a macroscopic rearrangment of the internal constituents of a system in response to a change in the thermodynamic conditions to which they are subject. A wide variety of physical systems undergo such transitions. Understanding the properties of phase transitions is fundamental to the study of soft and complex matter, as these systems often exhibit rich and subtle transformations between different states of organization. Whether in colloidal suspensions, polymer blends, liquid crystals, or biological materials, phase transitions underpin a wide range of physical behaviours, from self-assembly and pattern formation to critical phenomena and dynamical arrest. By analysing how macroscopic phases emerge from microscopic interactions and external conditions, one gains crucial insight into the principles that govern structure, stability, and functionality in these intricate systems. As such, an understanding of phase transitions not only enriches theoretical understanding but also informs practical applications across materials science, biophysics, and nanotechnology. For these reasons we will devote a large proportion of this course to the study of phase transitions.\nTwo classic examples of systems displaying phase transitions are the ferromagnet and fluid systems. For the magnet, a key observable is the magnetisation defined as the magnetic moment per spin, given by \\(m=M/N\\), with \\(N\\) the number of spins. \\(m\\) can be positive or negative, dependent on whether the spins are aligned ‘up’ or ‘down’. As the temperature of a ferromagnet is increased, its net magnetisation \\(|m|\\) is observed to decrease smoothly, until at a certain temperature known as the critical temperature, \\(T_c\\), it vanishes altogether (see left part of Figure 1.1). We define the magnetisation to be the order parameter of this phase transition.\nOne can also envisage applying a magnetic field \\(H\\) to the system which, depending on its sign (i.e. whether it is aligned (positive) or anti-aligned (negative) relative to the magnetisation axis), favours up or down spin states respectively, as shown schematically in Figure 1.1 (right part). Changing the sign of the magnetic field \\(H\\) for \\(T&lt;T_c\\) leads to a phase transition chacterised by a discontinuous jump in \\(m\\). We shall explore this behaviour in more detail in section 5.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of a simple magnet (schematic). Left: magnetisation as a function of temperature for zero applied magnetic field, \\(H=0\\). Right: Applying a magnetic field that is aligned or antialigned with the direction of the magnetisation leads to a phase transition. The \\(H=0\\) axis at \\(T&lt;T_c\\) is the coexistence curve for which positive and negative magnetisations are equally likely.\n\n\n\nSimilarly, a change of state from liquid to gas can be induced in a fluid system (though not in an ideal gas) simply by raising the temperature. Typically the liquid-vapour transition is abrupt, reflecting the large number density difference between the states either side of the transition. However the abruptness of this transition can be reduced by applying pressure. At one particular pressure and temperature the discontinuity in the density difference between the two states vanishes and the two phases coalesce. These conditions of pressure and temperature serve to locate the critical point for the fluid. We define the density difference \\(\\rho_{liq}-\\rho_{vap}\\) to be the order parameter for the liquid-gas phase transition. We shall meet order parameters for other, more complex, systems in section 5,\n\n\n\n\n\n\nFigure 1.2: Phase diagram of a simple fluid (schematic)\n\n\n\nIn the vicinity of a critical point, a system displays a host of remarkable behaviors known as critical phenomena. Chief among these is the divergence of thermal response functions—such as specific heat, compressibility, or magnetic susceptibility—which signal an enhanced sensitivity to external perturbations. These singularities arise from the emergence of large-scale cooperative interactions among the system’s microscopic constituents, as measured by a diverging correlation length (see Chapter 2). One visually striking manifestation of this is critical opalescence, particularly observed in fluids like CO\\(_2\\). As carbon dioxide nears its critical temperature and pressure, the distinction between its liquid and gas phases vanishes, giving rise to huge fluctuations in density. These fluctuations scatter visible light, rendering the fluid milky or opalescent. This scattering effect directly reflects the long-range correlations developing within the fluid. The movie below illustrates the effect as the critical temperature of CO\\(_2\\) is approached from above. Note the appearence of a liquid-vapour interface (meniscus) as the system enters the two-phase region.\n\nThe recalcitrant problem posed by the critical region is how best to incorporate such collective effects within the framework of a rigorous mathematical theory that affords both physical insight and quantitative explanation of the observed phenomena. This matter has been (and still is!) the subject of intense theoretical activity.\nThe importance of the critical point stems largely from the fact that many of the phenomena observed in its vicinity are believed to be common to a whole range of apparently quite disparate physical systems. Systems such as liquid mixtures, superconductors, liquid crystals, ferromagnets, antiferromagnets and molecular crystals may display identical behaviour near criticality. This observation implies a profound underlying similarity among physical systems at criticality, regardless of many aspects of their distinctive microscopic nature. These ideas have found formal expression in the so-called ‘universality hypothesis’ which, since its inception in the 1970s, has enjoyed considerable success.\nIn the next few lectures, principal aspects of the contemporary theoretical viewpoint of phase transitions and critical phenomena will be reviewed. Mean field theories of phase transitions will be discussed and their inadequacies in the critical region will be exposed. The phenomenology of the critical region will we described including power laws, critical exponents and their relationship to scaling phenomena. These will be set within the context of the powerful renormalisation group technique. The notion of universality as a phenomenological hypothesis will be introduced and its implications for real and model systems will be explored. Finally, the utility of finite-size scaling methods for computer studies of critical phenomena will be discussed, culminating in the introduction of a specific technique suitable for exposing universality in model systems. Thereafter we will consider some foundational concepts in the dynamics of complex disordered matter. We shall look at the processes by which one phase transform into another and introduce differential equations that allow us to deal with the inherent stochasticity of thermal systems. The wider applicability of these unifying concepts to complex disordered systems such as colloids, polymers, liquid crystals and glasses will be covered in part 2 of the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to phase behaviour</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html",
    "href": "phase-transitions/background.html",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "2.1 Observables and expectation values\nIn seeking to describe phase transition and critical phenomena, it is useful to have a quantitative measure of the difference between the phases: this is the role of the order parameter, \\(Q\\). In the case of the fluid, the order parameter is taken as the difference between the densities of the liquid and vapour phases. In the ferromagnet it is taken as the magnetisation. As its name suggest, the order parameter serves as a measure of the kind of orderliness that sets in when the temperature is cooled below a critical temperature.\nOur first task is to give some feeling for the principles which underlie the ordering process. Referring back to Section 1.2, the probability \\(p_a\\) that a physical system at temperature \\(T\\) will have a particular microscopic arrangement (alternatively referred to as a ‘configuration’ or ‘state’), labelled \\(a\\), of energy \\(E_a\\) is\n\\[\np_a=\\frac{1}{Z}e^{-E_a/k_BT}\n\\tag{2.1}\\]\nThe prefactor \\(Z^{-1}\\) is the partition function: since the system must always have some specific arrangement, the sum of the probabilities \\(p_a\\) must be unity, implying that\n\\[\nZ=\\sum_ae^{-E_a/k_BT}\n\\tag{2.2}\\] where the sum extends over all possible microscopic arrangements.\nThese equations assume that physical system evolves rapidly (on the timescale of typical observations) amongst all its allowed arrangements, sampling them with the probabilities Equation 2.1 the expectation value of any physical observable \\(O\\) will thus be given by averaging \\(O\\) over all the arrangements \\(a\\), weighting each contribution by the appropriate probability:\n\\[\\overline {O}=\\frac{1}{Z}\\sum_a O_a e^{-E_a/k_BT}\n\\tag{2.3}\\]\nSums like Equation 2.3 are not easily evaluated because the number of terms grows exponentially in the system size. Nevertheless, some important insights follow painlessly. Consider the case where the observable of interest is the order parameter, or more specifically the magnetisation of a ferromagnet.\n\\[\nQ=\\frac{1}{Z}\\sum_a Q_a e^{-E_a/k_BT}\n\\tag{2.4}\\]\nIt is clear from Equation 2.1 that at very low temperature the system will be overwhelmingly likely to be found in its minimum energy arrangements (ground states). For the ferromagnet, these are the fully ordered spin arrangements having magnetisation \\(+1\\), or \\(-1\\).\nNow consider the high temperature limit. The enhanced weight that the fully ordered arrangement carries in the sum of Equation 2.4 by virtue of its low energy, is now no longer sufficient to offset the fact that arrangements in which \\(Q_a\\) has some intermediate value, though each carry a smaller weight, are vastly greater in number. A little thought shows that the arrangements which have essentially zero magnetisation (equal populations of up and down spins) are by far the most numerous. At high temperature, these disordered arrangements dominate the sum in Equation 2.4 and the order parameter is zero.\nThe competition between energy-of-arrangements weighting (or simply ‘energy’) and the ‘number of arrangements’ weighting (or ‘entropy’) is then the key principle at work here. The distinctive feature of a system with a critical point is that, in the course of this competition, the system is forced to choose amongst a number of macroscopically different sets of microscopic arrangements.\nFinally in this section, we note that the probabilistic (statistical mechanics) approach to thermal systems outlined above is completely compatible with classical thermodynamics. Specifically, the bridge between the two disciplines is provided by the following equation\n\\[\nF=-k_BT \\ln Z\n\\tag{2.5}\\]\nwhere \\(F\\) is the “Helmholtz free energy”. All thermodynamic observables, for example the order parameter \\(Q\\), and response functions such as the specific heat or magnetic susceptibility are obtainable as appropriate derivatives of the free energy. For instance, utilizing Equation 2.2, one can readily verify (try it as an exercise!) that the average internal energy is given by\n\\[\\overline{E}=-\\frac{\\partial \\ln Z}{\\partial \\beta},\\]\nwhere \\(\\beta=(k_BT)^{-1}\\).\nThe relationship between other thermodynamic quantities and derivatives of the free energy are given in fig. Figure 2.1",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#observables-and-expectation-values",
    "href": "phase-transitions/background.html#observables-and-expectation-values",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "Figure 2.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#sec-correlations",
    "href": "phase-transitions/background.html#sec-correlations",
    "title": "2  Key concepts for phase transitions",
    "section": "2.2 Correlations",
    "text": "2.2 Correlations\n\n2.2.1 Spatial correlations\nThe two-point connected correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where the correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nIn isotropic fluids and particle systems, a closely related and more directly measurable quantity (particularly in simulations) is the radial distribution function \\(g(r)\\), which describes how particle density varies as a function of distance from a reference particle. For such systems, the two-point correlation function of the number density field \\(\\rho(\\vec{r})\\) is related to \\(g(r)\\) as follows:\n\\[\ng(r) = 1+\\frac{C(r)}{\\rho^2},\n\\] where \\(\\rho\\) is the average number density. This relation shows that \\(g(r)\\) encodes the same spatial correlations as \\(C(r)\\), but in a form that is more natural for discrete particle systems. Note that by definition \\(g(r)\\to 1\\) in the absence of correlations ie. when \\(C(r)=0\\). This is typically the case for \\(r\\gg\\xi\\).\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector and \\(d^dr\\) refers to the elemental volume (eg. \\(d^3r\\) in three dimensions).\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.\n\n\n2.2.2 Temporal correlations\nConsider a thermodynamic variable \\(x\\) with zero mean that fluctuates over time. Examples include the local magnetization in a magnetic system or the local density in a fluid. Here, \\(x\\) represents a deviation from the average value — a fluctuation.\nWe’re interested in how such fluctuations are correlated over time when the system is in thermal equilibrium. For instance, if \\(x\\) is positive at some time \\(t\\), it’s more likely to remain positive shortly after.\nThese temporal correlations are characterized by the two-time correlation function (also known as an auto-correlation function):\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle\n\\]\nIn equilibrium, the correlation function must be independent of the starting time \\(\\tau\\). Therefore, we define:\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle = M_{xx}(t)\n\\]\nThat is, \\(M_{xx}(t)\\) depends only on the time difference \\(t\\).\nWe typically expect \\(M_{xx}(t)\\) to decay exponentially over a characteristic correlation time \\(t_c\\):\n\\[\nM_{xx}(t) \\sim \\exp(-t / t_c)\n\\]\n\n\n\n\n\n\nFigure 2.2: Sketch of \\(M_{xx}(t)\\) against \\(t\\)\n\n\n\nThis exponential decay reflects how the memory of fluctuations fades with time.\nNow consider two different fluctuating variables, \\(x\\) and \\(y\\) (e.g., local magnetizations at different positions). Their cross-correlation function is defined as:\n\\[\n\\langle x(\\tau) y(\\tau + t) \\rangle = M_{xy}(t)\n\\]\nThis defines the elements of a dynamic correlation matrix, of which \\(M_{xx}(t)\\) is the diagonal.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/approach-to-criticality.html",
    "href": "phase-transitions/approach-to-criticality.html",
    "title": "3  The approach to criticality",
    "section": "",
    "text": "It is a matter of experimental fact that the approach to criticality in a given system is characterized by the divergence of various thermodynamic observables. Let us remain with the archetypal example of a critical system, the ferromagnet, whose critical temperature will be denoted as \\(T_c\\). For temperatures close to \\(T_c\\), the magnetic response functions (the magnetic susceptibility \\(\\chi\\) and the specific heat) are found to be singular functions, diverging as a power of the reduced (dimensionless) temperature \\(t \\equiv\n(T-T_c)/T_c\\):-\n\\[\n\\chi \\equiv \\frac{\\partial M}{\\partial H}\\propto t^{-\\gamma} ~~~~ (H=0)\n\\tag{3.1}\\]\n(where \\(M=mN\\)), \\[\nC_H \\equiv \\frac{\\partial E}{\\partial T}\\propto t^{-\\alpha} ~~~~ (H=\\textrm{ constant})\n\\tag{3.2}\\]\nAnother key quantity is the correlation length \\(\\xi\\), which measures the distance over which fluctuations of the magnetic moments are correlated. This is observed to diverge near the critical point with an exponent \\(\\nu\\).\n\\[\n\\xi \\propto t^{-\\nu} ~~~~ (T &gt; T_c,\\: H=0)\n\\tag{3.3}\\]\nSimilar power law behaviour is found for the order parameter \\(Q\\) (in this case the magnetisation) which vanishes in a singular fashion (it has infinite gradient) as the critical point is is approached as a function of temperature:\n\\[\nm \\propto t^{\\beta} ~~~~ (T &lt; T_c,\\: H=0)\n\\tag{3.4}\\] (here the symbol \\(\\beta\\), is not to be confused with \\(\\beta=1/k_BT\\)– this unfortunately is the standard notation.)\nFinally, as a function of magnetic field:\n\\[m \\propto h^{1/\\delta} ~~~~ (T = T_c,\\: |H|&gt;0) . \\tag{3.5}\\] with \\(h=(H-H_c)/H_c\\), the reduced magnetic field.\nAs examples, the behaviour of the magnetisation and correlation length are plotted in Figure 3.1 as a function of \\(t\\).\n\n\n\n\n\n\nFigure 3.1: Singular behaviour of the correlation length and order parameter in the vicinity of the critical point as a function of the reduced temperature \\(t\\).\n\n\n\nThe quantities \\(\\gamma, \\alpha, \\nu, \\beta\\) in the above equations are known as critical exponents. They serve to control the rate at which the various thermodynamic quantities change on the approach to criticality.\nRemarkably, the form of singular behaviour observed at criticality for the example ferromagnet also occurs in qualitatively quite different systems such as the fluid. All that is required to obtain the corresponding power law relationships for the fluid is to substitute the analogous thermodynamic quantities in to the above equations. Accordingly the magnetisation order parameter is replaced by the density difference \\(\\rho_{liq}-\\rho_{gas}\\) while the susceptibility is replaced by the isothermal compressibility and the specific heat capacity at constant field is replaced by the specific heat capacity at constant volume. The approach to criticality in a variety of qualitatively quite different systems can therefore be expressed in terms of a set of critical exponents describing the power law behaviour for that system (see the book by Yeomans for examples).\nEven more remarkable is the experimental observation that the values of the critical exponents for a whole range of fluids and magnets (and indeed many other systems with critical points) are identical. This is the phenomenon of universality. It implies a deep underlying physical similarity between ostensibly disparate critical systems. The principal aim of theories of critical point phenomena is to provide a sound theoretical basis for the existence of power law behaviour, the factors governing the observed values of critical exponents and the universality phenomenon. Ultimately this basis is provided by the Renormalisation Group (RG) theory, for which K.G. Wilson was awarded the Nobel Prize in Physics in 1982.\nMore about the scientists mentioned in this chapter:\nKenneth Wilson",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The approach to criticality</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html",
    "href": "phase-transitions/Ising-model.html",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "4.1 The 2D Ising model\nIn order to probe the properties of the critical region, it is common to appeal to simplified model systems whose behaviour parallels that of real materials. The sophistication of any particular model depends on the properties of the system it is supposed to represent. The simplest model to exhibit critical phenomena is the two-dimensional Ising model of a ferromagnet. Actual physical realizations of 2-d magnetic systems do exist in the form of layered ferromagnets such as K\\(_2\\)CoF\\(_4\\), so the 2-d Ising model is of more than just technical relevance.\nThe 2-d spin-\\(\\frac{1}{2}\\) Ising model envisages a regular arrangement of magnetic moments or ‘spins’ on an infinite plane. Each spin can take two values, \\(+1\\) (‘up’ spins) or \\(-1\\) (‘down’ spins) and is assumed to interact with its nearest neighbours according to the Hamiltonian\n\\[\n{\\cal H}_I=-J\\sum_{&lt;ij&gt;}s_is_j - H\\sum_i s_i\n\\tag{4.1}\\]\nwhere \\(J&gt;0\\) measures the strength of the coupling between spins and the sum extends over nearest neighbour spins \\(s_i\\) and \\(s_j\\), i.e it is a sum of the bonds of the lattice. \\(H\\) is a magnetic field term which can be positive or negative (although for the time being we will set it equal to zero). The order parameter is simply the average magnetisation:\n\\[m=\\frac{1}{N} \\langle \\sum_i s_i \\rangle\\:,\\] where \\(\\langle\\cdot\\rangle\\) means an average over typical configurations corresponding to the prescribed value of \\(J/k_BT\\).\nThe fact that the Ising model displays a phase transition was argued in Chapter 2. Thus at low temperatures for which there is little thermal disorder, there is a preponderance of aligned spins and hence a net spontaneous magnetic moment (ie. the system is ferromagnetic). As the temperature is raised, thermal disorder increases until at a certain temperature \\(T_c\\), entropy drives the system through a continuous phase transition to a disordered spin arrangement with zero net magnetisation (ie. the system is paramagnetic). These trends are visible in configurational snapshots from computer simulations of the 2D Ising model (see Figure 4.1). Although each spin interacts only with its nearest neighbours, the phase transition occurs due to cooperative effects among a large number of spins.\nAn interactive Monte Carlo simulation of the Ising model demonstrates the phenomenology, By altering the temperature you will be able to observe for yourself how the typical spin arrangements change as one traverses the critical region. Pay particular attention to the configurations near the critical point. They have very interesting properties. We will return to them later!\nAlthough the 2-d Ising model may appear at first sight to be an excessively simplistic portrayal of a real magnetic system, critical point universality implies that many physical observables such as critical exponents are not materially influenced by the actual nature of the microscopic interactions. The Ising model therefore provides a simple, yet quantitatively accurate representation of the critical properties of a whole range of real magnetic (and indeed fluid) systems. This universal feature of the model is largely responsible for its ubiquity in the field of critical phenomena. We shall explore these ideas in more detail later in the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "href": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "(a) \\(T=1.2T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(T=T_c\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(T=0.95T_c\\)\n\n\n\n\n\n\n\nFigure 4.1: Configurations of the 2d Ising model. The patterns depict typical arrangements of the spins (white=+1, black=−1) generated in a computer simulation of the Ising model on a square lattice of \\(N=512\\) sites, at temperatures (from left to right) of \\(T= 1.2T_c\\), \\(T=T_c\\), and \\(T=0.95T_c\\). In each case only a portion of the system containing \\(128\\) sites in shown. The typical island size is a measure of the correlation length \\(\\xi\\): the excess of black over white (below \\(T_c\\) is a measure of the order parameter.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "href": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "4.2 Exact solutions: the one dimensional Ising chain",
    "text": "4.2 Exact solutions: the one dimensional Ising chain\nOne might well ask why the 2D Ising model is the simplest model to exhibit a phase transition. What about the one-dimensional Ising model (ie. spins on a line)? In fact in one dimension, the Ising model can be solved exactly. It turns out that the system is paramagnetic for all \\(T&gt;0\\), so there is no phase transition at any finite temperature. To see this, consider the ground state of the system in zero external field. This will have all spins aligned the same way (say up), and hence be ferromagnetic. Now consider a configuration with a various “domain walls” dividing spin up and spin down regions:\n\n\n\n\n\n\nFigure 4.2: (a) Schematic of an Ising chain at \\(T=0\\). (b) At a small finite temperature the chain is split into domains of spins ordered in the same direction. Domains are separated by notional domain “walls”, which cost energy \\(\\Delta=2J\\). Periodic boundary conditions are assumed.\n\n\n\nInstead of considering the underlying spin configurations, we shall describe the system in terms of the statistics of its domain walls. The energy cost of a wall is \\(\\Delta = 2J\\), independent of position. Domain walls can occupy the bonds of the lattice, of which there are \\(N-1\\). Moreover, the walls are noninteracting, except that you cannot have two of them on the same bond. (Check through these ideas if you are unsure.)\nIn this representation, the partition function involves a count over all possible domain wall arrangements. Since the domain walls are non interacting (eg it doesn’t cost energy for one to move along the chain) they can be treated as independent. Independent contributions to the partition function simply multiply. So we can calculate \\(Z\\) by considering the partition function associated with a single domain wall being present or absent on some given bond, and then simply raise to the power of the number of bonds:\n\\[Z=Z_1^{N-1}\\]\nwhere\n\\[Z_1=e^{\\beta J} + e^{\\beta (J-\\Delta)}=e^{\\beta J}(1+e^{-\\beta\\Delta})\\] is the domain wall partition function for a single bond and represent the sum over the two possible states: domain wall absent or present. Then the free energy per bond of the system is\n\\[\\beta f\\equiv \\beta F/(N-1)=-\\ln Z_1=-\\beta J-\\ln(1+e^{-\\beta\\Delta})\\]\nThe first term on the RHS is simply the energy per spin of the ferromagnetic (ordered) phase, while the second term arises from the free energy of domain walls. Clearly for any finite temperature (ie. for \\(\\beta&lt;\\infty\\)), this second term is finite and negative. Hence the free energy will always be lowered by having a finite concentration of domain walls in the system. Since these domain walls disorder the system, leading to a zero average magnetisation, the 1D system is paramagnetic for all finite temperatures. Exercise: Explain why this argument works only in 1D.\nThe animation below lets you see qualitatively how the typical number of domain walls varies with temperature. If you’d lke to explore more quantitatively, a python code performing a Monte Carlo simulation is available. You will learn about Monte Carlo simulation in the coursework and in later parts of the course.\n\n\nShow python code\n#Monte Carlo simulation of the 1d Ising chain with periodic bounary conditions\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Slider\n\n# Number of spins\nN = 20 \n\n# Initialize spins (+1 or -1)\nspins = np.random.choice([-1, 1], size=N)\n\n# Initial temperature\nT = 2.0\n\n# Set up figure and axis for the spins\nfig, ax = plt.subplots(figsize=(10, 2))\nplt.subplots_adjust(bottom=0.25)  # make room for slider\nax.set_xlim(-0.5, N - 0.5)\nax.set_ylim(-1, 1)\nax.axis('off')\n\n# Create text objects for each spin\ntexts = []\nfor i in range(N):\n    arrow = '↑' if spins[i] == 1 else '↓'\n    t = ax.text(i, 0, arrow, fontsize=24, ha='center', va='center')\n    texts.append(t)\n\ndef update(frame):\n    \"\"\"Perform Metropolis updates over all spins, then refresh display.\"\"\"\n    global spins, T\n    for _ in range(N):\n        i = np.random.randint(N)\n        left = spins[(i - 1) % N]\n        right = spins[(i + 1) % N]\n        deltaE = 2 * spins[i] * (left + right)\n        # Metropolis criterion ensures configurations appear with the correct Boltzmann probability\n        if deltaE &lt; 0 or np.random.rand() &lt; np.exp(-deltaE / T):\n            spins[i] *= -1\n    # Update arrows on screen\n    for idx, t in enumerate(texts):\n        t.set_text('↑' if spins[idx] == 1 else '↓')\n    return texts\n\n# Create the animation with caching disabled and blit turned off\nani = FuncAnimation(\n    fig,\n    update,\n    interval=200,\n    blit=False,\n    cache_frame_data=False\n)\n\n# Add a temperature slider\nax_T = plt.axes([0.2, 0.1, 0.6, 0.03], facecolor='lightgray')\nslider_T = Slider(ax_T, 'Temperature T', 0.1, 5.0, valinit=T)\n\ndef on_T_change(val):\n    \"\"\"Callback to update T when the slider changes.\"\"\"\n    global T\n    T = val\n\nslider_T.on_changed(on_T_change)\n\n# Show the plot (ani is kept in scope so it won't be deleted)\nplt.show()\n\n\n\n\n\n Temperature T =\n\n2.0\n\n \n\n\n4.2.1 More general 1D spins systems: transfer matrix method\nGenerally speaking one-dimensional systems lend themselves to a degree of analytic tractability not found in most higher dimensional models. Indeed for the case of a 1-d assembly of \\(N\\) spins each having \\(m\\) discrete energy states, and in the presence of a magnetic field, it is possible to reduce the evaluation of the partition function to the calculation of the eigenvalues of a matrix–the so called transfer matrix.\nLet us start by assuming that the assembly has cyclic boundary conditions, then the total energy of configuration \\(\\{s\\}\\) is \\[\n\\begin{aligned}\n{\\cal H}(\\{s\\})=&-\\sum_{i=1}^N (J s_i s_{i+1}+Hs_i)\\\\\n\\:=&-\\sum_{i=1}^N (J s_i s_{i+1}+H(s_i+s_{i+1})/2)\\\\\n\\:=&\\sum_{i=1}^N E(s_i,s_{i+1})\n\\end{aligned}\n\\]\nwhere we have defined \\(E(s_i,s_{i+1})=-J s_i s_{i+1}-H(s_i+s_{i+1})/2\\).\nNow the partition function may be written\n\\[\\begin{aligned}\nZ_N =& \\sum_{\\{s\\}}\\exp\\left(-\\beta {\\cal H}(\\{s\\})\\right)\\nonumber \\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta[E(s_1,s_2)+E(s_2,s_3)+....E(s_N,s_1)]\\right) \\nonumber\\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta E(s_1,s_2)\\right)\\exp\\left(-\\beta E(s_2,s_3)\\right)....\\exp\\left(-\\beta E(s_N,s_1)\\right) \\nonumber\\\\\n=&\\sum_{i,j,...,l=1}^m V_{ij}V_{jk}...V_{li}\n\\end{aligned} \\tag{4.2}\\]\nwhere the \\(V_{ij}=\\exp(-\\beta E_{ij})\\) are elements of an \\(m \\times m\\) matrix \\(\\mathbf{V}\\), known as the transfer matrix (\\(i,j,k\\) etc are dummy indices that run over the matrix elements). You should see that the sum over the product of matrix elements picks up all the terms in the partition function and therefore Equation 4.2 is an alternative way of writing the partition function.\nThe reason it is useful to transform to a matrix representation is that it transpires that the sum over the product of matrix elements in Equation 4.2 is simply just the trace of \\(\\mathbf{V}^N\\) (check this yourself for a short periodic chain), given by the sum of its eigenvalues:-\n\n\n\n\n\n\nCautionProof (non examinable)\n\n\n\n\n\nLet \\(V\\) be an \\(n \\times n\\) matrix, and let \\(\\lambda_1, \\dots, \\lambda_n\\) denote its eigenvalues.\nEvery square matrix \\(V\\) can be written as \\[\nV = Q T Q^\\dagger,\n\\] where \\(Q\\) is a unitary matrix (\\(Q^\\dagger Q = I\\)), and \\(T\\) is upper triangular with the eigenvalues of \\(V\\) on its diagonal: \\[\nT = \\begin{pmatrix}\n\\lambda_1 & * & \\cdots & * \\\\\n0 & \\lambda_2 & \\cdots & * \\\\\n\\vdots & & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & \\lambda_n\n\\end{pmatrix}.\n\\]\nRaising both sides to the \\(N\\)th power gives \\[\nV^N = (Q T Q^\\dagger)^N = Q \\, T^N \\, Q^\\dagger.\n\\] The trace (ie. the sum of diagonal elements) is invariant under similarity transformations: \\[\n\\mathrm{Tr}(V^N) = \\mathrm{Tr}(T^N).\n\\]\nSince \\(T\\) is upper triangular, so is \\(T^N\\). The diagonal elements of \\(T^N\\) are simply the \\(N\\)th powers of the diagonal elements of \\(T\\), i.e. \\[\n(T^N)_{ii} = (T_{ii})^N = \\lambda_i^N.\n\\]\nTherefore, \\[\n\\mathrm{Tr}(T^N) = \\sum_{i=1}^n (T^N)_{ii} = \\sum_{i=1}^n \\lambda_i^N.\n\\]\n\n\n\n\\[Z_N=\\lambda_1^N+\\lambda_2^N+...\\lambda_m^N\\] For very large \\(N\\), this expression simplifies further because the largest eigenvalue \\(\\lambda_1\\) dominates the behaviour since \\((\\lambda_2/\\lambda_1)^N\\) vanishes as \\(N\\rightarrow \\infty\\). Consequently in the thermodynamic limit one may put \\(Z_N=\\lambda_1^N\\) and the problem reduces to identifying the largest eigenvalue of the transfer matrix.\nSpecializing to the case of the simple Ising model in the presence of an applied field \\(H\\), the transfer matrix takes the form\n\\[\\mathbf{V}(H)=\\left(\n\\begin{array}{cc}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{array} \\right)\\]\nThis matrix has two eigenvalues which can be readily calculated in the usual fashion as the roots of the characteristic polynomial \\(|\\mathbf{V}-\\lambda\\mathbf{I}|\\). They are\n\\[\\lambda_{\\pm}=e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\\]\nHence the free energy per spin \\(f=-k_BT\\ln \\lambda_+\\) is\n\\[f=-k_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\\]\nThe Ising model in 2D can also be solved exactly, as was done by Lars Onsager in 1940. The solution is extremely complicated and is regarded as one of the pinnacles of statistical mechanics. In 3D no exact solution is known.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html",
    "href": "phase-transitions/mean-field-theory.html",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "",
    "text": "5.1 Mean field solution of the Ising model\nOf the wide variety of models of interest to the critical point theorist, the majority have shown themselves intractable to direct analytic (pen and paper) assault. In a very limited number of instances models have been solved exactly, yielding the phase coexistence parameters, critical exponents and the critical temperature. The 2-d spin-\\(\\frac{1}{2}\\) Ising model is certainly the most celebrated such example, its principal critical exponents are found to be \\(\\beta=\\frac{1}{8}, \\nu=1, \\gamma=\\frac{7}{4}\\). Its critical temperature is \\(-2J/\\ln(\\sqrt{2}-1)\\approx 2.269J\\). Unfortunately such solutions rarely afford deep insight to the general framework of criticality although they do act as an invaluable test-bed for new and existing theories.\nThe inability to solve many models exactly often means that one must resort to approximations. One such approximation scheme is mean field theory.\nLet us look for a mean field expression for the free energy of the Ising model whose Hamiltonian is given in Equation 4.1 . Write\n\\[s_i=\\langle s_i\\rangle+(s_i-\\langle s_i\\rangle)=m+(s_i-m)=m+\\delta s_i\\]\nThen \\[\\begin{aligned}\n{\\cal H}_I=&-J\\sum_{&lt;i,j&gt;}[m+(s_i-m)][m+(s_j-m)]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{&lt;i,j&gt;}[m^2+m(s_i-m)+m(s_j-m)+\\delta s_i\\delta s_j]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{i}(qms_i-qm^2/2)-H\\sum_i s_i-J\\sum_{&lt;i,j&gt;}\\delta s_i\\delta s_j\n\\end{aligned} \\tag{5.1}\\] where in the last line we trnsformed from a sum over bonds to a sum over sites. Doing so makes use of the fact that when for each site \\(i\\) we perform the sum \\(\\sum_{&lt;i,j&gt;}\\) over bonds of a quantity which is independent of \\(s_j\\), then the result is just the number of bonds per site times that quantity. Since the number of bonds on a lattice of \\(N\\) sites of coordination \\(q\\) is \\(Nq/2\\) (because each bond is shared between two sites), there are therefore \\(q/2\\) bonds per site.\nNow the mean field approximation is to ignore the last term in the last line of Equation 5.1 giving the configurational energy as\n\\[\n{\\cal H}_{mf}=-\\sum_{i}H_{mf}s_i+NqJm^2/2\n\\] with \\(H_{mf}\\equiv qJm+ H\\) the “mean field” seen by spin \\(s_i\\). As all the spins are decoupled (independent) in this approximation we can write down the partition function, which follows by taking the partition function for a single spin (by summing the Boltzmann factor for \\(s_i=\\pm 1\\)) and raising to the power \\(N\\) to find\n\\[\nZ=e^{-\\beta qJm^2N/2}[2\\cosh(\\beta(qJm+H))]^N\n\\]\nThe free energy follows as\n\\[F(m)=NJqm^2/2-Nk_BT\\ln[2\\cosh(\\beta (qJm+H)]\\:.\\]\nand the magnetisation as\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\tanh(\\beta(qJm+H)),\n\\] where the first term drops out because we treat \\(m\\) as an independent variable when differentiating w.r.t. \\(H\\).\nThis is a self consistent equation because \\(m\\) appears on both the left and the right hand sides. To find \\(m(H,T)\\), we must numerically solve this last equation-self consistently. You will meet such an equation again later when you learn about mean field theories for liquid crystals.\nNote that we can obtain \\(m\\) in a different way. Consider some arbitary spin, \\(s_i\\) say. Then this spin has an energy \\({\\cal H}_{mf}(s_i)\\). Considering this energy for both cases \\(s_i=\\pm 1\\) and the probability \\(p(s_i)=e^{-\\beta{\\cal H}_{mf}(s_i)}/Z\\) of each, we have that\n\\[\\langle s_i\\rangle=\\sum_{s_i=\\pm 1}s_ip(s_i)\\] but for consistancy, \\(\\langle s_i\\rangle=m\\). Thus\n\\[\n\\begin{aligned}\nm & = \\sum_{s_i=\\pm 1}s_ip(s_i)\\nonumber\\\\\n\\: & = \\frac{e^{\\beta(qJm+H)}-e^{-\\beta(qJm+H)}} {e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\\nonumber\\\\\n\\: & = \\tanh(\\beta(qJm+H))\n\\end{aligned} \\tag{5.2}\\] as before.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-mfising",
    "href": "phase-transitions/mean-field-theory.html#sec-mfising",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "",
    "text": "CautionWhy self-consistent?\n\n\n\n\n\nIn mean-field theory, the many-body interaction is replaced by an effective one-body problem in which each degree of freedom experiences an average field generated by all the others. The quantity that characterises the ordered phase—the order parameter—is precisely this average. Because the effective (mean-field) Hamiltonian is constructed using a presumed value of that average, internal consistency requires that the order parameter obtained by solving the effective problem match the value assumed to define it. Enforcing this equality yields a self-consistency condition for the order parameter. In practice: choose the effective field determined by the putative order parameter, compute the corresponding thermal average, and require that the two coincide.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-breaking",
    "href": "phase-transitions/mean-field-theory.html#sec-breaking",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.2 Spontaneous symmetry breaking",
    "text": "5.2 Spontaneous symmetry breaking\n\n\n\n\n\n\nFigure 5.1: Schematic of the form of the free energy for a critical, subcritical and supercritical temperature\n\n\n\nThis mean field analysis reveals what is happening in the Ising model near the critical temperature \\(T_c\\). Figure 5.1 shows sketches for \\(\\beta F(m)/N\\) as a function of temperature, where for f simplicity we restrict attention to \\(H=0\\). In this case \\(F(m)\\) is symmetric in \\(m\\), Moreover, at high \\(T\\), the entropy dominates and there is a single minimum in \\(F(m)\\) at \\(m=0\\). As \\(T\\) is lowered, there comes a point (\\(T=T_c=qJ/k_B\\)) where the curvature of \\(F(m)\\) at the origin changes sign; precisely at this point\n\\[\\frac{\\partial^2 F}{\\partial m^2}=0.\\] At lower temperature, there are instead two minima at nonzero \\(m=\\pm m^\\star\\), where the equilibrium magnetisation \\(m^\\star\\) is the positive root (calculated explicitly below) of\n\\[m^\\star=\\tanh(\\beta Jqm^\\star)= \\tanh(\\frac{m^\\star T_c}{T})\\] The point \\(m=0\\) which remains a root of this equation, is clearly an unstable point for \\(T&lt;T_c\\) (since \\(F\\) has a maximum there).\nThis is an example of spontaneous symmetry breaking. In the absence of an external field, the Hamiltonian (and therefore the free energy) is symmetric under \\(m\\to -m\\). Accordingly, one might expect the actual state of the system to also show this symmetry. This is true at high temperature, but spontaneously breaks down at low ones. Instead there are a pair of ferromagnetic states (spins mostly up, or spins mostly down) which – by symmetry– have the same free energy, lower than the unmagnetized state.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#phase-diagram",
    "href": "phase-transitions/mean-field-theory.html#phase-diagram",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.3 Phase diagram",
    "text": "5.3 Phase diagram\nThe resulting zero-field magnetisation curve \\(m(T,H=0)\\) looks like Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: Phase diagram of a simple magnet in the \\(m\\)-\\(T\\) plane.\n\n\n\nThis shows the sudden change of behaviour at \\(T_c\\) (phase transition). For \\(T&lt;T_c\\) it is arbitrary which of the two roots \\(\\pm m^\\star\\) is chosen; typically it will be different in different parts of the sample (giving macroscopic “magnetic domains”). But this behaviour with temperature is qualitatively modified by the presence of a field \\(H\\), however small. In that case, there is always a slight magnetization, even far above \\(T_c\\) and the curves becomes smoothed out, as shown. There is no doubt which root will be chosen, and no sudden change of the behaviour (no phase transition). Spontaneous symmetry breaking does not occur, because the symmetry is already broken by \\(H\\). (The curve \\(F(m)\\) is lopsided, rather than symmetrical about \\(m=0\\).)\nOn the other hand, if we sit below \\(T_c\\) in a positive field (say) and gradually reduce \\(H\\) through zero so that it becomes negative, there is a very sudden change of behaviour at \\(h=0\\): the equilibrium state jumps discontinuously from \\(m=m^\\star\\) to \\(m=-m^\\star\\).\n\n\n\n\n\n\nFigure 5.3: Phase diagram of a simple magnet in the \\(H\\)-\\(T\\) plane.\n\n\n\nThis is called a first order phase transition as opposed to the “second order” or continuous transition that occurs at \\(T_c\\) in zero field. The definitions are:\nFirst order transition: magnetisation (or similar order parameter) depends discontinuously on a field variable (such as \\(h\\) or \\(T\\)).\nContinuous transition (criticality): Change of functional form, but no discontinuity in \\(m\\); typically, however, \\((\\partial m/\\partial T)_h\\) (or similar) is either discontinuous, or diverges with an integrable singularity.\nIn this terminology, we can say that the phase diagram of the magnet in the \\(H,T\\) plane shows a line of first order phase transitions, terminating at a continuous transition, which is the critical point.\n\n\n\n\n\n\nCautionAside on Quantum Criticality\n\n\n\n\n\nIn some magnetic systems such as \\(CePd_2Si_2\\), one can, by applying pressure or altering the chemical composition, depress the critical temperature all the way to abolute zero! This may seem counterintuitive, after all at \\(T=0\\) one should expect perfect ordering, not the large fluctuations that accompany criticality. It turns out that the source of the fluctuations that drive the system critical is zero point motion associated with the Heisenberg uncertainty principle. Quantum criticality is a matter of ongoing active research, and open questions concern the nature of the phase diagrams and the relationship to superconductivity. Although the subject goes beyond the scope of this course, there is an accessible article here if you want to learn more.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "href": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.4 A closer look: critical exponents",
    "text": "5.4 A closer look: critical exponents\nLet us now see how we can calculate critical exponents within the mean field approximation.\n\n5.4.1 Zero H solution and the order parameter exponent\nIn zero field\n\\[m=\\tanh(\\frac{mT_c}{T})\\] where \\(T_c=qJ/k_B\\) is the critical temperature at which \\(m\\) first goes to zero.\nWe look for a solution where \\(m\\) is small (\\(\\ll 1\\)). Expanding the tanh function and replacing \\(\\beta=(k_BT)^{-1}\\) yields\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +O(m^5)\\:.\\] Then \\(m=0\\) is one solution. The other solution is given by\n\\[m^2=3\\left(\\frac{T}{T_c} \\right)^3\\left(\\frac{T_c}{T} -1\\right)\\]\nNow, considering temperatures close to \\(T_c\\) to guarantee small \\(m\\), and employing the reduced temperature \\(t=(T-T_c)/T_c\\), one finds\n\\[m^2\\simeq -3t\\]\nHence\n\\[\\begin{aligned}\nm= 0  &    ~~~\\textrm{for } T&gt;T_c \\:\\:\\:  \\textrm{since otherwise $m$ imaginary}\\\\\nm= \\pm\\sqrt{-3t} & ~~\\textrm{ for}  \\:\\:\\: T&lt;T_c ~~\\textrm{ real}\n\\end{aligned}  \\tag{5.3}\\] This result implies that (within the mean field approximation) the critical exponent \\(\\beta=1/2\\).\n\n\n5.4.2 Finite (but small) field solution: the susceptibility exponent\nIn a finite, but small field we can expand Equation 5.2 thus:\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +\\frac{H}{kT}\\]\nConsider now the isothermal susceptibility\n\\[\n\\begin{aligned}\n\\chi  \\equiv & \\left(\\frac{\\partial m}{\\partial H}\\right)_T\\\\\n      =     & \\frac{T_c}{T}\\chi - \\left(\\frac{T_c}{T}\\right)^3 \\chi m^2 + \\frac{1}{k_BT}  \n\\end{aligned}\n\\]\nThen\n\\[\\chi \\left[ 1-\\frac{T_c}{T} +\\left(\\frac{T_c}{T}\\right)^3m^2  \\right]=\\frac{1}{k_BT}\\]\nHence near \\(T_c\\)\n\\[\\chi=\\frac{1}{k_BT_c}\\left(\\frac{1}{t+m^2}\\right)\\]\nThen using the results of Equation 5.3\n\\[\n\\begin{aligned}\n\\chi= (k_BT_ct)^{-1} & \\textrm{ for} ~~~ T&gt; T_c \\\\\n\\chi= (-2k_BT_ct)^{-1} & \\textrm{ for}  ~~~T \\le T_c\n\\end{aligned}\n\\]\nwhere one has to take the non-zero value for \\(m\\) below \\(T_c\\) to ensure +ve \\(\\chi\\), i.e. thermodynamic stability. This result implies that (within the mean field approximation) the critical exponent \\(\\gamma=1\\).\nThe schematic behaviour of the Ising order parameter and susceptibility are shown in Figure 5.5 (a) and (b) respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mean field behaviour of the Ising magnetisation (schematic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Mean field behaviour of the Ising susceptibility (schematic)\n\n\n\n\n\n\n\nFigure 5.4",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "href": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.5 Landau theory",
    "text": "5.5 Landau theory\nLandau theory is a slightly more general type of mean field theory than that discussed in the previous subsection because it is not based on a particular microscopic model. Its starting point is the Helmholtz free energy, which Landau asserted can be written in terms of power series expansion of the order parameter \\(\\phi\\):\n\\[\nF_(\\phi)=\\sum_{i=0}^{\\infty}a_i\\phi^i\n\\] The equilibrium value of \\(\\rho\\) is that which minimises the Landau free energy.\n\n\n\n\n\n\nCautionA note on order parameters\n\n\n\n\n\nWe have already seen examples of these in earlier sections, e.g., for the liquid-gas transition this was \\[\n\\rho_{liq} - \\rho_{gas}: \\quad \\textrm{difference in density of two coexisting phases},\n\\] while for the Ising magnet it is the magnetisation \\(m\\). Both quantities vanish at the critical point. These are examples of scalar order parameters – a single number is required to represent the degree of order (\\(n = 1\\)).\nIn the absence of a symmetry-breaking field, the Landau free-energy density \\(f_L\\) must have symmetry \\(f_L(-\\phi) = f_L(\\phi)\\) (Ising case).\nFor some other systems, \\(n\\) component vectors are required in order to represent the order:\n\\[\n\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_n)\n\\]\nThen \\(f_L(\\boldsymbol{\\phi})\\) should be symmetric under \\(O(n)\\) rotations in \\(n\\)-component \\(\\phi\\)-space.\nThe table below lists examples of order parameters for various physical systems.\n\n\n\n\n\n\n\n\nPhysical System\nOrder Parameter \\(\\varphi\\)\nSymmetry Group\n\n\n\n\nUniaxial (Ising) ferromagnet\nMagnetisation per spin, \\(m\\)\n\\(O(1)\\)\n\n\nFluid (liquid-gas)\nDensity difference, \\(\\rho - \\rho_c\\)\n\\(O(1)\\)\n\n\nLiquid mixtures\nConcentration difference, \\(c - c_c\\)\n\\(O(1)\\)\n\n\nBinary (AB) alloy (e.g., \\(\\beta\\)-brass)\nConcentration of one of the species, \\(c\\)\n\\(O(1)\\)\n\n\nIsotropic (vector) ferromagnet\n\\(n\\)-component magnetisation, \\(\\mathbf{m} = (m_1, m_2, \\dots, m_n)\\)\n\\(O(n)\\)\n\n\n\n\\(n = 2\\): xy model\n\\(O(2)\\)\n\n\n\n\\(n = 3\\): Heisenberg model\n\\(O(3)\\)\n\n\nSuperfluid He\\(^4\\)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nSuperconductor (s-wave)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nNematic liquid crystal\nOrientational order, \\(\\langle P_2(\\cos \\theta)\\rangle\\)\n\n\n\nSmectic A liquid crystal\n1-dimensional periodic density\n\n\n\nCrystal\n3-dimensional periodic density\n\n\n\n\nNotes:\n\nIn superfluid \\(^4He\\) the order parameter is\n\n\\[\n\\Psi = |\\Psi| e^{i\\theta},\n\\]\nthe complex wavefunction of the macroscopic condensate. Both the amplitude \\(|\\Psi|\\) and phase \\(\\theta\\) must be specified, so this corresponds to \\(n = 2\\).\nSuperconductors also correspond to \\(n = 2\\).\n\nIn a nematic liquid crystal, the orientational order parameter is\n\n\\[\n\\langle P_2(\\cos \\theta) \\rangle \\equiv \\frac{1}{2}\\langle 3\\cos^2 \\theta - 1\\rangle,\n\\]\nwhere \\(\\theta\\) is the angle a molecule makes with the average direction of the long axes of the molecules (known as the director \\(\\hat{n}\\)). Rotational symmetry is broken. For the case of an \\(n\\) component vector, the free energy should be a function of:\n\\[\n\\phi^2 \\equiv |\\boldsymbol{\\phi}|^2 = \\phi_1^2 + \\phi_2^2 + \\dots + \\phi_n^2 = \\sum_{i=1}^n \\phi_i^2\n\\] in the absence of a symmetry breaking field. Rotational symmetry is incorporated into the theory.\n\n\n\n\n\n\n\n\n\n\n\n(a) Schematic of the isotropic liquid phase of a system of elongated molcules.\n\n\n\n\n\n\n\n\n\n\n\n(b) Schematic of the nematic liquid phase of a system elongated molcules. This phase has uniaxial ordering.\n\n\n\n\n\n\n\nFigure 5.5: Isotropic and uniaxially ordered (nematic) phases of liquid crystal molecules.\n\n\n\n\n\n\nTo exemplify the approach, let us specialise to the case of a ferromagnet where \\(\\phi=m\\), the magnetisation and write the Landau free energy as\n\\[\nF(m)=F_0+a_2m^2+a_4m^4\n\\tag{5.4}\\]\nHere only the terms compatible with the order parameter symmetry are included in the expansion and we truncate the series at the 4th power because this is all that is necessary to yield the essential phenomenology. On symmetry grounds, the free energy of a ferromagnet should be invariant under a reversal of the sign of the magnetisation. Terms linear and cubic in \\(m\\) are not invariant under \\(m\\to -m\\), and so do not feature.\nOne can understand how the Landau free energy can give rise to a critical point and coexistence values of the magnetisation, by plotting \\(F(m)\\) for various values of \\(a_2\\) with \\(a_4\\) assumed positive (which ensures that the magnetisation remains bounded). This is shown in the following movie:\n\n\nThe situation is qualitatively similar to that discussed in Section 5.2. Thermodynamics tells us that the system adopts the state of lowest free energy. From the movie, we see that for \\(a_2&gt;0\\), the system will have \\(m=0\\), i.e. will be in the disordered (or paramagnetic) phase. For \\(a_2&lt;0\\), the minimum in the free energy occurs at a finite value of \\(m\\), indicating that the ordered (ferromagnetic) phase is the stable one. In fact, the physical (up-down) spin symmetry built into \\(F\\) indicates that there are two equivalent stable states at \\(m=\\pm m^\\star\\). \\(a_2=0\\) corresponds to the critical point which marks the border between the ordered and disordered phases. Note that it is an inflexion point, so has \\(\\frac{d^2F}{dm^2}=0\\).\nClearly \\(a_2\\) controls the deviation from the critical temperature, and accordingly we may write\n\\[a_2=\\tilde{a_2} t\\] where \\(t\\) is the reduced temperature. Thus we see that the trajectory of the minima as a function of \\(a_2&lt;0\\) in the above movie effective traces out the coexistence curve in the \\(m-T\\) plane.\nWe can now attempt to calculate critical exponents. Restricting ourselves first to the magnetisation exponent \\(\\beta\\) defined by \\(m=t^\\beta\\), we first find the equilibrium magnetisation, corresponding to the minimum of the Landau free energy:\n\\[\n\\frac{dF}{dm}=2\\tilde{a_2} tm+4a_4m^3=0\n\\tag{5.5}\\]\nwhich implies\n\\[m\\propto (-t)^{1/2},\\] so \\(\\beta=1/2\\), which is again a mean field result.\nLikewise we can calculate the effect of a small field \\(H\\) if we sit at the critical temperature \\(T_c\\). Since \\(a_2=0\\), we have\n\\[F(m)=F_0+a_4m^4-Hm\\]\n\\[\\frac{\\partial F}{\\partial m}=0 \\Rightarrow m(H,T_c)=\\left(\\frac{H}{4a_4}\\right)^{1/3}\\]\nor\n\\[H \\sim m^\\delta ~~~~~ \\delta=3\\] which defines a second critical exponent.\nNote that at the critical point, a small applied field causes a very big increase in magnetisation; formally, \\((\\partial m/\\partial H)_T\\) is infinite at \\(T=T_c\\).\nA third critical exponent can be defined from the magnetic susceptibility at zero field\n\\[\\chi=\\left(\\frac{\\partial m}{\\partial H}\\right)_{T,V} \\sim |T-T_c|^{-\\gamma}\\]\nExercise: Show that the Landau expansion predicts \\(\\gamma=1\\).\nFinally we define a fourth critical exponent via the variation of the heat capacity (per site or per unit volume) \\(C_H\\), in fixed external field \\(H=0\\):\n\\[C_H \\sim |T-T_c|^{-\\alpha}\\]\nBy convention, \\(\\alpha\\) is defined to be positive for systems where there is a divergence of the heat capacity at the critical point (very often the case). The heat capacity can be calculated from\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\]\nFrom the minimization over \\(m\\) Equation 5.5 one finds (exercise: check this) \\[\n\\begin{aligned}\nF = & 0 ~~~~T&gt;T_c\\nonumber\\\\\nF = & -a_2^2/4a_4 ~~~~ T &lt; T_c\n\\end{aligned}\n\\]\nUsing the fact that \\(a_2\\) varies linearly with \\(T\\), we have\n\\[\n\\begin{aligned}\nC_H =& 0 ~~~~ T\\to T_c^+\\nonumber\\\\\nC_H =& \\frac{T\\tilde a_2^2}{2a_4} ~~~~ T \\to T_c^-\\:,\n\\end{aligned}\n\\]\nwhich is actually a step discontinuity in specific heat. Since for positive \\(\\alpha\\) the heat capacity is divergent, and for negative \\(\\alpha\\) it is continuous, this behaviour formally corresponds to \\(\\alpha=0\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "href": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.6 Shortcomings of mean field theory",
    "text": "5.6 Shortcomings of mean field theory\nWhile mean field theories provide a useful route to understanding qualitatively the phenomenology of phase transitions, in real ferromagnets, as well as in more sophisticated theories, the critical exponents are not the simple fraction and integers found here. This failure of mean field theory to predict the correct exponents is of course traceable to their neglect of correlations. In later sections we shall start to take the first steps to including the effects of long range correlations.\n\n\n\nComparison of true Ising critical exponents with their mean field theory predictions in a number of dimensions.\n\n\n\\(\\:\\)\nMean Field\n\\(d=1\\)\n\\(d=2\\)\n\\(d=3\\)\n\n\nCritical temperature \\(k_BT/qJ\\)\n\\(1\\)\n\\(0\\)\n\\(0.5673\\)\n\\(0.754\\)\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\frac{1}{2}\\)\n-\n\\(\\frac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(1\\)\n\\(\\infty\\)\n\\(\\frac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(\\frac{1}{2}\\)\n\\(\\infty\\)\n\\(1\\)\n\\(0.63\\pm 0.001\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html",
    "href": "phase-transitions/scaling.html",
    "title": "6  The Static Scaling Hypothesis",
    "section": "",
    "text": "6.1 Experimental Verification of Scaling\nHistorically, the first step towards properly elucidating near-critical behaviour was taken with the static scaling hypothesis. This is essentially a plausible conjecture concerning the origin of power law behaviour which appears to be consistent with observed phenomena. According to the hypothesis, the basis for power law behaviour (and associated scale invariance or “scaling”) in near-critical systems is expressed in the claim that: in the neighbourhood of a critical point, the basic thermodynamic functions (most notably the free energy) are generalized homogeneous functions of their variables. For such functions one can always deduce a scaling law such that by an appropriate change of scale, the dependence on two variables (e.g. the temperature and applied field) can be reduced to dependence on one new variable. This claim may be warranted by the following general argument.\nA function of two variables \\(g(u,v)\\) is called a generalized homogeneous function if it has the property\n\\[g(\\lambda^au,\\lambda^bv)=\\lambda g(u,v)\\] for all \\(\\lambda\\), where the parameters \\(a\\) and \\(b\\) (known as scaling parameters) are constants. An example of such a function is \\(g(u,v)=u^3+v^2\\) with \\(a=1/3, b=1/2\\).\nNow, the arbitrary scale factor \\(\\lambda\\) can be redefined without loss of generality as \\(\\lambda^a=u^{-1}\\) giving\n\\[g(u,v)=u^{1/a}g(1,\\frac{v}{u^{b/a}})\\] A corresponding relation is obtained by choosing the rescaling to be \\(\\lambda^b=v^{-1}\\).\n\\[\\label{eq:sca2}\ng(u,v)=v^{1/b}g(\\frac{u}{v^{a/b}},1)\\]\nThis equation demonstrates that \\(g(u,v)\\) indeed satisfies a simple power law in \\(\\mathit{one}\\) variable, subject to the constraint that \\(u/v^{a/b}\\) is a constant. It should be stressed, however, that such a scaling relation specifies neither the function \\(g\\) nor the parameters \\(a\\) and \\(b\\).\nNow, the static scaling hypothesis asserts that in the critical region, the free energy \\(F\\) is a generalized homogeneous function of the (reduced) thermodynamic fields \\(t=(T-T_c)/T_c\\) and \\(h=(H-H_c)\\). Remaining with the example ferromagnet, the following scaling assumption can then be made:\n\\[F(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h) \\:.\n\\label{eq:scagibbs}\\]\nWithout loss of generality, we can set \\(\\lambda^a=t^{-1}\\), implying \\(\\lambda=t^{-1/a}\\) and \\(\\lambda^b=t^{-b/a}\\).\nThen \\[F(t,h)=t^{1/a}F(1,t^{-b/a}h)\\] where our choice of \\(\\lambda\\) ensures that \\(F\\) on the rhs is now a function of a single variable \\(t^{-b/a}h\\).\nNow, as stated in Chapter 2, the free energy provides the route to all thermodynamic functions of interest. An expression for the magnetisation can be obtained simply by taking the field derivative of \\(F\\) (cf. Figure 2.1)\n\\[m(t,h)=-t^{(1-b)/a}m(1,t^{-b/a}h)\n\\tag{6.1}\\]\nIn zero applied field \\(h=0\\), this reduces to\n\\[m(t,0)=(-t)^{(1-b)/a}m(1,0)\\] where the r.h.s. is a power law in \\(t\\). Equation 3.4 then allows identification of the exponent \\(\\beta\\) in terms of the scaling parameters \\(a\\) and \\(b\\).\n\\[\\beta=\\frac{1-b}{a}\\]\nBy taking further appropriate derivatives of the free energy, other relations between scaling parameters and critical exponents may be deduced. Such calculations (Exercise: try to derive them) yield the results \\(\\delta =\nb/(1-b)\\),\\(\\gamma = (2b-1)/a\\), and \\(\\alpha =(2a-1)/a\\) . Relationships between the critical exponents themselves can be obtained trivially by eliminating the scaling parameters from these equations. The principal results (known as “scaling laws”) are:- \\[\n\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\n\\end{aligned}\n\\]\nThus, provided all critical exponents can be expressed in terms of the scaling parameters \\(a\\) and \\(b\\), then only two critical exponents need be specified, for all others to be deduced. Of course these scaling laws are also expected to hold for the appropriate thermodynamic functions of analogous systems such as the liquid-gas critical point.\nThe validity of the scaling hypothesis finds startling verification in experiment. To facilitate contact with experimental data for real systems, consider again Equation 6.1. Eliminating the scaling parameters \\(a\\) and \\(b\\) in favour of the exponents \\(\\beta\\) and \\(\\delta\\) gives\n\\[\n\\frac{m(t,h)}{t^{\\beta}}=m(1,\\frac{h}{t^{\\beta\\delta}})\n\\] where the RHS of this last equation can be regarded as a function of the single scaled variable \\(\\tilde{H} \\equiv t^{-\\beta\\delta} h(t,M)\\).\nFor some particular magnetic system, one can perform an experiment in which one measures \\(m\\) vs \\(h\\) for various fixed temperatures. This allows one to draw a set of isotherms, i.e. \\(m-h\\) curves of constant \\(t\\). These can be used to demonstrate scaling by plotting the data against the scaling variables \\(M=t^{-\\beta}m(t,h)\\) and \\(\\tilde{H}=t^{-\\beta\\delta}h(t,M)\\). Under this scale transformation, it is found that all isotherms (for \\(t\\) close to zero) coincide to within experimental error. Reassuringly, similar results are found using the scaled equation of state of simple fluid systems such as He\\(^3\\) or Xe.\nIn summary, the static scaling hypothesis is remarkably successful in providing a foundation for the observation of power laws and scaling phenomena. However, it furnishes little or no guidance regarding the role of co-operative phenomena at the critical point. In particular it provides no means for calculating the values of the critical exponents appropriate to given model systems.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "href": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "title": "6  The Static Scaling Hypothesis",
    "section": "",
    "text": "Figure 6.1: Magnetisation of CrBr\\(_3\\) in the critical region plotted in scaled form (see text). From Ho and Lister (1969).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#sec-compsim",
    "href": "phase-transitions/scaling.html#sec-compsim",
    "title": "6  The Static Scaling Hypothesis",
    "section": "6.2 Computer simulation",
    "text": "6.2 Computer simulation\nIn seeking to employ simulation to obtain estimates of bulk critical point properties (such as the location of a critical point and the values of its associated exponents), one is immediately confronted with a difficulty. The problem is that simulations are necessarily restricted to dealing with systems of finite-size and cannot therefore accommodate the truly long ranged fluctuations that characterize the near-critical regime. As a consequence, the critical singularities in \\(C_v\\), order parameter, etc. appear rounded and shifted in a simulation study. Figure 6.2 shows a schematic example for the susceptibility of a magnet.\n\n\n\n\n\n\nFigure 6.2: Schematic of the near-critical temperature dependence of the magnet susceptibility in a finite-sized system.\n\n\n\nThus the position of the peak in a response function (such as \\(C_v\\)) measured for a finite-sized system does not provide an accurate estimate of the critical temperature. Although the degree of rounding and shifting reduces with system size, it is often the case, that computational constraints prevent access to the largest system sizes which would provide accurate estimates of critical parameters. To help deal with this difficulty, finite-size scaling (FSS) methods have been developed to allow extraction of bulk critical properties from simulations of finite size. FSS will be discussed in section 7.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html",
    "href": "phase-transitions/rg.html",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "7.1 The critical point: A many length scale problem\nOwing the the absence of a wholly appropriate textbook for the material covered in this section, I have supplied more detailed notes than used in other parts of the unit.\nThe critical region is characterised by correlated microstructure on \\(\\underline{all}\\) length-scales up to and including the correlation length.\nSuch a profusion of degrees of freedom can only be accurately characterized by a very large number of variables. Mean field theories and approximation schemes fail in the critical region because they at best incorporate interactions among only a few spins, while neglecting correlations over larger distances. Similarly, the scaling hypothesis fails to provide more than a qualitative insight into the nature of criticality because it focuses on only one length-scale, namely the correlation length itself. Evidently a fuller understanding of the critical region may only be attained by taking account of the existence of structure on all length-scales. Such a scheme is provided by the renormalisation group method, which stands today as the cornerstone of the modern theory of critical phenomena.\nA near critical system can be characterized by three important length scales, namely\nThe authentic critical region is defined by a window condition:\n\\[L_\\textrm{ max} \\gg \\xi \\gg L_\\textrm{ min}\\]\nThe physics of this regime is hard to tackle by analytic theory because it is characterized by configurational structure on all scales between \\(L_\\textrm{ min}\\) and \\(\\xi\\) (in fact it turns out that the near critical configurational patterns are fractal-like, cf. Figure 4.1 (b). Moreover different length scales are correlated with one another, giving rise to a profusion of coupled variables in any theoretical description. The window regime is also not easily accessed by computer simulation because it entails studying very large system sizes \\(L_\\textrm{\nmax}\\), often requiring considerable computing resources.\nA nice illustration of critical point scale invariance in the Ising model can be viewed here.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "href": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "The correlation length, \\(\\xi\\), ie the size of correlated microstructure.\nMinimum length scale \\(L_\\textrm{ min}\\), i.e. the smallest length in the microscopics of the problem, e.g. lattice spacing of a magnet or the particle size in a fluid.\nMacroscopic size \\(L_{max}\\) eg. size of the system.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-rgmethod",
    "href": "phase-transitions/rg.html#sec-rgmethod",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.2 Methodology of the RG",
    "text": "7.2 Methodology of the RG\nThe central idea of the renormalisation group (RG) method is a stepwise elimination of the degrees of freedom of the system on successively larger length-scales. To achieve this one introduces a fourth length scale \\(L\\). In contrast to the other three, which characterize the system itself, \\(L\\) characterises the description of the system. It may be thought of as typifying the size of the smallest resolvable detail in a description of the system’s microstructure.\nConsider the Ising model arrangements displayed in Figure 4.1. These pictures contain all the details of each configuration shown: the resolution length \\(L\\) in this case has its smallest possible value, coinciding with the lattice spacing i.e. \\(L=L_{\\min}\\). In the present context, the most detailed description is not the most useful: the essential signals with which we are concerned are hidden in a noise of relevant detail. A clue to eliminating this noise lies in the nature of the correlation length, i.e. the size of the largest droplets. The explicit form of the small scale microstructure is irrelevant to the behaviour of \\(\\xi\\). The small scale microstructure is the noise. To eliminate it, we simply select a larger value of the resolution length (or ‘coarse-graining’ length) \\(L\\).\nThere are many ways of implementing this coarse-graining procedure. We adopt a simple strategy in which we divide our sample into blocks of side \\(L\\), each of which contains \\(L^d\\) sites, with \\(d\\) the space dimensions . The centres of the blocks define a lattice of points indexed by \\(I=1,2,\\ldots,N/L^d\\). We associate with each block lattice point centre, \\(I\\), a coarse-grained or block variable \\(S(L)\\) defined as the spatial average of the local variables it contains:\n\\[\nS(L)=L^{-d}\\sum_i^Is_i\n\\tag{7.1}\\] where the sum extends over the \\(L^d\\) sites in the block \\(I\\). The set of coarse grained coordinates \\(\\{S(L)\\}\\) are the basic ingredients of a picture of the system having spatial resolution of order \\(L\\).\nThe coarse graining operation is easily implemented on a computer. In so doing one is faced with the fact that while the underlying Ising spins can only take two possible values, the block variables \\(S(L)\\) have \\(L^d+1\\) possible values. Accordingly in displaying the consequences of the blocking procedure, we need a more elaborate colour convention than that used in Figure 4.1. We will associate with each block a shade of grey drawn from a spectrum ranging from black to white.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(aiv)\n\n\n\n\n\n\n\n(biv)\n\n\n\n\n\n\nFigure 7.1: See text for details\n\n\n\nThe results of coarse-graining configurations typical of three different temperatures are shown in Figure 7.1 and Figure 7.2. Two auxiliary operations are implicit in these results. The first operation is a length scaling: the lattice spacing on each blocked lattice has been scaled to the same size as that of the original lattice, making possible the display of correspondingly larger portions of the physical system. The second operation is a variable scaling: loosely speaking, we have adjusted the scale (‘contrast’) of the block variable so as to match the spectrum of block variable values to the spectrum of shades at our disposal.\nConsider first a system marginally above its critical point at a temperature \\(T\\) chosen so that the correlation length \\(\\xi\\) is approximately 6 lattice spacing units. A typical arrangement (without coarse-graining) is shown in Figure 7.1(ai). The succeeding panels, (aii) and (aiii), show the result of coarse-graining with block sizes \\(L=4\\) and \\(L=8\\), respectively. A clear trend is apparent. The coarse-graining amplifies the consequences of the small deviation of \\(T\\) from \\(T_c\\). As \\(L\\) is increased, the ratio of the size of the largest configurational features (\\(\\xi\\)) to the size of the smallest (\\(L\\)) is reduced. The ratio \\(\\xi/L\\) provides a natural measure of how ‘critical’ is a configuration. Thus the coarse-graining operation generates a representation of the system that is effectively less critical the larger the coarse-graining length. The limit point of this trend is the effectively fully disordered arrangement shown in Figure 7.1(aiii) and in an alternative form in Figure 7.1(aiv), which shows the limiting distribution of the coarse grained variables, averaged over many realizations of the underlying configurations: the distribution is a Gaussian which is narrow (even more so the larger the \\(L\\) value) and centred on zero. This limit is easily understood. When the system is viewed on a scaled \\(L\\) larger than \\(\\xi\\), the correlated microstructure is no longer explicitly apparent; each coarse-grained variable is essentially independent of the others.\nA similar trend is apparent below the critical point. Figure 7.1(bi) show a typical arrangement at a temperature \\(T&lt;T_c\\) such that again \\(\\xi\\) is approximately \\(6\\) lattice spacings. Coarse-graining with \\(L=4\\) and \\(L=8\\) again generates representations which are effectively less critical as shown in panels (bii) and (biii)). This time the coarse-graining smoothes out the microstructure which makes the order incomplete. The limit point of this procedure is a homogeneously ordered arrangement in which the block variables have a random (Gaussian) distribution centred on the order parameter (Figure 7.1(biv)).\nConsider now the situation at the critical point. Figure 7.2(ai) shows a typical arrangement; panels (aii) and (aiii) show the results of coarse-graining with \\(L=4\\) and \\(L=8\\) respectively. Since the \\(\\xi\\) is as large as the system itself the coarse graining does not produce less critical representations of the physical system: each of the figures displays structure over all length scales between the lower limit set by \\(L\\) and the upper limit set by the size of the display itself. A limiting trend is nevertheless apparent. Although the \\(L=4\\) pattern is qualitatively quite different from the pattern of the local variables, the \\(L=4\\) and \\(L=8\\) patterns display qualitatively similar features. These similarities are more profound than is immediately apparent. A statistical analysis of the spectrum of \\(L=4\\) configurations (generated as the local variables evolve in time) shows (Figure 7.2(iv)) that it is almost identical to that of the \\(L=8\\) configurations (given the block variable scaling). The implication of this limiting behaviour is clear: the patterns formed by the ordering variable at criticality look the same (in a statistical sense) when viewed on all sufficiently large length scales.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(iv)\n\n\n\n\n\n\nFigure 7.2: See text for details\n\n\n\nLet us summarize. Under the coarse-graining operation there is an evolution or flow of the system’s configuration spectrum. The flow tends to a limit, or fixed point, such that the pattern spectrum does not change under further coarse-graining. These scale-invariant limits have a trivial character for \\(T&gt;T_c\\), (a perfectly disordered arrangement) and \\(T&lt; T_c\\), (a perfectly ordered arrangement). The hallmark of the critical point is the existence of a scale-invariant limit which is neither fully ordered nor fully disordered but which possesses structure on all length scales.\nA nice illustration of these points made by my former postdoc Douglas Ashton can be viewed here. (Note this video uses a different coarse-graining scheme called a “majority rule”: blocks variables are assigned to be \\(+1\\) or \\(-1\\) depending on whether spin-up or spin-down is in the majority in the underlying block. Thus in contrast to our scheme, there is no gray scale.)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#universality-and-scaling",
    "href": "phase-transitions/rg.html#universality-and-scaling",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.3 Universality and Scaling",
    "text": "7.3 Universality and Scaling\nEquipped with the coarse-graining technique, we now address the universality phenomenon. We aim to understand how it is that systems that are different microscopically can nevertheless display critical point behaviour which (in certain respects) is quantitatively identical.\nTo obtain initial insight we introduce a spin-1 Ising model in which the spins take on three values (\\(s_i=1,0,-1\\)), in contrast to the two values (\\(s_i=1,-1\\)) of the spin-1/2 Ising model. The two models have properties which are different: for example, \\(T_c\\) for the three-state model is some \\(30\\%\\) lower than that of the two-state model (for the same coupling \\(J\\)). However, there is abundant evidence that the two models have the same universal properties.\nLet us explore what is the same and what is different in the configurations of the two models at criticality. The configurations of the local variables \\(s_i\\) are clearly qualitatively different for the two models. Now consider the coarse-grained configurations (with \\(L=4\\) and \\(L=8\\) respectively) for the three-state model at the critical point. We have already seen that the coarse-graining operation bears the configuration spectrum of the critical two-state Ising model to a non-trivial scale-invariant limit. It is scarcely surprising that the same is true for the three-state model. What is remarkable is that the two limits are the same! This is expressed in Figure 7.2(iv), which shows the near coincidence of the distribution of block variables (grey-levels) for the two different coarse-graining lengths. Thus the coarse-graining erases the physical differences apparent in configurations where the local behaviour is resolvable, and exposes a profound configurational similarity.\n\n7.3.1 Fluid-magnet universality\nLet us now turn to fluid-magnet universality. In a magnet, the relevant configurations are those formed by the coarse-grained magnetisation (the magnetic moment averaged over a block of side \\(L\\)). In a fluid, the relevant configurations are those of the coarse-grained density (the mass averaged over a block if side \\(L\\)) or more precisely, its fluctuation from its macroscopic average (Figure 7.3). The patterns in the latter (bubbles of liquid or vapour) may be matched to pattern in the former (microdomains of the magnetisation), given appropriate scaling operations to camouflage the differences between the length scales and the differences between the variable scales.\n\n\n\n\n\n\nFigure 7.3: Schematic representation of the coarse graining operation via which the universal properties of fluids and magnets may be exposed.\n\n\n\nThe results is illustrated in Figure 7.4.\n\n\n\n\n\n\n\n\n2D critical Ising model and 2d critical Lennard-Jones fluid at small lengthscales\n\n\n\n\n\n\n\n\n\nSame models as above, but viewed at large lengthscales\n\n\n\n\n\n\nFigure 7.4: Snapshot configurations of the 2D critical Ising model (left) and the 2D critical Lennard-Jones fluid (right). When viewed on sufficiently large length scales the configurational patterns appear universal and self similar.\n\n\n\nA movie in which we progressively zoom out shows how the loss of microscopic details reveals the large lengthscale universal features.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#near-critical-scaling",
    "href": "phase-transitions/rg.html#near-critical-scaling",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.4 Near critical scaling",
    "text": "7.4 Near critical scaling\nThe similarity of coarse-grained configurations of different systems is not restricted to the critical temperature itself. Suppose we have a two state spin model and a three state spin model each somewhat above their critical points at reduced temperature \\(t\\). The two systems will have somewhat different correlation lengths, \\(\\xi_1\\) and \\(\\xi_2\\) say. Suppose however, we choose coarse-graining lengths \\(L_1\\) for \\(L_2\\) for the two models such that \\(\\xi_1/L_1=\\xi_2/L_2\\). We adjust the scales of the block variables (our grey level control) so that the typical variable value is the same for the two systems. We adjust the length scale of the systems (stretch or shrink our snapshots) so that the sizes of the minimum-length-scale structure (set by \\(L_1\\) and \\(L_2\\)) looks the same for each system. Precisely what they look like depends upon our choice of \\(\\xi/L\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-unipics",
    "href": "phase-transitions/rg.html#sec-unipics",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.5 Universality classes",
    "text": "7.5 Universality classes\nCoarse graining does not erase all differences between the physical properties of critical systems. Differences in the space dimension \\(d\\) of two critical systems will lead to different universal properties such as critical exponents. Thus, for instance, the critical exponents of the 2D magnet, match those of the 2d fluid, but they are different to those of 3d magnets and fluids.\n\n\n\n\n\n\n\n\n\n\\(d=2\\)\n\\(d=3\\)\n\n\n\n\nCritical temperature\n0.5673\n0.75\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\tfrac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(\\tfrac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(1\\)\n\\(0.63 \\pm 0.001\\)\n\n\n\nIn fact the space dimension is one of a small set of qualitative features of a critical system which are sufficiently deep-seated to survive coarse graining and which together serve to define the system’s universal behaviour, or universality class. The constituents of this set are not all identifiable a priori. They include the number of components \\(n\\) of the order parameter. Up to now, we have only considered order parameters which are scalar (for a fluid the density, for a magnet the magnetisation), for which \\(n=1\\). In some ferromagnets, the order parameter may have components along two axes, or three axes, implying a vector order parameter, with \\(n=2\\) (the so called XY model) or \\(n=3\\) (Heisenberg model), respectively. It is clear that the order-parameter \\(n\\)-value will be reflected in the nature of the coarse-grained configurations, and thus in the universal observables they imply.\nA third important feature which can change the universality class of a critical system is the range of the interaction potential between its constituent particles. Clearly for the Ising model, interactions between spins are inherently nearest neighbour in character. Most fluids interact via dispersion forces (such as the Lennard-Jones potential) which is also short ranged owing to the \\(r^{-6}\\) attractive interaction. However some systems have much longer ranged interactions. Notable here are systems of charged particles which interact via a Coulomb potential. The long ranged nature of the Coulomb potential (which decays like \\(r^{-1}\\)) means that charged systems often do not have the same critical exponents as the Ising model and fluid.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#critical-exponents",
    "href": "phase-transitions/rg.html#critical-exponents",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.6 Critical exponents",
    "text": "7.6 Critical exponents\nWe consider now how the critical exponents, may be computed via the coarse-graining procedure. In what follows we will refer only to the behaviour of a single typical coarse grained variable, which we shall denote \\(S(L)\\). We suppose that \\(t\\) is sufficiently small that \\(\\xi \\gg\nL_\\textrm{ min}\\). Universality and scaling may be expressed in the claim that, for any \\(L\\) and \\(t\\), scale factors \\(a(L)\\) and \\(b(L)\\) may be found such that the probability distribution \\(p(S(L),t)\\) can be written in the form\n\\[\np(S(L),t)=b(L)\\tilde{p}(b(L)S(L),a(L)t)\n\\tag{7.2}\\] where \\(\\tilde{p}\\) is a function unique to a universality class. The role of the scale factors \\(a\\) and \\(b\\) is to absorb the basic non-universal scales identified in Section 7.2. The critical exponents are implicit in the \\(L\\)-dependence of these scale factors. Specifically one finds:\n\\[\n\\begin{aligned}\na(L) & =a_0L^{1/\\nu} \\\\\nb(L) & =b_0L^{\\beta/\\nu}\n\\end{aligned}\n\\tag{7.3}\\] where the amplitudes \\(a_0\\) and \\(b_0\\) are system specific (non-universal) constants.\nThese results state that the critical exponents (in the form \\(1/\\nu\\) and \\(\\beta/\\nu\\)) characterize the ways in which the configuration spectrum evolves under coarse-graining. Consider, first the exponent ratio \\(\\beta/\\nu\\). Precisely at the critical point, there is only one way in which the coarse-grained configurations change with \\(L\\): the overall scale of the coarse-grained variable (the black-white contrast in our grey scale representation) is eroded with increasing \\(L\\). Thus the configurations of coarse-graining length \\(L_1\\) match those of a larger coarse-graining length \\(L_2\\) only if the variable scale in the latter configurations is amplified. The required amplification follows from Equation 7.2 and and Equation 7.3: it is\n\\[\n\\frac{b(L_2)}{b(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{\\beta/\\nu}\\:.\n\\] The exponent ratio \\(\\beta/\\nu\\) thus controls the rate at which the scale of the ordering variable decays with increasing coarse-graining length.\nConsider now the exponent \\(1/\\nu\\). For small but non-zero reduced temperature (large but finite \\(\\xi\\)) there is second way in which the configuration spectrum evolves with \\(L\\). As noted previously, coarse graining reduces the ratio of correlation length to coarse-graining length, and results in configurations with a less critical appearance. More precisely, we see from Equation 7.2 that increasing the coarse graining length from \\(L_1\\) to \\(L_2\\) while keeping the reduced temperature constant has the same effect on the configuration spectrum as keeping coarse-graining length constant which amplifying the reduced temperature \\(t\\) by a factor\n\\[\n\\frac{a(L_2)}{a(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{1/\\nu}\\:.\n\\] One may think of the combination \\(a(L)t\\) as a measure of the effective reduced temperature of the physical system viewed with resolution length \\(L\\). The exponent \\(1/\\nu\\) controls the rate at which the effective reduced temperature flows with increasing coarse-graining length.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-fss",
    "href": "phase-transitions/rg.html#sec-fss",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.7 Finite-size scaling",
    "text": "7.7 Finite-size scaling\nWe can exploit the fact that the scale factors \\(a(L)\\) and \\(b(L)\\) depend on critical exponents to estimate the values of these exponents using computer simulation. Consider the average of the block variable \\(S(L)\\). Consideration of Equation 7.1 shows that this is non other than the value of the order parameter \\(Q\\), measured over a block of side \\(L\\). Thus from the definition of an average\n\\[\nQ(L,t)=\\bar {S}(L,t)=\\int S(L)p(S(L),t)dS(L)\n\\] where \\(p(S(L))\\) is the probability distribution of \\(S(L)\\).\nMaking use of the representation of Equation 7.2, we then have that\n\\[Q\n(L,t) = \\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)dS(L)\n\\]\nTo integrate this we need to change the integration variable from \\(S(L)\\) to \\(b(L)S(L)\\). We have \\(d(b(L)S(L))=b(L)dS(L)\\) since \\(b(L)\\) does not fluctuate. Thus \\[\n\\begin{aligned}\nQ(L,t)  & =  b^{-1}(L)\\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)d(b(L)S(L))\\nonumber\\\\\n        & =  b^{-1}(L)f(a(L)t)\\nonumber\\\\\n       & =  b_0^{-1}L^{-\\beta/\\nu}f(a_0L^{1/\\nu}t)\n\\end{aligned}\n\\]\nwhere \\(f\\) is a universal function (defined as the first moment of \\(\\tilde{p}(x,y)\\) with respect to \\(y\\)).\nThe above results provide a method for determining the critical exponent ratios \\(\\beta/\\nu\\) and \\(1/\\nu\\) via computer simulations of near critical systems. For instance, at the critical point (\\(t=0\\)) and for finite block size, \\(Q(L,0)\\) will not be zero (the \\(T\\) at which Q vanishes for finite \\(L\\) is above the true \\(T_c\\), cf. Section 6.2. However, we know that its value must vanish in the limit of infinite \\(L\\); it does so like\n\\[Q(L,0)=b_0L^{-\\beta/\\nu}f(0)\\equiv Q_0L^{-\\beta/\\nu}\\]\nThus by studying the critical point \\(L\\) dependence of \\(Q\\) we can estimate \\(\\beta/\\nu\\). A similar approach in which we study two block sizes \\(L\\), and tune \\(t\\) separately in each case so that the results for \\(QL^{\\beta/\\nu}\\) are identical provides information on the value of \\(1/\\nu\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#summary-of-main-points",
    "href": "phase-transitions/rg.html#summary-of-main-points",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.8 Summary of main points",
    "text": "7.8 Summary of main points\nAs this is quite a long chapter let us summarise the main points:\n\nLimitations of conventional theories: Mean field theories and the scaling hypothesis are insufficient in the critical region due to their neglect of correlations across all relevant length scales.\nCritical region characteristics: Near-critical systems exhibit correlated microstructure on all length scales up to the correlation length. The complexity of this structure makes both analytical and computational study challenging.\nRelevant length scales:\n\nCorrelation length (\\(\\xi\\))\nMinimum microscopic scale (\\(L_\\textrm{min}\\))\nMacroscopic system size (\\(L_\\textrm{max}\\))\n\nWindow Condition for criticality: The true critical regime satisfies \\(L_\\textrm{max} \\gg \\xi \\gg L_\\textrm{min}\\), encompassing a broad range of scales where complex, often fractal-like, structures are present.\nRenormalisation Group (RG) methodology: RG involves the stepwise elimination of degrees of freedom by coarse-graining the system over increasing length scales. A fourth scale, \\(L\\), represents the resolution at which the system is described.\nEffect of coarse-graining: Coarse-graining changes the effective reduced temperature, captured by the relation \\(a(L)t\\), where \\(a(L)\\) scales with \\(L\\) as \\(L^{1/\\nu}\\). This helps describe how critical configurations evolve with resolution.\nUniversality:\n\nCoarse-graining reveals that microscopically different systems can exhibit identical critical behavior when observed at large scales.\nThe concept of universality explains why disparate systems, such as magnets and fluids, can show the same critical exponents and scaling laws.\nCritical behavior depends primarily on general features like dimensionality and symmetry, rather than microscopic details.\n\nFinite-Size scaling:\n\nThe average block variable \\(Q(L,t)\\) is related to block size \\(L\\) and reduced temperature \\(t\\) through scaling relations. Computer simulations exploit this to extract scaling functions and the values of critical exponents.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "href": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "title": "7  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "7.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)",
    "text": "7.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)\n\n\n\n\n\n\nCautionNotes for those interested in a different perspective on RG theory.\n\n\n\n\n\nLet us begin by returning to our fundamental Equation 2.1, which we rewrite as\n\\[p = Z^{-1}e^{-{\\cal H}}\\] where \\({\\cal H}\\equiv E/k_BT\\).\nThe first step is then to imagine that we generate, by a computer simulation procedure for example, a sequence of configurations with relative probability \\(\\exp(-{\\cal H})\\). We next adopt some coarse-graining procedure which produces from these original configurations a set of coarse-grained configurations. We then ask the question: what is the energy function \\({\\cal H}^\\prime\\) of the coarse-grained variables which would produce these coarse-grained configurations with the correct relative probability \\(\\exp(-{\\cal H}^\\prime)\\)? Clearly the form of \\({\\cal H}^\\prime\\) depends on the form of \\({\\cal H}\\) thus we can write symbolically\n\\[{\\cal H}^\\prime=R({\\cal H})\\]\nThe operation \\(R\\), which defines the coarse-grained configurational energy in terms of the microscopic configurational energy function is known as a renormalisation group transformation (RGT). What it does is to replace a hard problem by a less hard problem. Specifically, suppose that our system is near a critical point and that we wish to calculate its large-distance properties. If we address this task by utilizing the configurational energy and appealing to the basic machinery of statistical mechanics set out in Equation 2.1 and Equation 2.2, the problem is hard. It is hard because the system has fluctuations on all the (many) length scales intermediate between the correlation length \\(\\xi\\) and the minimum length scale \\(L_\\textrm{min}\\).\nHowever, the task may instead be addressed by tackling the coarse-grained system described by the energy \\({\\cal H}^\\prime\\). The large-distance properties of this system are the same as the large-distance properties of the physical system, since coarse-graining operation preserves large-scale configurational structure. In this representation the problem is a little easier: while the \\(\\xi\\) associated with \\({\\cal H}^\\prime\\) is the same as the \\(\\xi\\) associated with \\({\\cal H}\\), the minimum length scale of \\({\\cal H}^\\prime\\) is bigger than that of \\({\\cal H}\\). Thus the statistical mechanics of \\({\\cal H}^\\prime\\) poses a not-quite-so-many-length-scale problem, a problem which is effectively a little less critical and is thus a little easier to solve. The benefits accruing from this procedure may be amplified by repeating it. Repeated application of \\(R\\) will eventually result in a coarse- grained energy function describing configurations in which the \\(\\xi\\) is no bigger than the minimum length scale. The associated system is far from criticality and its properties may be reliably computed by any of a wide variety of approximation schemes. These properties are the desired large-distance properties of the physical system. As explicit reference to fluctuations of a given scale is eliminated by coarse-graining, their effects are carried forward implicitly in the parameters of the coarse-grained energy.\nIn order to setup the framework for a simple illustrative example, let is return to the lattice Ising model for which the energy function depended only on the product of nearest neighbour spins. The coefficient of this product in the energy is the exchange coupling, \\(J\\). In principle, however, other kinds of interactions are also allowed; for example, we may have a product of second neighbour spins with strength \\(J_2\\) or, perhaps, a product of four spins (at sites forming a square whose side is the lattice spacing), with strength \\(J_3\\). Such interactions in a real magnet have their origin in the quantum mechanics of the atoms and electrons and clearly depend upon the details of the system. For generality therefore we will allow a family of exchange couplings \\(J_1\\),\\(J_2\\),\\(J_3,\\dots\\), or \\(J_a, a =\n1,2,\\dots\\) In reduced units, the equivalent coupling strengths are \\(K_a =J_a/k_BT\\). Their values determine uniquely the energy for any given configuration.\n\nWe note that it is not only useful to allow for arbitrary kinds of interactions: if we wish to repeat the transformation several (indeed many) times, it is also necessary because even if we start with only the nearest neighbour coupling in \\({\\cal H}\\) the transformation will in general produce others in \\({\\cal H}^\\prime\\).\n\nNow consider the coarse-graining procedure. Let us suppose that this procedure takes the form of a ‘majority rule’ operation in which the new spins are assigned values \\(+1\\) or \\(-1\\) according to the signs of the magnetic moments of the blocks with which they are associated. The new energy function \\({\\cal H}^\\prime\\) will be expressible in terms of some new coupling strengths \\(K^\\prime\\) describing the interactions amongst the new spin variables (and thus, in effect, the interactions between blocks of the original spin variables). The RGT simply states that these new couplings depend on the old couplings: \\(K_1^\\prime\\) is some function \\(f_1\\) of all the original couplings, and generally\n\\[K^\\prime_a=f_a(K_1,K_2,\\dots) =f_a(\\mathbf {K}),\\quad a= 1, 2,\\dots\n\\tag{7.4}\\] where K is shorthand for the set \\(K_1, K_2,\\dots\\)\n\n7.9.1 A simple example\nThis example illustrates how one can perform the RG transformation Equation 7.4 directly, without recourse to a ‘sequence of typical configurations’. The calculation involves a very crude approximation which has the advantage that it simplifies the subsequent analysis.\n\n\n\n\n\n\nFigure 7.5: Coarse graining by decimation. The spins on the original lattice are divided into two sets \\(\\{s^\\prime\\}\\) and \\(\\{\\tilde{s}\\}\\). The \\(\\{s^\\prime\\}\\) spins occupy a lattice whose spacing is twice that of the original. The effective coupling interaction between the \\(\\{s^\\prime\\}\\) spins is obtained by performing the configurational average over the \\(\\{\\tilde{s}\\}\\)\n\n\n\nConsider an Ising model in two dimensions, with only nearest neighbour interactions as shown in Figure 7.5. We have divided the spins into two sets, the spins \\(\\{s^\\prime\\}\\) form a square lattice of spacing \\(2\\), the others being denoted by \\(\\{\\tilde{s}\\}\\). One then defines an effective energy function \\({\\cal H^\\prime}\\) for the \\(s^\\prime\\) spins by performing an average over all the possible arrangements of the \\(\\tilde{s}\\) spins\n\\[\n\\exp(-{\\cal H}^\\prime)=\\sum_{\\{\\tilde {s}\\}} \\exp(-{\\cal H}).\n\\tag{7.5}\\]\nThis particular coarse-graining scheme is called ‘decimation’ because a certain fraction (not necessarily one-tenth!) of spins on the lattice is eliminated. This formulation of a new energy function realizes two basic aims of the RG method: the long-distance physics of the ‘original’ system, described by \\({\\cal H}\\), is contained in that of the ‘new’ system, described by \\({\\cal H}^\\prime\\) (indeed the partition functions are the same as one can see by summing both sides over \\(s^\\prime\\)) and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing (‘minimum length scale’) has been reduced by a factor of \\(1/2\\) (the ratio of the lattice spacings of the two systems). We must now face the question of how to perform the configuration sum in Equation 7.5. This cannot in general be done exactly, so we must resort to some approximation scheme. The particular approximation which we invoke is the high temperature series expansion. In its simplest mathematical form, since \\({\\cal H}\\) contains a factor \\(1/k_BT\\), it involves the expansion of \\(\\exp(-{\\cal H})\\) as a power series:\n\\[\\exp(-{\\cal H}/k_BT)=1-{\\cal H}/k_BT +\\frac{1}{2!}({\\cal H}/k_BT)^2+.....\\]\nWe substitute this expansion into the right hand side of Equation 7.5 and proceed to look for terms which depend on the \\(s^\\prime\\) spins after the sum over the possible arrangements of the \\(\\tilde{s}\\) spins is performed. This sum extends over all the possible (\\(\\pm 1\\)) values of all the \\(\\tilde{s}\\) spins. The first term (the 1) in the expansion of the exponential is clearly independent of the values of the \\(s^\\prime\\) spins. The second term (\\({\\cal H}\\)) is a function of the \\(s^\\prime\\) spins, but gives zero when the sum over the \\(s^\\prime\\) spins is performed because only a single factor of any \\(s^\\prime\\) ever appears, and \\(+ 1 - 1 = 0\\). The third term (\\({\\cal H}^2/2\\)) does contribute. If one writes out explicitly the form of \\({\\cal H}^2/2\\) one finds terms of the form \\(K^2s_1^\\prime\\tilde{s}\\tilde{s}s_2^\\prime=K^2s_1^\\prime s_2^\\prime\\), where \\(s_1^\\prime\\) and \\(s_2^\\prime\\) denote two spins at nearest neighbour sites on the lattice of \\(s^\\prime\\) spins and \\(\\tilde{s}\\) is the spin (in the other set) which lies between them. Now, in the corresponding expansion of the left hand side of Equation 7.5, we find terms of the form \\(K^\\prime s_1^\\prime s_2^\\prime\\), where \\(K^\\prime\\) is the nearest neighbour coupling for the \\(s^\\prime\\) spins. We conclude (with a little more thought than we detail here) that\n\\[\nK^\\prime=K^2\n\\tag{7.6}\\]\nOf course many other terms and couplings are generated by the higher orders of the high temperature expansion and it is necssary to include these if one wishes reliable values for the critical temperature and exponents, However, our aim here is to use this simple calculation to illustrate the RG method. Let us therefore close our eyes, forget about the higher order terms and show how the RGT Equation 7.6 can be used to obtain information on the phase transition.\n\n\n\n\n\n\nFigure 7.6: Coupling flow under the decimation transformation described in the text.\n\n\n\nThe first point to note is that that mathematically Equation 7.6 has the fixed point \\(K^*= 1\\); if \\(K= 1\\) then the new effective coupling \\(K^\\prime\\) has the same value \\(1\\). Further, if \\(K\\) is just larger than \\(1\\), then \\(K^\\prime\\) is larger than \\(K\\), i.e. further away from \\(1\\). Similarly, if \\(K\\) is less than \\(1\\), \\(K^\\prime\\) is less than \\(K\\). We say that the fixed point is unstable: the flow of couplings under repeated iteration of Equation 7.6 is away from the fixed point, as illustrated in Figure 7.6. The physical significance of this is as follows: suppose that the original system is at its critical point so that the ratio of \\(\\xi\\) to lattice spacing is infinite. After one application of the decimation transformation, the effective lattice spacing has increased by a factor of two, but this ratio remains infinite; the new system is therefore also at its critical point. Within the approximations inherent in Equation 7.6, the original system is an Ising model with nearest neighbour coupling \\(K\\) and the new system is an Ising model with nearest neighbour coupling \\(K^\\prime\\). If these two systems are going to be at a common critically, we must identify \\(K^\\prime=\nK\\). The fixed point \\(K^*= 1\\) is therefore a candidate for the critical point \\(K_c\\), where the phase transition occurs. This interpretation is reinforced by considering the case where the original system is close to, but not at, criticality. Then \\(\\xi\\) is finite and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing is reduced by a factor of two. This instability of a fixed point to deviations of \\(K\\) from \\(K^*\\) is a further necessary condition for its interpretation as a critical point of the system. In summary then we make the prediction\n\\[\nK_c=J/k_BT_c=1\n\\tag{7.7}\\]\nWe can obtain further information about the behaviour of the system close to its critical point. In order to do so, we rewrite the transformation (Equation 7.6) in terms of the deviation of the coupling from its fixed point value. A Taylor expansion of the function \\(K^\\prime=K^2\\) yields\n\\[\n\\begin{aligned}\nK^\\prime =& (K^*)^2 +(K-K^*)\\left.\\frac{\\partial K^\\prime}{\\partial K}\\right|_{K=K^*}+\\frac{1}{2}(K-K^*)^2\\left.\\frac{\\partial^2 K^\\prime}{\\partial K^2}\\right|_{K=K^*}+\\ldots\\nonumber\\\\\nK^\\prime - K^* =& 2 (K - K^*)+ (K - K^*)^2\n\\end{aligned}\n\\]\nwhere in the second line we have used the fact that the first derivative evaluates to \\(2K^*=2\\) and \\((K^*)^2=K^*\\).\nFor a system sufficiently close to its critical temperature the final term can be neglected. The deviation of the coupling from its fixed point (critical) value is thus bigger for the new system than it is for the old by a factor of two. This means that the reduced temperature is also bigger by a factor of two:\n\\[t^\\prime= 2t\\]\nBut \\(\\xi\\) (in units of the appropriate lattice spacing) is smaller by a factor of \\(1/2\\):\n\\[\\xi^\\prime= \\xi/2\\]\nThus, when we double \\(t\\), we halve \\(\\xi\\), implying that\n\\[\\xi\\propto t^{-1}\\]\nfor \\(T\\) close to \\(T_c\\). Thus we see that the RGT predicts scaling behaviour with calculable critical exponents. In this simple calculation we estimate the critical exponent \\(\\nu=1\\) for the square lattice Ising model. This prediction is actually in agreement with the exactly established value. The agreement is fortuitous- the prediction in Eq. refeq:Kc for \\(K_c\\), is larger than the exactly established value by a factor of more than two. In order to obtain reliable estimates more sophisticated and systematic methods must be used.\nThe crude approximation in the calculation above produced a transformation, Equation 7.6, involving only the nearest neighbour coupling, with the subsequent advantages of simple algebra. We pay a penalty for this simplicity in two ways: the results obtained for critical properties are in rather poor agreement with accepted values, and we gain no insight into the origin of universality.\n\n\n7.9.2 Universality and scaling\nIn order to expose how universality can arise, we should from the start allow for several different kinds of coupling \\(J_a\\), and show how the systems with different \\(J_a\\) can have the same critical behaviour.\n\n\n\n\n\n\nFigure 7.7: General flow in coupling space\n\n\n\nFigure 7.7 is a representation of the space of all coupling strengths \\(K_a\\) in the energy function \\({\\cal H}/k_BT\\). This is of course actually a space of infinite dimension, but representing three of these, as we have done, enables us to illustrate all the important aspects. First let us be clear what the points in this space represent. Suppose we have some magnetic material which is described by a given set of exchange constants \\(J_1,J_2,J_3.....\\) As the temperature \\(T\\) varies, the coupling strengths \\(K_a=J_a/k_BT\\) trace out a straight line, or ray, from the origin of the space in the direction (\\(J_1,J_2,J_3 ....\\) ). Points on this ray close to the origin represent this magnet at high temperatures, and conversely points far from the origin represent the magnet at low temperatures. The critical point of the magnet is represented by a specific point on this ray, \\(K_a=\nJ_a/k_BT, a= 1,2,\\dots\\) The set of critical points on all of the possible rays forms a surface, the critical surface. Formally, it is defined by the set of all possible models (of the Ising type) which have infinite \\(\\xi\\). It is shown schematically as the shaded surface in Figure 7.7. (In the figure it is a two-dimensional surface; more generally it has one dimension less than the full coupling constant space, dividing all models into high and low temperature phases.)\nOur immediate goal then is to understand how the RGT can explain why different physical systems near this critical surface have the same behaviour. Let us turn now to the schematic representation of the RG flow in Figure 7.7. Suppose we start with a physical system, with coupling strengths \\(K_a,  a= 1,2, \\dots\\). What the RGT does is generate a new point in the figure, at the coupling strengths \\(K_a^{(1)}=f_a(\\mathbf {K})\\); these are the couplings appearing in the effective energy function describing the coarse-grained system. If we repeat the transformation, the new energy function has coupling strengths \\(K_a^{(2)}=f_a(\\mathbf {K})\\). Thus repeated application of the transformation generates a flow of points in the figure: \\(\\mathbf{K}\\to\nK^{(1)}\\to\\dots\\to K^{(n)}\\) where the superscript (\\(n\\)) labels the effective couplings after \\(n\\) coarse-graining steps. if the change in coarse-graining scale is \\(b\\) (\\(&gt; 1\\)) at each step, the total change in coarse-graining scale is \\(b^n\\) after \\(n\\) steps. In the process, therefore, the ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor of \\(b^{-n}\\). The dots in Figure 7.7 identify three lines of RG flow starting from three systems differing only in their temperature. (The flow lines are schematic but display the essential features revealed in detailed calculations.)\nConsider first the red dots which start from the nearest neighbour Ising model at its critical point. The ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor b at each step, but, since it starts infinite, it remains infinite after any finite number of steps. In this case we can in principle generate an unbounded number of dots, \\(\\mathbf{ K^{(1)}, K^{(2)},\\dots,K^{(n)}}\\), all of which lie in the critical surface. The simplest behaviour of such a sequence as \\(n\\) increases is to tend to a limit, \\(K^*\\), say. In such a case\n\\[K^*_a=f_a(K^*)~~~~ a= 1,2 .....\\]\nThis point \\(\\mathbf{K^*} \\equiv K_1^*, K_2^*, \\dots\\) is therefore a fixed point which lies in the critical surface.\nBy contrast, consider the same magnet as before, now at temperature \\(T\\) just greater than \\(T_c\\), its couplings \\(K_a\\), will be close to the first red dot (in fact they will be slightly smaller) and so will the effective couplings \\(K_a^{(1)},K_2^{(2)},\\dots\\) of the corresponding coarse-grained systems. The new flow will therefore appear initially to follow the red dots towards the same fixed point. However, the flow must eventually move away from the fixed point because each coarse-graining now produces a model further from criticality. The resulting flow is represented schematically by one set of black dots. The other set of black dots shows the expected flow starting from the same magnet slightly below its critical temperature.\nWe are now in a position to understand both universality and scaling within this framework. We will suppose that there exists a single fixed point in the critical surface which sucks in all flows starting from a point in that surface. Then any system at its critical point will exhibit large-length scale physics (large-block spin behaviour) described by the single set of fixed point coupling constants. The uniqueness of this limiting set of coupling constants is the essence of critical point universality. It is, of course, the algebraic counterpart of the unique limiting spectrum of coarse-grained configurations, discussed in Section 7.5. Similarly the scale-invariance of the critical point configuration spectrum (viewed on large enough length scales) is expressed in the invariance of the couplings under iteration of the transformation (after a number of iterations large enough to secure convergence to the fixed point).\nTo understand the behaviour of systems near but not precisely at critically we must make a further assumption (again widely justified by explicit studies). The flow line stemming from any such system will, we have argued, be borne towards the fixed point before ultimately deviating from it after a number of iterations large enough to expose the system’s noncritical character. We assume that (as indicated schematically in the streams of red and blue lines in Figure 7.7 the deviations lie along a single line through the fixed point, the direction followed along this line differing according to the sign of the temperature deviation \\(T-T_c\\). Since any two sets of coupling constants on the line (on the same side of the fixed point) are related by a suitable coarse-graining operation, this picture implies that the large-length-scale physics of all near- critical systems differs only in the matter of a length scale. This is the essence of near-critical point universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html",
    "href": "phase-transitions/master-equation-and-diffusion.html",
    "title": "8  Introduction to stochastic processes",
    "section": "",
    "text": "8.1 The Master Equation\nMany natural phenomena are stochastic — they involve randomness in their evolution over time.\nIn classical physics, this randomness may arise from our lack of knowledge about microscopic details. For example, in a gas, we do not know the precise positions and velocities of each particle, so their collisions with the container walls appear random.\nIn quantum mechanics, stochasticity is even more intrinsic. The fundamental objects are probability amplitudes, and outcomes are inherently probabilistic.\nTo describe these systems effectively, we use a coarse-grained probabilistic description, which tracks the likelihood of different outcomes rather than precise trajectories. One of the key tools in this approach is the master equation.\nConsider a system in microstate \\(i\\) with energy \\(E_i\\). It can transition to neighboring microstates \\(j\\), where the energy difference \\(|E_j - E_i|\\) is small (within \\(\\delta E\\)).\nLet \\(\\nu_{ij}\\) be the rate at which the system jumps from state \\(i\\) to state \\(j\\). Over an infinitesimal time interval \\(dt\\), the probability \\(p_i\\) changes as:\n\\[\ndp_i = \\left[ -p_i \\sum_j  \\nu_{ij} + \\sum_j \\nu_{ji} p_j \\right] dt\n\\]\nThis expression contains two terms:\nThe master equation becomes:\n\\[\n\\frac{dp_i}{dt} = -\\sum_j \\nu_{ij} p_i + \\sum_j \\nu_{ji} p_j\n\\]\nThis is a linear first-order differential equation for the vector of probabilities \\(\\{p_i\\}\\).\nAlternatively, in matrix form:\n\\[\n\\frac{d\\mathbf{p}}{dt} = W \\mathbf{p}\n\\]\nwhere \\(W\\) is the rate matrix with entries:\nThis structure ensures probability conservation: the total probability \\(\\sum_i p_i = 1\\) remains constant in time.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "title": "8  Introduction to stochastic processes",
    "section": "",
    "text": "A loss term: the system leaves state \\(i\\) at rate \\(\\nu_{ij}\\),\nA gain term: the system arrives in state \\(i\\) from other states \\(j\\) at rate \\(\\nu_{ji}\\).\n\n\n\n\n\n\n\n\n\\(W_{ij} = \\nu_{ji}\\) for \\(i \\neq j\\),\n\\(W_{ii} = -\\sum_{j \\neq i} \\nu_{ij}\\).\n\n\n\n\n\n\n\n\nCautionAn aside on entropy production\n\n\n\n\n\nThe master equation is first order in time and does not have time reversal symmetry so describes an irreversible process. This irreversibility arises from the coarse-graining process that throws away information about underlying microphysics which is described by Newton’s equations and which are time reversible. Only by doing so is the entropy allowed to increase which is required by the second law of thermodynamics for an irreversible process. Consequently the increase of entropy is linked to our knowledge about the system rather than anything it is doing internally in a manner that may appear dubious. Can it be possible that macroscopic and reproducible phenomena such as heat flow depend on how we handle information? Perhaps yes since the division between work and heat is somewhat arbitrary. Were we able to track all the particle positions there would be no need to talk about heat energy or heat flow.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "title": "8  Introduction to stochastic processes",
    "section": "8.2 From the Master Equation to the Diffusion Equation",
    "text": "8.2 From the Master Equation to the Diffusion Equation\nNow consider the situation where the state index \\(i\\) corresponds to a position in space, \\(x_i = i a\\), ie a one-dimensional lattice with lattice spacing \\(a\\), and transitions only occur between neighboring lattice sites.\nWe assume:\n\nTransition rates are symmetric: \\(\\nu_{i, i+1} = \\nu_{i, i-1} = \\nu\\),\nThe spacing \\(a\\to 0\\) in the continuum limit.\n\nThe master equation becomes a finite-difference equation:\n\\[\n\\begin{aligned}\n\\frac{dp_i}{dt} =  &\\sum_j\\nu_{ij}(p_j-p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i-1}-p_i) + \\nu(p_{i+1} - p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i+1} + p_{i-1} - 2p_i)\n\\end{aligned}\n\\]\nWe now define a continuous variable \\(x = i a\\), and a probability density \\(p(x, t)\\) such that \\(p_i(t) \\approx p(x, t)\\).\nUsing Taylor expansions:\n\\[\np(x \\pm a, t) = p(x, t) \\pm a \\frac{\\partial p}{\\partial x} + \\frac{a^2}{2} \\frac{\\partial^2 p}{\\partial x^2} + \\cdots\n\\]\nSubstituting into the master equation gives:\n\\[\n\\frac{\\partial p}{\\partial t} =\n\\nu a^2 \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nDefining the diffusion constant \\(D = \\nu a^2\\), we obtain the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThe diffusion equation describes the evolution of the probability density of a particle with diffusion constant (sometimes called diffusivity) given by \\(D = \\nu a^2\\). The dimensions of \\(D\\) are \\([\\text{length}]^2/[\\text{time}]\\). Typically, after expansion, we set the lattice spacing \\(a\\) to 1. Additionally, the diffusion equation can describe many non-interacting diffusing particles. In this case, we replace \\(p\\) with \\(\\rho\\), representing the density or concentration of particles, and use the normalization \\(\\int dx \\, \\rho = M\\), where \\(M\\) is the number of particles.\nThe diffusion equation, much like the master equation from which it originates, explicitly violates time-reversal symmetry, thus permitting entropy to increase.\nThe solution of the diffusion equation for an initial condition where the particle is initially localized at the origin (formally, \\(p(x,0) = \\delta(x)\\)) is a Gaussian:\n\\[\np(x,t) = (4 \\pi D t)^{-1/2} \\exp\\left[-\\frac{x^2}{4 D t}\\right]\n\\]\nWe explicitly see the arrow of time by examining this Gaussian solution at various times \\(t\\). As \\(t\\) increases, the Gaussian “bell-shaped” curve spreads out. Its width grows according to \\(\\langle x^2 \\rangle^{1/2} \\sim t^{1/2}\\). This is known as “diffusive scaling,” and it implies that, after time \\(t\\), a particle will typically be found at a distance roughly proportional to \\(t^{1/2}\\) from its starting point. Conversely, exploring a region of size \\(L\\) typically requires a time of order \\(O(L^2)\\).\nThe evolution of the solution to the 1d diffusion equation in a spatial region \\(x=[0,1]\\) as a function of times are shown in the movie below. The diffusion constant is \\(D=0.01\\). The movie corresponds to a particle initialised at \\(x=0.5\\). One sees how the probability density spreads out over the range as time increases. This can be used to model the diffusion of particles down a concentration gradient as you will see in the next part of the course.\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport os\n\n# Parameters\nL = 1.0        # Length of the domain\nT = 1.0        # Total time \nnx = 400       # Number of spatial points\nnt = 2000      # Number of time steps\nD = 0.1        # Diffusion coefficient\n\n# Discretization\ndx = L / (nx - 1)\ndt = T / nt\n\n# Stability condition auto-adjust\nif D * dt / dx**2 &gt; 0.5:\n    print(\"Adjusting dt and nt to satisfy stability condition...\")\n    dt = 0.4 * dx**2 / D\n    nt = int(T / dt)\n    dt = T / nt  # Recalculate dt exactly\n\nnt = int(nt / 50)  # Artificial slowdown for animation\n\nprint(f\"Using dt = {dt:.4e}, nt = {nt}\")\n\n# Initialize x and u\nx = np.linspace(0, L, nx)\n\n# Initial condition: smooth narrow Gaussian\nsigma = 0.01\nu = np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n# Normalize initial condition\nu /= np.sum(u) * dx\n\n# Setup figure\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\nfig, ax = plt.subplots()\nline, = ax.plot(x, u)\n\n# Compute peak height for setting y-axis\npeak_height = 1 / (np.sqrt(2 * np.pi) * sigma)\nax.set_ylim(0, peak_height * 1.05)\n\nax.set_xlabel(r'$x$', fontsize=20)\nax.set_ylabel(r'$p(x,t)$', fontsize=20)\n\nax.tick_params(axis='both', which='major', labelsize=14)\n\n# Tiny time counter text\ntime_text = ax.text(0.85, 0.05, '', transform=ax.transAxes, fontsize=10, verticalalignment='bottom')\n\n\n# Function to update the plot\ndef update(frame):\n    global u\n    unew = np.copy(u)\n    unew[1:-1] = u[1:-1] + D * dt / dx**2 * (u[2:] - 2*u[1:-1] + u[:-2])\n    u = unew\n\n    # Normalize at every step\n    u /= np.sum(u) * dx\n\n    line.set_ydata(u)\n    current_time = frame * dt\n    time_text.set_text(r'$t=%.4f$' % current_time)\n    return line, time_text\n\nani = animation.FuncAnimation(fig, update, frames=nt, interval=100, blit=True)\n\n\n# Save the animation as a movie\nWriter = animation.writers['ffmpeg']\nwriter = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nani.save(\"../Movies/diffusion_evolution.mp4\", writer=writer)\n\n\nplt.show()",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "href": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "title": "8  Introduction to stochastic processes",
    "section": "8.3 Consequences of time reversal symmetry",
    "text": "8.3 Consequences of time reversal symmetry\nAs we have seen by introducing a type of coarse graining, the master equation violates time reversal symmetry of the underlying Newtonian dynamics. Remarkably however, the fact that the underlying microphysics is actually time reversal symmetric has several deep consequences which survive the coarse graining procedure These results are some of the cornerstones of nonequilibrium thermodynamics\n\n8.3.1 Detailed balance\nRecall from year 2 Statistical Mechanics that for a system in equilibrium, the principle of equal a-priori probabilities of microstates holds. Therefore\n\\[\n\\nu_{ij} p_i^{eq} = \\nu_{ji} p_j^{eq}\n\\]\nHence, on average, the actual rate of quantum jumps from \\(i\\) to \\(j\\) (the left-hand side) is the same as from \\(j\\) to \\(i\\). This is a stronger statement than the master equation, which asserts only that there is overall balance between the rate of jumping into and out of state \\(i\\) in equilibrium. The result above is known as the principle of detailed balance.\nThis principle is powerful because it applies not only to individual states but also to any grouping of states.\nExercise: Show that for two groups of states, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\nu_{AB} p_A^{eq} = \\nu_{BA} p_B^{eq}\n\\]\nHence, detailed balance arguments can be extended to subsystems within a large isolated system, and even to systems that are not isolated. However, in such cases, the principle is far from obvious, because once states are grouped together:\n\\[\n\\nu_{AB} \\ne \\nu_{BA}, \\quad p_A \\ne p_B\n\\]\n(This can be easily demonstrated, for example, by considering two groups that contain different numbers of states with similar energies.) Nonetheless, the detailed balance relation holds, in equilibrium, in the general form above.\n\n\n8.3.2 Computer simulation\nIn computer simulation, good results will be obtained if one accurately follows the microscopic equations of motion. This is the molecular dynamics (MD) method which we now outline.\nMolecular dynamics\nMolecular Dynamics (MD) involves a system of classical particles interacting through specified interparticle forces. The motion of these particles is determined by numerically integrating Newton’s equations of motion. In MD simulations, averages of state variables are obtained as time averages over trajectories in phase space. Typically, the forces acting between particles are conservative, ensuring that the total energy \\(E\\) remains constant. This conservation implies that the motion is restricted to a \\((2dN - 1)\\)-dimensional surface in phase space, denoted by \\(\\Gamma(E)\\).\nA central aspect of MD is the averaging of observables. For a given observable \\(A\\), its average is computed as the time average along the trajectory. Mathematically, this is expressed as:\n\\[\n\\left\\langle A(\\{ \\mathbf{p}_i \\}, \\{ \\mathbf{r}_i \\}) \\right\\rangle = \\frac{1}{\\tau} \\int_{t_0}^{t_0+\\tau} dt\\, A(\\{ \\mathbf{p}_i(t) \\}, \\{ \\mathbf{r}_i(t) \\})\n\\]\nThis formula represents the integral of the observable over a time interval \\(\\tau\\), normalized by the length of that interval.\nThe practical steps of an MD simulation start with generating an initial random configuration of particle positions \\(\\{ \\mathbf{r}_i \\}\\) and momenta \\(\\{ \\mathbf{p}_i \\}\\). The system’s equations of motion are then iteratively solved using a suitable algorithm to allow it to reach equilibrium. After equilibration, a production run is performed over many time steps to collect meaningful data. Finally, relevant averages, such as pressure or kinetic energy, are calculated from the collected data.\nMonte Carlo\nHowever, to obtain the equilibrium properties of the system, it may be much faster to use a dynamics which is nothing like the actual equations of motion.\nAt first sight, this looks very dangerous; however, if one can prove that in the required equilibrium distribution, the artificial dynamics obey the principle of detailed balance, then it is (almost) guaranteed that the steady state found by simulation is the true equilibrium state.\nThe best known example is the Monte Carlo method, in which the dynamical algorithm consists of random jumps. The jump rates \\(\\nu_{AB}\\) for all pairs of states \\((A, B)\\) take the form:\n\\[\n\\nu_{AB} = \\nu_0 \\quad \\text{if } E_B \\le E_A\n\\]\n\\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)} \\quad \\text{if } E_B \\ge E_A\n\\]\nwhere \\(\\nu_0\\) is a constant.\nExercise: Show that this gives the canonical distribution in steady state.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nTo show that this jump rate rule gives the canonical distribution in steady state, we assume the system reaches a steady-state probability distribution \\(P_A\\) for state \\(A\\) and apply the condition of detailed balance.\nDetailed balance requires: \\[\nP_A \\nu_{AB} = P_B \\nu_{BA}\n\\]\nAssume the canonical distribution: \\[\nP_A = \\frac{1}{Z} e^{-\\beta E_A}, \\quad P_B = \\frac{1}{Z} e^{-\\beta E_B}\n\\]\nNow consider two cases for \\(\\nu_{AB}\\) and \\(\\nu_{BA}\\):\nCase 1: \\(E_B \\leq E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0, \\quad \\nu_{BA} = \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nSubstitute into the detailed balance condition: \\[\nP_A \\nu_0 = P_B \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A = P_B e^{-\\beta (E_A - E_B)}\n\\]\nUse canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} = \\frac{1}{Z} e^{-\\beta E_B} e^{-\\beta (E_A - E_B)} = \\frac{1}{Z} e^{-\\beta E_A}\n\\]\nVerified.\nCase 2: \\(E_B &gt; E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)}, \\quad \\nu_{BA} = \\nu_0\n\\]\nSubstitute: \\[\nP_A \\nu_0 e^{-\\beta (E_B - E_A)} = P_B \\nu_0\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A e^{-\\beta (E_B - E_A)} = P_B\n\\]\nAgain using canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} e^{-\\beta (E_B - E_A)} = \\frac{1}{Z} e^{-\\beta E_B} = P_B\n\\]\nVerified.\nHence, in both cases the detailed balance condition is satisfied with the canonical distribution, and the steady state is indeed the canonical distribution.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "title": "9  The Langevin Approach",
    "section": "",
    "text": "9.1 The Random Walk and the Langevin equation\nThe concept of a random walk and its continuum limit – diffusion – introduced in the previous chapter, expresses the time evolution of the probability distribution \\(p(x, t)\\) for a particle’s position \\(x\\) by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2},\n\\]\nwhich is a standard example of a so called Fokker-Planck equation, which is second-order in space and first-order in time.\nIn contrast, the Langevin equation provides a stochastic differential equation for the particle’s trajectory \\(x(t)\\). To understand it, consider the random hopping motion of a particle on a 1d lattice over a small time increment \\(\\Delta t\\):\n\\[\nx(t + \\Delta t) = x(t) + \\Delta x(t)\n\\]\nHere, \\(\\Delta x(t)\\) is a random displacement. If the lattice spacing is \\(a\\), we define the step statistics as:\n\\[\n\\Delta x(t) =\n\\begin{cases}\n+a & \\text{with probability } \\nu \\Delta t \\\\\n-a & \\text{with probability } \\nu \\Delta t \\\\\n0 & \\text{with probability } 1 - 2\\nu \\Delta t\n\\end{cases}\n\\]\nThis defines a discrete-time, discrete-space random walk. The average and variance of the step are easily derived (try as an exercise):\nThe steps \\(\\Delta x(t)\\) are uncorrelated across time.\nTo take the continuum limit, we let both \\(a \\to 0\\) and \\(\\Delta t \\to 0\\), while keeping the diffusion constant \\[\nD = \\nu a^2\n\\] fixed. This requirement implies that we need \\(\\nu\\propto a^{-2}\\), which is satisfied if \\[\na \\propto \\sqrt{\\Delta t}.\n\\]\nHence the step size must shrink as the square root of the time step. Only under this scaling does the random walk converge to a well-defined continuum process with finite diffusion constant, namely Brownian motion or the Langevin equation.\nIn this limit, we obtain the Langevin equation:\n\\[\n\\dot{x}(t) = \\eta(t)\n\\]\nwhere \\(\\eta(t)\\equiv\\frac{\\Delta x(t)}{\\Delta t}\\) is a stochastic noise satisfying:\n\\[\n\\langle \\eta(t) \\rangle = 0\n\\]\n\\[\n\\langle \\eta(t) \\eta(t') \\rangle = \\Gamma \\delta(t - t')\n\\]\nThis \\(\\eta(t)\\) is known as white noise — it has zero mean and is uncorrelated at different times. \\(\\Gamma\\) measures its amplitude.\nThe Langevin equation tells us that the velocity \\(\\dot{x}(t)\\) is purely driven by noise. We can formally integrate it:\n\\[\nx(t) - x_0 = \\int_0^t \\eta(t')\\, dt'\n\\]\nTaking ensemble averages, i.e. averaging over all possible realisations of the noise:\nComparing this with the diffusion equation result, we identify:\n\\[\n\\Gamma = 2D\n\\]\nHence, the Langevin description yields the same physical behavior — not just the mean-square displacement but also the full probability distribution \\(p(x, t)\\) — as the diffusion (Fokker-Planck) equation. This equivalence arises from the fact that the integral of many small, independent random steps leads to a Gaussian distribution, in agreement with the solution of the diffusion equation.\nFor more details, see: Stochastic Processes in Physics and Chemistry by N.G. van Kampen (North Holland, 1981).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "title": "9  The Langevin Approach",
    "section": "",
    "text": "Mean: \\(\\langle \\Delta x \\rangle = 0\\)\nVariance: \\(\\langle (\\Delta x)^2 \\rangle = 2 a^2 \\nu \\Delta t = 2D \\Delta t\\)\n\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nThe mean displacement is \\[\n\\langle \\Delta x \\rangle = (+a)(\\nu \\Delta t) + (-a)(\\nu \\Delta t) + 0(1 - 2\\nu \\Delta t) = 0.\n\\]\nThe mean square displacement is \\[\n\\langle (\\Delta x)^2 \\rangle = (+a)^2(\\nu \\Delta t) + (-a)^2(\\nu \\Delta t) + 0^2(1 - 2\\nu \\Delta t)\n= 2a^2\\nu \\Delta t.\n\\]\nHence the variance is \\[\n\\mathrm{Var}(\\Delta x) = \\langle (\\Delta x)^2 \\rangle - \\langle \\Delta x \\rangle^2 = 2a^2\\nu \\Delta t.\n\\]\nIdentifying \\(D = a^2\\nu\\), we obtain \\[\n\\langle (\\Delta x)^2 \\rangle = 2D\\Delta t.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean displacement: \\[\n\\langle x(t) - x_0 \\rangle = 0\n\\]\nMean square displacement: \\[\n\\langle [x(t) - x_0]^2 \\rangle = \\int_0^t \\int_0^t \\langle \\eta(t') \\eta(t'') \\rangle\\, dt'\\, dt'' = \\Gamma \\int_0^t dt' = \\Gamma t\n\\]",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "title": "9  The Langevin Approach",
    "section": "9.2 Brownian Motion",
    "text": "9.2 Brownian Motion\nLet us now examine Brownian motion, originally observed as the erratic motion of colloidal particles suspended in a fluid. These particles undergo constant collisions with surrounding (smaller) fluid molecules, which results in seemingly random movement.\nFrom a coarse-grained perspective — where we do not track each individual collision — this appears as motion under random forces. This statistical treatment introduces irreversibility at the macroscopic level, even though the underlying molecular dynamics are reversible.\nThe Langevin equation provides a way to model this behavior. For a particle of mass \\(m\\) in one dimension, Langevin proposed the equation:\n\\[\nm \\ddot{x} = -\\gamma \\dot{x} + f(t)\n\\]\nHere:\n\n\\(-\\gamma \\dot{x}\\) is a frictional damping force, where \\(\\gamma\\) is the damping coefficient.\n\\(f(t)\\) is a random force due to molecular collisions.\n\n\nOften, the mobility is defined as \\(\\mu = 1/\\gamma\\) — note that this is unrelated to chemical potential.\n\n\n9.2.1 Noise Properties\nIn principle the random forces are correlated in time since the molecular collisions which cause them are correlated and have some definite duration.\nLet us assume that there is some correlation time \\(t_c\\) over which \\(\\langle f(t_1) f(t_2) \\rangle = g(t_1 - t_2)\\) decays rapidly as shown in the sketch below:\n\n\n\n\n\n\nFigure 9.1: Sketch of \\(g(t_1−t_2)\\) against \\(|t_1− t_2|\\)\n\n\n\nThen as long as we consider timescales \\(\\gg t_c\\) we can safely replace \\(g(t_1 - t_2)\\) by a delta function. Thus we can make the approximation of white noise\n\\[\n\\langle f(t) \\rangle = 0\n\\]\n\\[\n\\langle f(t_1) f(t_2) \\rangle = \\Gamma \\delta(t_1 - t_2)\n\\]\n\n\n9.2.2 Solving the Langevin Equation (velocity)\nLet’s set \\(m = 1\\) for simplicity and solve the equation:\n\\[\n\\dot{v} + \\gamma v = f(t)\n\\]\nWe apply an integrating factor to get an exact differential on the LHS:\n\\[\n\\frac{d}{dt} \\left[ v e^{\\gamma t} \\right] = e^{\\gamma t} f(t)\n\\]\nIntegrating both sides:\n\\[\n\\int_0^t \\frac{d}{dt'}\\!\\left( v e^{\\gamma t'} \\right) dt' = \\int_0^t e^{\\gamma t'} f(t')\\, dt'\n\\]\n\\[\nv(t)e^{\\gamma t} - v_0 = \\int_0^t e^{\\gamma t'} f(t')\\, dt'\n\\]\n\\[\nv(t) = v_0 e^{-\\gamma t} + \\int_0^t e^{-\\gamma (t - t')} f(t')\\, dt'\n\\]\nWe now averaging over all possible realisations of the random force, i.e we imagine repeating the Brownian motion experiment infinitely many times, each with a different random sequence of microscopic kicks \\(f(t)\\). Then\n\\[\n\\langle v(t) \\rangle = v_0 e^{-\\gamma t}\n\\] where we have used the fact that \\(f(t)\\) is random with \\(\\langle f(t)\\rangle=0\\). Thus:\n\nAt short times: (\\(\\gamma t \\ll 1\\)): \\(\\langle v \\rangle \\approx v_0\\) ie. friction is negligible.\nAt long times: (\\(\\gamma t \\gg 1\\)): \\(\\langle v \\rangle \\to 0\\) ie. the system loses memory of the initial velocity.\n\n\n\n9.2.3 Mean-square velocity\nWe now compute (try it as an exercise):\n\\[\n\\langle v(t)^2 \\rangle = v_0^2 e^{-2\\gamma t} + \\Gamma \\int_0^t e^{-2\\gamma (t - t')} dt' = v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma} \\left(1 - e^{-2\\gamma t} \\right)\n\\]\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\text{From the solution for }v(t)\\text{ with }m=1:\\\\\nv(t)&=v_0 e^{-\\gamma t}+\\int_0^t e^{-\\gamma (t-t')}f(t')\\,dt'.\\\\\n\\Rightarrow\\; v(t)^2 &= v_0^2 e^{-2\\gamma t}\n+ 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} f(t')\\,dt'\n+ \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')} f(t')f(t'')\\,dt'\\,dt''.\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\Rightarrow\\; \\langle v(t)^2\\rangle &= v_0^2 e^{-2\\gamma t} + 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} \\underbrace{\\langle f(t')\\rangle}_{=\\,0}\\,dt' + \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')}\n\\underbrace{\\langle f(t')f(t'')\\rangle}_{=\\,\\Gamma\\,\\delta(t'-t'')}\\,dt'\\,dt''\\\\\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma \\int_0^t e^{-2\\gamma (t-t')}\\,dt'\\\\\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma\\left[\\,\\frac{1}{2\\gamma}e^{-2\\gamma (t-t')}\\,\\right]_{t'=0}^{t'=t}\\\\\n&= v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma}\\bigl(1-e^{-2\\gamma t}\\bigr).\n\\end{aligned}\n\\]\n\\[\n\\boxed{\\;\\langle v(t)^2\\rangle\n= v_0^2 e^{-2\\gamma t} + \\dfrac{\\Gamma}{2\\gamma}\\left(1-e^{-2\\gamma t}\\right)\\;}\n\\]\n\n\n\nImplying that at\n\nShort times: \\(\\langle v^2 \\rangle \\approx v_0^2\\)\nLong times: \\(\\langle v^2 \\rangle \\to \\Gamma / (2\\gamma)\\)\n\nAt equilibrium, the equipartition theorem gives:\n\\[\n\\frac{1}{2} m \\langle v^2 \\rangle = \\frac{1}{2} k_B T\n\\]\nUsing this to identify \\(\\Gamma\\):\n\\[\n\\Gamma = 2 \\gamma k_B T\n\\]\nThis important result relates the noise strength to the damping and temperature — they have the same microscopic origin (molecular collisions).\n\n\n9.2.4 Mean-square displacement\nWe now integrate \\(v(t)\\) again to get position \\(x(t)\\) (with \\(m = 1\\)):\nUsing the result above and substituting \\(\\Gamma = 2\\gamma k_B T\\), one finds (after integrating twice and some algebra):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle = \\frac{(v_0^2 - k_B T)}{\\gamma^2} (1 - e^{-\\gamma t})^2 + \\frac{2 k_B T}{\\gamma} \\left[ t - \\frac{1 - e^{-\\gamma t}}{\\gamma} \\right]\n\\]\n\n\n\n\n\n\nCautionDerivation (non-examinable)\n\n\n\n\n\nThe displacement over time \\(t\\) is\n\\[\n\\Delta x(t) = x(t) - x(0) = \\int_0^t v(t_1)\\,dt_1.\n\\]\nHence, the mean-squared displacement (MSD) is\n\\[\n\\langle \\Delta x^2(t) \\rangle\n= \\Big\\langle \\int_0^t dt_1 \\int_0^t dt_2\\, v(t_1)v(t_2) \\Big\\rangle.\n\\]\nSubstituting the expression for \\(v(t)\\) and averaging, we obtain (after some algebra)\n\\[\n\\langle v(t_1)v(t_2)\\rangle = \\frac{k_B T}{m}\\, e^{-\\gamma|t_1 - t_2|}.\n\\]\nInserting this result into the MSD expression gives\n\\[\n\\langle \\Delta x^2(t) \\rangle\n= \\frac{2k_B T}{m} \\int_0^t dt_1 \\int_0^{t_1} dt_2\\, e^{-\\gamma (t_1 - t_2)}.\n\\]\nEvaluating the inner integral first:\n\\[\n\\int_0^{t_1} e^{-\\gamma (t_1 - t_2)}\\,dt_2\n= \\frac{1 - e^{-\\gamma t_1}}{\\gamma}.\n\\]\nThen the outer integral becomes\n\\[\n\\int_0^t \\frac{1 - e^{-\\gamma t_1}}{\\gamma}\\,dt_1\n= \\frac{t}{\\gamma} - \\frac{1 - e^{-\\gamma t}}{\\gamma^2}.\n\\]\nTherefore, the mean-squared displacement is\n\\[\n\\boxed{\n\\langle \\Delta x^2(t)\\rangle\n= 2\\frac{k_B T}{m}\n\\left(\\frac{t}{\\gamma} - \\frac{1 - e^{-\\gamma t}}{\\gamma^2}\\right)\n= 2\\frac{k_B T}{m\\gamma^2}\\big(\\gamma t - 1 + e^{-\\gamma t}\\big).\n}\n\\]\n\n\n\nLimiting behaviours:\n\nShort times: (\\(\\gamma t \\ll 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx v_0^2 t^2\n\\]\n(correspinding to ballistic motion)\nLong time (\\(\\gamma t \\gg 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx \\frac{2 k_B T}{\\gamma} t\n\\]\n(corresponding to diffusive motion)\n\n\n\n\n\n\n\nCautionwhy?\n\n\n\n\n\nTo get the short and long time behaviour note that\n (\\(\\gamma t \\ll 1\\)):\n\\[\n\\begin{aligned}\ne^{-\\gamma t} &\\approx 1 - \\gamma t + \\tfrac{1}{2}\\gamma^2 t^2 + \\cdots, \\\\[4pt]\n1 - e^{-\\gamma t} &\\approx \\gamma t - \\tfrac{1}{2}\\gamma^2 t^2, \\\\[4pt]\n(1 - e^{-\\gamma t})^2 &\\approx (\\gamma t)^2.\n\\end{aligned}\n\\]\nSubstituting into the MSD expression: \\[\n\\begin{aligned}\n\\frac{(v_0^2 - k_B T)}{\\gamma^2}(1 - e^{-\\gamma t})^2\n&\\approx (v_0^2 - k_B T)t^2, \\\\[6pt]\n\\frac{2k_B T}{\\gamma}\\left[t - \\frac{1 - e^{-\\gamma t}}{\\gamma}\\right]\n&\\approx \\frac{2k_B T}{\\gamma}\\left(\\tfrac{1}{2}\\gamma t^2\\right)\n= k_B T t^2.\n\\end{aligned}\n\\]\nAdding both contributions: \\[\n\\boxed{\\langle [x(t) - x_0]^2 \\rangle \\approx v_0^2 t^2,}\n\\] which corresponds to ballistic motion (distance travelled is proportional to time).\n (\\(\\gamma t \\gg 1\\)):\n\\[\n\\begin{aligned}\ne^{-\\gamma t} &\\to 0, \\\\[4pt]\n\\frac{(v_0^2 - k_B T)}{\\gamma^2}(1 - e^{-\\gamma t})^2\n&\\to \\frac{(v_0^2 - k_B T)}{\\gamma^2}, \\\\[6pt]\n\\frac{2k_B T}{\\gamma}\\left[t - \\frac{1 - e^{-\\gamma t}}{\\gamma}\\right]\n&\\to \\frac{2k_B T}{\\gamma}\\left(t - \\frac{1}{\\gamma}\\right)\n= \\frac{2k_B T}{\\gamma}t - \\frac{2k_B T}{\\gamma^2}.\n\\end{aligned}\n\\]\nNeglecting constant terms at large \\(t\\) gives: \\[\n\\boxed{\\langle [x(t) - x_0]^2 \\rangle \\simeq \\frac{2k_B T}{\\gamma}\\, t,}\n\\] which corresponds to (distance travelled is proportional to \\(\\sqrt{t}\\)) with diffusion constant \\(D = \\tfrac{k_B T}{\\gamma}\\).\n\n\n\n\nThe effective diffusion constant is:\n\\[\nD = \\frac{k_B T}{\\gamma}\n\\]\nThis is the Einstein relation, connecting the rate of diffusion to temperature and damping. It is useful as it allows an explicit expression for the diffusion constant if one knows \\(\\gamma\\). A famous example is a sphere: the equation for fluid flow past a moving sphere may be solved and yields \\(\\gamma=6\\pi\\eta a\\) where \\(a\\) is the radius of the sphere and here \\(\\eta\\) is the fluid viscosity. This gives\n\\[\nD=\\frac{k_BT}{6\\pi\\eta a}\n\\] which is the Stokes-Einstein formula for the diffusion constant of a colloidal particle.\n\n\n9.2.5 External Forces and Mobility\nNow consider a charged particle with charge \\(q\\) under an external electric field \\(E\\). The Langevin equation becomes:\n\\[\nm \\dot{v} = -\\gamma v + qE\n\\]\nAt long times, the particle reaches a steady drift velocity:\n\\[\n\\langle v \\rangle = \\frac{qE}{\\gamma} = \\frac{qED}{k_B T}\n\\]\nDefining the mobility \\(\\mu\\) by \\(\\langle v \\rangle = \\mu qE\\), we get the Nernst-Einstein relation:\n\\[\n\\mu = \\frac{D}{k_B T}\n\\]\nThis relation connects the response of a system to an external perturbation (mobility) with its internal fluctuations (diffusivity).\n\n\n9.2.6 Molecular Dynamics simulation of Brownian motion for a colloid particle in a liquid suspension\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom numba import njit\n\n# Parameters\nn_fluid = 300\nbox_size = 20.0\nn_steps = 10000\nsigma_f = 1.0\nsigma_c = 10.0\nepsilon = 0.05\nmass_f = 1.0\nmass_c = 2.0\ndt = 1e-5\ndt_e = 1e-5\n\n# Echo the parameter values\nprint(\"Simulation Parameters:\")\nprint(f\"n_fluid = {n_fluid}\")\nprint(f\"box_size = {box_size}\")\nprint(f\"n_steps = {n_steps}\")\nprint(f\"sigma_f = {sigma_f}\")\nprint(f\"sigma_c = {sigma_c}\")\nprint(f\"epsilon = {epsilon}\")\nprint(f\"mass_f = {mass_f}\")\nprint(f\"mass_c = {mass_c}\")\nprint(f\"dt = {dt}\")\n \n # Derived quantities\n\nsigma_f6 = sigma_f ** 6\nsigma_f12 = sigma_f **12\nsigma_cf = 0.5 * (sigma_f + sigma_c)\nsigma_cf6 = sigma_cf ** 6\nsigma_cf12 = sigma_cf ** 12\n\nnp.random.seed(42)\n\n# Safe initialization to avoid overlaps with colloid and other fluid particles\ndef initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos, min_dist_factor=0.85):\n    min_dist_ff = min_dist_factor * sigma_f\n    min_dist_cf = min_dist_factor * 0.5 * (sigma_f + sigma_c)\n    positions = []\n    max_attempts = 20000\n\n    for _ in range(n_fluid):\n        for attempt in range(max_attempts):\n            trial = np.random.rand(2) * box_size\n            too_close = False\n\n            # Check distance to colloid center\n            if np.linalg.norm(trial - colloid_pos[0]) &lt; min_dist_cf:\n                too_close = True\n\n            # Check distances to already placed fluid particles\n            for existing in positions:\n                if np.linalg.norm(trial - existing) &lt; min_dist_ff:\n                    too_close = True\n                    break\n\n            if not too_close:\n                positions.append(trial)\n                break\n        else:\n            raise RuntimeError(\"Failed to place a fluid particle without overlap after many attempts.\")\n    \n    return np.array(positions)\n\ncolloid_pos = np.array([[box_size / 2, box_size / 2]])\nfluid_pos = initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos)\nfluid_vel = (np.random.rand(n_fluid, 2) - 0.5)\ncolloid_vel = np.zeros((1, 2))\n\n@njit\ndef compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid):\n    forces_f = np.zeros_like(fluid_pos)\n    force_c = np.zeros_like(colloid_pos)\n\n    for i in range(n_fluid):\n        # Fluid-colloid interaction\n        rij = fluid_pos[i] - colloid_pos[0]\n        rij -= box_size * np.round(rij / box_size)\n        dist2 = np.dot(rij, rij)\n        if dist2 &lt; (2.5 ** 2) * ((sigma_cf6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n            r2 = dist2\n            r6 = r2 ** 3\n            r12 = r6 ** 2\n            fmag = 48 * epsilon * ((sigma_cf12 / r12) - 0.5 * (sigma_cf6 / r6)) / r2\n            fvec = fmag * rij\n            forces_f[i] += fvec\n            force_c[0] -= fvec\n\n        for j in range(i + 1, n_fluid):\n            rij = fluid_pos[i] - fluid_pos[j]\n            rij -= box_size * np.round(rij / box_size)\n            dist2 = np.dot(rij, rij)\n            if dist2 &lt; (2.5 ** 2) * ((sigma_f6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n                r2 = dist2\n                r6 = r2 ** 3\n                r12 = r6 ** 2\n                fmag = 48 * epsilon * ((sigma_f12 / r12) - 0.5 * (sigma_f6 / r6)) / r2\n                fvec = fmag * rij\n                forces_f[i] += fvec\n                forces_f[j] -= fvec\n\n    return forces_f, force_c\n\n    fluid_history = []\ncolloid_history = []\nforces_f, force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n\nn_equilibration = 2000  # Number of steps to equilibrate before tracking\n\n# Equilibration phase (no history recorded)\n\nfor step in range(n_equilibration):\n    fluid_pos += fluid_vel * dt_e + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt_e + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt_e\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt_e\n    forces_f = new_forces_f\n    force_c = new_force_c\n    # if step &lt; 10:  # Log only the first few steps\n    #     force_mag = np.linalg.norm(force_c[0])\n    #     print(f\"Step {step:3d} | Colloid Pos: {colloid_pos[0]} | Vel: {colloid_vel[0]} | |F|: {force_mag:.4e}\")\n\n# Production phase (history recorded)\n\nfor step in range(n_steps):\n    fluid_pos += fluid_vel * dt + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt\n    forces_f = new_forces_f\n    force_c = new_force_c\n    if step % 10 == 0:\n        fluid_history.append(fluid_pos.copy())\n        colloid_history.append(colloid_pos.copy())\n\nfig, ax = plt.subplots()\n# Calculate figure and plot scale parameters\nfig_width_inch = fig.get_size_inches()[0]\ndpi = fig.dpi\naxis_length_pt = fig_width_inch * dpi\nmarker_scale = 0.1  # Scale factor for visibility\nfluid_marker_size = (marker_scale * axis_length_pt / box_size) ** 2\ncolloid_marker_size = 0.7*(marker_scale * sigma_c / sigma_f * axis_length_pt / box_size) ** 2\nfluid_scatter = ax.scatter([], [], s=fluid_marker_size, c='blue')\ncolloid_scatter = ax.scatter([], [], s=colloid_marker_size, c='red')\ntrajectory, = ax.plot([], [], 'r--', linewidth=1, alpha=0.5)\nax.set_xlim(0, box_size)\nax.set_ylim(0, box_size)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_aspect('equal')\ncolloid_traj = []\n\ndef init():\n    empty_offsets = np.empty((0, 2))\n    fluid_scatter.set_offsets(empty_offsets)\n    colloid_scatter.set_offsets(empty_offsets)\n    trajectory.set_data([], [])\n    return fluid_scatter, colloid_scatter, trajectory\n\ndef update(frame):\n    fluid_scatter.set_offsets(fluid_history[frame])\n    colloid_scatter.set_offsets(colloid_history[frame])\n    colloid_traj.append(colloid_history[frame][0])\n    traj_array = np.array(colloid_traj)\n    trajectory.set_data(traj_array[:, 0], traj_array[:, 1])\n    return fluid_scatter, colloid_scatter, trajectory\n\nani = animation.FuncAnimation(fig, update, frames=len(fluid_history), init_func=init, blit=True, interval=20)\nani.save(\"brownian_colloid.mp4\", writer=\"ffmpeg\", fps=30)\nprint(\"Simulation complete. Video saved as 'brownian_colloid.mp4'.\")\n\n\nMore about the scientists mentioned in this chapter:\nPaul Langevin\nRobert Brown\nAlbert Einstein\nWalther Nernst\nGeorge Stokes",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Langevin and Brownian dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html",
    "href": "phase-transitions/nucleation-and-growth.html",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "",
    "text": "10.1 Introduction to nucleation\nIn previous discussions, we considered first-order phase transitions but deferred a detailed analysis of the dynamical mechanism by which a system evolves from one phase to another. We now address this question explicitly.\nConsider again the Ising model at a temperature \\(T &lt; T_c\\), where the system is initially prepared in the majority spin-up phase at zero external field, \\(H = 0\\). We now examine the system’s response to the application of a small negative external field \\(H &lt; 0\\), which lowers the free energy of the spin-down phase relative to the spin-up phase.\nDespite the global free energy favoring the spin-down phase, the system does not undergo an instantaneous transition. This delay is a consequence of the free energy barrier associated with nucleating a region of the stable phase within the metastable one, as introduced in Section 5.2. The dynamical pathway of the phase transition proceeds via nucleation of localized regions—referred to as droplets—of the stable (spin-down) phase embedded within the metastable (spin-up) background. Once nucleated, these droplets may grow over time and ultimately coalesce to transform the system to the stable phase.\nThe nucleation of a droplet of the stable phase of size \\(n\\) spins entails a competition between bulk and interfacial contributions to the free energy. The bulk free energy gain is linear in \\(n\\), given by \\(-nH\\), due to the alignment of spins with the external field. However, this gain is offset by an interfacial free energy cost arising from broken bonds at the boundary between phases. For the Ising model, each broken bond contributes an energy cost of \\(+2J\\), so the total interfacial energy scales with the perimeter (in 2D) or surface area (in 3D) of the droplet. This interfacial contribution is referred to as the surface tension, and constitutes a true free energy cost: it includes not only the energetic penalty from broken bonds but also an entropic contribution due to the configurational degrees of freedom associated with the droplet shape.\nThe resulting competition between the extensive free energy gain and the sub-extensive interfacial cost leads to a free energy barrier for droplet formation. Only fluctuations that produce a droplet larger than a critical size \\(n_c\\) will grow; smaller droplets will shrink. This framework is formalized in classical nucleation theory, which provides a quantitative description of the nucleation rate, critical droplet size, and the associated activation energy barrier.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation, growth and spinodal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#sec-cnt",
    "href": "phase-transitions/nucleation-and-growth.html#sec-cnt",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.2 Classical Nucleation Theory: Homogeneous Nucleation",
    "text": "10.2 Classical Nucleation Theory: Homogeneous Nucleation\nWe now present the framework of classical nucleation theory (CNT) for the case of homogeneous nucleation, in which the nucleation of the stable phase occurs spontaneously and uniformly throughout the bulk of the metastable phase, without the aid of impurities, defects, or surfaces.\nLet us consider a droplet of the stable (spin-down) phase of radius \\(R\\) embedded within the metastable (spin-up) background. The total change in free energy \\(\\Delta F(R)\\) associated with forming such a droplet consists of two competing contributions:\n\nBulk free energy gain: The interior of the droplet consists of \\(V \\sim R^d\\) spins aligned with the external field \\(H &lt; 0\\), leading to a volume free energy change\n\\[\n\\Delta F_{\\text{bulk}}(R) = -|\\Delta f| \\, R^d,\n\\]\nwhere \\(|\\Delta f| \\propto |H|\\) is the free energy density difference between the metastable and stable phases, and \\(d\\) is the spatial dimensionality of the system.\nInterfacial free energy cost: The boundary between the two phases has a surface area scaling as \\(R^{d-1}\\), and incurs a free energy cost proportional to the surface tension \\(\\sigma\\):\n\\[\n\\Delta F_{\\text{surface}}(R) = \\sigma \\, S_d \\, R^{d-1},\n\\]\nwhere \\(S_d\\) is a geometrical factor (e.g., \\(S_2 = 2\\pi\\) in 2D and \\(S_3 = 4\\pi\\) in 3D).\n\nThe total free energy change is therefore given by\n\\[\n\\Delta F(R) = \\sigma \\, S_d \\, R^{d-1} - |\\Delta f| \\, V_d \\, R^d,\n\\]\nwhere \\(V_d\\) is another dimension-dependent constant (eg. \\(4\\pi/3\\) for \\(d=3\\)). This expression (Figure 10.1) exhibits a characteristic maximum at a critical droplet radius \\(R_c\\), obtained by extremizing \\(\\Delta F(R)\\) with respect to \\(R\\):\n\\[\n\\frac{d \\Delta F}{dR} = 0 \\quad \\Rightarrow \\quad R_c = \\frac{(d-1)\\sigma S_d}{d |\\Delta f| V_d}.\n\\]\n\n\n\n\n\n\nFigure 10.1: Free energy barrier \\(\\Delta F(r)\\) for nucleation of a spherical droplet as a function of radius \\(R\\) (schematic)\n\n\n\nThe corresponding free energy barrier for nucleation is\n\\[\n\\Delta F_c = \\Delta F(R_c) = \\frac{(d-1)^{d-1}}{d^d} \\cdot \\frac{(S_d)^d \\, \\sigma^d}{(|\\Delta f|)^{d-1} \\, (V_d)^{d-1}}.\n\\]\nThis barrier must be surmounted by thermal fluctuations in order for a critical nucleus to form and grow. The nucleation rate per unit volume is given (in the Arrhenius approximation) by\n\\[\nI \\sim I_0 \\exp\\left( -\\frac{\\Delta F_c}{k_B T} \\right),\n\\]\nwhere \\(I_0\\) is a prefactor determined by microscopic kinetics, and \\(k_B\\) is Boltzmann’s constant.\n\n10.2.1 Interpretation and Scaling Behavior\nSeveral key features emerge from this analysis:\n\nBarrier scaling: The nucleation barrier \\(\\Delta F_c \\sim \\sigma^d / |\\Delta f|^{d-1}\\) diverges as \\(H \\to 0\\), reflecting the increasing stability of the metastable phase near the coexistence point.\nCritical radius: The critical droplet size \\(R_c \\sim \\sigma / |\\Delta f|\\) also diverges as \\(|\\Delta f| \\to 0\\), indicating that larger fluctuations are required to initiate nucleation close to the coexistence line.\nDimensional dependence: Both \\(\\Delta F_c\\) and \\(R_c\\) exhibit strong dependence on the spatial dimension \\(d\\), with nucleation becoming increasingly suppressed in higher dimensions due to the dominance of interfacial cost.\n\nIn summary, homogeneous nucleation in a first-order transition is governed by a delicate balance between surface tension and bulk free energy gain. Only droplets exceeding a critical size can overcome the barrier and initiate a transition. This sets an intrinsic timescale for the dynamics of phase transformation, which can become extremely long near coexistence due to the exponentially small nucleation rate.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation, growth and spinodal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "href": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.3 Domain growth",
    "text": "10.3 Domain growth\nFollowing successful nucleation of a supercritical droplet, the system enters a regime where the global transformation is driven by the deterministic growth of domains of the stable phase. Such domain growth involves more atoms or molecules attaching to these nuclei, causing them to expand into larger structures (e.g., growing crystals or droplets).\nGrowth rate depends on factors like temperature, concentration, and the availability of building blocks.\nThe shape and structure of the final phase often depend on how growth occurs (e.g., slow growth may form perfect crystals; rapid growth may be irregular).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation, growth and spinodal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "href": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "title": "10  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "10.4 Spinodal decomposition",
    "text": "10.4 Spinodal decomposition\nPreviously we have considered a scenario in which we move our system to a statepoint just inside the coexistence region so that the original phase remains metastable. We then wait for a fluctuation that yields a critical nucleus and subsequent growth.\nNow imagine that rather than positioning the system just inside the coexistence region we move immediately to a state point well inside the coexistence region (cf. Figure 10.2) such that there is no metastable minimum in the free energy. Then there is no nucleation and growth, rather the system is unstable and immediately starts to phase separate at all points in the system. This so called spinodal decomposition.\n\n\n\n\n\n\nFigure 10.2: Schematic phase diagram for a magnet showing the regions in which one observes nucleation and growth, or spinodal decomposition\n\n\n\nFormally the spinodal is located by the locus of points that satisfy \\(\\partial F/\\partial Q=\\partial^2 F/\\partial Q^2=0\\), where \\(F(Q)\\) is the free energy as a function of the order parameter \\(Q\\) at a given temperature. This corresponds to the points at which the metastable minimum is lost and the system can simply run downhill in free energy to the stable phase.\nThe nature of spinodal decomposition and late-time domain growth depends sensitively on whether the order parameter is conserved or not.\n\n\n10.4.1 Non-Conserved Order Parameter Dynamics (Model A)\nIn systems with a non-conserved order parameter, such as the Ising model with single spin flip (so called Glauber) dynamics, the order parameter can relax locally without constraint. Motion of interfaces is driven by local chemical potential differences. In the jargon of the field, this is called ‘model A’ dynamics.\nThe typical domain size \\(L(t)\\) grows algebraically with time:\n\\[\nL(t) \\sim t^{1/2},\n\\]\ncorresponding to a dynamic exponent \\(z = 2\\), where in general \\(L(t)\\sim t^{1/z}\\). The local chemical potential difference across a domain wall depends on its curvature. Thus regions of high curvature (small domains) shrink and are absorbed by larger, flatter ones.\nLet is assume that domain walls move with velocity proportional to curvature: \\[\n  v \\sim \\frac{1}{L}\n  \\] Then with the typical domain size being \\(L(t)\\) it follows that\n\\[\n  \\frac{dL}{dt} \\sim \\frac{1}{L}\n  \\]\nIntegrating both sides yields the \\(t^{1/2}\\) domain growth law.\nIt turns out that the detailed evolution of the coarse-grained order parameter field \\(\\phi(\\mathbf{r}, t)\\) is governed by the Allen–Cahn equation:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = - \\frac{\\delta F[\\phi]}{\\delta \\phi},\n\\] where the derivative on the RHS is the local chemical potential.\n\\(F[\\phi]\\) is a coarse-grained Ginzburg–Landau free energy functional:\n\\[\nF[\\phi] = \\int d^d x \\left[ \\frac{1}{2} (\\nabla \\phi)^2 + V(\\phi) \\right],\n\\]\nwith \\(V(\\phi)\\) typically a double-well potential such as \\(V(\\phi) = \\frac{1}{4}(\\phi^2 - 1)^2\\).n The squared gradient term serves to penalise interfaces in the system.\n\n\n\n10.4.2 Conserved Order Parameter Dynamics (Model B)\nFor systems with a conserved order parameter, such as phase separation in binary alloys or the Ising model with spin-swap (so called ‘Kawasaki’) dynamics, the order parameter (e.g., composition or particle number) must be conserved locally. This imposes a diffusive constraint on the dynamics (so-called ‘model B’ dynamics).\nThe domain size again grows algebraically, but with a different exponent:\n\\[\nL(t) \\sim t^{1/3},\n\\]\ncorresponding to a dynamic exponent \\(z = 3\\).\nA chemical potential difference drives diffusion and is given by \\[\n\\Delta \\mu \\sim \\frac{\\sigma}{L}\n\\] (again due to curvature, where \\(\\sigma\\) is surface tension)\nNow the flux is proportional to the chemical potential gradient (Fick’s law): \\[\n\\text{Flux} \\sim -\\nabla \\mu \\sim \\frac{\\Delta \\mu}{L} \\sim \\frac{\\sigma}{L^2}\n\\]\nand the rate of change of domain size is proportional to this flux: \\[\n\\frac{dL}{dt} \\sim \\frac{1}{L^2}\n\\quad\\Rightarrow\\quad\nL(t) \\sim t^{1/3}\n\\]\nIt turns out that the detailed dynamics are described by the Cahn–Hilliard equation, a continuity equation of the form:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = \\nabla^2 \\left( \\frac{\\delta F[\\phi]}{\\delta \\phi} \\right),\n\\]\nreflecting that the order parameter can only evolve via diffusion of its conjugate chemical potential. This leads to the slow transport of material across domains and a more sluggish coarsening process compared to the non-conserved case.\n\n\n10.4.3 Schematic of Domain Growth in 2D Ising model\nHere are schematic illustrations of domain growth for:\n\nNon-conserved dynamics (Model A): Domains coarsen rapidly, with smoother and larger regions due to free relaxation of the order parameter.\nConserved dynamics (Model B): Coarsening is slower and domains are more intricate, reflecting the constraint of local conservation.\n\n\n\n\n\n\n\n\n\nNon-Conserved Dynamics (Model A)\n\n\n\n\n\n\n\nConserved Dynamics (Model B)\n\n\n\n\n\n\nFigure 10.3: Schematic illustrations of domain morphology resulting from spinodal decomposition or late-time domain growth for non-conserved and conserved dynamics.\n\n\n\n\n\n10.4.4 Dynamic Scaling Hypothesis\nAt late times, both conserved and non-conserved systems exhibit dynamic scaling: the statistical properties of the domain morphology become self-similar under rescaling of lengths by \\(L(t)\\).\nFor example, the equal-time two-point correlation function satisfies\n\\[\nC(r, t) = f\\left(\\frac{r}{L(t)}\\right),\n\\]\nwhere \\(f(x)\\) is a time-independent scaling function. Plots of \\(C(r, t)\\) collapse when plotted as a function of \\(r/L(t)\\).\nThe structure factor \\(S(k, t)\\), which is the Fourier transform of the correlation function, is experimentally accessible eg via X-ray or neutron scattering, and also obeys dynamic scaling:\n\\[\nS(k, t) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r, t).\n\\]\nSubstituting the scaling form of \\(C(r, t)\\) into this expression, and changing variables to \\(\\vec{u} = \\vec{r}/L(t)\\), gives:\n\\[\nS(k, t) = L(t)^d \\int d^d u \\, e^{-i \\vec{k} \\cdot L(t) \\vec{u}} \\, f(u) = L(t)^d \\, g(kL(t)),\n\\]\nwith \\(g(x)\\) a universal scaling function dependent on the dynamical class and dimensionality.\n\n\n10.4.5 Summary of Growth Laws\n\n\n\n\n\n\n\n\n\n\nDynamics Type\nConservation\nEquation Type\nGrowth Law\nDynamic Exponent\n\n\n\n\nModel A (e.g. Glauber)\nNo\nAllen–Cahn\n\\(L(t) \\sim t^{1/2}\\)\n\\(z = 2\\)\n\n\nModel B (e.g. Kawasaki)\nYes\nCahn–Hilliard\n\\(L(t) \\sim t^{1/3}\\)\n\\(z = 3\\)\n\n\n\n\n\nRemarks: The domain growth exponents \\(1/z\\) are robust under many conditions, but can be modified in the presence of disorder, long-range interactions, or hydrodynamic effects.\n\n\nIn both the model A and model B cases, the system coarsens until it reaches equilibrium, characterized by a uniform macroscopic phase and the complete elimination of interfaces.\n\n\nThe approach to equilibrium is algebraically slow (described by power laws) due to the scale-free nature of domain dynamics.\n\nMore about the scientists mentioned in this chapter:\nJohn Cahn",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Nucleation, growth and spinodal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html",
    "href": "phase-transitions/precourse-reading.html",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Ensembles and free energies\nComplex disordered systems are composed of an enormous number of interacting components—typically on the order of \\(\\sim 10^{23}\\). These interactions can lead to fascinating emergent behaviour, but they also render the systems analytically intractable; it is clearly impossible to solve Newton’s equations for such vast numbers of particles. To address this difficulty, we turn to Statistical Mechanics, which you first encountered in your second year. Statistical Mechanics provides the essential framework for connecting the microscopic behaviour of individual constituents with the macroscopic thermodynamic and dynamical properties of the system as a whole.\nIn this section, we will revisit and expand upon key concepts relevant to our discussion, with particular emphasis on the free energy—a central quantity that captures the balance between energy minimisation and entropy maximisation in determining the system’s equilibrium state. If any of these ideas feel unfamiliar, you may find it useful to revise the Statistical Mechanics material from your Year 2 Thermal Physics course notes.\nStatistical mechanics can be formulated in a variety of ensembles reflecting the relationship between the system and its environment. In what follows we summarise the formalism, focussing on the case of a particle fluid. Analogous equations apply to lattice spin models (see lectures and the book by Yeomans). Key ensembles are:",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "href": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Microcanonical ensemble\nApplies to a system of \\(N\\) particles (or spins) in a fixed volume \\(V\\) having adiabatic walls so that the internal energy \\(E\\) is constant. Denoted as constant-\\(NVE\\). Let \\(\\Omega\\) be the number of (micro)states having the prescribed energy:\n\\[\n\\Omega=\\sum_\\textrm{all states having energy E}\n\\]\nThermodynamically, the states favored in the canonical ensemble are those that maximise the entropy:\n\\[\nS=k_B\\ln \\Omega\\: .\n\\]\nwhere \\(k_B\\) is Boltzmann’s constant The microcanonical ensemble is useful for defining the entropy, but is little used in practice.\n\n\nCanonical ensemble\nApplies to a system of \\(N\\) particles in a fixed volume \\(V\\) and coupled to a heat bath at temperature \\(T\\). Denoted as constant-\\(NVT\\). A central quantity is the partition function\n\\[\nZ_{NVT}=\\sum_\\textrm{ all states i}e^{-\\beta E_i},~~~~~\\beta=1/(k_BT)\n\\tag{1}\\] which is a weighted sum over the states. The partition function provides the normalisation constant in the probability of finding the system in a given state \\(i\\).\n\\[\nP_i=\\frac{e^{-\\beta E_i}}{Z_{NVT}}.\n\\tag{2}\\]\nThe states favored in the canonical ensemble are those that minimise the free energy:\n\\[\nF_{NVT}=-\\beta^{-1}\\ln Z_{NVT}\\:.\n\\]\n\\(F_{NVT}\\) is known as the Helmholtz free energy. Thermodynamics also supplies a relation for the Helmholtz free energy:\n\\[\nF_{NVT}=E-TS\\:,\n\\] where \\(E\\) is the average internal energy. In minimising the free energy, the system strikes a compromise between low energy and high entropy. The temperature plays the role of arbiter, favouring high entropy at high \\(T\\), and low energy at low \\(T\\). The canonical ensemble is usually used to describe systems such as magnets, or a fluid held at constant volume. It is the ensemble we shall use most in this course.\n\n\nGrand canonical ensemble\nApplies to a system with a variable number of particle in a fixed volume \\(V\\) coupled to both a heat bath at temperature \\(T\\) and a particle reservoir with chemical potential \\(\\mu\\) (which is the field conjugate to \\(N\\)). Denoted as constant-\\(\\mu VT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty e^{\\beta\\mu N}Z_{NVT}\n\\] and a state probability analogous to Equation 2 holds. One can recast this in a form similar to Equation 1:\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty\\:\\sum_\\textrm{all~states~i}e^{-\\beta {\\cal H}_i},\n\\tag{3}\\] where \\({\\cal H}_i=E_i-\\mu N\\) is the form of the Hamiltonian in the grand canonical ensemble.\nStatistically, the states favored in the grand canonical ensemble are those that minimise the free energy:\n\\[\nF_{\\mu VT}=-\\beta^{-1}\\ln Z_{\\mu VT}\n\\] \\(F_{\\mu VT}\\) is known as the grand potential. It can also be derived from thermodynamics, from which one finds\n\\[\nF_{\\mu VT}=E-TS-\\mu N=-pV,\n\\] where \\(p\\) is the pressure.\nThe grand canonical ensemble is usually used to describe systems such as fluid connected to a particle reservoir. Sometimes for a magnet we consider the effects of an applied magnetic field, which is analogous to working in the grand canonical ensemble: the magnetic field (which is conjugate to the magnetisation) plays a similar role to the chemical potential in a fluid.\n\n\nIsothermal-isobaric ensemble\nApplies to a system with a fixed number of particles \\(N\\) that is coupled to a heat bath at temperature \\(T\\) and a reservoir that exerts a constant pressure \\(p\\) which allows the sample volume to fluctuate. Denoted as constant-\\(NpT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}\n\\] or \\[\nZ_{NpT}=\\int_0^\\infty dV\\:\\sum_\\textrm{i}e^{-\\beta {\\cal H}_i},\n\\tag{4}\\] where \\({\\cal H}_i=E_i+pV\\) is the form of the Hamiltonian in the constant-\\(NpT\\) ensemble. Again a state probability analogous to Equation 2 holds.\nStatistically, the states favored in the constant-\\(NpT\\) ensemble are those that minimise the free energy:\n\\[\nF_{NpT}=-\\beta^{-1}\\ln Z_{NpT}\n\\] \\(F_{NpT}\\) is known as the Gibb’s free energy (often denoted \\(G\\)). It can also be derived from thermodynamics, from which one finds\n\\[\nF_{NpT}=E-TS+pV=\\mu N\n\\]\nThe constant-\\(NpT\\) ensemble is usually used to describe systems such as a fluid subject to a variable pressure, or a magnet coupled to a magnetic field \\(H\\). In the latter case the quantity \\(HM\\) plays the role of \\(pV\\) and\n\\[\nF_{NpT}=E-TS-MH\\:,\n\\] with \\(M\\) the total magnetisation.",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "href": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "title": "Tools for understanding complex disordered matter",
    "section": "From free energies to observables",
    "text": "From free energies to observables\nFree energies are not directly observable quantities. However, all physical observables can be expressed in terms of derivatives of the free energy. One can derive the appropriate relations either from Thermodynamics, or the corresponding statistical mechanics (Revise your year-2 Thermal Physics notes on this if necessary). As an example let us consider a fluid in the isothermal-isobaric ensemble for which the appropriate free energy is \\(F_{NpT}=E-TS+pV\\), and where the volume fluctuates in response to the prescribed pressure. We shall seek an expression for the average volume in terms of the free energy. First lets us take the thermodynamic route. Differentiating the free energy and applying the chain rule we have:\n\\[\ndF=dE-TdS-sdT+pdV+VdP\\:.\n\\] But from the first law of thermodynamics, \\(dE=TdS-pdV\\), so\n\\[\ndF=-SdT+Vdp\\:,\n\\] and rearranging yields \\[\nV=\\left(\\frac{\\partial F}{\\partial p}\\right)_T\\:.\n\\]\nWe can now show that this result is consistent with the definition of \\(F_{NpT}\\) in terms of the partition function. Write\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}=\\int_0^\\infty dV\\sum_{all~states~i}e^{-\\beta (p V_i+E_i)}\n\\]\nThen\n\\[\\begin{align}\n\\left(\\frac{\\partial F}{\\partial p}\\right)_T\n&= -\\frac{1}{\\beta} \\left(\\frac{\\partial \\ln Z_{NpT}}{\\partial p}\\right)_T \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\frac{\\partial Z_{NpT}}{\\partial p} \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\int_0^\\infty dV \\int_{\\text{all states}} (-\\beta V) e^{-\\beta (p V + E)} \\\\\n&= \\langle V \\rangle_T \\,.\n\\end{align}\\]\nwhere in the last step we have used the fact that the probability of a state is defined to be \\(e^{-\\beta (p V_i+E_i)}/Z_{NpT}\\).\nExercise. Repeat these manipulations to find an expression for the mean particle number \\(N\\) in the grand canonical ensemble\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nIn the grand canonical ensemble (GCE), the relevant free energy is\n\\[\nF_{\\mu VT} = E - TS - \\mu N\n\\]\nFrom the first law of thermodynamics changes in the internal energy are given by:\n\\[\ndE = TdS - PdV + \\mu dN=TdS+\\mu dN\n\\] where we have used the fact that \\(V\\) is fixed in the GCE, so \\(dV=0\\).\nDifferentiating \\(F_{\\mu VT}\\):\n\\[\ndF_{\\mu VT} = dE - TdS - SdT - \\mu dN - N d\\mu\n= -S dT - N d\\mu\n\\] where for the last equality we have substitued for \\(dE\\) from above.\nThus \\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V} = -N\n\\quad \\Rightarrow \\quad\n\\langle N \\rangle = -\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V}\n\\]\nNow consider the statistical mechanics route to calculate \\(\\langle N\\rangle\\):\n\\[\nZ_{\\mu V T} = \\sum_{N=0}^{\\infty} \\sum_{\\text{states}} e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nThe grand potential (now written as \\(F_{\\mu VT}\\)) is:\n\\[\nF_{\\mu VT} = -k_B T \\ln Z_{\\mu V T}\n\\]\nWe now differentiate:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\left( \\frac{1}{Z} \\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} \\right)\n\\]\nFrom the partition function\n\\[\n\\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} = \\sum_{N=0}^\\infty \\sum_{\\text{states}} \\left( \\beta N \\right) e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nSubstitute:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\cdot \\beta \\cdot \\frac{1}{Z_{\\mu V T}} \\sum_{N=0}^\\infty \\sum_{\\text{states}} N\\, e^{-\\beta (E_{N,i} - \\mu N)} = - \\langle N \\rangle\n\\]\nwhere in the last step we have used the fact that in the GCE the Boltzmann probability of a microstate is defined to be \\(e^{-\\beta (E_{N,i}-\\mu N)}/Z_{\\mu VT}\\).",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html",
    "href": "phase-transitions/problems.html",
    "title": "Unifying concepts: Problems",
    "section": "",
    "text": "Although you should try all of these questions, some of them are deliberately quite challenging. If you don’t get very far with some, don’t worry. We’ll be going over them in problems classes, so you can just regard them as worked examples.\n\n1. Existence of a phase transition in \\(d=2\\).\nIn lectures it was argued that no long ranged order occurs at finite-temperatures in a one dimensional system because of the presence of domain walls. Were macroscopic domain walls to exist in two dimensions at finite temperature, they would similarly destroy long ranged order and prevent a phase transition. By calculating the free energy of a 2D domain wall for an Ising lattice, show that domain walls do not in fact exist for sufficiently low \\(T\\).\n(Hint: Model the domain wall as a non-reversing \\(N\\)-step random walk on the lattice and find an expression for its energy and -from the number of random walk configurations- its entropy.)\n\n\n\n2. Correlation Length\nFor a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small ((as occurs at low \\(T\\))?\n\n\n\n3. A model fluid\nThe van der Waals (vdW) equation of state is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nWrite a Python script to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of \\(P\\) vs \\(V\\) on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\n4. Mean field theory of the Ising model heat capacity\nUsing results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle E\\rangle/\\partial T\\) has the behaviour\n\\[\n\\begin{aligned}\nC_H=& 0 \\quad T&gt;T_c\\\\\nC_H=& 3Nk_B/2 \\quad T\\le T_c\n\\end{aligned}\n\\] —\n\n\n5. Magnetisation and fluctuations\nA system of spins on a lattice in the presence of an applied field \\(h\\), has a Hamiltonian \\[\n{\\cal H}=E - hM\n\\] where \\(E\\) is the spin-spin interaction energy, \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\n\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\n\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\n6. Spin-1 Ising model\nA set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j - h\\sum_i s_i\\]\nFind the partition function in the mean field approximation and hence show that in the same approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\n7. Transfer Matrix.\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\n8. Landau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\n9. Scaling equation of state\nConsider a Landau expression for the free energy of a magnetic system having magnetisation \\(m\\):\n\\[\nF=F_0+\\tilde{a}_2tm^2+a_4m^4-Hm\\:,\n\\] where \\(t=T-T_c\\) and \\(H\\) is an applied magnetic field; \\(\\tilde{a}_2\\) and \\(a_4\\) are positive constants and \\(F_0\\) is a constant background term.\nShow that the equation of state for the model is\n\\[\nH=2\\tilde{a}_2tm+4a_4m^3\\:.\n\\]\nUse the near-critical power law behaviour of \\(m\\) to show that the equation of state may be written in the scaling form\n\\[\n\\frac{H}{m^\\delta}=g\\left(\\frac{t}{m^{1/\\beta}}\\right)\\:,\n\\] and find the (mean field) values of the critical exponents \\(\\delta\\) and \\(\\beta\\).\nDeduce that \\(g(x)=x+1\\) up to a choice of scale for \\(\\tilde{a}_2\\) and \\(a_4\\).\n\n\n\n10. Scaling laws\nUsing the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\n\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\n\\]\nHence derive the scaling laws among the critical exponents:\n\\[\n\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=& 2 \\\\\n\\alpha+2\\beta+\\gamma =& 2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\n\\] (Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 8: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\n\n\n\n11. Classical nucleation theory\nA supercooled liquid metal is undergoing solidification. According to classical nucleation theory, the Gibbs free energy change \\(\\Delta G\\) for forming a spherical solid nucleus of radius \\(r\\) in the liquid is given by:\n\\[\n\\Delta G(r) = \\frac{4}{3}\\pi r^3 \\Delta G_v + 4\\pi r^2 \\sigma\n\\] where \\(\\Delta G_v &lt; 0\\) is the free energy change per unit volume due to the phase change, and \\(\\sigma &gt; 0\\) is the interfacial free energy (“surface tension”) between the solid and liquid phases.\n(a) Derive the expression for the critical radius \\(r^*\\) at which the nucleus becomes stable and begins to grow.\n(b) Show that the critical energy barrier for nucleation \\(\\Delta G^*\\) is given by:\n\\[\n\\Delta G^* = \\frac{16\\pi \\sigma^3}{3 (\\Delta G_v)^2}\n\\]\n(c) Explain qualitatively how the degree of undercooling \\(\\Delta T\\) affects the rate of nucleation. You may use the fact that \\(\\Delta G_v \\propto \\Delta T\\) to support your answer.\n\n\n\n12. Colloidal diffusion\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + \\frac{e^{-\\gamma t}}{M} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{k_BT}{M} + \\left( u_0^2 - \\frac{k_BT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k_B\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\n\n13. Einstein’s expression for the diffusion coefficient\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle [X(t) - X(0)]^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nUse the fact that \\(X(t) - X(0)\n= \\int_{0}^{t} u(t')\\,\\mathrm{d}t'\\) to show that the Einstein relation may be written as\n\\[\n\\mu = \\frac{1}{\\gamma} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\n\n14. Master equation\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\n\n15. Detailed balance\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\n\n16. Jump processes\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump reversibly between states \\(1\\) and \\(2\\) and between states \\(2\\) and \\(3\\) but not directly between \\(1\\) and \\(3\\). Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the rate matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_partial.html",
    "href": "phase-transitions/Solutions_partial.html",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "",
    "text": "1. Existence of a phase transition in \\(d=2\\).\nHere we present outline solutions to the problems.\nConsider the simplest elementary excitation that will destroy long range order in the 2d system: a domain wall of \\(N\\) segments which divides an Ising system of \\(L\\times L\\) spins into a spin up and a spin down part.\nThe associated energy cost is \\(2JN \\equiv \\Delta E\\).\nTo evaluate the entropy gain due to a domain wall in the system we have to estimate \\(\\Omega\\) the number of possible paths for the domain wall. If we start at the left hand side then there are \\(L\\) starting positions. At each step the domain wall can move to the right, move up or move down. This implies that the number of domain walls is approximately\n\\[\n\\Omega\\approx L3^N\n\\] Hence the entropy gain is:\n\\[\n\\Delta S=Nk_B\\ln 3+k_B\\ln L\\approx Nk_B\\ln 3  \n\\]\nAccordingly, the change in the free energy associated with inserting such a domain wall into an ordered system is\n\\[\n\\Delta F=\\Delta E-T\\Delta S= N(2J-k_BT\\ln 3)\n\\]\nFor small enough \\(T &lt;2J/(k_B\\ln 3)\\), the free energy change is positive. Thus the ordered phase is free energetically stable against formation of a wall. Accordingly there will be a non zero value for \\(T_c\\) in two dimensions.",
    "crumbs": [
      "Unifying concepts",
      "Solutions to problems"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_partial.html#einsteins-expression-for-the-diffusion-coefficient",
    "href": "phase-transitions/Solutions_partial.html#einsteins-expression-for-the-diffusion-coefficient",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "13. Einstein’s expression for the diffusion coefficient",
    "text": "13. Einstein’s expression for the diffusion coefficient\nStart from Einstein’s definition\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle [X(t) - X(0)]^2 \\rangle,\n\\qquad\nX(t) - X(0) = \\int_0^t u(t')dt'.\n\\]\nHence\n\\[\n\\langle [X(t) - X(0)]^2 \\rangle\n= \\int_0^t  \\int_0^t \\langle u(t_1) u(t_2) \\rangle\\, dt_1 dt_2\n= \\int_0^t \\int_0^t C(|t_1 - t_2|)\\, dt_1 dt_2,\n\\]\nwhere \\(C(\\tau) = \\langle u(t_0) u(t_0 + \\tau) \\rangle\\) is the stationary (ie. equilibrium) velocity autocorrelation function which depends solely on the lag \\(\\tau=|t_1-t_2|\\). The modulus is because in equilibirum the velocity autocorrelation is time symmetric i.e. flipping the sign of time doesn’t change the average of the correlation of velocities.\n\n\n\n\n\n\nFigure 4: Change of variables from \\(t1,t@\\) to the lad \\(\\tau=|t1-t2|\\)\n\n\n\nNow for fixed \\(\\tau = |t_1 - t_2|\\), the square \\([0,t]^2\\) splits into two equal triangles: one where \\(t_1 &gt; t_2\\) and one where \\(t_2 &gt; t_1\\).\nFor each \\(\\tau\\), the allowable range of \\(t_1\\) (or \\(t_2\\)) is \\(t - \\tau\\) (because if \\(t_1 \\in [\\tau, t]\\), then \\(t_2 = t_1 - \\tau \\in [0, t - \\tau]\\)).\nTherefore,\n\\[\n\\int_0^t \\int_0^t C(|t_1 - t_2|)\\, dt_1 dt_2\n= 2 \\int_0^t (t - \\tau) C(\\tau)\\, d\\tau.\n\\]\nThe factor of \\(2\\) comes from the two triangles, and the \\((t - \\tau)\\) is the length of each diagonal slice corresponding to lag \\(\\tau\\).\nFor \\(t\\) much larger than the correlation time of \\(C(\\tau)\\), we can approximate \\((t - \\tau) \\approx t\\), and set the upper limit to \\(\\infty\\) giving\n\\[\n\\langle [X(t) - X(0)]^2 \\rangle \\approx 2t \\int_0^\\infty C(\\tau)\\, d\\tau,\n\\]\nso that\n\\[\nD = \\int_0^\\infty \\langle u(t_0) u(t_0 + \\tau) \\rangle\\, d\\tau.\n\\]\nFinally, from \\(D = \\mu kT\\) and \\(\\mu = 1 / \\gamma\\), we have\n\\[\n\\mu = \\frac{D}{kT}\n= \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle, dt,\n\\qquad\n\\gamma = \\frac{kT}{D}.\n\\]\n\n\n14. Master equation\nThe rates of change of \\(n_+\\) and \\(n_-\\) are given by the master equations:\n\\[\n\\frac{dn_+}{dt} = -n_+ \\lambda_{+-} + n_- \\lambda_{-+};\n\\]\n\\[\n\\frac{dn_-}{dt} = n_+ \\lambda_{+-} - n_- \\lambda_{-+}.\n\\] where since the particles are independent we have written \\(n_i=Np_i\\).\nAt equilibrium the right-hand sides of the above master equations must vanish. Hence the ratio of the transition probabilities is given by:\n\\[\n\\left. \\frac{\\lambda_{-+}}{\\lambda_{+-}} = \\frac{n_+}{n_-} \\right|_{\\text{eq}} = e^{-2\\beta \\varepsilon}.\n\\]\nwith \\(\\beta = 1/k_BT\\), as usual.\nFollowing the lead in the question, we define \\(n = n_- - n_+\\). Subtracting one of the above master equations from the other, we obtain:\n\\[\n\\frac{dn}{dt} = 2n_+ \\lambda_{+-} - 2n_- \\lambda_{-+}\n\\]\nThen, noting that \\(N = n_- + n_+\\), we have that:\n\\[\nn_- = \\frac{N + n}{2} \\quad \\text{and} \\quad n_+ = \\frac{N - n}{2},\n\\]\nallowing to rewrite as:\n\\[\n\\frac{dn}{dt} = -n(\\lambda_{+-} + \\lambda_{-+}) + N(\\lambda_{+-} - \\lambda_{-+})\n\\]\nThe equilibrium value of \\(n(t)\\) follows by setting \\(\\frac{dn}{dt}=0\\) to find\n\\[\nn_{\\text{eq}} = N \\frac{\\lambda_{+-} - \\lambda_{-+}}{\\lambda_{+-} + \\lambda_{-+}}.\n\\]\nRewrite the differential equation in the suggestive form:\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) - \\frac{N(\\lambda_{+-} - \\lambda_{-+})}{\\lambda_{+-} + \\lambda_{-+}} \\right]\n\\]\nwhere\n\\[\n\\tau = \\frac{1}{\\lambda_{+-} + \\lambda_{-+}}.\n\\]\nInserting the result (above) for \\(n_{\\text{eq}}\\) yields\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) - n_{\\text{eq}} \\right],\n\\]\nMaking a change of variables and separating variables in the standard way, one finds the solution to be:\n\\[\nn(t) = n_{\\text{eq}} + e^{-t/\\tau} [n(0) - n_{\\text{eq}}].\n\\]\nYou should note the behaviour of this solution as \\(t \\to 0\\) and \\(t \\to \\infty\\).\n\n\n\n15. Detailed balance\n(a) For individual states, the rates of transition are:\n\\[\n1 \\rightarrow 2 \\quad = \\quad v_{12} p_1\n\\]\n\\[\n2 \\rightarrow 1 \\quad = \\quad v_{21} p_2\n\\]\nDefine groups of states:\nGroup A: \\(\\alpha = 1, \\cdots, N\\)\nGroup B: \\(\\beta = 1, \\cdots, M\\)\nThen\n\\[\n\\sum_{\\alpha=1}^N p_\\alpha = p_A \\quad ; \\quad \\sum_{\\beta=1}^M p_\\beta = p_B\n\\]\n(Note: \\(p_A \\ne p_B\\))\nOverall rate\n\\[\nA \\rightarrow B = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^m v_{\\alpha \\beta} p_\\alpha\n\\]\nOverall rate\n\\[\nB \\rightarrow A = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^m v_{\\beta \\alpha} p_\\beta\n\\]\nwhere\n\\(v_{\\alpha \\beta} = v_{\\beta \\alpha}\\) : jump rate symmetry\n\\(p_\\alpha = p_\\beta\\) : principle of equal a priori probabilities (for microstates)\nHence:\n\\[\n\\text{rate}(A \\rightarrow B) \\equiv v_{AB} p_A^{eq}\n\\]\n\\[\n\\text{rate}(B \\rightarrow A) \\equiv v_{BA} p_B^{eq}\n\\]\nare equal in equilibrium.\n(b)\nThe probability \\(p_A\\) of a group of states \\(A\\) is given by\n\\[\np_A = \\frac{e^{-\\beta F_A}}{Z},\n\\]\nfor the canonical ensemble, and a similar result may be written for group \\(B\\).\nAs this system is in the canonical ensemble, we can assume that it is in a large heat bath, with the ‘system + heat bath’ being treated as ‘isolated’.\nTake \\(A\\) to be the microstate \\(i\\) of the system + any state of the heat bath, such that\n\\[\nE_{system} + E_{reservoir} = E \\, \\text{(fixed)}.\n\\]\nFor a large reservoir,\n\\[\np_i = \\frac{e^{-\\beta E_i}}{Z} = \\textrm{``$p_A$''},\n\\]\nwith a similar result for \\(B\\).\nHence the result of part (a), that\n\\[\n\\text{Rate}(A \\rightarrow B) = \\text{Rate}(B \\rightarrow A)\n\\]\nmeans here that\n\\[\nv_{ij} p_i = v_{ji} p_j,\n\\]\neven though, for states of different system energy \\(E_i\\),\n\\[\np_i \\ne p_j \\quad \\text{and} \\quad v_{ij} \\ne v_{ji}.\n\\]\nThis implies:\n\\[\n\\frac{v_{ij}}{v_{ji}} = \\frac{p_j}{p_i} = e^{\\beta(E_j - E_i)}\n\\]\nThis result has important implications for Monte Carlo simulation\n\n\n\n16. Jump processes\n(a) Label by \\(p_i(t)\\) the probability to be in state \\(i\\) at time \\(t\\). From the question:\n\nFrom state 1 one can only jump to 2 with rate \\(\\lambda_0\\).\nFrom state 2 one can jump to 1 or to 3, each with rate \\(\\lambda_0\\).\nFrom state 3 one can only jump to 2 with rate \\(\\lambda_0\\).\n\nHence the gain–loss equations are \\[\n\\begin{aligned}\n\\dot p_1 &= -\\lambda_0\\,p_1 + \\lambda_0\\,p_2,\\\\\n\\dot p_2 &= \\lambda_0\\,p_1 - 2\\lambda_0\\,p_2 + \\lambda_0\\,p_3,\\\\\n\\dot p_3 &= \\lambda_0\\,p_2 - \\lambda_0\\,p_3.\n\\end{aligned}\n\\]\nWriting \\(\\mathbf p=(p_1,p_2,p_3)^{\\!T}\\), one checks immediately that \\[\n\\dot{\\mathbf p}\n\\;=\\;\n\\lambda_0\n\\begin{pmatrix}\n-1 & 1 & 0\\\\\n1 & -2 & 1\\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\mathbf p\n\\;=\\;M\\,\\mathbf p,\n\\] as claimed. Note that the columns of \\(M\\) sum to zero, ensuring \\(\\sum_i\\dot p_i=0\\) (probability conservation).\n(b) An equilibrium state \\(\\mathbf p^\\textrm{ eq}\\) satisfies \\[\nM\\,\\mathbf p^\\textrm{ eq}=0,\n\\quad\n\\sum_{i=1}^3 p^\\textrm{ eq}_i=1.\n\\] From the first row of \\(M\\,\\mathbf p=0\\) we get \\[\n-\\,p^\\textrm{ eq}_1 + p^\\textrm{ eq}_2 =0\n\\;\\Longrightarrow\\;\np^\\textrm{ eq}_1 = p^\\textrm{ eq}_2.\n\\] From the third row, \\[\np^\\textrm{ eq}_2 - p^\\textrm{ eq}_3=0\n\\;\\Longrightarrow\\;\np^\\textrm{ eq}_2 = p^\\textrm{ eq}_3.\n\\] Hence all three components are equal. Normalizing, \\[\np^\\textrm{ eq}_1 = p^\\textrm{ eq}_2 = p^\\textrm{ eq}_3 = \\tfrac13,\n\\] i.e.  \\[\n\\mathbf p^\\textrm{ eq} = \\bigl(\\tfrac13,\\tfrac13,\\tfrac13\\bigr).\n\\]\n(c)\nWe have \\[\nM = \\lambda_0\n\\begin{pmatrix}\n-1 & 1 & 0\\\\\n1 & -2 & 1\\\\\n0 & 1 & -1\n\\end{pmatrix}.\n\\] Its characteristic polynomial is \\[\n\\det(M -\\mu I)\n=-\\mu(\\mu+\\lambda_0)(\\mu+3\\lambda_0),\n\\] so the eigenvalues are \\[\n\\mu_1=0,\\quad \\mu_2=-\\lambda_0,\\quad \\mu_3=-3\\lambda_0.\n\\] Since the zero‐eigenvalue has multiplicity one, the null space of \\(M\\) is one-dimensional. Imposing \\(\\sum_i p_i=1\\) picks out a unique vector in that null space. Hence the equilibrium \\((\\tfrac13,\\tfrac13,\\tfrac13)\\) is the only steady-state probability vector.",
    "crumbs": [
      "Unifying concepts",
      "Solutions to problems"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html",
    "href": "phase-transitions/assignment_October2025.html",
    "title": "PHYSM0071: First coursework assignment",
    "section": "",
    "text": "1. Introduction and background\nIn this coursework assignment, you will explore the phenomena of spatial correlations and their relationship to phase behaviour in a simple lattice gas model. This model is a crude representation of a fluid in which particles can occupy the sites of a hypercubic lattice. The occupancy of a site \\(i\\) is specified by the variable \\(c_i=1\\) (occupied) or \\(c_i=0\\) vacant. The complete list of these occupancies \\(\\{c\\}\\) specifies a microstate. The instantaneous particle number density (fraction of occupied sites) is given by\n\\[\n\\rho=L^{-d}\\sum_i c_i\n\\]\nwhere \\(L\\) is the linear extent of the lattice (i.e. the number of unit cells along a coordinate axis) and \\(d\\) its dimensionality.\nWithin the Grand Canonical ensemble (see precourse reading) the Hamiltonian of the lattice gas model is\n\\[{\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j - \\mu\\sum_ic_i\\]\nwhere \\(\\epsilon\\) is an attraction energy between a pair of particles on adjacent (nearest neighbouring) sites and \\(\\mu\\) is a field known as the chemical potential which couples to the particle number density \\(\\rho\\). The number density typically fluctuates around a mean value controlled by the prescribed value of \\(\\mu\\).\nOne can also consider the model under conditions in which the particle number is fixed to some prescribed value - then there is no chemical potential and the corresponding set of microstates define the canonical ensemble (see precourse reading) of the lattice gas at that density.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "href": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.1 Mapping between lattice gas and Ising model",
    "text": "1.1 Mapping between lattice gas and Ising model\nThe lattice gas model is interesting because whilst being a plausible model for a fluid, it maps onto the Ising model. We say that they are isomorphic to one another. This isomorphism extends the applicability of the Ising model. To expose the mapping we write the grand partition function of the lattice gas:\n\\[ \\Xi=\\sum_\\textrm{ state}\\exp-\\beta{\\cal H}_{LG}=\\sum_{\\{c\\}}\\exp\\left[\\beta \\epsilon\\sum_{&lt;i,j&gt;}c_ic_j +\\beta\\mu\\sum_ic_i\\right] \\] where the sum runs over all possible combinations of occupancies of the lattice sites.\nWe now change variables to\n\\[c_i=(1+s_i)/2; ~~~~ J=\\frac{\\epsilon}{4}; ~~~~\nh=\\frac{\\epsilon q+2\\mu}{4},\n\\] where \\(q\\) is the lattice coordination number.\nOne finds,\n\\[{\\cal H}_{LG}={\\cal H}_\\textrm{ I} + \\textrm{ constant}\\] Since the last term does not depend on the configuration, it feeds through as an additive constant in the free energy; and since all observables feature as derivatives of the free energy, the constant has no physical implications.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#phase-diagram",
    "href": "phase-transitions/assignment_October2025.html#phase-diagram",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.2 Phase diagram",
    "text": "1.2 Phase diagram\nUsing the isomorphism we can plot the phase diagram of the lattice gas in the plane of average number density and temperature.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of the lattice gas model in the density-temperature plane.\n\n\n\nIn the \\(\\mu-T\\) plane there is a line of first order phase transitions terminating at a critical point which has density \\(\\rho_c=0.5\\). The first order line means that if \\(T&lt;T_c\\) and starting from the gas phase we smoothly increase the chemical potential through the coexistence value of \\(\\mu\\), the average number density of particles on our lattice \\(\\rho=N/L^d\\) jumps discontinuously from a low value \\(\\rho_\\textrm{gas}\\) to a high value \\(\\rho_\\textrm{liquid}\\). This is the gas-liquid phase transition. The values of \\(\\rho_\\textrm{gas}\\) and \\(\\rho_\\textrm{liquid}\\) merge at \\(T_c\\), the gas-liquid critical temperature. At higher temperatures, the distinction between the phases disappears.\n\n\n\n\n\n\nCaution1.3 Aside on Real Fluids\n\n\n\n\n\nYou may wish to compare the phase diagram of the lattice gas mode with the results of (say) van der Waals equation (see recommended textbooks for the required phase diagram). The main difference is that the lattice gas has so-called “particle-hole” symmetry, \\(\\rho\\to 1-\\rho\\) (inherited from the up-down symmetry of the Ising model) which is not present for a real fluid. Accordingly, the phase diagram in a real fluid looks like a lopsided version of the above picture as shown in Figure 1.2. See here for some real experimental data showing the asymmetry of the coexistence curve in liquid metals.\n\n\n\n\n\n\nFigure 1.2: Schematic of the liquid-gas phase diagram in the \\(\\rho-T\\) plane for a realistic fluid .",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc-algorithm",
    "href": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc-algorithm",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.1 Grand Canonical Monte Carlo (GCMC) algorithm",
    "text": "2.1 Grand Canonical Monte Carlo (GCMC) algorithm\nA suitable algorithm for this case is to repeatedly update the system via the following fours steps:\n\nSelect a random lattice site \\((i,j)\\).\nIf the site is empty, propose inserting a particle. If it is occupied, propose removing the particle.\nCompute the change in energy \\(\\Delta E\\) and particle number \\(\\Delta N = \\pm 1\\).\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta {\\cal H}_{LG}&lt; 0, \\\\\ne^{-\\beta \\Delta {\\cal H}_{LG}} & \\text{otherwise},\n\\end{cases}\n\\]\nwhere \\(\\Delta {\\cal H}_{LG} = \\Delta E - \\mu \\Delta N\\) is the change in the grand canonical Hamiltonian.\nThis update scheme allows the system to explore states with varying particle numbers according to grand canonical statistics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "href": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.2 Fixed-particle-number case (Canonical Ensemble)",
    "text": "2.2 Fixed-particle-number case (Canonical Ensemble)\nIn contrast, we may for simplicity want to simulate with the total number of particles \\(N\\) fixed, as in the canonical ensemble. Then particle insertions and deletions are not allowed and the chemical potential term drops out of the Hamiltonian. We can use Kawasaki dynamics, which conserves particle number by moving particles between lattice sites. An update consists of the following four steps.\n\nSelect two sites \\((i,j)\\) and \\((i',j')\\) at random.\nIf one site is occupied and the other is empty, propose exchanging the particle and vacancy.\nCompute the energy change \\(\\Delta E = E(C') - E(C)\\) associated with this proposed particle move.\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta E &lt; 0, \\\\\ne^{-\\beta \\Delta E} & \\text{otherwise}.\n\\end{cases}\n\\]\nThis approach ensures particle number is conserved while still allowing the system to explore equilibrium configurations of the canonical (ie. fixed \\(N\\)) ensemble.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "href": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.3 Observables and measurement",
    "text": "2.3 Observables and measurement\nA Monte Carlo sweep in either ensemble involves performing \\(L^2\\) update attempts on a lattice of size \\(L \\times L\\).\nThe expectation value of a macrovariable \\(O\\) (e.g., energy, particle density, correlation functions) is estimated by averaging over sampled configurations:\n\\[\n\\overline O \\approx \\frac{1}{{\\cal N}} \\sum_{n=0}^{\\cal N} O_n,\n\\]\nwhere \\(O_n\\) is the value measured in configuration \\(C_n\\), and \\({\\cal N}\\) is the total number of configurations use for sampling.\nOther observables (those which are second derivatives of the free energy) such as the specific heat capacity can’t be calculated as simple averages. Instead they are calculated as a variance. The formula for the specific heat (cf. problem set) is\n\\[\nC=\\frac{1}{k_BT^2} \\left[\\overline{E^2} - \\overline{E}^2\\right] = \\frac{(\\Delta E)^2}{k_BT^2}\n\\]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#setup",
    "href": "phase-transitions/assignment_October2025.html#setup",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThis should be completed in the week prior to the release of the assignment to make sure that any technical problems are resolved.\n\nOn the PHYSM0071 Blackboard page open the Unit Information and Resources tab\nScroll down to Notable and open it (if off campus make sure you have the UoB VPN enabled)\nSelect the Jupyter Notebook (Legacy) notebook server option\nWhen the notebook has opened click the +Gitrepo button\nUnder enter Git Repository insert: https://github.com/nbwilding/Lattice-gas-coursework\nPress the “clone” button. This will download a notebook called lattice-gas.ipynb into Jupyter\nCheck that the program runs\nFamiliarise yourself with the main features of the program. Pay attention to how to change the temperature, number of lattice sites \\(N=L^2\\), and the number of equilibration and sampling sweeps. Also check how to toggle live visualisation on and off. Be aware that the program can take several minutes to run depending on the system size, and equilibration/sampling parameters.\n\nThe assignment itself will be released at 12:30pm on Monday 13th October 2025 on Blackboard via the Unit Assessment tab. The submission deadline is Monday 27th October 2025 at 09:30.\nPlease contact me (nigel.wilding@bristol.ac.uk) if you have trouble with the setup described above, detailing the problem you encountered.\nThe various elements of the assignment are set out below. Instructions are given in italics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "href": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.2 Isomorphism of the lattice gas and Ising model",
    "text": "3.2 Isomorphism of the lattice gas and Ising model\nBy referring to section 1.1 above, show that the Hamiltonian of the Lattice Gas model in the grand canonical ensemble\n\\[\n    {\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j -\\mu\\sum_ic_i\n\\] is transformed to that of the Ising model by means of the change of variable\n\\[\n    s_i=2c_i-1;~~~~ J=\\frac{\\epsilon}{4};~~~~\n    h=\\frac{\\epsilon q+2\\mu}{4}.\n\\] where \\(q\\) is the lattice coordination number (\\(q=4\\) in 2-dimensions and \\(q=6\\) in 3d).\n(Hint: Note that when doing sums over bonds \\(\\sum_{\\langle i,j\\rangle}\\) for a lattice of coordination \\(q\\) there are \\(q/2\\) bonds per site since each bond is shared between two sites.) [4 marks]\nGiven that the critical temperature of the 2d Ising model is \\(T_c\\approx 2.269\\: J/k_B\\), use the above mapping to find the value of the critical temperature of the 2d lattice gas model in units of \\(\\epsilon/k_B\\). [2 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "href": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.3 Code modification tasks",
    "text": "3.3 Code modification tasks\nThe program provided to you simulates the 2D lattice gas model in the canonical ensemble (i.e., at a fixed particle number density \\(\\rho\\)), using the Kawasaki swap algorithm as described above. It allows the user to specify the number of lattice sites and the temperature for the simulation. The program computes the pair correlation function (also known as the radial distribution function) \\(g(r)\\) and the structure factor \\(S(\\mathbf{k})\\) (refer to Chapter 2 of the notes for the definitions of these quantities).\nIn this implementation, the particle density is fixed at \\(\\rho = \\rho_c = 0.5\\). This choice ensures that the system approaches the critical point as the temperature is lowered within the supercritical regime. For convenience, the program uses dimensionless units by setting \\(\\epsilon = k_B = 1\\). The linear system size is intially set to \\(L=50\\) lattice units.\n\nReview the program thoroughly and gain a clear understanding of its functionality.\nThe program calculates the average form of the 2d structure factor \\(S(\\mathbf{k})\\). Using this, obtain and plot the radially averaged structure 1d factor \\(S(|k|)\\).  [6 marks]\nAdd a function to calculate and print the dimensionless total energy and specific heat of the system. For the latter you will need the expression given in section 2.3, above.\n[5 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "href": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.4 Computational investigations of correlations on the approach to criticality",
    "text": "3.4 Computational investigations of correlations on the approach to criticality\n\nPerform simulations at several (six or seven) temperatures within the super-critical range \\(T_c \\le T \\lesssim 2T_c\\). For each temperature, save the final configuration and describe how the overall structural characteristics evolve with temperature. Consider how you can be sure that your final configuration is equilibrated [4 marks]\nPlot and compare the radial distribution function \\(g(r)\\) and the structure factor \\(S(\\mathbf{k})\\), and its radially averaged form, \\(S(|k|)\\) at each temperature. [4 marks]\nInterpret the peak structure in \\(g(r)\\) and \\(S(|k|)\\) and discuss the relationship to clustering or ordering. [3 marks]\nThe full width at half maximum (FWHM) of the radially averaged structure factor can be used to estimate the correlation length \\(\\xi\\) through \\(\\xi=1/FWHM\\). For each temperature studied, estimate this quantity from your data. Plot the temperature dependence of \\(\\xi\\) and comment on its behaviour. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "href": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.5 Temperature and system size dependence of the specific heat",
    "text": "3.5 Temperature and system size dependence of the specific heat\nExamine how the specific heat capacity varies with dimensionless temperature over a broad range, including both subcritical and supercritical regimes (e.g \\(T_c/2&lt;T&lt;2T_c\\)). Describe the overall behavior of the specific heat across this temperature range and offer an explanation for its form. Describe how you checked that your sampling was from reasonably well equilibrated configurations. [4 marks]\nRepeat the analysis that you performed for the \\(L=50\\) system, for system sizes \\(L=20\\) and \\(L=35\\). Compare the results for all three system size and highlight any observed differences. Discuss possible explanations for these size-dependent effects. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#your-report",
    "href": "phase-transitions/assignment_October2025.html#your-report",
    "title": "PHYSM0071: First coursework assignment",
    "section": "4.0 Your report",
    "text": "4.0 Your report\nYou should produce a short skeleton report (upto 4 sides of A4 including figures and in no less than 11 pt fontsize) focussing on and summarising the results of the above investigations and your comments/observations on the findings.\nAs well as a title and your name, the report should be laid out using the headings above:\n\nIsomorphism of the lattice gas and Ising model\nCode modifications. Include snippets of your modified code, highlighting the modifications in yellow. Do not include the whole code, just enough to see what you changed and where you did it.\n\nComputational investigations of correlations on the approach to criticality\nTemperature and system size dependence of the specific heat\nInclude a line with following declaration at the end of the report: “I have not made use of Artificial Intelligence tools in completing this assessement beyond those of category 2 in the University’s categorisation scheme.”\n\nThere is no need for any other section headings ie. you don’t need abstract, introduction, references etc. Marks will be deducted for reports that exceed four sides (faces) of A4.\nSubmit your report in pdf format for grading via blackboard. The report will be marked out of 40, and will count for \\(30\\%\\) of the unit mark.\nLike any lab report, the marking focus will be on:\n\nDo your results look physically plausible?\nDid you describe the pertinent features?\nAre your descriptions clear?\nDo you explain them correctly using appropriate concepts?",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "experimental/part1.html",
    "href": "experimental/part1.html",
    "title": "11  Macroscopic measurements",
    "section": "",
    "text": "11.1 Introduction\nIn the course so far you have discussed the theoretical origins of phase transitions and have been given a theoretical introduction to the properties of disordered materials. In this section we will look briefly at some of the experimental techniques and experimental data that have driven and tested the theoretical work. Much of the quantitative experimental work is challenging and in many cases would take many lectures to justify properly. So, in these notes we will concentrate on the ideas underpinning the experiments and I will try to give a justification for the results obtained. The number of techniques discussed is limited and not exhaustive, you may find others as you read along.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#macroscopic-observation-and-experiments",
    "href": "experimental/part1.html#macroscopic-observation-and-experiments",
    "title": "11  Macroscopic measurements",
    "section": "11.2 Macroscopic observation and experiments",
    "text": "11.2 Macroscopic observation and experiments\nBefore discussing experiments in detail it is worth taking a little time to think about our observation of phase transitions and the different types of material we encounter in our everyday life. In the following I will concentrate mainly on water and its states as we know them well. You may consider it as a liquid at room temperature, a solid at cold temperatures and a gas at high temperatures. However, even for this `simple’ material we can observe unusual effects as we change its state. So in the following I will use water as an example even though, hopefully, you can see how the ideas will apply in general to other materials.\n\n11.2.1 1st Order Phase Transitions - ice and water as an example.\nPhase transitions are all around us in our everyday life. Some take place naturally, for example, the freezing of water on a cold winter night, some are exploited (and maybe hidden) in the technology that surrounds us in for example, liquid crystal displays or non-volatile computer memory.\nLet’s take water as our example and consider our everyday experience. In a warm building at a temperature of 20\\(^o\\)C, water in a beaker is a liquid, it freely flows and you can pour it from one container to another. What happens as we lower the temperature slowly (so that we keep, as best we can, a uniform temperature in the liquid). Common experience tells us it remains as a liquid until we reach its freezing point at 0\\(^o\\)C when it starts to solidify and form solid water - ice. But how does happen and does it always happen in the same way? Let’s think about this a little more deeply.\nSuppose we put our beaker of water at 20\\(^o\\)C straight into a freezer at, say, -10\\(^o\\) C. To start with the water in the beaker will lose heat to the surroundings (mainly by conduction and convection) and temperature gradients will appear in the water (the water at the edges will be cooler than the water in the middle). As the outside approaches 0\\(^o\\)C we expect the water at the edges to freeze (we’ll discuss this further later) and a block of solid ice will eventually form in the beaker. (We might break our container as, unusually, the density of ice is less than water so the total volume occupied by the water tries to get larger). During this process we will have an ice-water \\(\\textit{interface}\\) that stays at 0\\(^o\\)C but with large temperature gradients in the freezer and beaker - we are far from having the uniform temperature we mentioned above. As we can see from this short description, the temperature gradients mean that in practise phase transitions never take place uniformly in a material and there will always be some localization of the processes taking place. We can already see that freezing is more complicated than we might first imagine!\nIf we are going to look at the ice-water transition more carefully experimentally and to reduce these non-local effects we try to do things more slowly and try to maintain a uniform temperature by e.g. stirring. Ideally we’d also like to know qualitatively how quickly we are removing the heat and how much heat is removed in the process (the latent heat). What happens in this case? Well, if the outside temperature of the beaker is very slightly below zero then heat will be lost slowly from the (stirred) water and we expect small regions of ice mixed with the water to form. The water in the beaker does not freeze instantaneously but the ratio of the amount of ice to water steadily increases as heat is gradually removed (we should describe this as quasi-equilibrium where we consider we are moving gradually from one thermodynamic equilibrium state to another). As more ice forms we get a slushy mixture. If we carefully measure the temperature of the ice-water mixture (slush) during this process we will see that it remains at 0\\(^o\\)C until all the water has changed to ice at which point the ice will reach equilibrium with its surroundings (that we remember was just below zero). If, at any time during the process we prevent heat entering or leaving the system, the ice-water mixture will maintain the same ratio (although changes in the sizes of the regions may take place). The ice and water are then said to be in co-existence at the \\(\\textit{phase ~ transition ~temperature}\\) of 0\\(^o\\)C.\nCo-existence of two phases at the transition temperature is the typical characteristic of a first order transition.\nYou may wish to remind yourself of this process when we discuss thermal analysis later.\n\n\n11.2.2 First Order transitions - Experimental measurements.\nIn the discussion above I tried to define more carefully how we were cooling the system. This hints at some of the difficulties in actually trying to measure the details of about what happens when the transition is taking place. Fundamentally we are trying to use equlibrium thermodynamics to explain what happens in a system that is not in equilibrium but in the process of equilibrating. The study of non-equilibrium processes is a very active and important area of study. We might note that the ice-water co-existence is an equilibrium state - it is all at 0\\(^o\\)C but can have very different appearances (slushy, an ice cube surrounded by water etc.). The result is that in all careful (accurate calorimetry) experiments care is taken to avoid non-unformity in the system (i.e. temperature gradients, field gradients in magnetic systems, …). The problem of non-uniform temeperature is even more acute for second order phases transitions as we will discuss below.\nThere are many ways in which you might study phase transitions apart from visually, but the most common, quantative and closely related techniques are Differential Thermal Analysis (DTA) and Differential Scanning Calorimetry (DSC). Machines for doing this are commercially available and are ubiquitous in materials science labs.\nWe will look at these techniques in more detail later.\n\n\n11.2.3 First Order Phase transitions - nucleation and growth.\nThe form of the system after the transition has taken place may be of more interest for practical applications. For example, we know we can get ice by cooling water below 0\\(^o\\)C and if you are looking to cool your drink, the water in the form of a large ice cube is ideal (‘Scotch on the rocks’) whereas it wouldn’t be nice to ‘eat’ on a hot summers day. In contrast, if we can make ice crystals as small as possible (10’s of \\(\\mu\\)m), we have the basis for making good ice cream. Indeed if the crystals are small enough the texture is creamy without the need for any ‘cream’ at all. Hence, the way in which macroscopic properties through the transition can be controlled may be as important as understanding the transition itself. For example, one favourite method of making ice cream is to use liquid nitrogen. This has the effect, with rapid stirring, of forming the tiny crystals needed for good ice cream. The processing of materials through phase transitions is ubiquitous. It’s perhaps no surprise to see that major industries including food processing, cosmetics, engineering materials are all heavily dependent on our knowledge of what happens as phase transitions take place.\nTo deepen our knowledge of how the transitions take place we need to think beyond the thermodynamics and ask what happens at the microscopic (atomic) level. So what happens to the atoms/particles as a first order freezing transition takes place? To make progress we need to introduce the ideas and theories of the nucleation and growth of crystals at the phase transition temperature. These concepts have been introduced in the theory part of these lectures but let’s thing about what happens in our water/ice system. If I start with liquid water at 0\\(^o\\)C and remove a small amount of heat from the system I expect, from our discussion above, to start forming an ice/water mixture. But how does the first ice crystal start to form?\nThe first thing to note and you may be surprised to learn is that crystallization doesn’t have to take place at all at 0\\(^o\\)C! It is perfectly possible to have stable liquid water below 0\\(^o\\)C even under standard atmosphere and pressure. This is known as supercooling or undercooling. It can also occur at other transitions, for example you can find superheated water above 100\\(^o\\)C. Although supercooled/superheated materials are not the equilbrium states they can stay in this \\(\\textit{metastable}\\) state for many weeks/months on end unless disturbed.\nHence, if we are careful we can cool water to well below 0\\(^o\\)C without it freezing. So what happens as we cool our water below 0\\(^o\\)C and why, in some cases does it not freeze? What observations measurements could we make? Firstly, if it supercools, no first-order phase transition has taken place and there is no release of latent heat. Hence, in heat capacity measurements (DSC) we would expect to see a steady and smooth decrease in temperature. The physical properties of the supercooled liquid water would stay broadly the same - it will still flow and pour as a liquid but we might observe steady changes in, for example, its volume or viscosity (which we will discuss in relation to the glass transition later) as the temperature decreases. But where has the heat (energy) associated with the latent heat of freezing (if the sample had frozen) gone? The answer is it hasn’t gone anywhere, it remains in the water even though the temperature has decreased. In other words there is more energy in supercooled water at -10 \\(^o\\)C than ice at the same temperature. Hence, the metastable state is not the \\(\\textit{global}\\) minimum energy configuration of the system but is at a \\(\\textit{local}\\) energy minimum (i.e. it will return to this local equilibrium after small deviations from the equilibrium state). In order for the global minimum to be reached from the local minimum there is an energy barrier to overcome. In order to freeze from the supercooled state this energy barrier needs to be overcome. The idea that there may be one or more possible local minima for a configuration is known as the \\(\\textit{energy landscape}\\) of the material. This is illustrated conceptually in the figure below. Any configuration that lies above the landscape (the squiggly line) is a possible state of the system. For a given energy the system is rapidly changing configuration by moving to a neighbouring state on the configuration axis (in practise the energy landscape is multidimensional). At high energies (above the blue dotted line) it is possible to reach any configuration of the system in time. This would be equivalent to the normal liquid state. The lowest minimum (green dotted line) represents the equilibrium (minimum energy state) state e.g. ice. Now we imagine starting in the liquid state to the left of the vertical purple line and remove some energy. Once we are below this line we see that we cannot access any states to its right - the global minimum, ice, has become inaccessible. If we continue to remove energy we may find ourselves in the local minimum in this part of configuration space. This could perhaps be the supercooled liquid state. To reach the global minimum we have to overcome the energy barrier separating the local from the global minimum. Natural fluctuations in energy may be sufficient and explain why under normal cicumstances we get to the global minimum. However, if the local minimum is deep enough we get stuck in the metastable state.\n\n\n\nIn principle we could get ‘stuck’ in any of the local minima if we process our material in a certain way.\nSo what happens as we cool the water further - there are two possibilities: it will suddenly freeze to form ice (we find the global minimum energy), or its viscosity becomes so high that the internal molecular motion of the atoms is slowed down so much that the disordered arrangements of the atoms become ‘frozen’ so that it forms a ‘solid’ disordered structure that we call a glass (i.e. we’ve got stuck in a local minimum). In fact, no one has ever been able to produce ‘glassy’ water in this way \\(^*\\) but the transition to a glass on cooling does a occur in many materials. We’ll leave discussion of the glass transition and how we observe it until later.\nSo, if we observe our water as we cool it, we find it will suddenly and rapidly appear to freeze to form ice. If you seacrh for `supercooled water’ on youtube (or other) you will find many videos of this happening. So what is happening? In our supercooled water there will always be small regions that spontaneously start to form the atomic arrangements reminiscent of the crystal you’d expect to see at that temperature. But, as you’ve seen in the theory lectures, there is an energy barrier to overcome before the crystals become large enough to start growing continuously. In other words our embryonic crystals start to grow but dissolve again before they have the chance to grow further. However, if a crystal gets large enough, there is no energy barrier to stop more atoms/molecules attaching to the nascent crystal and it will continue to grow.\nOnce the crystals are macroscopically large we start to see them and can observe the growth. However, there are a few things we should note. Firstly, this spontaneous crystallization (known as \\(\\textit{homogeneous nucleation}\\)), is a ‘local’ event - it could happen anywhere in the liquid. There are many questions that can then be asked. Is it only one crystal that forms and grows, or does homogeneous nucleation take place simultaneously in different parts of fluid at the same time? This poses some very difficult questions and when you investigate further you realise that the simple theory of nucleation and growth is just that - in reality how and where nucleation physically takes place in different materials is hard to predict! In some materials you might see a small number of large crystal form, in others, lots of smaller crystals start to form at the same time. In many cases you see a `front’ of nucleation advancing rapidly from the original nucleation event. We don’t have time in these lectures to explore this fascinating subject but the consequences for materials science (i.e the strength and physical properties of the materials produced) are important.\nThere are still a few things we should note. We know in our example that there is more energy in the supercooled liquid water than the same amount of water in the form of ice. So what happens when an ice crystal starts to grow spontaneously in our supercooled water? As the crystal starts to grow, the latent heat associated with the transition, that has remained in the liquid, must be released, so locally around the forming crystal the temperature of the liquid and the crystal will start to rise. In other words temperature gradients will start to form around the crystal and heat will only leave via transport processes (conduction, convection, possibly radiation) through the liquid. So, as the nucleation takes place the material is far from equlibrium with large variations in temperature present etc. What is the final state of the system once equlibrium has been reached in an isolated system? Well, as we noted, as nucleation takes place, the temperature goes up. So, do we end up with ice at a temperature below zero or a mixture of ice and water, which we know should be at 0\\(^o\\)C (at stp)? The answer is the latter, you end up with an ice/water mixture at 0\\(^o\\)C with the ratio of ice to water dependent on the level of supercooling. The more deeply supercooled the greater the proportion of ice to water.\nExercise. Look up the latent heat of freezing for water and estimate the temperature at which supercooled water needs to be if it was form only ice (i.e. no water) at 0\\(^o\\)C on nucleation.\nFrom the discussion above you can see that the local and microscopic nature of the nucleation processes make precise experiments difficult. The results will depend, on the size of the sample, the rate of heat transfer through the sample, the randomness, of the nucleation processes, whether the process was at fixed pressure or constant volume (remember ice at stp has lower density than water and think about the implications if the process takes place at constant volume) etc.\nSo how does a first order transition from a supercooled state manifest itself in a calorimetry experiment, for example in a DSC? Well, if the transition takes place at the freezing temperature we’d expect to see the temperature of the sample remain at the transition temperature until the transition is complete, at which point the temperatures starts to drop steadily again at a rate corresponding to the heat capacity of the material. If however the material supercools its temperature will decrease steadily through the the transition temperature (no transition takes place). When the material eventually nucleates and crystallizes you observe a sudden rise in temperature (back to the transtion temperature) where it stays until the transition is complete and the temperature decreases as before. The deeper the supercooling the bigger the temperature jump. As noted above, the precise shape of the reheating peak observed depends on the sample size, thermal conductivity etc.\nThe sudden reheating of a material from a supercooled state is called \\(\\textit{recalescence}\\).\nExercise. Recalescence is not confined to liquid/solid/gas transitions. It may occur for any first order transition, for example, the austenitic transition in stainless steel which is of commerical importance. Can you find other examples.\nFinal comments about 1st order phase transitions.\n\nSupercooling is often difficult to achieve. The reason is that in real situations we need to hold the material in containers in which there might be impurities or specks of dirt etc. These impurities can act very much like embryonic crystals that form in homogeneous nucleation and they lower the free energy barrier for nucleation. Processing materials under very clean conditions in smooth walled containers is very difficult. Nucleation on impurities is known as \\(\\textit{heterogeneous}\\) nucleation and is often the process most observed unless great attention is taken to the preparation method.\nIf there is a known transition and neither the transition or recalescence is observed in for example a DSC scan, then it is possible that the material formed a glass. This is discussed below.\n\n\\(^*\\) Although it is possible to make amorphous ice - a topic great interest but we don’t have time to study it here.\n\n\n11.2.4 Phase Separation in mixtures - spinodal decomposition.\nIn the previous section we saw that a first order transition doesn’t necessarily take place at the transition temperature. That is, rather than observing two phases in coexistence at the transition temperature, the material may remain in a metastable supercooled or superheated single state. The crucial observation is that for the transition to start, a nucleation event needs to take place, from which the second phase will grow. The next question you might ask is what happens if I continue to cool the material? Firstly, as the temperature difference increases the likelihood of homogeneous nucleation (and recalescence) increases. However, there are other possibilities, the material may form a glass (which we will discuss shortly) or we reach the point where spinodal decomposition takes place. You have discussed spinodal decomposition in the theoretical section of this course, but briefly there comes a point where the barrier to the separation disappears and the formation of two distinct phases occurs throughout the material (it does not rely on a nucleation centre). This is especially true if you are below and close to the critical point. This is a dynamic process as the size of the different regions grows over time - eventually two distinct macroscopically large phases appear. Spinodal decomposition, has an important role to play in material processing as it has an effect on the microsctructure of the material produced. If the material is quenched fast the growth of the separate regions that form will be rapidly arrested leading to a fine grained solid. If the quench is slow the resulting structure will be coarse grained. This graining of the microstructure will affect the physical properties of the material produced. This behaviour is most associated with phase transitions that involve the separation of components in composition, such as metallic alloys and polymer blends rather than pure liquid/gas, liquid/solid transitions in which spinodal decomposition is far harder to observe.\n\n\n11.2.5 The Glass Transition.\nIn the section above we discussed how, particularly in the case of compositional phase separation, the microstucture of a material may be related to the speed in which it is quenched. As a consequence the end point is a material that is composed of a mixture of different phases rather than a single phase - the result of a true first order phase transition. So, what happens if we manage to cool without nucleation or spinodal decomposition occuring? i.e. what happens if we rapidly quench the sample to the extent that the dynamics driving for example spinodal decomposition are ‘frozen’ out. If we manage to slow the diffusion of the atoms enough, then the atomic re-arrangements needed to cause crystallization or spinodal decomposition cannot take place and the atoms/molecules appear to be fixed in place, but with a disordered arrangement. However we note that overall the material remains homogeneous and uniform - there is no microstructure. As in the case of supercooled liquids, this is not the equilibrium state, but another form of metastable state known as a glass. An example, is typically the material in our windows, `glass’, that is the result of cooling liquid silicates. Not all materials readily form glasses and this is often expressed loosely as the glass forming ability of the material. A good glass former is a material that can be cooled quite slowly, a poor glass former is one that needs to be quenched fast (there are more complex definitions but this is the gist). A characteristic of the glass is that no transition appears to take place in the material as it is quenched. There is no latent heat released, there is no recalescence and there is no evidence of phase separation (microstructure) in the material produced. In other words we imagine the glass forming from the supercooled liquid. So what do we mean by the glass transition and the glass transition temperature \\(T_g\\)? To answer this question, it is interesting to ask what happens as we heat our glass up? If, all we had was a very very high viscosity (behaving like a solid), but metastable supercooled liquid phase, then we would eventually return to the true liquid phase (above the melting point) without any evidence of a phase transition taking place (unless nucleation and recalescence took place). However, when we heat glass, we do see heat evolved at a temperature below the melting point (we’ll see this later on our summary of thermal analysis). For any given glass this occurs consistently at roughly the same temperature (but notably not well defined) and the material will start to change into a more viscous, `rubbery’ state. This is known as the glass transition. It is not a true thermodynamic transition and there are many ways in which it may defined (all occuring at roughly the same temperature). It depends on cooling/heating rate and can be observed in a change in for example viscosity, thermal expansion coefficients, heat capacity etc. It also depends on the thermal history of the material.\nIn summary, the existence of a glassy state of matter is readily apparent. However, the transformation of a liquid into a glass and how and why the structure changes on heating/cooling at the glass transition are still very poorly understood and the subject of much research. Indeed, there is no universal definition of the glass transition despite many attempts to define it in a rigorous thermodynamic way.\nThe terms glassy and amorphous materials are often used synonymously. Stricter definitions state that a (true) glass is formed by quenching from the liquid state whereas an amorphous solid includes materials that have been formed by for example deposition from a vapour, preciptation from a solution, mechanical processing. Amorphous materials also show a glass transition temperature but the manner in which they form is more complicated to describe theoretically.\n\n\n11.2.6 Second Order Phase transitions\nIn the discussion above we charaterised a first order phase transtion as one in which latent heat is absorbed/emitted. i.e. one in which two phases coexist at the transition temperature and the temperature stays constant until the transition is complete. However, we can also identify other transitions in which no latent heat is associated with the change in phase taking place. Typical examples might be the liquid-vapour transition at the critical point or the ferromagnetic transition. The key point is that the transition between the two states is continuous. For example, in the ferromagnetic transition the magnetization is zero above the transition and increases steadily from zero below the transition. In this case it is the discontinuity in slope of the magnetization vs. temperature that is the signature of the transition taking place. In other cases a discontinuity in the gradient of the temperature dependence of the heat capacity is observed (while the heat capacity itself is continuous across the transition). These transitions have been discussed in the the theory section of this course and their beauty is the observation that the behaviour for a variety of supposedly very different physical transitions may be characterised in terms of universal behaviour. The \\(\\textit{universality class}\\) of the transition is characterised by its critical exponents. You might note that many attempts have been made (unsuccessfully) to describe the glass transition in terms of an underlying second order phase transition.\nExperiments to determine, for example, the critical exponents of the transition (rather than observing the transition itself) are extremely difficult. Precise control and accuracy of the temperature measurements is needed, great care must be taken to make any temperature gradients in the material as small as possible and in order to get accurate values of the critical exponents you need to get very close to the transition temperature.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#examples-of-macroscopic-measurements.",
    "href": "experimental/part1.html#examples-of-macroscopic-measurements.",
    "title": "11  Macroscopic measurements",
    "section": "12.1 Examples of macroscopic measurements.",
    "text": "12.1 Examples of macroscopic measurements.\n\n12.1.1 Differential Thermal analysis.\nDifferential Thermal Analysis (DTA) methods are now a common and routine way of establishing the existenceo of phase transitions in materials in both a qualitative and quantitative way. The methods are all based on the same principle, namely the response to heating/cooling of the material under study is compared to that of a known calibration material that does not have any phase transitions in the region of study. I describe the two most common forms below.\n\n12.1.1.1 Heat flux DSC (Boersma DTA)\nThis is the simplest thermal analysis technqiue. Two materials, a sample and a reference are heated together in a single furnace as shown below.\n\n\n\nThe temperature of the two materials is monitored carefully as the furnace is heated steadily. The reference is known not to show any phase transitions over the temperature range studied and will therefore heat steadily. If there is a phase change in the sample on heating/cooling, it will show as a temperature difference between the sample and reference. If the change is endothermic the sample will be at a lower temperature until the transition is complete after which the sample will catch up to the base temperature of the furnace, if the change is exothermic its temperature will increase until the transition is complete and then return to the furnace temperature. If the mass of the sample and reference is known and the heat capacity of the reference known, quantative values of, for example, latent heat, can be obtained. In quantitative measurements it is sometimes known as a heat flux DSC. A schematic of a heat flux DSC trace is shown below.\n\n\n\n\n\n12.1.1.2 Heat flow DSC\nThis technique is similar to heat flux DSC but the sample and reference are located in their own separate furnaces (it is a double furnace technique). The power needed to steadily heat the sample and reference (and hence the heat added to the system) is controlled in such a way as to keep the temperature of the sample and reference the same.\n\n\n\nThe difference in power supplied to the furnaces indicates when a transition takes place and with the mass of the reference and sample allows the energy absorbed/released in the transition be determined. If on heating there is a first order phase transition then more power is needed to overcome the latent heat to maintain the temperature. If an endothermic transition takes place then the power needed would decrease. The output from the instrument is usually presented as the heat flow to the sample as the temperature is raised. The trace is very similar in form to the heat flux DSC above except that the peaks and troughs observed are the opposite directions. (Increased heat flow into the sample is equivalent to the temperature of the sample being lower in the heat heat flux DSC). With computer control and a carefully designed and insulated chamber accurate measurements of latent heat etc. may be made.\n\n\n12.1.1.3 Practical considerations\nIn use, practical questions involve the rate at which you wish to heat/cool the sample (transitions don’t take place instantaneously) and the sensitivity to small changes you are tryinh to observe. One common extension to the technique is \\(\\textit{Temperature ~ Modulated ~ DSC}\\). In this case the heating/cooling may be modulated (e.g. stopped or reversed periodically. Application of this method allows the kinetics of the transition in the DSC to be observed. In particular, it can distinguish between a reversible second order phase transition and an irreversible transition such as the glass transition.\nA typical DSC analysis would involve monitoring the sample as it is heated to some target temperature, for example, to above its melting point. (The limit would be set by the maximum temperature limit of the apparatus, the material stability (does it breakdown/oxidise) or the onset of a reaction between sample and container). If the sample was initially in a non equilibrium state (glassy, supercooled so you pass through \\(T_g\\) or recalescence takes place) then you do not expect to see transitions at the same place in the heating/cooling cycles. Similarly, observations of the differences in the DSC trace of materials with different thermal histories will be different.\n\n\n\n12.1.2 Other techniques (non exhaustive)\nDifferential thermal analysis is not the only method for studying phase transitions. There are many other ways, quantitative and qualitative that may be used. Here are a few examples.\n\\(\\bf{Thermal ~ mechanical ~ analysis ~ (TMA)}\\). In this case the mechanical properties (modulus) of the material is observed on heating/cooling. For example, on heating, at the glass transition temperature, a glass will start to soften. Note, the glass transition temperature measured in this way is often slightly different to that measured by DSC and is indicative of the vagueness of the definition of \\(T_g\\) itself!\n\\(\\bf{Thermogravimetric ~ analysis}\\). Observation of changes in sample mass with temperature.\n\\(\\bf{Imaging}\\). It is quite possible to observe (and hence video) the phase transition taking place and getting some insight into the mechanism of the transition taking place.\n\\(\\bf{Magnetic ~ susceptibilty}\\). To observe transitions in para to ferro/antiferro magnetic transitions. (e.g. Curie and Néel points),\n\\(\\bf{Electrical ~ Conductivity}\\). In addition to structural changes in a material changes in other properties, for example, the electronic conductivity may also take place. For example, some chalcogenide (S,Se,Te) based materials are good glass formers and have very high resistivity (low conductivity) in the glassy state. By contrast, in the crystalline state they have high conductivity. The associated glass and melting transitions may be observed easily by measuring resistance. Indeed, the glass transition in these materials, is exploited in non-volatile memory devices.\n\\(\\bf{Pyrometric ~ measurements}\\). This is an example from my own research on oxide glasses (that melt in excess of 2000K). In this case, the glass transition, recalescence, and freezing my all be observed in a semi-quantitative way by measuring the temperature of a cooling sample as a function of time.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part1.html#keypoints",
    "href": "experimental/part1.html#keypoints",
    "title": "11  Macroscopic measurements",
    "section": "12.2 Keypoints",
    "text": "12.2 Keypoints\nWe have taken some time here to discuss various aspects about phase transitions as macroscopically observable changes in a material. We have introduced the ideas of\n\nfirst and second order phase transitions,\nnucleation and growth,\nsupercooling and superheating,\nspinodal decomposition,\nthe formation of glasses\nandhave given a brief introduction as how to observe and measure them with especial emphasis on differential thermal analysis techniques.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Macroscopic measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html",
    "href": "experimental/part2.html",
    "title": "12  Measurements in real space",
    "section": "",
    "text": "12.1 A simple description of \\(g(r)\\)\nIn the theory part of the course you have been introduced formally to the radial distribution function \\(g(r)\\) and seen how it is a useful description of the arrangement of particles in a disordered system. In particular, in the absence of the long range periodic order that is associated with crystalline materials it is one of the first things we wish to find for liquid or glassy material. In our examples we assume that we are looking at a uniform and isotropic material, that is, there is no preferred direction in the material and no density variations on length scales much greater than the particle size. That is, macroscopically the material appears is of uniform density. It is also possible to give a more general definition \\(g(\\bf{r})\\) where we recognise that there is some preferred orientation of the atoms around each other in the material but we will not consider it here.\nIn layman’s terms the radial distribution function is often described loosely as the probability of finding another particle at a distance from a particle at the origin. If we think about it this way we must remember that \\(g(r)\\) is a statistical function so what we are really trying to express is the average of this distribution taking each atom in the material in turn as the ‘origin’. Expressed in this way you might worry about what this means for atoms on the ‘edge’. Normally this is not a concern for large samples, but beware about the definition of \\(g(r)\\)if you start approaching ‘nano-sized’ materials.\nA better definition is that \\(g(r)\\) represents the deviation from the mean particle density, normalised to one, as you move outward from a particle at the origin. At large \\(r\\) we expect this to equate to a constant, related to the mean particle density of the material. When properly normalised to the mean density \\(g(r) _{r\\rightarrow \\infty} \\rightarrow 1\\). The figure below shows the key features of a typical radial distribution function.\nPoints to note are",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#a-simple-description-of-gr",
    "href": "experimental/part2.html#a-simple-description-of-gr",
    "title": "12  Measurements in real space",
    "section": "",
    "text": "that \\(g(r) = 0\\) for a distance below a ‘cutoff’ distance. This recognises that our particles have a finite size and when we mean ‘position of particle’ we mean its centre. Hence this cut off represents the diameter of the particles. What we mean by diameter of particle and what this means for \\(g(r)\\) we will discuss shortly.\n\\(g(r)\\) typically rises from 0 to give a ‘first peak’. The position of this peak is referred to as the nearest neighbour distance. (NB. it is not the same as the diameter of the particles).\nAfter the nearest neighbour peak, \\(g(r)\\) typically dips below the mean density and is characterised by damped oscillating peaks and troughs until \\(g(r)\\) reaches the constant mean density.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#what-can-we-learn-from-gr",
    "href": "experimental/part2.html#what-can-we-learn-from-gr",
    "title": "12  Measurements in real space",
    "section": "12.2 What can we learn from \\(g(r)\\)?",
    "text": "12.2 What can we learn from \\(g(r)\\)?\n\n12.2.1 Peak positions.\nThe peaks observed in \\(g(r)\\) correspond to the most common interparticle seperation. For an atomic system, they can typically be associated with chemical bonds and as you move further in \\(r\\) the \\(\\textit{medium}\\) (i.e. next nearest neighbour …) range ordering. However, we must remember that this is only a pair corrrelation function. It does not \\(\\it{per~se}\\) tell us about for example bond angles. These are formally be obtained from higher order correlation functionz (triplet, …) but these are not obtainable directly from \\(g(r)\\) itself.\nNote. If we have a crystalline material the peaks in \\(g(r)\\) will be narrow (they will have an intrinsic width at least due to thermal vibration) and at low values of \\(r\\), \\(g(r)\\) will be zero between them. However, even in a crystalline material these peaks will start to broaden and get smaller at high \\(r\\) until, again, \\(g(r)\\) reaches mean density.\n\n\n12.2.2 Coordination numbers.\n\\(g(r)\\) is a dimensionless function that normalises to a mean density of one at large \\(r\\). For a material with a \\(\\textit{number~density}\\), \\(\\rho_N\\), the mean number density at distance \\(r\\) is hence \\(\\rho_N g(r)\\). A thin spherical shell of thickness \\(\\Delta r\\) at a distance \\(r\\) will have a volume \\(4\\pi r^2 \\Delta r\\) so that the mean number of atoms in the shell will be,\n\\(4\\pi r^2 \\rho_N(r) \\Delta r\\).\nHence the mean number of atoms in a spherical shell of inner radius \\(r_1\\) and outer radius \\(r_2\\) will be given by,\n\\(\\bar{n} = 4\\pi\\rho_N\\int_{r1}^{r2}r^2g(r)dr\\).\nIf we define a peak in \\(g(r)\\) by \\(r_1\\) and \\(r_2\\) we call \\(n\\) the coordination number of the particles in the peak.\n\n\n\nIt is important to note that the coordination number is an average (\\(g(r)\\) is a statistical distribution) and in general will vary particle by particle in the material. It does not have to be integer and unless \\(g(r_1)=g(r_2)=0\\) it is an ill defined quantity. Nevertheless, it is a commonly quoted number in research papers so it is important to read carefully how it has been caclulated to obtain meaningful conclusions that can be compared to figures in other research work.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#how-to-we-measure-gr-directly-for-a-material",
    "href": "experimental/part2.html#how-to-we-measure-gr-directly-for-a-material",
    "title": "12  Measurements in real space",
    "section": "12.3 How to we measure \\(g(r)\\) directly for a material?",
    "text": "12.3 How to we measure \\(g(r)\\) directly for a material?\nIn order to measure \\(g(r)\\) directly, we need to obtain a 3D (unless we are looking at a 2D system) image of the material in which all the particle coordinates \\(r_i(x,y,z)\\) can be obtained. Although some modern techniques can obtain images with atomic resolution (for example scanning tunneling microscopy) they are mostly limited to surface imaging and not suitable for studying liquids (where atoms are moving around). Although some X-ray and electron microscopy techniques (see for example ptychography) can produce atomic scale resolution (using coherent imgaing techniques) these are again largely confined to 2D surfaces and are again not suitable for atomic liquids.\nHistorically (1950-60s) the first attempts to determine \\(g(r)\\) came from measurement of the packing of real particles made by literally constructing disordered structures with balls and sticks, gelatine balls, plasticine etc. and then painstakingly finding the particle coordinates during a careful and systematic disassembly. Needless to say this wasn’t a particularly accurate method but it did reveal, what suprised many at the time, dense packing of particles in liquids was characterised by 5-fold coordination.\nToday there is much literature on the study of disorder and phase changes in colloidal systems (particles of the order of 100s of nanometres in size and greater) and we will take a look at some of the methods used to do these experiments. The spirit of these experiments is the same - we try to measure the positions of the particles in space and ideally also over time (to see how they move).",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#the-structure-of-colloidal-systems.",
    "href": "experimental/part2.html#the-structure-of-colloidal-systems.",
    "title": "12  Measurements in real space",
    "section": "12.4 The structure of colloidal systems.",
    "text": "12.4 The structure of colloidal systems.\nColloids are complex systems composed of mesoscopic particles suspended in a host liquid. They and emulsions (liquid drops in a host liquid) are found everywhere around us. They are used extensively in food processing and paints etc. Much work has been done to stabilize and control their properties but here we will look briefly at how they have been used to study phase transitions, glasses and particle coordination in ‘simple’ systems.\nIn its simplest form a colloid consists of small (mesoscopic) particles suspended in a solvent. However, if you simply mix and stir say small spherical particles in a solvent you will often observe \\(\\textit{flocculation}\\). Fundamentally, there is an attractive force, called the depletion force, that is driven by the excluded volumes (and associated entropy) around hard spheres as they come into contact. That is, the particles tend to clump together into larger aggregates. So, in order to produce a homogeneous phase of uniform particle density, the colloid needs to be \\(\\textit{stabilised}\\). The stabilization of colloids and emulsions is a vast subject, but in short, by adding additional chemicals (e.g. surfactants) to the solvent it is possible to introduce some repulsion between the particles that overcomes the tendency to flocculate. The type and concentration of the added chemicals causes a change in the electrostatic attraction/repulsion between the particles and gives rise to the zeta (\\(\\zeta\\)) potential. To mimic ideal `hard sphere’ behaviour the aim is to have a colloid with a zeta potential of zero. i.e. there is no net attraction or repulsion (except the hard sphere surface itself) between the particles.\nThere are other ways to stabilize colloids for example, steric repulsion. In this method long chain molelcules are incorporated into the particles to make them `hairy’. The presence of these hairs reduces the excluded volume around the particles and hence reduces the depletion force between them.\nFrom here on we will assume that we are able to create such an ideal hard sphere colloid! For a comphrensive and detailed review of studies of colloidal hard sphere systems see the review of Royall et. al. 2024",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#how-to-we-obtain-gr-and-information-about-coordination-from-colloid-systems.",
    "href": "experimental/part2.html#how-to-we-obtain-gr-and-information-about-coordination-from-colloid-systems.",
    "title": "12  Measurements in real space",
    "section": "12.5 How to we obtain \\(g(r)\\) and information about coordination from colloid systems.",
    "text": "12.5 How to we obtain \\(g(r)\\) and information about coordination from colloid systems.\nIf we have our ideal colloid there are several things we may wish to control. For example, what is the size of the particles (there will be a tendency for large particles to sediment), are the particles all the same size (polydispersivity), can we make systems with different shapes (e.g. ovoid), can we make systems with mixed particle sizes, can we introduce directional interactions (Janus particles) and how do you control the number density of the particles (the density of the particles is the crucial parameter for describing the hard sphere phase diagram)? These all pose theoretical and experimental difficulties of their own. Finally, once we’ve prepared our colloid with a controlled density … how do we study its structure (and dynamics)?",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#confocal-microscopy.",
    "href": "experimental/part2.html#confocal-microscopy.",
    "title": "12  Measurements in real space",
    "section": "12.6 Confocal microscopy.",
    "text": "12.6 Confocal microscopy.\nThere are a few further steps we need to achieve to be able to measure and track the positions of the particles in our hard sphere colloid. Firstly, it is important that the particles are transparent, otherwise, we will not be able to see into the depth of the sample and secondly we need to match, as closely as possible, the refractive index of the solvent. If we don’t do this the difference in refractive index will mean light is quickly scattered in all directions meaning we will just have an opaque material. However, by making the particles all but invisible how to we determine their positions? The trick is to include a fluorescent dye their centre. When illuminated with the correct wavelength of light (using a laser) the particles will glow. This works well but how do we measure the positions of the particles in 3D. The answer is to use a confocal microscope as illustrated below (figure take from Royall et. al.2024).\n\n\n\nIn the confocal microscope laser light is passed through a small pinhole and then brought to focus at a point in the sample by a lens, known as the condenser lens. The fluorescent light coming back from the illuminated samples is then focused by the objective lens (which in this case is the same lens as the condenser) and the microscopy image is obtained by the camera. However, although the laser light is brought to a tight focus in the sample there are nevertheless still regions that are illuminated by the laser that, under normal circumstances, would also bring unwanted light into the camera. The key point of the confocal system is that a second pinhole is placed precisely at the focus (hence confocal) of the objective lens for the light coming from the sample. The effect of this pinhole is to reduce the volume of the sample that is illuminated and then seen by the camera. The method is good enough that the fluorescence from individual particles can be observed clearly and the ultimate ability to resolve the individual particles is given by the diffraction limit imposed by the wavelength of the laser light used. (There are also ways in which this has been overcome to an extent in practical systems). The figures below, also taken from Royall et. al. 2024 show examples of confocal images used to map out the phase diagram of the hard sphere system.\n\n\n\nNote, moving the sample up and down changes the ‘z’ direction and a three dimensional image may be obtained by ‘stacking’ vertically scanned 2D images.\nOnce the particles positions have been determined it is possible to plot \\(g(r)\\) for the different packing densities to show how it varies over the phase diagram as shown below\n\n\n\nAs mentioned previously, the early attempts at understanding the packing of particles in disordered materials came to te conclusion that 5 fold coordination (inconsistent with the long range order required for crystals) was the characteristic of simple dense packed structures. In the same spirit as these early experiments the results from confocal imaging have allowed the prevalence of different types of local ordering to be investigated. This leads to the mapping out the \\(\\textit{topological cluster distribution}\\) (i.e. the different types of local structure) that occur in the system. As noted before, these higher order correlations may be calculated from the particle positions (once we have them) but cannot be determined unambiguously from measurements of \\(g(r)\\) alone.\n\n\n\nThese are just a few examples of the areas studied using colloids and confocal microscopy techqniues.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#calculation-of-gr-directly-from-simulation",
    "href": "experimental/part2.html#calculation-of-gr-directly-from-simulation",
    "title": "12  Measurements in real space",
    "section": "12.7 Calculation of \\(g(r)\\) directly from simulation",
    "text": "12.7 Calculation of \\(g(r)\\) directly from simulation\nAlthough simulation is not strictly an experimental method, it is neither a truly theoretical method either. This said, from the experimentalist point of view, especially in the case of the colloind systems studied above, they provide a direct comparison in real space with the data. As such the experiemntal results are an important check of the veracity of the simulation methods especially with regard to the application of for example theoretical potentials. The simulation methods naturally exploit the interaction of the particles in real space (we know the positions of the particles and we calculate how they move under the influence of the interparticle forces. Hence, it is quite straight forward to compare the real space configurations measured in experiment with the particle coordinates in the simulation. The comparison between the experiment and simulation and the level of agreement that can be achieved can be strikingly seen in the comparison of \\(g(r)\\) for hard spheres shown above. Simulations however are not the answer to everything. They get more and more computer intensive as larger numbers of particles are included and the time scales over which they are carried out are extremely short compared to those in experiments. The latter is a particular issue with studies of the glass transition (as always) where the physical processes taking place may occur over a very wide range of timescales.\nThe comparison of simulation results with experimental diffraction data is more difficult. Diffraction experiments are a reciprocal space measurement and to compare to simulation we need to work out the corresponding real space (\\(g(r)\\) ) distributions. Hence when comparing experiment with simulation we have to choose whether to transform our reciprocal space data to real space (with associated problems) or convert the real space data from our simulations to reciprocal space for comparison (that also has problems). This is the subject of the next part of this course.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#dynamics-in-real-space",
    "href": "experimental/part2.html#dynamics-in-real-space",
    "title": "12  Measurements in real space",
    "section": "12.8 Dynamics in real space",
    "text": "12.8 Dynamics in real space\nConfocal microscopy experiments are not only useful for static measurements of \\(g(r)\\) but also open up the possibility for studying the dynamics of the system (how the particles move over time), that is, measurement of the time dependent pair correlation function \\(G(r,t)\\) I’ll not say much about dynamics at the present time but will leave it to the last part of this section of the course. Needless to say the process involves taking confocal images of the particles over time scales in which they will move only slightly from their initial position. With sophisticated software it is then possible to label the particles and track their position and hence coordinates in succesive image frames. This work well if you are in the sweet spot for the measurements but is a challenge, for example in glass transition experiments, where the experimental timescales become longer and longer as you approach the conditions for the glass transition to take place. This is still a very active field.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "experimental/part2.html#summary-and-keypoints",
    "href": "experimental/part2.html#summary-and-keypoints",
    "title": "12  Measurements in real space",
    "section": "12.9 Summary and keypoints",
    "text": "12.9 Summary and keypoints\nIn this section we have considered the experimental determination of \\(g(r)\\).\n\nWe have seen that this is not realistically possible for atmonic systems,\nwe have seen how colloid systems are ideal systems for studying the properties of hard-sphere and closely related systems on the 100nm scale,\nwe have seen how \\(g(r)\\) for colloids may be obtained from the coordinates obtained from confocal microscopy,\nwe have seen how \\(g(r)\\) and higher order correlations may be obtained from these coordinates,\nwe have seen how we can extend the confocal microscopy method to study particle dynamics.",
    "crumbs": [
      "First experimental interlude",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Real-space measurements</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_intro.html",
    "href": "soft-matter/soft-matter_intro.html",
    "title": "13  Entropy matters",
    "section": "",
    "text": "13.1 Systems and definitions\nMost of the matter around us does not simply fit within the idealized pictures of crystalline solids or simple liquids: examples include colloids, polymers, surfactants, liquid crystals, foams, gels, and biological materials such as proteins, DNA, and cell membranes.\nThis means that cellular life itself (the very constituents that make us) obeys principles that go beyond the standard patterns of conventional solid-state physics.\nThis branch of physics is called soft condensed matter physics, or macromolecular physics, or the physics of complex fluids. Specifically, soft matter refers to an area of condensed matter focused on systems that can be easily deformed.\nIn this course, we will emphasize the fact that many such systems are not crystalline: thermal noise and disordered configurations play a key role in their phase behavior, and hence we think of them as complex disordered systems.\nWhile we often think about problems in physics as a matter of energy minimization, in soft-matter physics a key role is played by fluctuations. Typically (but not exclusively) these are thermal fluctuations. This means that entropy and not only the energy from the interactions plays a key role.\nThis is because soft matter systems are typically composed of many microscopic constituents in contact with an environment. The appropriate description of the macroscopic state of such systems is therefore statistical and uses the language of statistical mechanics. The relevant energy, therefore, is the free energy of the statistical ensemble representative of the system under consideration. For example, in the canonical ensemble, this is the Helmholtz free energy\n\\[F = U-TS\\]\nwhere \\(U\\) is the internal energy, \\(T\\) the temperature and \\(S\\) is the entropy of the system. Therefore, in a broader sense, soft matter is the physics of those systems for which the internal energy and the entropy are on comparable scales.\nIn other words, fluctuations of the internal energy are on the same scale as thermal fluctuations:\n\\[\\Delta U \\sim k_BT \\]\nwhere \\(k_B\\) is the Boltzmann constant and \\(\\Delta U\\) indicates standard deviations from the average internal energy.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Entropy matters</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_intro.html#systems-and-definitions",
    "href": "soft-matter/soft-matter_intro.html#systems-and-definitions",
    "title": "13  Entropy matters",
    "section": "",
    "text": "13.1.1 Elementary constituents and energy scales\nSoft matter covers a wide spectrum of deformable systems. Each is constituted of many parts . Each is deformable because the interactions amongst such parts are weak compared to the perturbing forces (e.g. thermal fluctuations or mechanical loading).\nIn hard condensed matter, the elementary constituents are the atoms themselves, eventually with their subatomic particles. Between atoms, the scale of the interaction energies is in the 0.1 to 10 eV: for example the carbon-carbon covalent bond is approximately 3.6 eV.\nThe main units of soft matter are not atoms. They are instead themselves aggregates of atoms such as:\n\ncolloids, micrometer- to nanometer-sized particles dispersed in a fluid.\npolymers, macromolecules composed of long chains such as DNA, proteins, plastics\nsurfactants, macromolecules wuth polar head and tails that lead to the spontaneous formation of structures such as bilayers (e.g. the cellular membrane).\n\nAmongst such units, the dominating forces of soft matter are much weaker than in hard condensed matter:\n\nVan der Waals forces are of the order of 0.001-0.01 eV\nweak interactions such as hydrogen-bonds are typically in the 0.01-0.2 eV range\nthe thermal energy at room temperature is \\(k_B T \\approx 0.025 eV\\) (check it for yourself)\n\nAt the microscopic level, all these interactions have essentialy one source: the electrostatic force. However, this information is practically of no use when we want to understand how the units of soft matter come together to give rise to macroscopic properties of soft matter systems, such as their elastic properties, their viscosities, their plasticity. In fact, the emphasis on the atomistic details of the various units is fundamentally misleading: atomistically very different objects (e.g. colloids and micelles) can in fact share very similar macroscopic behaviours.\nTherefore, theories of soft matter leverage the concept of coarse-graining, rooted in the renormalisation group notions explored earlier. Coarse-graining means integrating out the unimportant degrees of freedom and only describing the units in terms of a few important parameters. For example, instead of taking a full atomistic representation of the DNA we may want to focus on the fact that structurally it is a long chain with specific bending energies: we may want to include the fact that it is formed by a double helix but we may not want to specifically construct every single atom in the sugar chain the forms the backbone. An example of DNA coarse garining is provided by the succesful model oxDNA (see picture below):\n\n\n\noxDNA model: (a) Base structure on one strand; (b) planarity of the bonding; (c) an example of the resulting double strand.\n\n\n\n\n\n\n\n\nNoteCoarse graining\n\n\n\n\n\nCoarse graining is often motivated by intuition, experimental insights or a simple desire of simplification. It allows for a multi-scale description of the problem which permits to describe large systems and log time scales.\nNonetheless, coarse-graining can also be made mathematically rigorous. For example, rigorous coarse graining in statistical mechanics can be performed by projecting the dynamics onto a relevant and an irrelevant (fluctutating) part, in the so called Mori-Zwanzig formalism (Mori 1965).\n\n\n\n\n\n13.1.2 Classes of systems\nIn our exploration of soft matter we will focus on six main classes of systems which display different physics:\n\ncolloidal dispersions\npolymeric systems\nliquid crystals\nsurfactant aggregates\narrested systems\nactive matter\n\n\nColloidal dispersions\nColloidal dispersions are systems where small particles, typically in the nanometer to micrometer range, are dispersed in a continuous medium (a solvent). Prototypical colloids are spherical particles of various sizes (e.g. as those present in paint) but colloidal science has achieved a high degree of sophistication, with colloidal particles with various different shapes and interactions.\nColloids are often thought as big atoms: they exhibit Brownian motion, can form ordered structures (colloidal crystals), and display phase transitions similar to atomic systems. However, their larger size and slower dynamics make them ideal for studying phenomena that are difficult to observe in atomic systems.\n\n\nPolymeric systems\nPolymer physics is a field on its own. Polymers are macromolecules composed of repeating structural units (monomers) connected by covalent bonds. Their unique properties arise from their string-like structure and the interplay between entropic and energetic contributions.\nPolymers can be classified into two main categories:\n\nSynthetic polymers, including plastics (e.g., polyethylene, polystyrene) and synthetic rubbers.\nBiopolymers, such as polymers like DNA, RNA, and proteins.\n\n\nimport py3Dmol #install this with: pip install py3dmol\n\npdb_id = \"1XQ8\"  # Human micelle-bound alpha-synuclein, an analog of long polymers \n# Fetch the PDB structure from the RCSB PDB server\nview = py3Dmol.view(query='pdb:' + pdb_id)\n# Customize visualization (you can change the style or the color)\nview.setStyle({}, {'sphere': {'color': 'spectrum'}});\nview.setBackgroundColor('white')  # Set background color to white\nview.show()\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages.\n        \n\n3D Rendering of the bio-polymer synuclein as extracted from the RCSB Protein Data Bank (click to rotate)\n\n\nCompared to colloids, polymers have distiunctive characteristics common to all chain-like molecules, such as their topological constraints due to teh fact that two polymers cannot cross each other (a phenomenon known as polymer entanglement).\n\n\nLiquid crystals\nWhen we take soft matter units that are highly anisotropic (e.g. elongate in particular directions) thermal fluctuations and high packing lead to equilibrium states woth a degree of order that is intermediate between the complete disorder of a liquid and the long-ranged, three-dimensional order of a crystal.\nSuch states are referred to as liquid crystal and can be described successffully with continuum free energy theories that take into account the symmetries of the order parameters.\nComponents that form liquid crystals are called mesogens and include highly anisotropic organic macromolecules (as used in liquid crystal displays), rod-like molecules or polymeric aggregates, as well as disk-shaped molecules and particles(such as triphenylene and derivatives).\n\n\nSurfactant aggregates\nWhen two distinct fluid phases are put into contact, a free energy cost per unit area ensues: this is the surface tension. It is possible to control the tension by introducing molecules that sit at the interface between the two phases, called surfactants.\nHence, surfactants are molecules which contain chemical groups with different affinities (they are amphiphilic). A key example is the case of phospholipids, which posses both hydrophilic (water-preferring) heads and hyrdrophobic (water avoiding) tails. As surfactants sit at inetrface they are able to self-assemble and separate different fluid phases, forming equilibrium bilayers and vesicles that are ubiquitous in cell biology.\n\n\nArrested systems\nWe have stressed that the thermal energy is distinctive of soft matter systems. It would be then natural to assume that as we reduce the temperature, we should converge readily to the solid state physics of crystalline solids. In fact, on the way to low temperatures, the lack of long-range order of most soft-matter systems has important consequences: many such systems find themselves trapped in states that are not corresponding to the global energy minimum (i.e. the crystal) and instead display non trivial mechanisms of structural relaxation. These systems are disordered a bit like liquids, but share various mechanical properties, such as emergence rigidity, elasticity and plasticity.\nExamples of arrested systems include:\n\nGlasses, such as silica glass or metallic glasses, where the system is kinetically trapped in a disordered state.\nGels, which are networks of interconnected particles or polymers that span the entire system, providing rigidity despite being mostly liquid.\n\nThese systems are arrested as their relaxation towards equilibrium is so slow that is longer than any observable timescales. This makes them fundamentally out-of-equilibrium systems, escaping from an ordinary description in terms of equilibrium statistical mechanics.\n\n\nActive matter\nActive matter refers to systems composed of units that consume energy to generate motion or mechanical stresses. Unlike passive systems, active matter is inherently out of equilibrium due to the continuous energy input at the microscopic level. Examples include:\n\nBiological systems, such as bacterial colonies, cell tissues, and flocks of birds.\nSynthetic systems, like self-propelled colloids or active gels.\n\nThe study of active matter focuses on understanding how individual activity leads to emergent collective behaviors, such as swarming, clustering, or pattern formation, often described using hydrodynamic theories or agent-based models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMori, Hazime. 1965. “Transport, Collective Motion, and Brownian Motion.” Progress of Theoretical Physics 33 (3): 423–55. https://bris.on.worldcat.org/oclc/8091001741.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Entropy matters</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html",
    "href": "soft-matter/soft-matter_colloids.html",
    "title": "14  Colloids",
    "section": "",
    "text": "14.1 Kinds of colloids\nColloids are mixtures where one substance is dispersed throughout another. They consist of particles that are larger than typical atoms/molecules but small enough to remain suspended without settling. Examples include milk, fog, and paint.\nColloids can be classified based on the state of the dispersed phase and the dispersion medium. Depending on the particular mixture, one can obtain a wide variety of soft materials, with unique mechanical, optical, and thermal properties.\nClearly, colloidal materials form an incredibly diverse class, and surround us in our everyday lives. Here are some visual examples from the table above:\nThe IUPAC definition of colloids is based on the idea that the particles are dispersed in a medium, creating a subdivision at the colloidal scale, which is approximately 1 nm to 1 µm.\nThe small size of colloids means that they are constantly subject to collisions with the atoms/molecules/particles of the medium, triggered by thermal fluctuations. Due to this, the colloids undergo Brownian motion. For each particle, the amount of energy received from the medium is of the order of \\(k_B T\\) (the reference energy scale of soft matter). This energy can be compared with the potential energy associated with a displacement of a particle radius to produce a dimensionless number, the gravitational Péclet number\n\\[{\\rm Pe}_g = \\dfrac{\\Delta m g R}{k_B T}\\]\nwhere \\(R\\) is the radius of the colloid, \\(g\\) is the acceleration due to gravity, and \\(\\Delta m\\) is the buoyant mass, which for a spherical colloid is expressed as \\(\\Delta m = \\dfrac{4\\pi}{3}\\Delta\\rho R^3\\), with \\(\\Delta \\rho\\) being the density difference between the particle and the dispersion medium (the solvent). We have a colloidal suspension only when \\({\\rm Pe}_g \\lesssim 1\\), i.e. when Brownian motion is only marginally perturbed by the effects of gravity.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#kinds-of-colloids",
    "href": "soft-matter/soft-matter_colloids.html#kinds-of-colloids",
    "title": "14  Colloids",
    "section": "",
    "text": "Dispersion Phase\n\nDispersion Medium\n\n\n\n\n\nSolid\nLiquid\n\n\nSolid\nSolid suspension:\npigmented plastics,\nstained glass,\nruby glass, opal, pearl\nSol, colloidal suspension:\nmetal sol,\ntoothpaste, paint,\nink, clay slurries, mud\n\n\nLiquid\nSolid emulsion:\nbituminous road paving,\nice cream\nEmulsion:\nmilk, mayonnaise,\nbutter, pharmaceutical creams\n\n\nGas\nSolid foam:\nzeolites, expanded polystyrene,\n‘silica gel’\nFoam:\nfroths, soap foam,\nfire-extinguisher foam\n\n\n\n\n\n\n\nFirst row: opal, paint, smoke. Middle row: ice-cream (gelato), milk, fog. Bottom row: expanded polystyrene and foam. Source: unsplash.com",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#stability-of-colloids-and-colloid-colloid-interactions",
    "href": "soft-matter/soft-matter_colloids.html#stability-of-colloids-and-colloid-colloid-interactions",
    "title": "14  Colloids",
    "section": "14.2 Stability of colloids and colloid-colloid interactions",
    "text": "14.2 Stability of colloids and colloid-colloid interactions\nA colloidal dispersion is said to be stable if it is able to remain dispersed and Brownian for a long time (typically, significantly longer than the experimental observation time). Unstable colloids undergo aggregation or sedimentation due to the dominance of internal attractive forces or external fields such as gravity.\nFor example, consider a colloidal suspension like milk. Milk is an emulsion where droplets of fat are dispersed in water and stabilised by proteins. If lemon juice1 is added, the dispersion medium (water) changes, the pH drops, and the emulsion is destabilised. This causes the proteins to coagulate, leading to the separation of curds (solid) and whey (liquid).\n\n\n\nCurds and whey resulting from the destabilisation of milk, a colloidal emulsion. (Wikimedia)\n\n\nIt is clear from this example that the nature of the dispersed phase and the dispersed medium ultimately determines the stability of colloidal dispersions. What ultimately matters for the stability is whether the colloids have a propensity to aggregate or not. This propensity is quantified in terms of colloidal interactions.\n\n14.2.1 Fundamental and effective forces\nAt the colloidal scales, only two kinds of fundamental forces are relevant: gravity and electro- (and occasionally magneto-) static forces. As we have seen above, when a system is truly colloidal, gravitational contributions are assumed to be small, so in effect, for a non-magnetic colloid (which is the vast majority of colloidal systems), only electrostatic forces are fundamentally important.\nHowever, colloidal dispersions consist of large particles that carry many charges both in the dispersed phase and the surrounding medium, arranged in a disordered manner at the atomic scale. This makes a microscopic description of all charges and the resulting electrostatic fields not only unfeasible but also ineffective for understanding colloidal systems in terms of key characteristics such as particle size, density, and spatial distribution. The fundamental issue is that of time scales: the motion of colloids is much slower than the motion of individual ions or molecules. Over such longer timescales, many interactions at the microscopic atomistic scale occur, and we can consider taking averages to extract macroscopic, effective interactions at the colloidal scale 2.\nThis is especially important due to quantum fluctuations: the uncertainty principle means that electron clouds around atoms are not fixed but exhibit intrinsic spatial fluctuations in their charge distribution. Perturbative approaches allow us to capture the effective forces resulting from such fluctuations.\nThere are many examples of such effective forces (Lekkerkerker, Tuinier, and Vis 2024). One you may already know is the Van der Waals interaction. For colloids, we have additional relevant forces, such as the double-layer interaction and the depletion interaction. We detail them below.\n\n\n14.2.2 Van der Waals interaction\nThe London-van der Waals dispersion forces arise from the interaction between instantaneous dipoles in overall neutral atoms or molecules. We know from classical electrostatics that static dipoles interact via the dipole-dipole interaction, with a potential strength which decays like \\(1/r^3\\), where \\(r\\) is the separation between two dipoles.\nMore generally, neutral atoms or molecules have electronic clouds that are fluctuating and not symmetrically distributed, creating a temporary dipole. Such an instantaneous dipole induces a dipole in a neighbouring atom or molecule by distorting its electron cloud.\nThe interaction energy between two dipoles \\(\\mu_1\\) and \\(\\mu_2\\) separated by a distance \\(r\\) is proportional to:\n\\[U \\propto -\\frac{\\mu_1 \\mu_2}{r^3}\\]\n\n\n\nThe dipole moments fluctuate due to quantum mechanical effects. The average interaction energy is derived using perturbation theory and is proportional to the polarizabilities \\(\\alpha_1\\) and \\(\\alpha_2\\) of the two particles:\n\n\\[\n    U(r) = -\\frac{C}{r^6}\n\\]\n\nwhere \\(C \\propto \\alpha_1 \\alpha_2\\) depends on the polarizabilities and ionization energies of the particles.\nThe resulting \\(r^{-6}\\) dependence is also referred to as the London dispersion force, which can be derived within quantum-mechanical perturbation theory (London 1937).\nWe can then assume two identical spherical colloids of radius \\(R\\) at distance \\(h\\) and that every volume element of such spheres interacts with the London dispersion force (see the semi-classical approach of Hamaker Hamaker (1937) for illustration). Integrating over all volume elements yields the colloidal spheres’ Van der Waals attractive potential in the form\n\n\\[W_{vdW}(h)=-\\dfrac{A_H}{6}f(h/R)\\]\n\nwhere \\(A_H\\) is the Hamaker constant and \\(f(h/R)\\) is\n\\[f(h/R) = \\left[ \\frac{2R^2}{h^2 - 4R^2} + \\frac{2R^2}{h^2} + \\ln\\left( \\frac{h^2 - 4R^2}{h^2} \\right) \\right]\\]\n\n\n\n\n\n\nNoteSketch of a derivation of colloid-colloid Van der Waals potential\n\n\n\n\n\nEach volume element in one sphere interacts with each in the other sphere via: \\[\n\\phi(r) = -\\frac{C}{r^6}\n\\] So the total interaction energy is: \\[\nU(h) = -C \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\] Let:\n\nSphere 1 be centered at \\((0, 0, 0)\\),\nSphere 2 be centered at \\((0, 0, h)\\),\n\\(\\mathbf{r}_1 = \\mathbf{r}\\), \\(\\mathbf{r}_2 = \\mathbf{r}'\\)\n\nThen \\(|\\mathbf{r}_1 - \\mathbf{r}_2| = |\\mathbf{r} - \\mathbf{r}'|\\), and the integral becomes: \\[\nU(h) = -C \\iiint_{|\\mathbf{r}| \\leq R} \\iiint_{|\\mathbf{r}' - h\\hat{z}| \\leq R} \\frac{1}{|\\mathbf{r} - \\mathbf{r}'|^6} \\, d^3\\mathbf{r} \\, d^3\\mathbf{r}'\n\\] Hamaker evaluated this by integrating over the densities of interacting atoms with number density \\(\\rho\\) in both spheres. The result: \\[\nU(h) = -\\rho^2 C \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\] Defining the Hamaker constant: \\[\nA_H = \\pi^2 \\rho^2 C\n\\] So, \\[\nU(h) = -\\frac{A_H}{\\pi^2} \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\] This six-dimensional integral can be evaluated analytically for spheres, yielding: \\[\nU(h) = -\\frac{A_H}{6} \\left[ \\frac{2R^2}{h^2 - 4R^2} + \\frac{2R^2}{h^2} + \\ln\\left( \\frac{h^2 - 4R^2}{h^2} \\right) \\right]\n\\]\n\n\n\n\n\n\n\n\n\nVan der Waals interactions are regarded as short-range forces because their decay rate (e.g., London’s \\(1/r^6\\)) is much faster than that of Coulombic interactions (\\(1/r\\)). Importantly, since their origin lies in the fluctuation of charges on the colloids, their strength is not additive: simply summing all the pairwise interactions does not fully account for many-body effects. We often rely on such two-body approximations, but we should be aware that they are a simplified scenario.\n\n\n14.2.3 Double-layer interaction\nColloids are often charged. The solution they are immersed in also has an inhomogeneous distribution of ions. There will be\n\nco-ions (same charge as the colloid) that will be pushed away from the colloid surface, while\ncounter-ions (opposite charge) will accumulate at the surface.\n\nThese two different concentrations of oppositely charged ions form what is called a double layer and its properties (such as its width) are controlled by the number of ions in the solvent, which can be tuned by adding or removing, for example, salts.\nSuppose we now have two colloids of the same size and total charge in the solvent. The charges in their double layers will interact giving rise to a repulsive interaction. This interaction is referred to as screened-Coulomb (as the electrostatic interaction is screened by the presence of the ions) or double layer repulsion. We are not going to derive it here, but it can be shown that, for a colloid of radius \\(R\\) in solvent with salt density \\(n_s\\) it can be approximated by an exponential decay\n\n\\[\nW_{DR}(h) = B\\dfrac{R}{\\lambda_B}\\exp{(-h/\\lambda_D)}\n\\]\n\nwhere \\(\\lambda_D\\) is called the Debye length \\[\n\\lambda_D = \\sqrt{\\dfrac{1}{8\\pi\\lambda_B n_s}}\n\\] while \\(\\lambda_B\\) is the Bjerrum length, which is itself derived from the characteristic distance at which two elementary charges have energy \\(k_B T\\) 3\nThe coefficient \\(B\\) is a material property that depends on the surface potential. We are not going into the details of these features, which are however important for the design of colloidal experiments.\n\nFrom our point of view, what matters is that identical colloids in solution appear to have two interactions of opposite sign:\n\na van der Waals component, typically attractive and emergent from induced dipole–dipole interactions resulting from quantum fluctuations of the electronic clouds;\na double layer component, repulsive in nature, resulting from the electrostatic repulsions induced by ions and counterions.\n\nThe sum of the two gives rise to the DLVO (Derjaguin–Landau–Verwey–Overbeek) interaction which is an elementary model for colloid stability and aggregation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantActivity\n\n\n\nModify the script above and test graphically that:\n\nWhen the salt concentration is low, the double layer repulsion dominates the DLVO interaction.\nThere are salt concentrations where two minima occur in the DLVO potential: one at very close distance (dominated by the Van der Waals attraction) and one, much shallower, at intermediate distances comparable to \\(2(R+\\lambda_D)\\). This minimum can lead to weak aggregation of colloids.\nLarge salt concentrations depress the DLVO maximum altogether and eventually the Van der Waals interaction dominates.\n\n\n\n\n\n14.2.4 Steric interactions and depletion interactions\nWe have seen that quantum fluctuations from the uncertainty principle can be recast via a semi-classical approach into effective short-range interactions (van der Waals). Another fundamental quantum principle—Pauli’s exclusion principle—is also at the source of key effective interactions that can be understood in a semi-classical picture. Indeed, the main consequence of the exclusion principle is that since electrons cannot occupy the same quantum state, there are minimal distances below which atoms cannot be brought together.\nThis simply means that as we take a pair of atoms, below a certain distance they repel each other with very strong forces that emerge from the overlapping electron clouds, even in the absence of double layer interactions. These repulsive interactions can be approximated in various ways: their strength depends on the details of the atoms and hence, in the case of colloids, on the details of the materials composing the colloids. We call these excluded volume interactions steric interactions4.\nThese fundamentally repulsive interactions mean that, in a system with \\(N\\) colloids, an additional \\((N+1)\\)th colloid does not have access to the entire configuration space: there is a large excluded volume due to the presence of the other colloids.\nWhat is intriguing about these interactions is that, even if they are purely repulsive, they can collectively give rise to effective attractive interactions: attraction through repulsion. The origin of this effect is purely statistical and hence generic.\n\n14.2.4.1 A simple example: particles in a one-dimensional line (hard rods)\nAs an introductory example, let us consider a very simple, idealised system of purely repulsive objects. Let us confine them along a one-dimensional line, bounded by hard (repulsive and impenetrable) walls separated by the distance \\(L\\). Assume the objects to be \\(N\\) spheres of diameter \\(d\\), or (equivalently) hard rods of length \\(d\\). They cannot overlap so, once they are placed along the line, their order cannot change.\nWhat we want to know is how these hard objects, that interact solely via repulsive interactions, distribute themselves along the line. The problem is a classic of statistical mechanics and thanks to its one-dimensional nature can be addressed extensively using analytical methods.\nHere we take a more algorithmic approach, and directly sample the probability density distribution \\(\\rho(x)\\) of finding a particle center at position \\(x\\).\nTo do so, we do the following:\n\n\n\n\n\n\nNoteAlgorithm\n\n\n\n\nposition the particles in a valid configuration (no overlaps with other particles or the walls)\npick a particle at random\nmove it slightly along the line\ntest if the new position is valid (no overlaps)\nif valid, accept the move otherwise, reject\ngo back to (2)\n\n\n\nThis very simple algorithm is based on a trial move and an acceptance/rejection step. This is the heart of a very popular molecular simulation method, named Metropolis Monte-Carlo Chain method.\nHere below you have a very simple implementation in python.\n\n\n\n\n\n\nThe code should produce a final probability distribution along the \\(x\\) axis. The main control parameter here is the packing fraction, i.e. \\(\\phi = \\dfrac{dN}{L}\\), the coverage of the line. If you take high values for the packing fraction you will observe an interesting effect: the distribution \\(\\rho(x)\\) displays distinctive modulations. These reflect the layering of the particles along the line, due to their hard-core interactions. However, the oscillations are even more interesting as we observe that the particles are more likely to be found near the walls than away from them.\nReflect on this point. In the complete absence of any attractive interactions, we find that the hard particles are preferentially located close to the walls. It is as if the walls exerted an attractive force capable of pulling the particles close to them. In reality, the force is purely statistical in nature. It is merely the result of the entropic advantage that the entire system acquires when the particles are closer to the walls: simply put, if the first (and last) particles are close to the walls, there is more space for the particles in the middle, hence a larger number of configurations and hence larger configurational entropy.\nEven if the force is statistical, it is not less real: in this simplified case, it leads to the layering of the density profile. Its generalisation to less idealised conditions leads to a family of forces that are essential for the aggregation of soft matter which are called depletion interactions.\n\n\n\n14.2.5 Asakura-Oosawa depletion potential\n\n\n\nMixture of colloids (yellow) and polymers (squiggly lines inside red circles). The depletion layers are as thick as the polymer radius (red circles) and are indicated with the dashes around the colloids. When the two layers do not overlap, the osmotic pressure due to the polymers on the colloids is balanced. When there is overlap, there is a region inaccessible to the polymers (purple) and the pressure is unbalanced, leading to aggregation.\n\n\nThe simple unidimensional scenario can be extended to a more interesting situation of hard-core colloidal particles dispersed in a medium where other smaller, repulsive particles (e.g. coiled polymers) are also dispersed. It is not important at this stage to know the details of such polymeric structures. We will ignore their internal structure and we will also ignore their mutual interactions. We will only consider for the moment how they interact with the colloids and how this mediates an interaction between the colloids. We call these idealised polymers penetrable hard spheres.\nThis assumption yields a great simplification: the polymers, on their own, are an ideal gas. Their chemical potential is given by\n\\[\\mu_p = k_BT \\ln \\rho_p^r \\tag{14.1}\\]\nwhere \\(\\rho_p\\) is the number density of polymers in the ideal reservoir related to their volume fraction \\(\\phi_p^{r}=\\rho_p v_p = \\frac{\\pi}{6} \\rho_p^{r} \\sigma_p^{3}\\), with \\(v_p= \\dfrac{\\pi}{6}\\sigma_p^{3}\\).\nWe assume also to work in an ensemble at fixed volume \\(V\\), temperature \\(T\\) and chemical potential \\(\\mu\\): this is the grand canonical ensemble. This means that we imagine that there is some ideal reservoir with which we can exchange polymers in order to maintain the chemical potential constant.\nThe grand potential for such ideal polymers is given by\n\\[\\Omega = -k_BT e^{\\mu_p/k_B T}V_{\\rm accessible}\\]\nwhere \\(V_{\\rm accessible}\\) is the accessible volume. In the absence of the colloids, the entire volume \\(V\\) is accessible.\nLet’s imagine to introduce two colloids at separation \\(r\\). For all \\(r&gt; 2(R+\\delta)=R_d\\), the accessible volume is \\(V_{\\rm accessible}^{\\infty}=V-2V_{\\rm exclusion}\\), where \\(R_d\\) is the depletion radius and \\(V_{\\rm exclusion}\\) is the inaccessible region due to the colloid-polymer interaction around each colloid: \\[\n\\begin{aligned}\nV_{\\rm exclusion}& = V_{\\rm outer}-V_{\\rm inner}\\\\\n&= \\dfrac{4\\pi}{3}\\left((R+\\delta)^3-R^3\\right)\n\\end{aligned}\n\\] Changing the separation between the two colloids but maintaining \\(r&gt;2R_d\\) does not change the grand potential: there is no free energy advantage and hence no effective interaction.\nInstead, it is only when we take the colloids closer than \\(2R_d\\) that we see a free energy difference. For \\(2R &lt; r &lt; 2R_d\\), an overlap region (lens-shaped) is formed. Its volume can be calculated geometrically as\n\\[\nV_{\\text{overlap}}(r)\n=\\frac{\\pi}{12}\\,\n\\bigl(4(R+\\delta)+r\\bigr)\\,\n\\bigl(2(R+\\delta)-r\\bigr)^{2},\n\\qquad\n2R \\le r \\le 2(R+\\delta),\n\\]\nwhich gives the exact Asakura-Oosawa potential:\n\\[\nW_{\\rm AO}(r) = - k_B T \\rho_p V_{\\rm overlap}(r).\n\\]\nThe approximate form commonly used for small polymer-to-colloid size ratios \\(q = \\delta / R \\ll 1\\) is obtained by expanding the overlap volume as a cubic polynomial in \\(r/(R+\\delta)\\):\n\\[\nV_{\\rm overlap}(r) \\approx \\frac{4 \\pi}{3} (R+\\delta)^3\n\\left[ 1 - \\frac{3}{4}\\frac{r}{R+\\delta} + \\frac{1}{16}\\left(\\frac{r}{R+\\delta}\\right)^3 \\right],\n\\quad 2R \\le r &lt; 2(R+\\delta),\n\\]\nwhich yields the approximate AO potential:\n\\[\nW_{\\rm AO}(r) \\approx - \\frac{4 \\pi \\rho_p k_B T}{3} (R+\\delta)^3\n\\left[ 1 - \\frac{3}{4}\\frac{r}{R+\\delta} + \\frac{1}{16}\\left(\\frac{r}{R+\\delta}\\right)^3 \\right].\n\\]\nThis cubic polynomial approximation is valid for small \\(q \\ll 1\\) and for center-to-center distances near contact. For larger polymers or separations near the interaction cutoff \\(2(R+\\delta)\\), the exact formula should be used.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteForce-based derivation of the AO interaction\n\n\n\n\n\nWhen the colloids are so close the polymers cannot enter the lens-shaped region between the two colloids. This gap leads therefore to a uniform distribution of polymers which results in a pressure difference: the outer polymers push the colloids together, producing an effective attractive force. Such pressure resulting from an uncompensated concentration gradient is called osmotic.\nThe lens-shaped overlap region between the two colloids consists of two identical spherical caps, each subtending an angle \\(\\theta_0\\) at the center of the colloid. From the given geometry, the angle is such that \\[\\cos\\theta_0 = \\dfrac{r}{2R_d}.\\]\nThe uncompensated pressure acts on such surface.\nFor symmetry reasons, only the forces along the axis connecting the two spheres contribute to the total force. For a given angle \\(\\theta\\) the component is proportional to \\(P\\cos\\theta\\) where \\(P\\) is the pressure exerted by the ideal gas of polymers: \\(P=\\eta_p^r k_B T\\) where \\(\\eta_p^r\\) is the polymer concentration. The surface element on which this pressure acts for a small increment \\(d\\theta\\) is \\[dS = 2\\pi R_d^2\\sin\\theta d\\theta\\]. Integrating over the range \\([0,\\theta_0]\\) yields the total force \\(F_d\\): \\[\n\\begin{aligned}\nF_d(r) &= -2\\pi \\eta_p^r k_B T R_d^2 \\int_0^{\\theta_0} \\sin\\theta \\cos\\theta \\, d\\theta \\\\\n&= -\\pi R_d^2 \\eta_p^r k_B T \\left[1 - \\left(\\frac{r}{2R_d}\\right)^2\\right]\n\\end{aligned}\n\\] Notice the negative sign, chosen to reflect the fact that the force is attractive.\nIntegrating the force yields the interaction potential.\n\n\n\n\n\n\n\n\n\nNoteSummary of main colloid-colloid interactions\n\n\n\n\n\n\n\n\n\n\n\nInteraction Type\nDescription\nRange\n\n\n\n\nVan der Waals\nAttractive forces arising from induced dipoles between particles.\nShort-range\n\n\nDouble Layer\nElectrostatic repulsion due to overlapping electrical double layers around charged particles.\nLong-range\n\n\nDLVO\nCombination of van der Waals attraction and double layer repulsion.\nShort and long range\n\n\nDepletion\nTypically attractive interactions emerging from purely entropic interactions\nShort range",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#colloids-as-big-atoms",
    "href": "soft-matter/soft-matter_colloids.html#colloids-as-big-atoms",
    "title": "14  Colloids",
    "section": "14.3 Colloids as big atoms",
    "text": "14.3 Colloids as big atoms\nColloids can be viewed as “big atoms”: they are large particles suspended in a medium, exhibiting thermal motion and interactions in various ways analogous to atoms, but at much larger length and time scales. As we have seen above, the interactions can have statistical or even quantum-mechanical origin, but are ultimately cast in a classical form that is amenable to a classical treatment. At the same time, the large scales of colloids mean that via dedicated microscopy techniques one is able to identify individual colloids, study their arrangements in detail, and follow their dynamics.\nAs mentioned earlier, there is a huge variety of colloids and a vast literature characterising their properties but also producing theoretical and computational models.\nHere we focus on the most elementary model of a colloid. The simplest such example is the purely repulsive hard-sphere colloid. While an idealisation, quasi-hard-sphere colloids can be prepared in the laboratory by various techniques (see Royall et al. (2024)). This includes synthesizing spheres of bundled polymers (you will learn about polymers in the next chapter) such as polymethyl methacrylate (PMMA), but also simply silica (small spheres of non-crystalline \\(\\rm SiO_2\\)) micron-sized beads carefully treated to screen and minimize the electrostatic interactions that would lead to DLVO-like contributions.\n\n14.3.1 The archetype: hard-spheres\nA hard-sphere is an idealized particle model in which each particle is represented as a perfectly rigid sphere of fixed radius \\(R\\) and diameter \\(\\sigma=2R\\). Hard spheres interact only through excluded volume: they cannot overlap, but otherwise experience no attraction or repulsion. The interaction potential \\(U(r)\\) between two hard spheres separated by a center-to-center distance \\(r\\) is: \\[\nU(r) = \\begin{cases}\n\\infty & \\text{if } r &lt; \\sigma \\\\\n0 & \\text{if } r \\geq \\sigma\n\\end{cases}\n\\]\nThis model captures the essential physics of excluded volume which, as we said earlier, fundamentally emerges from Pauli’s exclusion principle (electrons cannot occupy the same quantum state so electronic clouds of different atoms exclude each other).\n\n\n   r  U(r)      2R    ∞  0  forbidden  Hard Sphere Potential\n\n\nPhase behaviour of hard spheres\nSince the interaction potential is only based on excluded volume, the energy of hard spheres is trivial: it is always zero. A naive interpretation of such trivial energetics may lead to conclude that nothing interesting happens to a collection of hard spheres, since they are always at their energy minimum (namely, zero). However, it is clear from the earlier discussion of depletion forces that the interaction energy is only a part of the picture for systems subject to thermal fluctuations: indeed, for any \\(T&gt;0\\), entropic contributions to the free energy are always present. In the specific case of a fixed number of hard spheres in a fixed volume, they are the only contribution to the free energy.\nIn this sense, hard-spheres are completely entropy-driven systems. For a collection of identical (monodisperse) hard spheres the entropy is solely configurational and corresponds to the number of possible arrangements. This is constrained only by the accessible volume, of which we have seen an instance when considering the depletion interactions. Notice that changing the temperature does not really affect the statistics of the configurations: the Boltzmann factor \\(e^{U(\\mathbf{r}^N)/k_BT}\\) is always \\(1\\) for all valid configurations. The only way we can change the state of the system is by varying the accessible volume. For a system of \\(N\\) identical hard spheres in a volume \\(V\\) this can only be done in two ways\n\nby adding more spheres (of the same kind)\nby varying the volume \\(V\\)\n\nThe two routes essentially amount to varying one single parameter, which is the packing fraction (also called, volume fraction) of the system, defined as\n\\[\\phi = N\\dfrac{v_{\\rm sphere}}{V} = \\dfrac{\\pi \\sigma^6 N}{6V}\\]\nwhich can also be expressed in terms of the number density \\(\\rho=N/V\\) as \\(\\phi=\\pi\\sigma^3\\rho/6\\).\nTo change the phase behaviour of hard spheres we have a single control parameter, \\(\\phi\\), meaning that (differently from fluids like water) we are bound to have a one-dimensional phase diagram.\nWe explore the phase behaviour of hard spheres by considering different regimes of packing fraction.\n\n\nLow packing fractions\nThe hard repulsion between hard spheres means that each sphere is surrounded by an excluded volume in which the center of other spheres cannot be placed. Since the distance of closest approach between two spheres is \\(\\sigma\\) such excluded volume per particle is simply \\(v_{\\rm ex} =4\\pi\\sigma^3/3\\) and it is much larger than the volume per particle \\(v= V/N\\). A collection of \\(N\\) hard spheres has a total excluded volume \\(V_{\\rm ex}\\neq N v_{\\rm ex}\\) simply because the excluded volumes of individual particles can in general overlap, as we saw earlier with the case of depletion.\nAt very low densities, most hard spheres are isolated. So, in this case, we can approximate the total excluded volume as \\(V_{\\rm ex} \\approx N v_{\\rm ex}\\) so that the accessible volume (the volume not occupied by the spheres) is\n\\[V_{\\rm accessible} = V-Nv_{\\rm ex}\\]\nWe use this to perform an appropriate statistical mechanical calculation for the Helmholtz free energy of system \\(F\\), which we know is purely entropic \\(F=-TS\\). The partition function \\(\\mathcal{Z}\\) is \\[\n\\mathcal{Z} = \\frac{1}{N! \\Lambda^{3N}} \\int_{V_{\\rm accessible}} d\\mathbf{r}_1 \\ldots d\\mathbf{r}_N\n\\] where \\(\\Lambda\\) is the thermal de Broglie wavelength\n\\[\\Lambda = h/\\sqrt{2\\pi mk_B T},\\] and originates from the integration over the Maxwell-Boltzmann distribution of momenta for particles of mass \\(m\\), while \\(h\\) is Planck’s constant.\nFor hard spheres, the integral is over all configurations where no two spheres overlap (i.e., \\(|\\mathbf{r}_i - \\mathbf{r}_j| \\geq \\sigma\\) for all \\(i \\neq j\\)), and \\(V_{\\rm accessible}\\) is the total accessible volume. The result is that\n\\[\\mathcal{Z} = \\dfrac{(V-Nv_{\\rm ex}/2)^N}{N!\\Lambda^{3N}},\\]\nwith the \\(1/2\\) factor coming from the fact that we avoid double counting the excluded volume of pairs. The entropy is \\[\nS = k_B \\ln \\mathcal{Z} = k_B \\left[ N \\ln(V - N v_{\\rm ex}/2) - \\ln N! - 3N \\ln \\Lambda \\right]\n\\]\nFrom the partition function, we use Stirling’s formula \\(\\ln N! = N\\ln N -N\\) and obtain\n\\[\nS = k_B \\left[ N \\ln(V - N v_{\\rm ex}/2) - (N \\ln N - N) - 3N \\ln \\Lambda \\right]\n\\]\nwhich reads \\[\nS = N k_B \\left[ \\ln\\left( \\frac{V - N v_{\\rm ex}/2}{N \\Lambda^3} \\right) + 1 \\right]\n\\] The phase behaviour is encoded in the equation of state (the equation that links the three thermodynamic variables \\(P,T\\) and \\(\\phi\\)). To obtain it, we calculate the pressure\n\\[P = -\\left(\\dfrac{\\partial F}{\\partial V}\\right)_{N,T} =T\\left(\\dfrac{\\partial S}{\\partial V}\\right)_{N,T}= \\dfrac{k_B T }{v-v_{\\rm ex}/2}\\]\nThis expression can be simplified (do it as an exercise) to obtain the equation of state\n\\[Z_{\\rm comp}= \\dfrac{PV}{Nk_BT}=\\dfrac{1}{1-4\\phi} \\quad (\\phi\\ll 1)\\]\nalso known as the compressibility factor \\(Z_{\\rm comp}\\). Since \\(\\phi\\) is very small the expression is in fact\n\n\nThe choice of the letter “Z” is unfortunate. Do not confuse the compressibility factor with the partition function!\n\\[Z_{\\rm comp} = 1+4\\phi+O(\\phi^2)\\]\nThis expression makes it apparent that the first term linear in \\(\\phi\\) is a correction to the ideal gas law. This is an example (to very low order) of what is called the virial expansion. This, in general takes the form\n\\[Z_{\\rm comp}= 1 + B_2 \\rho + B_3 \\rho^2+ \\dots\\]\nwhere \\(B_2, B_3, \\dots\\) are the virial coefficients and for systems that are not hard spheres they depend also on the temperature, \\(B_2(T), B_3(T),\\dots\\). They are important as they encode the effects of correlations:\n\n\\(B_2\\) accounts for pairwise correlations (how the presence of one particle affects the probability of finding another nearby),\n\\(B_3\\) for three-body correlations, and so on.\n\nGiven an interaction potential the second virial coefficient \\(B_2\\) can be calculated independently from the equation state \\[\nB_2(T)=-\\frac{1}{2} \\int_0^{\\infty}\\left(\\exp \\left(-\\frac{U(r)}{k_B T}\\right)-1\\right) 4 \\pi r^2 d r\n\\] For hard spheres this results in \\[\nB_2 =  \\frac{2\\pi}{3} \\sigma^3\n\\] which is exactly what is predicted by the equation of state above, once you recognise that \\(\\phi = \\frac{\\pi\\sigma^3}{6}\\rho\\).\n\n\nExercise: check this calculation.\nHigher-order coefficients become increasingly complex to compute and reflect more complex many-body correlations that can be established even if the interaction potential is purely two-body (as in the case of hard spheres). The higher order coefficients become more and more important as the packing fraction is increased. Eventually something surprising occurs at a sufficiently high volume fraction.\n\n\nDense packing: metastability and crystalisation\nAs we increase the packing fraction, the accessible (free) volume reduces rapidly. At some point, disordered packing of spheres are so tightly packed that thermal motion becomes extremely inefficient or even impossible. Such disordered (random) packing of spheres are described as jammed: they are so densely packed that they are no longer behaving like a fluid but they instead display some of the rigidity that we typically associate with solids. Such jammed configurations are indeed examples of amorphous solids. We will explore them more in detail in the chapter dedicated to arrested systems.\nIt is important to note, however, that the disordered packing at high volume fractions are not minima of the free energy. The densest possible packing of hard spheres is achieved by arranging them in a crystalline lattice. In three dimensions, the densest packings are the face-centered cubic (FCC) and hexagonal close-packed (HCP) structures, both of which have a maximum packing fraction of \\[\n\\phi_{\\text{max}} = \\frac{\\pi}{3\\sqrt{2}} \\approx 0.74\n\\] This means that, at most, about 74% of the available volume can be filled by the spheres, with the remainder being empty space. This result was conjectured by Kepler in 1611 and proved rigorously only in 1998 (the Kepler conjecture).\nIn contrast, random close packing (the densest disordered arrangement of spheres) yields a lower packing fraction, typically around \\(\\phi_{\\text{rcp}} \\approx 0.64\\). Notice that this value is approximate and variations can be observed (due to randomness and, more importantly, to the method by which such packing are reached, i.e. the compression protocol). This means that the only way to compress spheres at very high packing has to lead (spontaneously) to the formation of crystals. We can indeed imagine to perform the following experiment:\n\nprepare a disordered assembly of colloidal hard spheres\ncompress them gradually to high and higher packing fraction, taking care to monitor the systems so that time correlations decay and the system is at thermal equilibrium\nmeasure the resulting packing fraction\n\nExperiments of this sort have been performed historically, leveraging for example the slow sedimentation of colloids, which on long time scales leads to the accumulation of dense packings of spheres. Such experiments reveal that beyond the so-called freezing packing fraction \\(\\phi_{f}=0.494\\) the system spontaneously forms small regions of crystals mixed with the fluid. At the high melting packing fraction \\(\\phi_{m}=0.545\\) the entirety of the system is then crystalline and its optical properties consequently change.\n\n\n\nPhase behaviour of hard-sphere colloids from Pusey et al. (2009)\n\n\nA simple cell model can help us rationalise what is taking place. When the spheres are packed within their FCC cell, they can move very little beyond their own diameter \\(\\sigma\\). Assume that the volume per particle again is \\(v\\) and the (geometrically constrained) close packed volume is \\(v_{cp}\\).\nThe maximum displacement is \\[\\delta=\\frac{\\sigma}{\\sqrt{2}}\\left(\\left(\\frac{v}{v_{c p}}\\right)^{1 / 3}-1\\right)\n\\] The corresponding free volume is then \\(v_f=\\frac{4 \\pi}{3} \\delta^3\\) from which we can calculate the entropy \\[\nS = -N k_B T \\ln \\left(v_f / \\Lambda^3\\right)\n\\] and the resulting pressure \\[\nP=\\frac{N k_B T}{v_{c p}} \\frac{\\left(v / v_{c p}\\right)^{-2 / 3}}{\\left(v / v_{c p}\\right)^{1 / 3}-1}\n\\] Rearranging and expressing everything in terms of packing fraction \\(\\phi = \\dfrac{\\pi\\sigma^3}{6v}\\) yields \\[\nZ_{\\rm comp}=\\frac{1}{1-\\left(\\phi / \\phi_{c p}\\right)^{1 / 3}}\n\\] What is notable here is that the expression we have obtained is a completely different functional form compared to the low density fluid regime. This is indicative of a discontinuous, first-order phase transition between the fluid and the crystalline phases. First order phase transitions are characterised by coexisting regions where the system can be found in partial fluid and partial crystalline state, as illustrated in the experiments above. This also means that we can prepare a disordered packing at very high density: this will not be its equilibrium state (global minimum of the free energy), but will still be stable for some (finite) time. This local equilibrium state is called a metastable state.\n\n\n\nSchematic free energy for hard spheres compressed at high pressures: the fluid branch becomes metastable and the free energy minimum is located in the crystal phase\n\n\nBut where does the free energy advantage of the crystal over the disordered fluid come from? From the discussion above the answer is obvious: crystals can accommodate higher packings, meaning that they use the available volume more efficiently. This in fact means that (on average) every hard sphere has more available volume if it arranged in the crystalline state compared to the fluid phase: the increased volume (available for thermal fluctuations and random particle displacements) is translated into an increased entropy. So, in fact the ordered, crystalline state has overall a higher entropy than the disordered fluid. This is an important instance in which the conventional storytelling, where entropy is just disorder, simply does not hold. As we have seen with depletion forces earlier, entropy can lead to structure: in the case of hard spheres, it is the only mechanism leading to structure, and such structure is the most orderly one can think of: long-range, crystalline order.\nThe video below shows instead the results of a Monte-Carlo simulation at packing \\(\\phi=0.493\\) for a small system of 32 particles. Small systems have enhanced fluctuations, and since the overall packing fraction is very close to the freezing line, we see spontaneous freezing and unfreezing of the small system (wait until second 14 in the movie).\n\nIn conclusion, colloidal hard spheres have a one-dimensional phase diagram, that depends only on the packing fraction but with various distinct phases\n\n\n\nHard-spheres phase diagram\n\n\n\n\n\n14.3.2 Beyond hard-spheres: simple liquids\nA simple liquid is a system of particles interacting via short-range, spherically symmetric (isotropic) pair potentials. The most common model is the Lennard-Jones (LJ) potential, which captures both the short-range repulsion (due to Pauli exclusion) and longer-range van der Waals attraction: \\[\nU_{\\mathrm{LJ}}(r) = 4\\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\n\\] where \\(\\epsilon\\) sets the depth of the potential well (interaction strength) and \\(\\sigma\\) is the particle diameter (distance at which \\(U=0\\)). The \\(r^{-12}\\) term models steep repulsion, while \\(r^{-6}\\) describes the attractive tail.\nThe Lennard-Jones fluid exhibits rich phase behavior: at low temperatures and densities, it forms a gas; at intermediate conditions, a liquid; and at high densities, a solid. The LJ model is widely used to study atomic and molecular liquids, and serves as a reference for understanding real fluids and their phase transitions.\nIn the phase diagram below these different phases have been highlighted. It is clear that, compared to hard spheres which only display a fluid and a crystalline solid phase, systems like the Lennard-Jones fluid display an additional phase, the liquid. This phase emerges from the presence of the attractive well in the interaction potential. This allows for short-range attractive interactions that cause a region of the phase diagram in the same class of universality of the Ising model and the lattice gas.\n\n\n\nPhase diagram of the Lennard-Jones Fluid, adapted from Wikimedia.\n\n\nThe Lennard-Jones interaction was designed to model noble gases (e.g. Argon) but has over time been used to model many other systems due to its simplicity and computational convenience: in particular it is used to construct coarse-grained models of macromolecules, as well as colloids and nanoparticles. It belongs to a wider class of classical, effective pair-wise interaction models used extensively to calculate properties and phase diagrams of a variety of condensed matter systems. These are useful because they allow one to, for example, long molecular simulations that are normally unreachable when considering atomistic calculations.\nThe following table provides you with a few example potentials and their functional forms:\n\n\n\n\n\n\n\n\nPotential Name\nMathematical Form\nTypical Systems/Features\n\n\n\n\nLennard-Jones (LJ)\n\\(U(r) = 4\\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\\)\nSimple atomic fluids, coarse-grained molecular interactions\n\n\nHard Sphere\n\\(U(r) = \\begin{cases} \\infty & r &lt; \\sigma \\\\ 0 & r \\geq \\sigma \\end{cases}\\)\nColloids, granular materials, excluded volume\n\n\nYukawa (Screened Coulomb)\n\\(U(r) = \\epsilon \\frac{e^{-\\kappa r}}{r}\\)\nCharged colloids, plasmas, electrolytes\n\n\nDipolar\n\\(U(r) = \\frac{\\mu_0}{4\\pi} \\frac{\\mu_1 \\mu_2}{r^3} (1 - 3\\cos^2\\theta)\\)\nMagnetic colloids, polar molecules",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#characterisation-of-colloidal-systems",
    "href": "soft-matter/soft-matter_colloids.html#characterisation-of-colloidal-systems",
    "title": "14  Colloids",
    "section": "14.4 Characterisation of colloidal systems",
    "text": "14.4 Characterisation of colloidal systems\nWhen looking at colloidal systems, we typically focus on two main aspects of their physics:\n\ntheir structural features, characterised by the spatial correlations between their constituents\ntheir dynamical features, characterised by the mobility of single particles or groups of particles.\n\nHere below, we briefly account of some main approaches to characterise these two dimensions.\n\n14.4.1 Structural properties: the radial distribution function\nStructural features of disordered (but also ordered) systems are described in terms of correlation functions. The underlying idea is that we are provided with an ensemble of stochastic variables (the positions of the colloids) which have a characteristic spatial distribution. For \\(N\\) particles we have an N-body probability distribution function \\(\\rho_N(\\mathbf{r}^N)\\) which contains all the necessary statistical mechanical information to describe the thermodynamics of the system. However, this is very difficult to access directly. In experiments or theoretical calculations we typically only have access to some lower order marginalisation of the distribution, in terms of few-body distributions.\nOne of the simplest assumptions we can make when we describe a system is that its potential energy is expressed in terms of purely pairwise additive potential, meaning that\n\\[ U_N (\\mathbf{r}^N)=\\sum_{i=1}^{N-1}\\sum_{j=i+1}^N V(r_{ij}) = \\dfrac{1}{2}\\sum_{i\\neq j}V(r_{ij})\\]\nwhere \\(V(r_{ij})\\) is the pairwise, radial interaction potential that only depends on the distance between particle centres \\(r_{ij} = |\\mathbf{r}_i-\\mathbf{r}_j|\\).\nFor systems with pairwise interactions, the natural spatial correlation function is a two-body correlation \\(g(\\mathbf{r}_i, \\mathbf{r}_j)\\), where \\(\\mathbf{r}_i\\) and \\(\\mathbf{r}_j\\) are two randomly chosen particles. In the case of non-crystalline, disordered systems the correlation function has to be\n\ntranslationally invariant, so that it only depends on the difference \\(\\mathbf{r}_i-\\mathbf{r}_j\\)\nrotationally invariant, so that there is no angular component and hence only the distance \\(r_{ij} =| \\mathbf{r}_i-\\mathbf{r}_j|\\) matters\n\nThis function \\(g(r)\\) is called the radial distribution function and plays a crucial role in the characterisation of fluids, crystals, glasses and much more. For systems with two-body interactions only, it contains in principle all the thermodynamic information necessary to reconstruct the free energy, as one can write \\[\n\\frac{F_{\\mathrm{ex}}}{k_B T}=2 \\pi \\rho N \\int_0^{\\infty}[g(r) \\ln g(r)-g(r)+1] r^2 d r+\\frac{\\rho N}{2 k_B T} \\int V(r) g(r) d^3 r\n\\] where the first term represents the entropic contributions and the second term represents the energetic contribution.\nBut how is it calculated? Very simply. You can think of constructing a histogram of the distances between all of the particles and suitably normalising such histogram by the density of the system. Mathematically this reads as\n\n\\[\ng(r) = \\frac{1}{\\rho N} \\left\\langle \\sum_{i=1}^N \\sum_{j \\neq i} \\delta(|\\mathbf{r}_i - \\mathbf{r}_j| - r) \\right\\rangle\n\\]\n\nwhere \\(\\rho = N/V\\) is the number density, and the angle brackets denote an ensemble average and \\(\\delta\\) is a Dirac delta function.\n\nFor an ideal gas, \\(g(r) = 1\\) everywhere (no correlations).\nFor hard spheres, \\(g(r) = 0\\) for \\(r &lt; \\sigma\\) (no overlap), and \\(g(r)\\) shows oscillations at higher \\(r\\) due to packing effects.\n\nNotice that the radial distribution function is an instance of the more general class of pair-wise correlations that have been introduced earlier, see Section 2.2 and it is paired with its reciprocal space Fourier transform, the structure factor:\n\\[\nS(k) = 1 + \\rho \\int \\left[ g(r) - 1 \\right] e^{-i \\mathbf{k} \\cdot \\mathbf{r}} d\\mathbf{r}\n\\]\nFor isotropic systems, this reduces to:\n\\[\nS(k) = 1 + 4\\pi \\rho \\int_0^\\infty r^2 [g(r) - 1] \\frac{\\sin(kr)}{kr} dr\n\\]\nExperimentally, one can measure \\(S(k)\\) using scattering techniques or \\(g(r)\\) via direct imaging in colloidal systems. They provide insight into short-range order, local structure, and phase transitions in soft matter.\n\n\n\n\n\n\nThe radial distribution function \\(g(r)\\) has two possible interpretations\n\nIt represents the probability density of finding a particle at distance \\(r\\) away from a reference particle relative to the probability density of an ideal gas at the same number density.\nIf a given reference particle is taken at the origin, then the local average density at distance \\(r\\) is \\(\\rho g(r)\\).\n\n\n\n\nHere below we show two codes to calculate the radial distribution function for an ideal gas (which is trivial) and then for an assembly of hard spheres\n\n\n\n\n\n\nWe can compare this with arrangements of hard spheres\n\n\n\n\n\n\nThe radial distribution function is the central object of much of liquid state theory, which aims to predict the shape of the correlation functions (such as \\(g(r)\\)) from the sole knowledge of the interaction potential between the elementary particles (see Santos (2016) for a gentle introduction to the topic). In particular, one can imagine the correlations between two particles \\(1\\) and \\(2\\) in a fluid to have two possible origins\n\na direct correlation between the two particles, mediated by direct interactions (e.g. collisions) between 1 and 2.\nan indirect correlation, mediated by other particles in the fluid\n\nThe \\(g(r)\\) contains both direct and indirect correlations. We can assume that a suitable function \\(c(r)\\) exists to express the direct correlations. In such case, for two-body interactions, we can write a hierarchy of equations via a central result of liquid state theory, the Ornstein-Zernike (OZ) equation\n\\[\nh\\left(r_{12}\\right)=c\\left(r_{12}\\right)+\\rho \\int c\\left(r_{13}\\right) h\\left(r_{32}\\right) d \\mathbf{r}_3\n\\tag{14.2}\\]\nwhere we defined the total correlation function as \\(h(r)=g(r)-1\\). The OZ equation is an integral equation. In Fourier space (assuming the isotropicity of a fluid) it becomes an algebraic equation \\[\n\\tilde{h}(k)=\\frac{\\tilde{c}(k)}{1-\\rho \\tilde{c}(k)}\n\\] Since both \\(h(r)\\) and \\(c(r)\\) are unknown, an additional relationship is needed, called a closure. This is constructed through physical arguments. A common one is the so-called Percus-Yevick closure\n\\[\nc(r)=[1+h(r)]\\left[1-e^{\\beta U(r)}\\right]\n\\] where the pairwise interaction enters explicitly.\nSolving the OZ equation with the Percus-Yevick closure produces realistic radial distribution functions in a wide range of packing fractions for hard spheres.\n\n\n\n\n\n\n\n\n\n\n\n14.4.2 Dynamics: single vs collective displacements\nWe have seen earlier that if we are given a number of particles evolving microscopically according to the Langevin equation of drag \\(\\gamma\\) and zero-mean noise \\(\\eta\\) \\[\nm \\frac{d \\vec{v}}{d t}=-\\gamma\\vec{v}+\\vec{\\eta}(t)\n\\] then an ensemble of independent particles will follow the diffusion equation:\n\\[\\frac{\\partial \\rho(\\mathbf{r}, t)}{\\partial t} = D \\nabla^2 \\rho(\\mathbf{r}, t) \\]\nwhere \\(\\rho(\\mathbf{r},t)\\) is the probability of finding a particle at position \\(\\mathbf{r}\\) at time \\(t\\) with diffusivity \\(D\\). The diffusivity \\(D\\) is related to the microscopic Langevin equation parameters via the fluctuation-dissipation relation (or Einstein relation) \\[\nD = \\frac{k_B T}{\\gamma}\n\\] where \\(k_B\\) is Boltzmann’s constant, \\(T\\) is temperature, and \\(\\gamma\\) is the friction (drag) coefficient.\n\n\n\n\n\n\nNoteSolution of the diffusion equation using Laplace transform\n\n\n\n\n\nThe 1D diffusion equation is: \\[\n\\frac{\\partial \\rho(x, t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}\n\\]\nSuppose the initial condition is a delta function at the origin: \\[\n\\rho(x, 0) = \\delta(x)\n\\] Take the Laplace transform in time: \\[\n\\tilde{\\rho}(x, s) = \\int_0^\\infty \\rho(x, t) e^{-st} dt\n\\]\nThe Laplace transform of the time derivative: \\[\n\\mathcal{L}\\left[\\frac{\\partial n}{\\partial t}\\right] = s\\tilde{\\rho}(x, s) - \\rho(x, 0)\n\\] So the transformed equation is: \\[\ns\\tilde{\\rho}(x, s) - \\delta(x) = D \\frac{\\partial^2 \\tilde{\\rho}(x, s)}{\\partial x^2}\n\\]\nRearrange: \\[\nD \\frac{\\partial^2 \\tilde{\\rho}}{\\partial x^2} - s\\tilde{\\rho} = -\\delta(x)\n\\] For \\(x \\neq 0\\), the homogeneous equation: \\[\nD \\frac{\\partial^2 \\tilde{\\rho}}{\\partial x^2} - s\\tilde{\\rho} = 0\n\\]\nGeneral solution: \\[\n\\tilde{\\rho}(x, s) = A e^{-\\lambda |x|}, \\quad \\lambda = \\sqrt{\\frac{s}{D}}\n\\] The delta function at \\(x=0\\) gives a discontinuity in the derivative: \\[\n\\left.\\frac{\\partial \\tilde{\\rho}}{\\partial x}\\right|_{x=0^+} - \\left.\\frac{\\partial \\tilde{\\rho}}{\\partial x}\\right|_{x=0^-} = -\\frac{1}{D}\n\\]\nCompute derivatives: \\[\n\\frac{\\partial \\tilde{\\rho}}{\\partial x} = -A \\lambda \\, \\text{sgn}(x) e^{-\\lambda |x|}\n\\] So at \\(x=0^+\\): \\(-A\\lambda\\), at \\(x=0^-\\): \\(A\\lambda\\). The jump is \\(-2A\\lambda\\).\nSet equal to \\(-1/D\\): \\[\n-2A\\lambda = -\\frac{1}{D} \\implies A = \\frac{1}{2D\\lambda}\n\\]\nSo: \\[\n\\tilde{\\rho}(x, s) = \\frac{1}{2D\\lambda} e^{-\\lambda |x|} = \\frac{1}{2\\sqrt{Ds}} e^{-|x|\\sqrt{\\frac{s}{D}}}\n\\] Invert the Laplace transform (using tables or convolution theorem): \\[\n\\rho(x, t) = \\frac{1}{\\sqrt{4\\pi D t}} \\exp\\left(-\\frac{x^2}{4Dt}\\right)\n\\]\nThis is the fundamental solution (Green’s function) of the diffusion equation.\n\n\n\nThe diffusion equation can also be read as a simple consequence of two requirements:\n\ncontinuity of the distribution of mass (no mass is lost during the transport), as expressed in the continuity equation \\[\n\\frac{\\partial \\rho(\\mathbf{r}, t)}{\\partial t} + \\nabla \\cdot \\mathbf{J}(\\mathbf{r}, t) = 0\n\\] where the divergence \\(\\nabla \\cdot \\mathbf{J}(\\mathbf{r}, t)\\) represents the net outflow of particles from a given region due to the flux \\(\\mathbf{J}\\).\nthe flux of particles is assumed to be proportional to the gradient in the density (or concentration of the particles). This can be taken as an empirical assumption, a reasonable approximation (i.e. a perturbative approach) or a consequence of underlying Brownian dynamics. It is known as Fick’s law:\n\n\\[\n\\mathbf{J}(\\mathbf{r}, t) = -D \\nabla \\rho(\\mathbf{r}, t)\n\\]\nThe diffusivity constant \\(D\\) is therefore central for the diffusion. It is possible to link the microscopic motion of the particles to the collective behaviour of the density distribution \\(\\rho(x,t)\\) by exploiting the connection provided by the mean squared displacement, which is simply the variance of the distribution \\(\\rho(x,t)\\) at time \\(t\\).\nAs discussed earlier, in \\(d\\) dimensions this is equal to \\[\\langle r(t)^2\\rangle-\\langle r(t)\\rangle^2 = 2d D t\\]\nHence, measuring the average squared displacements is sufficient to recover the diffusivity and to reconstruct the distribution.\n\n14.4.2.1 Diffusion and interactions\nWe have up to now considered the dilute (or non-interacting) limit, where collisions between the colloids are ignored. Let’s now consider instead simple colloids (again, hard spheres) and their dynamics.\nWe are interested in the mean squared displacement \\(\\left\\langle r^{2}(t)\\right\\rangle\\) as a function of time for different volume fractions. At low volume fractions, the particles undergo Brownian motion (random-walk diffusion) due to collisions with liquid molecules. The mean squared displacement (in three dimensions) is \\[\\left\\langle\\underline{r}^{2}(t)\\right\\rangle=6 D_{0} t\\] where the meaning of the subscript 0 will be apparent shortly.\nFor sufficiently dense hard spheres (e.g. above \\(\\phi \\sim 0.3\\)), however, different regimes are observed. At short times the particles diffuse with the short time (self) diffusion constant \\(D_{s}\\). This is determined from the short time limit and is smaller than the \\(D_{0}\\) measured for \\(\\phi \\rightarrow 0\\). The motion of the particles (self diffusion) is still driven by collisions with the liquid molecules, but in addition the interactions between particles become significant.\nWhile the particles are diffusing in their cages formed by their neighbours, the hydrodynamic interaction with the neighbours, transmitted through flows in the liquid, causes slowing down relative to the free diffusion at low concentrations. At intermediate times the particles encounter the neighbours and the interactions slow the motion down. To make further progress, the particle has to break out of the cage formed by its neighbours. Now the particles experience a further interaction, direct interactions (hard-sphere interactions), in addition to the hydrodynamic interactions. The long-time and long-ranged movement is also diffusive, i.e. we still have \\[\\langle r^{2}(t)\\rangle \\propto t\\] when the particles undergo large-scale random-walk diffusion through many cages.\nHowever, the motion is further slowed and a smaller diffusion constant relative to the motion in the short time limit is observed, the long time (self) diffusion constant \\(D_{L}\\).\n\n\n\n\n\n\n\n\n\nThe slowing down due to collisions eventually dominates the physics of hard spheres at high densities. This is more broadly true also for generic dense colloidal suspensions, where the short range interactions dominate over other mechanisms of motion (including the hydrodynamics). Eventually, for very dense packings one observes the emergence of a new physical regime where relaxation becomes anomalous and non-diffusive: this is the glassy regime, which we will revisit in a following chapter.\n\n\n\n14.4.3 Stokes-Einstein relation\nSuppose that the particles are subjected to an external force F in the x direction, e.g. gravity. In thermal equilibrium the Maxwell-Boltzmann distribution is valid, i.e. the particle density \\(n(x)\\) is given by\n\\[\nn(x)=n_{0} \\exp \\left(-U(x) / k_{B} T\\right)=n_{0} \\exp \\left(-F x / k_{B} T\\right)\n\\]\nwhere we assumed a constant force \\(F\\) in the last equation. In the case of gravity \\(F=m_{B} g\\) with \\(m_{B}\\) the buoyant mass. \\(n(x)\\) results from the balance between the motion of the particles due to the external force setting up a concentration gradient, and the resultant diffusion given by Fick’s law.\n\n{\n  const width = 600;\nconst height = 400;\nconst numCircles = 300;\n\nlet temperature = 300;    // Arbitrary scale\nlet viscosity = 0.02;     // Arbitrary scale\nconst particleRadius = 5; // pixels\nlet timeStep = 1;\n\nconst kB = 1; // reduced units\nlet gravityFactor = 0.1;\nlet gravity = 10 * gravityFactor;\n\n// Compute damping as velocity decay per timestep (Langevin friction)\nfunction computeDamping(viscosity, radius, dt) {\n  const gamma = 6 * Math.PI * viscosity * radius; // friction coeff\n  return Math.exp(-gamma * dt);\n}\n\nlet damping = computeDamping(viscosity, particleRadius, timeStep);\n\n// Create SVG\nconst svg = d3.create(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .style(\"border\", \"1px solid #ccc\")\n  .style(\"background\", \"#f9f9f9\");\n\n// Initialize circles\nconst circles = Array.from({ length: numCircles }, (_, i) =&gt; ({\n  id: i,\n  x: Math.random() * width,\n  y: Math.random() * height * 0.3,\n  vx: (Math.random() - 0.5) * 2,\n  vy: (Math.random() - 0.5) * 2,\n  radius: particleRadius,\n  color: d3.interpolateYlOrBr(Math.random() * 0.5 + 0.25)\n}));\n\n// Create circles in SVG\nconst circleElements = svg.selectAll(\"circle\")\n  .data(circles)\n  .enter()\n  .append(\"circle\")\n  .attr(\"r\", d =&gt; d.radius)\n  .attr(\"fill\", d =&gt; d.color)\n  .attr(\"opacity\", 0.8)\n  .attr(\"stroke\", \"#333\")\n  .attr(\"stroke-width\", 1);\n\n// Collision detection & response\nfunction handleCollisions() {\n  for (let i = 0; i &lt; circles.length; i++) {\n    for (let j = i + 1; j &lt; circles.length; j++) {\n      const c1 = circles[i];\n      const c2 = circles[j];\n      const dx = c2.x - c1.x;\n      const dy = c2.y - c1.y;\n      const dist = Math.hypot(dx, dy);\n      const minDist = c1.radius + c2.radius;\n\n      if (dist &lt; minDist && dist &gt; 0) {\n        const overlap = minDist - dist;\n        const nx = dx / dist;\n        const ny = dy / dist;\n\n        c1.x -= nx * overlap / 2;\n        c1.y -= ny * overlap / 2;\n        c2.x += nx * overlap / 2;\n        c2.y += ny * overlap / 2;\n\n        const dvx = c2.vx - c1.vx;\n        const dvy = c2.vy - c1.vy;\n        const vn = dvx * nx + dvy * ny;\n\n        if (vn &gt; 0) continue;\n\n        const restitution = 1.0;\n        const impulse = (-(1 + restitution) * vn) / 2;\n\n        c1.vx -= impulse * nx;\n        c1.vy -= impulse * ny;\n        c2.vx += impulse * nx;\n        c2.vy += impulse * ny;\n      }\n    }\n  }\n}\n\n// Update damping when viscosity or timestep changes\nfunction updateDamping() {\n  damping = computeDamping(viscosity, particleRadius, timeStep);\n}\n\n// Animate function\nfunction animate() {\n  const diffusion = (kB * temperature) / (6 * Math.PI * viscosity * particleRadius);\n\n  circles.forEach(c =&gt; {\n    // Apply gravity\n    c.vy += gravity * timeStep;\n\n    // Diffusion random kick scaled by sqrt(2*D/dt)\n    const diffusionForce = Math.sqrt(2 * diffusion / timeStep);\n    c.vx += (Math.random() - 0.5) * diffusionForce;\n    c.vy += (Math.random() - 0.5) * diffusionForce;\n\n    // Apply damping\n    c.vx *= damping;\n    c.vy *= damping;\n\n    // Update position\n    c.x += c.vx * timeStep;\n    c.y += c.vy * timeStep;\n\n    // Boundary collision\n    if (c.x - c.radius &lt; 0) {\n      c.x = c.radius;\n      c.vx *= -1;\n    }\n    if (c.x + c.radius &gt; width) {\n      c.x = width - c.radius;\n      c.vx *= -1;\n    }\n    if (c.y - c.radius &lt; 0) {\n      c.y = c.radius;\n      c.vy *= -1;\n    }\n    if (c.y + c.radius &gt; height) {\n      c.y = height - c.radius;\n      c.vy *= -1;\n    }\n  });\n\n  handleCollisions();\n\n  circleElements\n    .attr(\"cx\", d =&gt; d.x)\n    .attr(\"cy\", d =&gt; d.y);\n\n  diffusionDisplay.text(`Diffusion: ${diffusion.toFixed(4)}`);\n}\n\n// Start timer\nconst timer = d3.timer(animate);\n\n// Controls container\nconst controls = d3.create(\"div\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"padding\", \"10px\")\n  .style(\"background\", \"#f0f0f0\")\n  .style(\"border-radius\", \"8px\")\n  .style(\"width\", `${width}px`);\n\n// Slider factory\nfunction createSlider(labelText, min, max, initial, step, onChange) {\n  const container = controls.append(\"div\")\n    .style(\"margin-bottom\", \"10px\")\n    .style(\"display\", \"flex\")\n    .style(\"align-items\", \"center\")\n    .style(\"gap\", \"10px\");\n\n  container.append(\"label\")\n    .text(labelText)\n    .style(\"min-width\", \"120px\")\n    .style(\"font-weight\", \"bold\");\n\n  const slider = container.append(\"input\")\n    .attr(\"type\", \"range\")\n    .attr(\"min\", min)\n    .attr(\"max\", max)\n    .attr(\"step\", step)\n    .attr(\"value\", initial)\n    .style(\"flex\", \"1\");\n\n  const valueDisplay = container.append(\"span\")\n    .text(initial.toFixed(step &lt; 1 ? 4 : 0))\n    .style(\"width\", \"50px\")\n    .style(\"text-align\", \"right\")\n    .style(\"font-family\", \"monospace\");\n\n  slider.on(\"input\", function () {\n    const val = +this.value;\n    onChange(val);\n    valueDisplay.text(val.toFixed(step &lt; 1 ? 4 : 0));\n  });\n\n  return slider;\n}\n\n// Create sliders\ncreateSlider(\"Gravity\", -100, 100, gravity / gravityFactor, 10, v =&gt; gravity = v * gravityFactor);\n\ncreateSlider(\"Temperature\", 0, 1000, temperature, 10, v =&gt; temperature = v);\n\ncreateSlider(\"Viscosity\", 0.001, 0.1, viscosity, 0.001, v =&gt; {\n  viscosity = v;\n  updateDamping();\n});\n\n// createSlider(\"Time Step\", 0.1, 5, timeStep, 0.1, v =&gt; {\n//   timeStep = v;\n//   updateDamping();\n// });\n\n// Diffusion display\nconst diffusionDisplay = controls.append(\"div\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"font-family\", \"monospace\")\n  .style(\"font-weight\", \"bold\")\n  .text(\"\");\n\n// Reset button\ncontrols.append(\"button\")\n  .text(\"Reset Positions\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"padding\", \"6px 12px\")\n  .on(\"click\", () =&gt; {\n    circles.forEach(c =&gt; {\n      c.x = Math.random() * width;\n      c.y = Math.random() * height * 0.3;\n      c.vx = (Math.random() - 0.5) * 2;\n      c.vy = (Math.random() - 0.5) * 2;\n    });\n  });\n\n// Start/Stop toggle\nconst startStopBtn = controls.append(\"button\")\n  .text(\"Stop\")\n  .style(\"margin-left\", \"10px\")\n  .style(\"padding\", \"6px 12px\")\n  .on(\"click\", function () {\n    if (timer._call) {\n      timer.stop();\n      this.textContent = \"Start\";\n    } else {\n      timer.restart(animate);\n      this.textContent = \"Stop\";\n    }\n  });\n\n// Container div\nconst container = d3.create(\"div\");\ncontainer.node().appendChild(svg.node());\ncontainer.node().appendChild(controls.node());\n\nreturn container.node();\n}\n\n\n\n\n\n\nIn the case of gravity, this leads to a sedimentation equilibrium. (a) Flux due to external force, \\(J_{F}\\)\nThe velocity of a particle under an applied force \\(F\\) in a viscous fluid can be written as \\(v=F / \\xi\\) which defines the friction coefficient \\(\\xi\\). Hence\n\\[\nJ_{F}=n(x) v=\\frac{n(x) F}{\\xi}\n\\]\nDiffusive flux, \\(J_{D}\\) is given by Fick’s Law (see above):\n\\[\nJ_{D}(x)=-D \\frac{\\partial n(x)}{\\partial x}\n\\]\nEquating the two fluxes \\(J_{F}=J_{D}\\) we get\n\\[\n\\frac{n(x) F}{\\xi}=-D \\frac{\\partial n(x)}{d x}=D \\frac{F}{k_{B} T} n(x)\n\\]\nThe second equality is obtained by differentiating the Maxwell-Boltzmann distribution. This gives the relation between the diffusion and friction coefficients:\n\\[\nD=\\frac{k_{B} T}{\\xi}=\\frac{k_{B} T}{6 \\pi \\eta R}\n\\]\nThe last equation applies to a spherical particle of radius \\(R\\) in a fluid of viscosity \\(\\eta\\), for which Stokes’s Law gives \\(\\xi=6 \\pi \\eta R\\) (which applies only at low Reynold’s number, \\(\\rho R v / \\eta\\) \\(\\ll 1\\) ) resulting in the Stokes-Einstein (and Sutherland) relation.\n\nThe Stokes-Einstein relation is a very deep result. It relates equilibrium fluctuations in a system to the energy dissipation when the system is driven off equilibrium. Here, the fluctuations in the fluid give rise to the diffusive motion of the suspended particle and \\(D\\) is therefore the response to the ‘fluctuations’ $k_B T $. A sheared fluid will dissipate energy because of its finite viscosity and thus \\(\\eta\\) represents the dissipative part.\nMore generally, Brownian motion sets a natural limit to the precision of physical measurements. Example from the Feynman Lectures:\n\nA mirror suspended on a torsion fibre reflects a spot of light onto a scale. The spot will jiggle due to the random impact of air molecules and the random motion of atoms in the quartz fibre. To reduce the jiggle, the apparatus has to be cooled. The relation between fluctuation and dissipation tells us where to cool. ‘This depends upon where [the mirror] is getting its ’kicks’ from. If it is through the fibre, we cool it … if the mirror is surrounded by a gas and is getting hit mostly by collisions in the gas, it is better to cool the gas. As a matter of fact, if we know where the damping of the oscillations comes from, it turns out that that is always the source of the fluctuations. (Adapted from Feynman, Chapter 41)\n\n\n\n\n\n\n\n\nImportantCheck your understanding\n\n\n\n\nColloids are mixtures with dispersed particles (1 nm – 1 μm) that remain suspended due to Brownian motion.\nStability of colloids depends on the balance between attractive (van der Waals) and repulsive (double layer) interactions.\nThe DLVO theory combines van der Waals attraction and double layer repulsion to explain colloidal stability and aggregation.\nEntropic effects (e.g., depletion interactions) can induce effective attractions even in purely repulsive systems.\nHard-sphere colloids are a fundamental model: their phase behavior is controlled by packing fraction, leading to fluid, crystalline, and metastable (jammed) states.\nThe radial distribution function \\(g(r)\\) characterizes spatial correlations and structure in colloidal systems.\nDynamics of colloids are governed by Brownian motion, with diffusion slowed at higher densities due to interactions.\nThe Stokes-Einstein relation links diffusion to temperature, viscosity, and particle size.\nEntropy can drive ordering (e.g., crystallization of hard spheres), showing that entropy is not always associated with disorder.\nColloidal systems serve as accessible models for studying fundamental concepts in soft matter and statistical mechanics.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#references",
    "href": "soft-matter/soft-matter_colloids.html#references",
    "title": "14  Colloids",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHamaker, Hugo C. 1937. “The London—van Der Waals Attraction Between Spherical Particles.” Physica 4 (10): 1058–72. https://www.sciencedirect.com/science/article/pii/S0031891437802037.\n\n\nLekkerkerker, Henk NW, Remco Tuinier, and Mark Vis. 2024. Colloids and the Depletion Interaction. Springer Nature. https://bris.on.worldcat.org/oclc/1428319704.\n\n\nLondon, Fritz. 1937. “The General Theory of Molecular Forces.” Transactions of the Faraday Society 33: 8b–26.\n\n\nPusey, PN, E Zaccarelli, C Valeriani, E Sanz, Wilson CK Poon, and Michael E Cates. 2009. “Hard Spheres: Crystallization and Glass Formation.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1909): 4993–5011. https://bris.on.worldcat.org/oclc/8582157462.\n\n\nRoyall, C Patrick, Patrick Charbonneau, Marjolein Dijkstra, John Russo, Frank Smallenburg, Thomas Speck, and Chantal Valeriani. 2024. “Colloidal Hard Spheres: Triumphs, Challenges, and Mysteries.” Reviews of Modern Physics 96 (4): 045003. https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.96.045003.\n\n\nSantos, Andrés. 2016. “A Concise Course on the Theory of Classical Liquids.” Lecture Notes in Physics 923: 064601–4. https://bris.on.worldcat.org/oclc/949904359.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#footnotes",
    "href": "soft-matter/soft-matter_colloids.html#footnotes",
    "title": "14  Colloids",
    "section": "",
    "text": "The citric acid in lemon juice lowers the pH of the water, making it more acidic. The pH is simply the negative logarithm of the concentration of hydrogen ions.↩︎\nThis is an instance of the idea of coarse graining that we have introduced earlier.↩︎\nAs you see, scaling by \\(k_B T\\) is a recurrent energy scale of soft matter. \\[\n\\lambda_B = \\dfrac{e^2}{4\\pi \\varepsilon_0\\varepsilon_r k_B T}\n\\]↩︎\nFrom the Greek στερεός, “solid, three-dimensional”.↩︎",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html",
    "href": "soft-matter/soft-matter_polymers.html",
    "title": "15  Polymers",
    "section": "",
    "text": "15.1 General molecular properties\nA polymer is a large molecule made up of many small, simple chemical units, joined together by chemical bonds. The basic unit of this sequence is called a monomer, and the number of units in the sequence is called the degree of polymerisation.\nIt is possible to have polymers containing over \\(10^{5}\\) units and there are naturally occurring polymers with a degree of polymerisation exceeding \\(10^{9}\\). For example, polystyrene with a degree of polymerisation of \\(10^{5}\\) has a molecular weight of about \\(10^{7} \\mathrm{~g} / \\mathrm{mol}\\) and, if fully stretched out, would be about 25 \\(\\mu \\mathrm{m}\\) long.\nPolymers play a central role in many fields, ranging from technology to biology. This is reflected in a huge number of different chemical structures. Given this manifold and the complexity of polymer molecules, the theories are astonishingly simple. This is possible because of the characteristic feature of polymers: The molecule itself is very large (compared to the individual units) and the macroscopic behaviour is dominated by this large-scale property of the molecule. Theories focus on such large-scale properties, whereas the small scale fine structure is typicaly resolved by computer simulations.\nPolymers exist in very different architectures, such as\nFurthermore, a large variation in the chemical structure may be achieved by combining different monomers (copolymerisation).\nFor simplicity, we will focus on linear homopolymers, i.e. with no branch points and with all identical subunits.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#general-molecular-properties",
    "href": "soft-matter/soft-matter_polymers.html#general-molecular-properties",
    "title": "15  Polymers",
    "section": "",
    "text": "linear: straight chains of repeating units (e.g., polyethylene).\nbranched: polymers with a main chain and side chains (branches) attached. The branching affects properties like density and melting point (e.g., low-density polyethylene).\nstar: polymers with multiple linear chains (arms) radiating from a central core. These structures can have unique properties like lower viscosity.\ncross-linked: polymers where chains are interconnected by covalent bonds, forming a network. This structure makes them rigid and heat-resistant (e.g., vulcanized rubber).\n\n\n\n\n15.1.1 Example structures\nHere below you can see the chemical structures of (sections) of a few common polymers. Hovering on the diagram should highlight the repeated units.\n\n\n\n\n\n\nNoteHow to use the visualisers\n\n\n\n\n\nCick on the 🔧 symbol to customise the visualisation:\n\nyou should see a State Tree to your left\nclick on the last item (highlighted in light blue, by default Ball & Stick)\nclick on Update 3D representation and select a suitable Type. For example the Cartoon mode will correspond to the kinnd of coarse graining we will have in mind when talking about polymer chains.\n\n\n\n\n\nPolystyrene \\(\\rm (C_8H_8)_n\\)\n    \n    \n    \n\n\nPoly(methyl methacrylate) (PMMA) \\(\\rm (C_5H_8O_2)_n\\)\nThis polymer is also known as acrylic, acrylic glass, or plexiglass. This a very common material also to fabricate colloids (PMMA particles).\n    \n    \n    \n\n\nNatural rubber \\(\\rm (C_5H_8)_n\\)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#models-for-the-conformation-of-polymers",
    "href": "soft-matter/soft-matter_polymers.html#models-for-the-conformation-of-polymers",
    "title": "15  Polymers",
    "section": "15.2 Models for the conformation of polymers",
    "text": "15.2 Models for the conformation of polymers\nIn order to understand the properties of most substances, we must consider a large assembly of molecules. In the case of polymers, however, the molecules themselves are very large and due to their flexibility they can take up an enormous number of configurations by rotation of chemical bonds. The shape of the polymer can therefore only be usefully described statistically and one need to use statistical mechanics to calculate the characteristics of even an isolated polymer.\nTo be able to investigate the properties of a single polymer and to neglect interactions between polymers, the polymer is placed in a very dilute solution. In this chapter, we will theoretically investigate the properties of an isolated, single polymer chain in solution (which in addition is linear and consists of only one kind of monomers).\n\n15.2.1 Freely-jointed chain\nMany polymers are highly flexible and are coiled up in solution. In a simple model we thus describe a polymer with two assumptions :\n\nit is composed of a large number of segments freely joined up\nall angles between segments are assumed to be equally likely.\n\nThe momomers are located at positions \\(\\mathbf{R}_{\\mathrm{j}}\\) and connected by bonds \\(\\mathbf{r}_{j}=\\mathbf{R}_{j}-\\mathbf{R}_{j-1}\\) of length \\(\\left|\\mathbf{r}_{j}\\right|=b_{0}\\).\nThe end-to-end vector \\(\\mathbf{R}\\) is then simply the vector linking the head to the tail of the chain:\n\\[\\mathbf{R} = \\mathbf{r}_1+\\mathbf{r}_2+\\dots+\\mathbf{r}_N = \\sum_{j=1}^N \\mathbf{r}_j.\\]\nAt any instant, the configuration (arrangement) of the polymer is one realisation of an N-step random walk in three dimensions.\n\n\n\n\n\n\n\nEnd-to-end vector\nIn time, the various segments undergo Brownian motion and the polymer fluctuates between all possible configurations of the random walk. Remember however that it does this with fixed inter-monomer distance.\nThe mean squared end-to-end distance is then simply \\[\\begin{aligned}\\langle\\mathbf{R}^2\\rangle &= \\left\\langle \\left( \\sum_{i=1}^N \\mathbf{r}_i\\right) \\cdot  \\left( \\sum_{j=1}^N \\mathbf{r}_j\\right) \\right\\rangle\\\\\n&= \\left\\langle \\sum_{i=1}^N \\sum_{j=1}^N  \\mathbf{r}_i \\mathbf{r}_j \\right\\rangle\\\\\n\\end{aligned}\\] We can split the sum into the terms where \\(i=j\\) and the rest. This yields in general\n\\[\\langle \\mathbf{R}^2\\rangle = Nb_0^2+\\langle \\mathbf{r}_i \\mathbf{r}_j  \\rangle\\]\nwhere the second term simply encodes the covariances between the (random) variables \\(\\mathbf{r}_i,\\mathbf{r}_j\\). Since we assumed (in this simplistic case) that they are independent, only the first term remains for the free-jointed chain\n\\[\\langle \\mathbf{R}^2\\rangle = Nb_0^2\\]\nThis is essentially the same result as for the mean squared displacement of a random walk, provided that we recognise that the number of monomers has taken the role played by time in the case of the walk. We had \\({\\rm MSD}\\propto t\\) and here we have\n\\[\\langle \\mathbf{R}^2\\rangle\\propto N\\]\nSince we formally inherit all the results holding for a random walk, we can also predict what happens to the distribution of possible end-to-end distances in the limit of long polymers, i.e. large \\(N\\).\nThe mean squared end-to-end distance can be decomposed as\n\\[\n\\left\\langle\\mathbf{R}^2\\right\\rangle=\\left\\langle R_x^2\\right\\rangle+\\left\\langle R_y^2\\right\\rangle+\\left\\langle R_z^2\\right\\rangle=3 \\sigma^2=N b_0^2 \\Rightarrow \\sigma^2=\\frac{N b_0^2}{3}\n\\] where \\(\\sigma\\) is the variance per component.\nWe know that the random walk converges to a Gaussian distribution in 3d with each component having variance \\(\\sigma\\). Thus, for large \\(N\\), the probability distribution for the end-to-end vector \\(\\mathbf{R}\\) is \\[\nP(\\mathbf{R}) = \\left( \\frac{3}{2\\pi N b_0^2} \\right)^{3/2} \\exp\\left( -\\frac{3 \\mathbf{R}^2}{2 N b_0^2} \\right)\n\\]\nThis means that, for long chains, the end-to-end distance is distributed as a 3D Gaussian, centered at zero, with variance proportional to \\(N\\). Also, we can\n\n\nRadius of gyration\nWhile the end-to-end distance represents a well-defined quantity for a linear chain, we need a more versatile measure of the size for more complicated architectures, such as branched or star-shaped polymers. This is provided by the so called radius of gyration, a measure of the (average) extent of the polymer chain.\nThe radius of gyration is a generic quantity that can be measured from any point cloud. It is closely linked to the (co)-variance of the set of points.\nWe define the center of mass as the average position\n\\[\\mathbf{R}_{\\rm CM}=\\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{R}_{j}\\]\nA general way to describe the spatial extent and shape of the polymer is to construct the gyration tensor (also called the configuration tensor):\n\\[\n\\mathbf{S} = \\frac{1}{N} \\sum_{j=1}^N (\\mathbf{R}_j - \\mathbf{R}_{\\rm CM}) \\otimes (\\mathbf{R}_j - \\mathbf{R}_{\\rm CM})\n\\]\nwhere \\(\\otimes\\) denotes the outer product, and \\(\\mathbf{S}\\) is a \\(3 \\times 3\\) symmetric matrix. The elements of \\(\\mathbf{S}\\) are given by\n\\[\nS_{\\alpha\\beta} = \\frac{1}{N} \\sum_{j=1}^N (R_{j,\\alpha} - R_{{\\rm CM},\\alpha})(R_{j,\\beta} - R_{{\\rm CM},\\beta})\n\\]\nwhere \\(\\alpha, \\beta \\in \\{x, y, z\\}\\).\nThe radius of gyration squared is then simply the trace of this tensor:\n\\[\nR_g^2 = \\mathrm{Tr}(\\mathbf{S}) = S_{xx} + S_{yy} + S_{zz}\n\\]\nThe eigenvalues and eigenvectors of \\(\\mathbf{S}\\) provide information about the principal axes and shape anisotropy of the polymer coil. The tensor of gyration corresponds to the covariance matrix of the positions \\(\\mathbf{R}_j\\).\nThe radius of gyration is directly proportional to the end-to-end vector.\n\n\\[\n\\langle R_g^2 \\rangle = \\frac{1}{6} \\langle R^2 \\rangle\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe radius of gyrations is \\[\nR_g^2=\\frac{1}{N} \\sum_{j=1}^N\\left(\\mathbf{R}_j-\\mathbf{R}_G\\right)^2\n\\]\nWe can rewrite it as \\[\nR_g^2 = \\frac{1}{2N^2} \\sum_{j=1}^N \\sum_{k=1}^N (\\mathbf{R}_j - \\mathbf{R}_k)^2\n\\]\nand note that, from our previous result on the end-to-end distance, the mean squared distance between monomers \\(j\\) and \\(k\\) in a free jointed chain is \\[\n\\langle (\\mathbf{R}_j - \\mathbf{R}_k)^2 \\rangle = |j - k| b_0^2\n\\]\nSo, \\[\n\\langle R_g^2 \\rangle = \\frac{b_0^2}{2N^2} \\sum_{j=1}^N \\sum_{k=1}^N |j - k|\n\\]\nEvaluating the double sum gives: \\[\n\\sum_{j=1}^N \\sum_{k=1}^N |j - k| = \\frac{N^3 - N}{3}\n\\]\nThus, \\[\n\\langle R_g^2 \\rangle = \\frac{b_0^2}{2N^2} \\cdot \\frac{N^3 - N}{3} = \\frac{b_0^2}{6} (N - \\frac{1}{N})\n\\]\nFor large \\(N \\gg 1\\): \\[\n\\langle R_g^2 \\rangle \\approx \\frac{1}{6} N b_0^2\n\\]\n\n\n\n\n\n\n15.2.2 Freely-rotating chain\nIn a polymer molecule the bond angles are usually restricted, which leads to a limited flexibility of the molecule. Let us consider the case of \\(n\\)-butane:\n\\[\n\\mathrm{H}_{3} \\mathrm{C}-\\mathrm{CH}_{2}-\\mathrm{CH}_{2}-\\mathrm{CH}_{3}\n\\]\n\nN-butane\n    \n    \n    \nThe valence angle (also called the bond angle) is the angle formed between two adjacent chemical bonds originating from the same atom. In the context of polymers, it is the angle between two consecutive bonds along the polymer backbone. The value of the valence angle is determined by the chemical structure of the monomer and affects the flexibility and overall conformation of the polymer chain. For example, in \\(n\\)-butane, the C–C–C bond angle is about \\(112^\\circ\\). Still rotations about the C-C bond are possible.\n\n\n\nFour conformers of butane, from LibreText Chemistry\n\n\nIndeed, the potential energy of a polymer configuration depends on the dihedral angle.\n\n\n\nPotential energy of the above conformers\n\n\nAt low temperatures ( \\(k_{B} T &lt; {\\mathrm {\\text{configurational energy}}}\\)) the configuration will thus be predominantly of type A (an anti conformation). As the temperature is increased ( \\(k_{B} T \\sim\\) config. energy), there will also be C (gauche) configurations and at high temperatures ( \\(k_{B} T \\gg\\) config. energy), any angle will be possible.\nThis suggests that a good model of polymeric conformation can take fixed angles between bonds, but needs to include free rotation about the bonds. This model is called the freely-rotating chain.\n\n\n\nWe start with a fixed configuration of \\(\\mathbf{r}_{l}\\), \\(\\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\) and then add the next segment \\(\\mathbf{r}_{j}\\). While the bond angle \\(\\theta\\) is given by the chemistry of the molecule, the segment can still freely rotate about the axis defined by \\(r_{j-1}\\), i.e. \\(\\varphi\\) can take any value \\(0 \\leq \\varphi \\leq 2 \\pi\\). If we average \\(\\mathbf{r}_{j}\\) over \\(\\varphi\\), while keeping \\(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\) fixed, only the component in \\(\\mathbf{r}_{j}\\) direction remains:\n\\[\\langle \\mathbf{r}_{j} \\rangle_{\\mathbf{r}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\quad {\\rm fixed}}=\\cos \\theta \\mathbf{r}_{j-1}\\]\nMultiplying both sides by \\(\\mathbf{r}_k\\) and averaging over all configurations gives\n\\[\n\\left\\langle\\mathbf{r}_j \\cdot \\mathbf{r}_k\\right\\rangle=\\cos \\theta\\left\\langle\\mathbf{r}_{j-1} \\cdot \\mathbf{r}_k\\right\\rangle .\n\\]\nApplying this relation recursively leads to\n\\[\n\\left\\langle\\mathbf{r}_j \\cdot \\mathbf{r}_k\\right\\rangle=b_0^2(\\cos \\theta)^{|j-k|}\n\\]\nSince \\(\\cos \\theta&lt;1\\), correlations between \\(\\mathbf{r}_{j}\\) and \\(\\mathbf{r}_{k}\\) decrease with increasing distance \\(|j-k|\\) between the links and the orientations of distant links become uncorrelated.\nThe end-to-end distance \\(\\left\\langle R^{2}\\right\\rangle\\) of a freely-rotating chain is hence \\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =\\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left\\langle\\mathbf{r}_{j} \\cdot \\mathbf{r}_{k}\\right\\rangle=b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\cos ^{(j-k \\mid} \\theta\\right)=N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{j-1} \\cos ^{(j-k)} \\theta \\\\\n& =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\theta \\sum_{k=1}^{j-1} \\cos ^{-k} \\theta\n\\end{aligned}\n\\]\nwhere we used the same argument as above to deal with \\(|j-k|\\). To calculate these two sums we consider the geometric progression:\n\\[\n\\begin{aligned}\n& S=\\sum_{m=1}^{M} x^{m}=x+x^{2}+\\ldots .+x^{M} \\quad \\therefore x S=x^{2}+x^{3}+\\ldots .+x^{M+1} \\quad \\therefore S-x S=x-x^{M+1} \\\\\n& \\therefore S=\\frac{x-x^{M+1}}{1-x}\n\\end{aligned}\n\\]\nWith the help of this formula we get\n\\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\theta \\frac{\\frac{1}{\\cos \\theta}-\\frac{1}{\\cos ^{j} \\theta}}{1-\\frac{1}{\\cos \\theta}}=N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\theta-1}\\left(\\sum_{j=1}^{N} \\cos ^{j} \\theta-\\sum_{j=1}^{N} \\cos \\theta\\right) \\\\\n& =N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\theta-1}\\left(\\frac{\\cos \\theta-\\cos ^{N+1} \\theta}{1-\\cos \\theta}-N \\cos \\theta\\right)\n\\end{aligned}\n\\]\nFor large N this can be simplified\n\\[\\left\\langle R^{2}\\right\\rangle \\approx N b_{0}^{2}+\\frac{2 b_{0}^{2}}{1-\\cos \\theta} N \\cos \\theta=N b_{0}^{2}\\left(\\frac{1+\\cos \\theta}{1-\\cos \\theta}\\right)=C N b_{0}^{2}\\]\nwith \\(C=(1+\\cos \\theta) /(1-\\cos \\theta)\\).\nTo get a better idea of the effect of a fixed angle \\(\\theta\\), i.e. going from a freely-jointed to a freely-rotating chain, we look at a few special (but not necessarily very realistic) cases:\n\n\\(\\theta \\rightarrow 0 \\Rightarrow \\cos \\theta \\rightarrow 1-\\frac{\\theta^{2}}{2} \\Rightarrow C=\\frac{2-\\theta^{2} / 2}{\\theta^{2} / 2} \\approx \\frac{4}{\\theta^{2}} \\quad\\left(\\mathrm{C} \\approx 500\\right.\\) for \\(\\left.\\theta=5^{0}\\right)\\)\n\n\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle \\gg N b_{0}^{2}\n\\]\n\nThis corresponds to a nearly straight chain, i.e., a rigid rod. The end-to-end distance is much larger than that of a flexible chain with the same number of segments.\n\n\n                                                \n\n\n\n\n\\[\n\\begin{aligned}\n\\theta \\rightarrow \\pi-\\delta \\quad & \\Rightarrow \\cos \\theta \\rightarrow-1-\\frac{\\delta^{2}}{2} \\Rightarrow C=\\frac{\\delta^{2} / 2}{2-\\delta^{2} / 2} \\approx \\frac{\\delta^{2}}{4} \\quad\\left(\\mathrm{C} \\approx 2 \\times 10^{-3} \\text { for } \\theta=175^{0}\\right) \\\\\n\\end{aligned}\n\\]\n\n\\[\\therefore\\left\\langle R^{2}\\right\\rangle \\ll N b_{0}^{2}\\]\n\nThis corresponds to the opposite limit, where the chain is compact and forms a globular, collapsed assembly. Examples include:\n\npolypeptides (the constituents of amino acids) with strong hydrophobic interactions (e.g. alanine, leucine, methionine)\npolystyrene in water\nchromatin (the genetic information condensed in the nucleus of cells)\n\n\n\n\n                                                \n\n\n\n\n\\(\\theta \\rightarrow \\pi / 2 \\Rightarrow \\cos \\theta \\rightarrow 0 \\Rightarrow C=1\\)\n\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle=N b_{0}^{2}\\]\n\n\nThese are ideal conditions, where the random walk model of the free-jointed chain works exactly.\nAgain, we get \\(\\left\\langle R^{2}\\right\\rangle \\propto N\\), which suggests that a generic long freely-rotating chain can be represented by an effective freely-jointed chain with \\(N'\\) segments of length \\(b\\).\n\n\nThis is another example of coarse-graining.\nReal and effective chain must have the same actual length, i.e. \\[\nN b_0 = N' b\n\\] and the same end-to-end distance, \\[\nC N b_0^2 = N' b^2.\n\\] Solving these two equations gives \\(b = C b_0\\) and \\(N' = N / C\\).\nThese constraints result in \\(b=C b_{0}\\) and \\(N^{\\prime}=N / C\\). This has important consequences:\n\nAll sufficiently long flexible chains have identical behaviour as regards their dimensions: the chemical details are hidden in \\(N^{\\prime}\\) and \\(b.\\)\nWhile individual monomer pairs are not totally flexible, groups of monomers are\n\\(C\\) represents the number of monomers over which the orientational correlation is lost\n\\(b\\) is the so called “Kuhn” statistical segment length and defines a related characteristic known as the “persistence length” \\(l_{p}=b/2\\), also calculated directly via the correlation of bond vectors along the chain: \\[\n\\langle \\mathbf{r}_i \\cdot \\mathbf{r}_{i+n} \\rangle = b_0^2 \\, \\langle \\cos \\theta \\rangle^n = b_0^2 \\, e^{-n b_0 / l_p}\n\\] where \\(b_0\\) is the bond length, \\(\\theta\\) is the angle between consecutive bonds, and \\(n\\) is the number of bonds separating the two segments.\n\nFor the freely-rotating chain, \\[\nl_p = -\\frac{b_0}{\\ln \\langle \\cos \\theta \\rangle}\n\\]\nFor small angles, this simplifies to \\(l_p \\approx \\frac{b_0}{1 - \\langle \\cos \\theta \\rangle}\\).\n\n\nOriginal contour length: 200.0000\nCoarse-grained contour length: 163.8304\n\n\n                                                \n\n\n\n\n\nThis visualisation doesn’t strictly satisfy the equal length requirement we set out above, but satisfies the equal end-to-end vector for the sake of visualisation. The Kuhn length should be thought of as a statistical quantity and the coarse-grained chain as a polymer with the equivalent statistical properties to a chain of subunits of the original chain.\n\n\n\n15.2.3 Excluded volume effects\nThe fact that two monomers cannot occupy the same space has consequences on different length scales.\n\nOn a local length scale this prevents neighbouring monomers from coming too close together. This effect is taken into account in terms of a restricted bond-angles, which prevent them from overlaping.\nNon-overlap, i.e. excluded volume, of distant monomers along the chain has also to be taken into account and can have surprisingly large effects.\n\nTo estimate the importance of this effect, we consider the fraction of coil volume actually occupied by monomers:\n\\[\\quad V_{N}=N V_{1} \\sim N b^{3},\\]\nwhere \\(\\mathrm{V}_{1}\\) is the volume of a monomer.\nThe overall volume occupied by the whole coil is\n\\[\nV_{\\text {coil }}=\\frac{4 \\pi}{3}\\langle R_{g}^{2}\\rangle^{3 / 2} \\sim \\frac{4 \\pi}{3} N^{3 / 2} b^{3}\n\\]\n\\[\\therefore \\frac{V_{N}}{V_{\\text {coil }}}=\\frac{N b^{3}}{(4 \\pi / 3) N^{3 / 2} b^{3}} \\sim N^{-1 / 2}\\]\nThis means that for \\(N=10^{4}\\) monomers occupy only about \\(1 \\%\\) of the whole coil volume.\nThe overall chain size as estimated by the radius of gyration \\(\\langle R_{g}^{2}\\rangle^{1 / 2}\\) is determined by the competition of two effects:\n\nEntropy (and chain connectivity) favour a compact chain and avoid the more unlikely stretched configurations\nRepulsive excluded volume interactions want to expand the chain to avoid overlap.\n\nBased on this balance we will estimate the effect of excluded volume in a very hand-waving way. (Due to a fortuitous cancellation of errors introduced by various approximations, the result is practically identical to more rigorous treatments, which are very involved.)\nWe consider the Helmholtz function of a single chain, which is regarded as an assembly of particles with constant volume \\(\\mathrm{NV}_{1}\\) at constant temperature T :\n\\[\nF=U-T S\n\\]\nThe entropy S is given by\n\\[\nS=k_{B} \\ln (\\text {number of configurations })\n\\]\nwhere for a given end-to-end vector \\(\\mathbf{R}\\) the number of configurations is expected to be proportional to\n\\[\nP(\\mathbf{R})=\\left(\\frac{3}{2 \\pi\\langle R^{2}\\rangle}\\right)^{3 / 2} e^{-\\frac{3 R^{2}}{2\\langle R^{2}\\rangle}}\n\\]\nand hence\n\\[\nS \\sim \\frac{-3 k_{B} R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nThe internal energy \\(U\\) includes the kinetic and potential energy. However, the kinetic energy is independent of the configuration and thus of \\(\\mathbf{R}\\) and we only have to consider the potential energy.\nTo estimate the potential energy, we disregard the connectivity of the chain and calculate the interaction energy of a ‘segment gas’ confined in a volume \\(R^{3}\\). The probability of a monomer to lie in this volume is given by the fraction of total coil volume occupied by monomers, which we estimated above to be \\(N V_{1} / R^{3}\\)\nThus the probability of monomer-monomer contacts is \\(N^{2} V_{1} / R^{3} \\sim N^{1 / 2}\\). With an energy \\(\\varepsilon\\) of a monomer-monomer contact, the potential energy \\(U \\sim \\varepsilon N^{2} V_{1} / R^{3}\\). We thus obtain\n\\[\nF=\\frac{\\varepsilon N^{2} V_{1}}{R^{3}}+\\frac{3 k_{B} T R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nwhich can be minimized with respect to \\(R\\), i.e. \\(d F / d R=0\\), yielding\n\\[R^{5}=\\frac{\\varepsilon V_{1} b^{2}}{k_{B} T} N^{3} \\sim \\frac{\\varepsilon}{k_{B} T} N^{3} b^{5}\\]\n\\[ \\therefore R \\sim N^{3 / 5} b\\]\nSimulations give a very similar scaling, \\(\\mathrm{R} \\sim N^{0.588}\\). The chain can no longer be modelled by a random walk, but has to be described by a self-avoiding random walk. The distribution of end-to-end distances is also not Gaussian. Note that the exponent \\(3/5\\) here is an example of a critical exponent (though not in exactly the same sense that you saw when studying magnets and fluids). Inclusion of excluded volume effects changes the universality class of the scaling of the chain length and radius of gyration with \\(N\\) compared to the Gaussian chains that we looked at previously.\nAlthough the difference between an exponent of 0.5 (as is characteristic for the freely jointed and freely-rotating chains, i.e. a random walk) and 0.6 (excluded volume chain, i.e. self-avoiding random walk) seems small, it has a large effect at large N . For example, for \\(N=10^{4}, R=N^{0.5} b=100 b\\), while \\(R=N^{0.6} b=251 b\\), which corresponds to a swelling of the chain by a factor of 2.5.\n\n\nThis is an example of the importance of accurate scaling exponents and how they can impact our predictions.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#good-poor-and-theta-solvents",
    "href": "soft-matter/soft-matter_polymers.html#good-poor-and-theta-solvents",
    "title": "15  Polymers",
    "section": "15.3 Good, poor and theta solvents",
    "text": "15.3 Good, poor and theta solvents\nSo far we only considered monomer-monomer interactions, which we assumed to be purely repulsive, and neglected the influence of the solvent.\nHowever, the type of solvent has a great effect on the polymer size. If there is a high affinity with the solvent (‘good solvent’) the polymer swells, while it will shrink in a ‘poor solvent’.\nSolvent affinity refers to how strongly the solvent molecules interact with the polymer (or monomer) molecules compared to how they interact with themselves or with other monomers. Mathematically, this is captured by the interaction energy \\(\\varepsilon_{sp}\\) (solvent-monomer), compared to the average of solvent-solvent (\\(\\varepsilon_{ss}\\)) and monomer-monomer (\\(\\varepsilon_{pp}\\)) interactions.\nWe consider a lattice model, where each lattice site has \\(z\\) nearest neighbours and there are \\(N_{s}\\) solvent molecules and \\(N_{p}\\) monomers, with \\(\\mathrm{N}_{\\mathrm{sp}}\\) solvent-monomer contacts.\nThe energies of interaction are\n\n\\(\\varepsilon_{s s}\\) for the solvent-solvent\n\\(\\varepsilon_{\\mathrm{pp}}\\) for monomer-monomer\n\\(\\varepsilon_{s p}\\) for solvent-monomer interactions.\n\nThen the energy of mixing \\(\\Delta U_{\\operatorname{mix}}\\) is given by\n\\[\n\\Delta U_{\\operatorname{mix}}=U-\\left(U_{S}+U_{p}\\right)\n\\]\nwhere energy of pure solvent is \\[U_{s}=\\frac{z N_{s} \\varepsilon_{s s}}{2}\\]\nand the energy of pure polymer is \\[U_{p}=\\frac{z N_{p} \\varepsilon_{p p}}{2}\\]\n\nresulting in an energy of solution \\[U=N_{s p} \\varepsilon_{s p}+\\frac{\\left(z N_{s}-N_{s p}\\right) \\varepsilon_{s s}}{2}+\\frac{\\left(z N_{p}-N_{s p}\\right) \\varepsilon_{p p}}{2}\\]\nHence we obtain for the energy of mixing\n\\[\n\\Delta U_{\\mathrm{mix}}=N_{s p}\\left[\\varepsilon_{s p}-\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right)\\right]\n\\]\nwhich can be either positive or negative:\n\nGood solvent: \\(\\varepsilon_{s p}&lt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&lt;0\\)\n\n\nThis is the case of a ‘good solvent’, because the monomers prefer to be near the solvent molecules. Excluded volume effects then expand the chain.\n\nPoor solvent: \\(\\varepsilon_{s p}&gt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&gt;0\\)\n\nThis is the case of a ‘poor solvent’, because the monomers prefer to be near to each other (and similarly for the solvent molecules). The attraction between the different monomers offset the excluded volume effect.\nThe importance of the attractions generally depends on temperature. At very high temperatures the coil is expanded and the solvent quality is good. In contrast, at very low temperatures, the solvent quality is poor, attraction dominates, the coil collapses and phase separation is observed. In between these two limits, there is a temperature, the so-called theta temperature \\(\\theta\\), where the coil has ideal dimensions and the effects of excluded volume and attraction cancel each other. The solvent at \\(T=\\theta\\) is called a ‘theta solvent’. The stronger the attractions the higher \\(\\theta\\) will be, while for weak attractions \\(\\theta\\) is low. A full treatment of the coil expansion is rather involved and has to take into account excluded volume, attractions, configurational entropy and entropy of mixing. You can read more about the theta condition in Jones (2002).\n\n\n\n\n\n\nImportantCheck your understanding\n\n\n\n\nA polymer has characteristic features on different length scales.\nOn a very global length scale, it has a molar mass \\(M\\) and an overall size which can be characterised by the root mean square end-to-end distance \\(\\langle R^{2}\\rangle^{1 / 2}\\) or radius of gyration \\(\\langle R g^{2}\\rangle^{1 / 2 }\\propto N^{\\nu} \\propto M^{\\nu}\\), where \\(\\nu=1 / 2\\) for a freely-jointed or freely-rotating chain (random walk) and \\(\\nu=3 / 5\\) for an excluded volume chain (self-avoiding random walk).\nOn a smaller length scale the behaviour will be dominated by the finite flexibility or persistence of the chain, which is characterised by the Kuhn length b. The chain will essentially behave like a stiff rod on this length scale. This rod typically has a constant mass per length, \\(M / L\\), and thus \\(M \\alpha L\\).\nFinally, the local cross-sectional structure will be observed on an even smaller length scale.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#concentrated-polymer-solutions",
    "href": "soft-matter/soft-matter_polymers.html#concentrated-polymer-solutions",
    "title": "15  Polymers",
    "section": "15.4 Concentrated polymer solutions",
    "text": "15.4 Concentrated polymer solutions\nUp to now we considered a single polymer in a very dilute solution. Now we increase the concentration in steps until we reach bulk polymers.\nThe most important regimes of concentration are:\n\n15.4.1 Dilute regime\n\nThe polymer coils are well-separated on average. Call \\(c\\) the concentration expressed as mass per unit volume, then it satisfies\n\\[ \\dfrac{c}{M}N_A \\times \\dfrac{4\\pi}{3}R_g^3\\ll 1\\] where \\(N_A\\) is Avogadro’s number, \\(M\\) is the molar mass of a single chain and \\(R_g\\) is the radius of gyration of the chain.\n\n\n\nOverlap occurs when the volume fraction of coils reaches unity and thus\n\\[\n\\frac{c^{*}}{M} N_{A} \\frac{4 \\pi}{3} R_{g}^{3} \\sim 1 \\quad \\therefore c^{*}=\\frac{3 M}{4 \\pi N_{A} R_{g}^{3}}\n\\]\nusing \\(R g=\\langle R g^2\\rangle^{1 / 2}=B M^{\\nu}\\) gives\n\\[\nc^{*}=\\frac{3}{4 \\pi N_{A} B^{3}} M^{1-3 \\nu}\n\\]\nFor example, polystyrene with \\(M=10^{6} \\mathrm{~g} \\mathrm{~mol}^{-1}\\) in a good solvent ( \\(v=0.6\\) ) and \\(B=0.028 \\mathrm{~nm}\\left(\\mathrm{~g} \\mathrm{~mol}^{-1}\\right)^{-0.6}\\) leads to \\(\\mathrm{c}^{*}=0.29 \\mathrm{~kg} \\mathrm{~m}^{-3}=0.29 \\mathrm{mg} / \\mathrm{ml}\\). With the density of polystyrene \\(\\rho=1050 \\mathrm{~kg} \\mathrm{~m}^{-3}\\), the volume fraction of monomers is \\(\\mathrm{c}^{*} / \\rho=0.28 \\times 10^{-3},. Hence, \\mathrm{c}^{*}\\) can be very small for large polymers.\n\n\n15.4.2 Semi-dilute\n\nThe concentration is larger than the overlap concentration \\(\\mathrm{c}^{*}\\), but still much smaller than the bulk density. The coils interpenetrate and entangle, but the solution is still mostly solvent.\n\n\n15.4.3 Concentrated\nIn this case the concentration is very close to the bulk density and the polymer monomers occupy a significant fraction of the total volume.\n\n\n15.4.4 Bulk polymers\nA bulk polymer refers to a polymeric material in which the polymer chains occupy a significant fraction of the total volume, with little or no solvent present. In this regime, the properties of the material are dominated by polymer-polymer interactions rather than polymer-solvent interactions. Bulk polymers can be amorphous, semi-crystalline, or crystalline, and their mechanical, thermal, and optical properties are determined by the arrangement and mobility of the polymer chains.\nBulk polymers are divided into two main classes, characterised by whether they are cross-linked or not.\n\nThere are elastomers or rubbers, which have a low degree of cross-linking that allows for flexibility and elasticity (the material can stretch and return to its original shape), and thermosets, which have a high degree of cross-linking that creates a rigid, three-dimensional network structure making them hard and brittle once formed.\nThe second class are thermoplastics, which are not cross-linked. Most everyday plastic products are thermoplastics. We will briefly discuss their behaviour upon cooling, which shows similarities to the behaviour of colloids.\n\nFor thermoplastics, at high temperature the free energy is dominated by the entropic terms. The melt resembles a random assembly of mobile, intertwined, flexible coils with a density similar to the density of the corresponding monomer liquid. Upon cooling the potential energy takes over and the bonds are restricted in their rotation leading to configurations which are more straightened out. Below the melting temperature \\(\\mathrm{T}_{\\mathrm{m}}\\), a crystal is the lowest free energy state. Crystallisation, however, requires significant ordering of the initially random melt and is only possible if cooling occurs slow enough. If the melt is rapidly cooled below the glass transition temperature \\(T_{g}\\left(&lt;T_{m}\\right)\\), then instead of a crystal a glass is formed, which represents an amorphous metastable, but long-lived state. Although the polymers can still vibrate, they can no longer move. Solid thermoplastics are frequently a mixture of crystalline and amorphous structures.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#references",
    "href": "soft-matter/soft-matter_polymers.html#references",
    "title": "15  Polymers",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html",
    "href": "soft-matter/soft-matter_liquidcrystals.html",
    "title": "16  Liquid crystals",
    "section": "",
    "text": "16.1 Anisotropic particles\nOrdered phases of matter are typically characterised by spatial correlations, which can be positional or orientational in nature.\nPositional order is the regular arrangement of particle positions in space, often forming a repeating lattice structure. Orientational order refers to the angular alignment of the neighbours of a particle, influenced for example by the anisotropy of the particle itself (e.g. rods tending to align with other rods).\nIn crystalline phases, both positional and orientational order are long-ranged, meaning that the arrangement and orientation of particles are correlated over macroscopic distances. In contrast, simple liquids exhibit only very short-range (positional and orientational) correlations.\nLiquid crystals represent an intermediate case, where orientational order can persist over long distances while positional order remains short-ranged or absent.\nThe decoupling between orientational and translational degrees of freedom is favoured by individual microscopic units (particles) that already have preferential axis of symmetry, i.e. anisotropic particles. Anisotropic particles have shapes that are not spherically symmetric, such as rods, ellipsoids, or plates. Their interactions depend not only on the distance between particles but also on their relative orientations.\nThere are many kinds of anisotropic particles. The table below contains a non-exhaustive list.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#anisotropic-particles",
    "href": "soft-matter/soft-matter_liquidcrystals.html#anisotropic-particles",
    "title": "16  Liquid crystals",
    "section": "",
    "text": "Particle Shape\nDescription\nExample Materials\nTypical Applications\n\n\n\n\nRods\nCylindrical, length &gt; width\nGold nanorods, Tobacco virus\nPhotothermal therapy, plasmonics\n\n\nEllipsoids\nElongated or flattened spheroids\nHematite ellipsoids\nAnisotropic optics, directed self-assembly\n\n\nPlates/Discs\nFlat, disk-like shapes\nGraphene oxide, clay platelets\nBarrier materials, viscosity control\n\n\nPolyhedra\nMulti-faceted, highly symmetric shapes\nGold nanocubes, silica polyhedra\nPhotonic crystals, catalysis\n\n\n\n\n\n\nVarious assemblies of anisotropic particles. Left to right: Golden nanorods (from Stanford Advanced Materials), tobacco virus rods (adapted from Knapp and Lewandowski (2001)), ellipsoidal silica-coated hematite particles (adapted from Sánchez-Ferrer et al. (2010)), hard platelets in the isotropic phase (from Atashpendar, Ingenbrand, and Schilling (2020)), and an arrangement of octahedra simulated using hard particle Monte Carlo with hoomdblue",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#liquid-crystal-phases",
    "href": "soft-matter/soft-matter_liquidcrystals.html#liquid-crystal-phases",
    "title": "16  Liquid crystals",
    "section": "16.2 Liquid crystal phases",
    "text": "16.2 Liquid crystal phases\nThe additional orientational degrees of freedom makes opens up the possibilities of phases that do not normally exist for spherical, isotropic particles. We progress from the most disordered to the most ordered phase:\n\nIsotropic fluid: this phase is not very different from the ordinary fluid phase of spherical particles. There is no long-range positional or orientational order and all correlations decay rapidly and on short lengthscales.\nNematic phase: still without translational order, the constituents arrange themselves spontaneously in a preferential, average direction, called the director. Since there is no positional order, the transition from isotropic to nematic can only be detected by taking into account the relative orientations between the constituents, as the individual centres of mass are as disordered in the nematic as they are in the isotropic. If the particles are themselves chiral (i.e., the particle differs from its mirror image) the director tends to form a helix due to the propensity of the molecules to align at some angle between each other. This leads to so called chiral nematic phase (also known as cholesteric).\nsmectic phase: in this phase, particles not only align along a common direction (as in the nematic phase) but also exhibit partial translational order. The centers of mass of the particles tend to form well-defined layers, with orientational order within each layer. However, within a layer, the positions of the particles remain disordered, similar to a liquid. Smectic phases can be further classified (e.g., smectic A, smectic C) depending on the relative orientation of the director with respect to the layer normal.\nColumnar phase: here, anisotropic particles (often disc-like) stack into columns, which then arrange themselves into a two-dimensional lattice. There is long-range positional order in two directions (within the plane perpendicular to the columns) and orientational order along the column axis.\nCrystal: the most ordered phase, where both positional and orientational order are long-ranged in all directions, forming a true three-dimensional periodic lattice.\n\n\n\n\n\n\n\n\n\nPhase\nPositional Order\nOrientational Order\n\n\n\n\nIsotropic fluid\nNo\nNo\n\n\nNematic\nNo\nYes (long-range)\n\n\nChiral nematic\nNo\nYes (helical)\n\n\nSmectic\nYes (1D: layered)\nYes (within layers)\n\n\nColumnar\nYes (2D: columns)\nYes (along columns)\n\n\nCrystal\nYes (3D: lattice)\nYes (long-range)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#orientational-correlations-and-the-isotropicnematic-transition",
    "href": "soft-matter/soft-matter_liquidcrystals.html#orientational-correlations-and-the-isotropicnematic-transition",
    "title": "16  Liquid crystals",
    "section": "16.3 Orientational correlations and the isotropic/nematic transition",
    "text": "16.3 Orientational correlations and the isotropic/nematic transition\nWhile for the spherical colloids we focused on positional correlation functions like the radial distribution function, for anisotropic particle we need to quantify orientational correlations.\nSuppose we have \\(N\\) rod-like particles, each with an orientation angle \\(\\theta_i\\) (in 2D) or a unit vector \\(\\mathbf{u}_i\\) (in 3D). We want to determine the director, i.e. the average direction of the long molecular axes of all molecules in the liquid crystal.\nThe rods are head-tail symmetric, so, we cannot truly distinguish which one is the head or the tail (e.g. an orientation of \\(\\  theta\\) in 2D is the same as an orientation of \\(\\theta+\\pi\\)). We cannot therefore just take the average of the vectors to extract the common orientation of the various vectors, otherwise we would incur into a cancellation.\nInstead, we construct a second moment tensor called the alignment tensor\n\n\\[\n\\mathbf{Q}=\\dfrac{d}{2}\\left\\langle\\mathbf{u}_i \\otimes \\mathbf{n}_i-\\dfrac{1}{d}\\mathbf{I}\\right\\rangle\n\\]\n\nwhere \\(d\\) is the dimensionality. The removed \\(\\dfrac{1}{d}\\mathbf{I}\\) term ensures that the tensor is traceless and does not include isotropic components.\nThe analysis of the tensor yield the main characteristics of the orientation of the system:\n\nthe largest eigenvalue of \\(\\mathbf{Q}\\) is the scalar nematic order parameter \\(\\mathcal{S}\\)\nthe eigenvector corresponding to the largest eigenvalues is called the director \\(\\mathbf{n}\\) and corresponds to the main collective orientation of the system.\n\nIn two dimensions, the director is simply characterised by the angle \\(\\psi\\) expressed as \\[\n\\psi = \\frac{1}{2} \\operatorname{atan2}(\\sin{2\\theta_i}, \\cos{2\\theta_i}).\n\\] and the nematic order parameter is simply\n\\[\n\\mathcal{S}=\\sqrt{\\left\\langle\\cos 2 \\theta_i\\right\\rangle^2+\\left\\langle\\sin 2 \\theta_i\\right\\rangle^2}\n\\]\n\n\nNotice that \\(\\mathcal{S}\\) is not the entropy \\(S\\)! These commonly used symbols should not be confused.\nwhile in three dimensions this gives\n\n\\[\n\\mathcal{S}=\\dfrac{1}{2} \\left\\langle 3\\cos ^2 \\theta_i-1\\right\\rangle\n\\]\n\n(the expression for the director in 3d requires explicit diagonalisation).\n\n\n\n\n\n\nNoteDerivation of 2D director\n\n\n\n\n\nFor the purpose of illustrating how the calculations for the director are carried out, we provide a detailed example in two-dimensions.\nIn two dimensions, each particle has an orientation unit vector\n\\[\n\\mathbf{u}_i = \\begin{pmatrix} \\cos \\theta_i \\\\ \\sin \\theta_i \\end{pmatrix},\n\\]\nand the alignment tensor is defined as\n\\[\n\\mathbf{Q} = \\left\\langle \\mathbf{u}_i \\otimes \\mathbf{u}_i \\right\\rangle = \\begin{pmatrix}\n\\langle \\cos^2 \\theta_i \\rangle & \\langle \\cos \\theta_i \\sin \\theta_i \\rangle \\\\\n\\langle \\cos \\theta_i \\sin \\theta_i \\rangle & \\langle \\sin^2 \\theta_i \\rangle\n\\end{pmatrix}.\n\\]\nUsing double-angle trigonometric identities,\n\\[\n\\cos^2 \\theta = \\frac{1 + \\cos 2\\theta}{2}, \\quad \\sin^2 \\theta = \\frac{1 - \\cos 2\\theta}{2}, \\quad \\cos \\theta \\sin \\theta = \\frac{1}{2} \\sin 2\\theta,\n\\]\nwe rewrite \\(\\mathbf{Q}\\) as\n\\[\n\\mathbf{Q} = \\frac{1}{2} \\begin{pmatrix}\n1 + C & D \\\\\nD & 1 - C\n\\end{pmatrix},\n\\]\nwhere\n\\[\nC = \\langle \\cos 2\\theta_i \\rangle, \\quad D = \\langle \\sin 2\\theta_i \\rangle.\n\\]\nTo find the eigenvalues \\(\\lambda\\), solve\n\\[\n\\det(\\mathbf{Q} - \\lambda \\mathbf{I}) = 0,\n\\]\nwhich yields the largest eigenvalue\n\\[\n\\lambda_{\\max} = \\frac{1}{2} + \\frac{S}{2},\n\\]\nwith the scalar nematic order parameter\n\\[\n\\mathcal{S} = \\sqrt{C^2 + D^2}.\n\\]\nWhere \\(\\mathcal{S} \\to 1\\) indicates strong alignment and \\(\\mathcal{S} \\to 0\\) indicates isotropic order.\nThe director \\(\\mathbf{n}\\) is the eigenvector corresponding to \\(\\lambda_{\\max}\\), satisfying\n\\[\n(\\mathbf{Q} - \\lambda_{\\max} \\mathbf{I}) \\mathbf{v} = 0,\n\\]\nwhich gives\n\\[\n\\mathbf{Q} - \\lambda_{\\max} \\mathbf{I} = \\frac{1}{2} \\begin{pmatrix}\nC - S & D \\\\\nD & -C - S\n\\end{pmatrix}.\n\\]\nThe eigenvector is\n\\[\n\\mathbf{v} \\propto \\begin{pmatrix} 1 \\\\ \\frac{S - C}{D} \\end{pmatrix}.\n\\]\nNormalizing \\(\\mathbf{v}\\) gives the director \\(\\mathbf{n}\\).\nThe director angle \\(\\psi\\) is given by\n\\[\n\\tan \\psi = \\frac{S - C}{D}.\n\\]\nThis simplifies to\n\\[\n\\tan \\psi = \\frac{D}{S + C}.\n\\]\nUsing the double-angle formula,\n\\[\n\\psi = \\frac{1}{2} \\operatorname{atan2}(D, C).\n\\]\nThe angle \\(\\psi\\) represents the director’s orientation modulo \\(\\pi\\).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#landau-de-gennes-mean-field-theory",
    "href": "soft-matter/soft-matter_liquidcrystals.html#landau-de-gennes-mean-field-theory",
    "title": "16  Liquid crystals",
    "section": "16.4 Landau-de Gennes mean field theory",
    "text": "16.4 Landau-de Gennes mean field theory\nLet us consider the uniaxial 3D case, which is the most commonly discussed case (this focuses on nematic order, excluding cholesteric and smectic order) and attempt to construct a Landau theory centred on the order parameter in the spirit of what we saw in Section 5.5. If the system is uniaxial, then the order is characterized by a single director and the \\(\\mathbf{Q}\\) must be symmetric and traceless with a preferred axis \\(\\mathbf{n}\\).\nThe tensorial order parameter can be rewritten leveraging that the scalar order parameter is an eigenvalue of the director. i.e. \\(\\mathbf{Qn} = S\\mathbf{n}\\). The result is that\n\\[\n\\mathbf{Q}=\\mathcal{S}\\left(\\mathbf{n} \\otimes \\mathbf{n}-\\frac{1}{d} \\mathbf{I}\\right)\n\\]\nfor some scalar value \\(\\mathcal{S}\\) (the order parameter between 0 for the isotropic and 1 for the nematic). If we choose now the \\(z\\) axis along \\(\\mathbf{n}\\) we can write this explicitly as\n\\[\n\\mathbf{Q}=\\mathcal{S}\\left(\\begin{array}{ccc}\n-\\frac{1}{3} & 0 & 0 \\\\\n0 & -\\frac{1}{3} & 0 \\\\\n0 & 0 & \\frac{2}{3}\n\\end{array}\\right)\n\\]\nwith \\(\\mathcal{S}=\\frac{3}{2}\\left\\langle(\\mathbf{u} \\cdot \\mathbf{n})^2-\\frac{1}{3}\\right\\rangle\\) as defined earlier.\nWe can now make an extra step by considering how to express the scalar order parameter in terms of the probability distribution \\(p(\\Omega)\\) of finding a rod with pair of polar angles \\(\\Omega=(\\theta,\\phi)\\) in a coordinates system with polar axis along \\(\\mathbf{n}\\) (which is the reference frame we have just chosen). The infinitesimal probability is \\(p(\\Omega)d\\Omega\\).\nAs often in physics, we need to make a symmetry observation, and remark that the nematic phase has full cylindrical symmetry around the director. This means that, for us \\(p(\\Omega)=p(\\theta, \\phi)\\) is in fact independent of The azimuthal angle \\(\\phi\\) and only depends on the deviation from the director \\(\\theta\\).\nWe also note that the expression for the nematic order parameter satisfies head-tail symmetry by employing the first non-trivial Legendre polynomial, \\(P_2(\\cos\\theta)\\) as\n\\[\\mathcal{S}=\\dfrac{3}{2}\\left\\langle(\\mathbf{u} \\cdot \\mathbf{n})^2-\\frac{1}{3}\\right\\rangle = \\dfrac{1}{2}\\langle 3 \\cos^2\\theta-1\\rangle=\\langle P_2(\\cos\\theta)\\rangle\\]\nIn terms of the probability distribution this is\n\\[\\mathcal{S}=2\\pi \\int_0^{\\pi}P_2(\\cos\\theta)p(\\theta)\\sin\\theta d\\theta\\]\nThe Landau free energy density for the isotropic-nematic transition is constructed by scalar combinations of the tensor \\(\\mathbf{Q}\\). The idea is to expand the free energy density \\(f\\) around the transition and write it as\n\\[\nf=f_0+\\frac{A}{2} Q_{i j} Q_{j i}-\\frac{B}{3} Q_{i j} Q_{j k} Q_{k i}+\\frac{C}{4}\\left(Q_{i j} Q_{i j}\\right)^2\n\\]\nor in more compact form\n\\[\nf=f_0+\\frac{A}{2} \\operatorname{tr} \\mathbf{Q}^2-\\frac{B}{3} \\operatorname{tr} \\mathbf{Q}^3+\\frac{C}{4}\\left(\\operatorname{tr} \\mathbf{Q}^2\\right)^2\n\\] where we stop the construction to terms of fourth order in \\(\\mathbf{Q}\\) and \\(A,B,C\\) are temperature dependent coefficients.\nWe note that given our definitions above the following identities hold \\[\n\\operatorname{Tr}\\left(\\mathbf{Q}^2\\right)=\\frac{2}{3} S^2 \\quad,\\quad\n\\operatorname{Tr}\\left(\\mathbf{Q}^3\\right)=\\frac{2}{9} S^3\n\\]\nso that\n\\[\nf=f_0+\\frac{A}{3}S^2-\\frac{2B}{27}S^3+\\frac{C}{9}S^4\n\\]\n\n\nWe can expand \\(A,B,C\\) around the reference temperature \\(T^\\ast\\) as \\[ A(T)=a\\left(T-T_*\\right)+\\cdots \\] and at zeroth order for both \\(B\\) and \\(C\\) as \\(B(T)= b+\\dots\\) , \\(C(T)=c+\\dots\\), with \\(a,b,c&gt;0\\) to preserve the most relevant terms.\nWe can follow the scheme used for Landau phase transitions and obtain the so-called Landau-de Gennes theory result\n\\[\nf-f_0=\\frac{a}{3}\\left(T-T^{\\ast}\\right) S^2-\\frac{2 b}{27} S^3+\\frac{c}{9} S^4 .\n\\]\nThis has the form of standard Landau free energy (combinations of power of S up to the power of 4 with real coefficients that depend on the temperature).  It can be shown that this corresponds to a first order phase transition without critical point (because the isotropic and nematic phase have different symmetry) with transition temperature\n\\[\nT_{N I}=T^{\\ast}+\\frac{b^2}{27 a c}\n\\] where \\(T^{\\ast}\\) is a material dependent temperature, identifying where the quadratic term of the free energy vanishes, and where the isotropic phase becomes unstable (spinodal).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#maier-saupe-theory",
    "href": "soft-matter/soft-matter_liquidcrystals.html#maier-saupe-theory",
    "title": "16  Liquid crystals",
    "section": "16.5 Maier-Saupe theory",
    "text": "16.5 Maier-Saupe theory\nThe Landau-de Gennes theory illustrates how the simple existence of a suitably defined nematic order is compatible with the construction of a theory that predicts a first order phase transition. A complementary approach is to consider the minimal physical ingredients that lead to the transition itself. As in the case of the colloids, we have essentially two classes\n\nattractive interactions that can favour the new symmetries (e.g. the nematic order)\nentropic contributions that make it easier to pack the rods in the new symmetry (i.e. when they are aligned)\n\nPhenomenologically, we can put these two together assuming a form for the energetic and entropic contributions to write down a free energy of the following form\n\\[f = f_0 - u\\dfrac{S^2}{2}+k_BT \\int_0^\\pi p(\\theta) \\ln (4\\pi p(\\theta)) \\sin\\theta d\\theta\\]\nwhere:\n\n\\(u\\) is the coupling strength parameter that quantifies the energetic preference for orientational alignment (higher \\(u\\) favors stronger nematic order),\nthe \\(4\\pi\\) term assumes that \\(p(\\theta)\\) is correctly normalised on the sphere and \\(\\sin\\theta d\\theta\\) is the usual Jacobian.\n\nThe two terms above identify two simple contributions:\n\nan energetic contribution that is a simple quadratic function of the order parameter (more order, lower energy, with the isotropic phase paying the highest cost). Near the isotropic phase the free energy needs to be even due to the head-tail symmetry.\na generic entropic contribution of the form \\(k_B\\int p(\\Omega)\\ln p(\\Omega) d\\Omega\\) using the Gibbs entropy formula.\n\nThe minimization of the free energy with respect to \\(p(\\theta)\\), under the normalization constraint \\(\\int_0^\\pi p(\\theta) \\sin\\theta d\\theta = 1\\), is performed using a Lagrange multiplier \\(\\lambda\\). The variational problem is:\n\\[\n\\delta \\left[ f + \\lambda \\left( \\int_0^\\pi p(\\theta) \\sin\\theta d\\theta - 1 \\right) \\right] = 0\n\\]\nTaking the functional derivative with respect to \\(p(\\theta)\\) and setting it to zero yields:\n\\[\n\\frac{\\delta f}{\\delta p(\\theta)} + \\lambda \\sin\\theta = 0\n\\]\nPlugging in the expression for \\(f\\) and simplifying, we obtain the solution:\n\\[\np(\\theta) = \\frac{1}{Z} \\exp\\left( \\lambda P_2(\\cos\\theta) \\right)= \\frac{1}{Z} \\exp\\left( \\frac{u S}{k_B T} P_2(\\cos\\theta) \\right)\n\\]\nwhere \\(\\mathcal{S}\\) is the nematic order parameter to be determined self-consistently, and \\(Z\\) is the normalization constant (partition function):\n\\[\nZ = \\int_0^\\pi \\exp\\left( \\frac{u S}{k_B T} P_2(\\cos\\theta) \\right) \\sin\\theta d\\theta\n\\]\nThis is the Maier-Saupe self-consistent equation for the orientational distribution in the mean-field theory of nematic liquid crystals. In the code below, we can see how this leads to a non trivial energy profile which corresponds to the emergence of a ordered state (a minimum at high \\(\\mathcal{S}\\)) corresponding to the nematic phase.\nThe transition is weakly-first order, as two separate basins of stability (separated typicaly by a small barrier) are formed.\n\n\n\n\n\n\nThese profiles are example of simple free energy landscapes. We will see more about this later in Section 18.1.\n\n16.5.1 Lattice model: the Lebwohl-Lasher model\nThe Lebwohl-Lasher model is a lattice model designed to capture the orientational ordering of anisotropic particles, such as those found in nematic liquid crystals and is a lattice version of the Maier-Saupe mean field model. In this model, each lattice site \\(i\\) is associated with a unit vector \\(\\mathbf{n}_i\\) representing the local orientation (the “director”) of a particle at that site.\nThe Hamiltonian of the Lebwohl-Lasher model is given by:\n\\[\nH = -\\epsilon \\sum_{\\langle i, j \\rangle} \\left[ \\frac{3}{2} (\\mathbf{n}_i \\cdot \\mathbf{n}_j)^2 - \\frac{1}{2} \\right]\n\\]\nwhere: - \\(\\epsilon &gt; 0\\) is the coupling constant favoring alignment, - the sum \\(\\langle i, j \\rangle\\) runs over all pairs of nearest-neighbor sites, - \\((\\mathbf{n}_i \\cdot \\mathbf{n}_j)^2\\) measures the degree of alignment between neighboring directors.\nThis Hamiltonian favors parallel (or antiparallel) alignment of neighboring directors, capturing the essential physics of the isotropic-nematic transition in liquid crystals.\nIn two dimensions, the Lebwohl-Lasher model is similarly defined, but the directors \\(\\mathbf{n}_i\\) are restricted to lie in the plane. Each director can be represented by a unit vector \\(\\mathbf{n}_i = (\\cos\\theta_i, \\sin\\theta_i)\\), where \\(\\theta_i\\) is the orientation angle at site \\(i\\).\nThe Hamiltonian in 2D becomes:\n\\[\nH = -\\epsilon \\sum_{\\langle i, j \\rangle} \\left[ \\frac{3}{2} \\cos^2(\\theta_i - \\theta_j) - \\frac{1}{2} \\right]\n\\]\nwhere the sum is over nearest-neighbor pairs on a 2D lattice. This model captures the essential features of orientational ordering and the isotropic-nematic transition in two-dimensional systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantActivity\n\n\n\nModify the script above to check the following\n\nThe high temperature regime (\\(T\\gg 0.5\\)$) is mostly disordered (isotropic)\nThe low temperature regime forms larger and larger patches with coherent orientations\nDefine and plot a global nematic order parameter. Τhe global nematic order parameter S is defined as:\n\n\\[ S = \\langle \\cos(2(\\theta - \\psi))\\rangle\\]\nwhere \\(\\psi\\) is the director (average orientation), which in 2D is\n\\[\\psi = \\dfrac{1}{2} {\\rm atan2}(\\langle \\sin(2\\theta)\\rangle, \\langle \\cos(2\\theta)\\rangle)\\]",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#splay-twist-and-bend",
    "href": "soft-matter/soft-matter_liquidcrystals.html#splay-twist-and-bend",
    "title": "16  Liquid crystals",
    "section": "16.6 Splay, twist and bend",
    "text": "16.6 Splay, twist and bend\nIn the continuum limit, the nematic liquid crystal can be characterised in terms of the deformation of a vector field \\(\\mathbf{n}(\\mathbf{r})\\) which represents the director at every point \\(\\mathbf{r}\\) (a local director, resulting from the average over a large number of particles in a single volume).\nThe Frank free energy density for distortions of the director field is\n\\[\nf = \\frac{1}{2} K_1 (\\nabla \\cdot \\mathbf{n})^2\n    + \\frac{1}{2} K_2 [\\mathbf{n} \\cdot (\\nabla \\times \\mathbf{n})]^2\n    + \\frac{1}{2} K_3 [\\mathbf{n} \\times (\\nabla \\times \\mathbf{n})]^2\n\\]\nwhere: - \\(K_1\\) is the splay elastic constant, - \\(K_2\\) is the twist elastic constant, - \\(K_3\\) is the bend elastic constant.\nEach term in the Frank free energy penalizes a specific type of distortion: splay, twist, or bend. The total elastic free energy is found by integrating \\(f\\) over the entire sample volume. This formulation is named after Charles Frank, a pioneer in the study of crystal dislocations (which are defects in crystalline solids) and what we call today soft matter at the University of Bristol, for whom the Frank lecture theatre is named.\n\n\n\nThe formulation allows one to model the behaviour of liquid crystals with finite element methods on much larger scales than the typical size of the elementary constituents. It also allows to phenomenologically add more deformation modes, to include for example the description of chirality.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#topological-defects",
    "href": "soft-matter/soft-matter_liquidcrystals.html#topological-defects",
    "title": "16  Liquid crystals",
    "section": "16.7 Topological defects",
    "text": "16.7 Topological defects\nThe continuum limit description of liquid crystals allows us to enhance their description with the notion of topological defects.\nTopological defects are singularities in the director field that can not be removed by a continuous deformation of the director field: they are topologically protected. We can see this with a few examples in 2d by drawing lines of the director (remember that the head-tail symmetry means that the lines do not have any preferential direction but indicate local orientation):\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_half_defect(ax, charge, title):\n    # Grid\n    x = np.linspace(-1, 1, 21)\n    y = np.linspace(-1, 1, 21)\n    X, Y = np.meshgrid(x, y)\n\n    # Polar coordinates\n    theta = np.arctan2(Y, X)\n\n    # Director angle for nematic disclination\n    # Only ±1/2 defects\n    phi = charge * theta  # charge = +0.5 or -0.5\n\n    # Director vectors (head-tail symmetry)\n    U = np.cos(phi)\n    V = np.sin(phi)\n    mask = U &lt; 0\n    U[mask] *= -1\n    V[mask] *= -1\n\n    # Plot director field as short lines\n    ax.quiver(\n        X, Y, U, V,\n        pivot='middle', color='black',\n        scale=18, headwidth=0, headlength=0, headaxislength=0\n    )\n\n    # Defect core\n    ax.plot(0, 0, 'o', color='orange', markersize=12)\n\n    ax.set_title(title, fontsize=12)\n    ax.set_aspect('equal')\n    ax.set_xlim(-1.1, 1.1)\n    ax.set_ylim(-1.1, 1.1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.axis('off')\n\n# Plotting\nfig, axs = plt.subplots(1, 4, figsize=(8, 3))\ncharges = [0.5, -0.5, +1, -1]\ntitles = [\"+1/2 defect\", \"-1/2 defect\", \"+1 defect\",  \"-1 defect\"]\n\nfor ax, charge, title in zip(axs, charges, titles):\n    plot_half_defect(ax, charge, title)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese topological defects are places where the orientation changes discontinuously and are also called disclinations.\nIn two dimensions, the disclinations are characterized by a topological charge (or strength) \\(s\\), defined by the total rotation of the director around a closed loop enclosing the defect:\n\\[\n\\Delta \\theta = 2\\pi s\n\\]\n\n\n\n\n\n\n\n\n\nCommon disclination strengths in nematic liquid crystals are \\(s = \\pm \\frac{1}{2}\\) and \\(s = \\pm 1\\). For example, a \\(+1/2\\) disclination corresponds to a director field that rotates by \\(+\\pi\\) as one encircles the defect, while a \\(-1/2\\) disclination rotates by \\(-\\pi\\).\nWhen two disclinations of opposite charges meet, they annihilate. This means the defects can cancel each other out, restoring uniform orientational order in the region. The annihilation of defect pairs is a key mechanism by which nematic liquid crystals relax toward equilibrium after being disturbed. This is because the idealise equilibrium state should in principle be free of any such defects, as they cost energy: they deform the dirctor field on long distances, leading to a deformation cost, and they also engenders a complete loss of orientational order at the singularity point, which costs energy by itself.\nIn three dimensions, disclinations are line defects rather than point defects. These line defects represent regions where the orientational order of the director field is singular along a curve, rather than at isolated points. The topology and dynamics of disclination lines in 3D nematic liquid crystals are richer and more complex than in 2D, allowing for phenomena such as defect loops, entanglements, and reconnections. Disclination lines can form closed loops, terminate at surfaces, or interact with other defects, and play a crucial role in the response of liquid crystals to external fields, boundary conditions, and during phase transitions.\nThe classification of disclination in 3D involves the rotation group SO(3) (that generalises the construction we have seen in two dimensions) and are more complex in general. They include lines, closed loops, point-like features (hedgehogs and monopoles) and complex textures.\nYou can read more about liquid crystal’s defects and their coupling to the material properties in Jones (2002).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#references",
    "href": "soft-matter/soft-matter_liquidcrystals.html#references",
    "title": "16  Liquid crystals",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtashpendar, Arshia, Tim Ingenbrand, and Tanja Schilling. 2020. “Shape, Geometric Percolation, and Electrical Conductivity of Clusters in Suspensions of Hard Platelets.” Phys. Rev. E 101 (March): 032706. https://doi.org/10.1103/PhysRevE.101.032706.\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.\n\n\nKnapp, Elisabeth, and Dennis J Lewandowski. 2001. “Tobacco Mosaic Virus, Not Just a Single Component Virus Anymore.” Molecular Plant Pathology 2 (3): 117–23.\n\n\nSánchez-Ferrer, Antoni, Mathias Reufer, Raffaele Mezzenga, Peter Schurtenberger, and Hervé Dietsch. 2010. “Inorganic–Organic Elastomer Nanocomposites from Integrated Ellipsoidal Silica-Coatedhematite Nanoparticles as Crosslinking Agents.” Nanotechnology 21 (18): 185603.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html",
    "href": "soft-matter/soft-matter_surfactants.html",
    "title": "17  Surfactants",
    "section": "",
    "text": "17.1 Hydrophobicity and amphiphiles\nThe hydrophobic interaction between solutes is a statistical force mediated by water (the solvent). Traditional explanations focus on the role played by hydrogen bonds (an attractive interaction due to the difference in electronegativity between oxygen and hydrogen in water) and the structure of water around solutes.\nPolar molecules have regions with partial positive and negative charges due to differences in electronegativity between atoms, resulting in an uneven distribution of electrons. This allows them to interact strongly with water (hydrophilic) and other polar substances.\nApolar (or nonpolar) molecules, instead, have a more even distribution of electrical charge, lacking distinct poles. They do not mix well with water and tend to aggregate with other nonpolar substances.\nIn this chapter we discuss the behaviour of a special class of molecules. In these molecules one end contains a hydrophilic (literally, water-loving) part, while the other end is hydrophobic (water-fearing). For their nature, they are called amphiphilic (loving both) molecules, which reflects their structure, or surfactants (from SURFace ACTive AgeNT), which refers to their behaviour in solution.\nFor very small solutes (a few atoms) the traditional picture of hydro-phobicity/philicity is that the energetic advantage due to hydrogen bonding favors configurations that minimally disrupt the local structure of water, and this means that multiple apolar solutes tend to come together to minimise such disruption since they cannot form such bonds. However, for larger molecules (such as large proteins or longer macromolecular chains) interfaces between the bulk water and the solutes are formed, to which we can associate characteristic density fluctuations and surface tensions. Phase separation between a local vapor-like layer and the bulk water becomes then important and aggregation due to hydrophobicty becomes very much a many-body effect. This is an active topic of current research (even by some of us, see Wilding and Turci (2025) ) and demonstrates how even very foundational concepts in soft matter remain to be explored.\nIn the rest of the chapter, we will look mostly phenomenologically at the structure of surfactants and how they collectively come together to form meso-scale structures via the process of self-assembly, and assume that there exist solvent-mediated interactions such as hydrophobicity that control the free energies of these assemblies.\nA few chemical structures are shown below. They illustrate that the hydrophobic tail usually consists of hydrocarbon chains of different lengths",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#hydrophobicity-and-amphiphiles",
    "href": "soft-matter/soft-matter_surfactants.html#hydrophobicity-and-amphiphiles",
    "title": "17  Surfactants",
    "section": "",
    "text": "Schematic model of a surfactant (adapted).\n\n\n\n\n\n\nSDS (dodecyl sulfate)\nIn SDS (sodium dodecyl sulfate) the head is the hydrophilic sulfate group (\\({\\rm –OSO_3}\\)), which is negatively charged and water-attracting while the tail is the long hydrophobic 12-carbon alkyl chain (dodecyl group), which is water-repelling.\n    \n    \n     \n\n\nCholesterol\nIn cholesterol the head is the small polar hydroxyl (-OH) group and the tail is the nonpolar hydrocarbon isooctyl side chain at the opposite end.\n    \n    \n    \nIn general, the hydrophilic head might either be positively or negatively charged, zwitterionic (with both charges but overall neutral) or uncharged.\nThe hydrocarbon chains are insoluble in water. The molecules are thus preferentially located at the surface, which allows the hydrophilic head to be surrounded by water and the hydrophobic chains to avoid contact with water. There is always an equilibrium between surfactants at the surface and in the bulk of the solution. The coverage of the surface leads to a reduction of the surface tension with increasing surfactant concentration.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#self-assembled-structures",
    "href": "soft-matter/soft-matter_surfactants.html#self-assembled-structures",
    "title": "17  Surfactants",
    "section": "17.2 Self-assembled structures",
    "text": "17.2 Self-assembled structures\nSelf-assembly is a fundamental property of surfactant molecules in solution. It is also a highlight feature of soft-matter systems: under the influence of thermal fluctuations (typically mediated by a solvent), the constituents of soft matter systems self-organise into more complex, meso or macroscopic structures. This is typically due to driving thermodynamic forces that make initially uniform systems find (local ord global) minima of their free energy which distinctive structural features. In this sense, soft matter systems explore free energy landscapes whose complexity is tuned by the strength of thermal fluctuations.\nIn many ways, we have already seen various forms of self assembly: crystallisation of colloids is itself self-organised but so is also the phase transition between the gasous and the liquid phase, or the isotropic-nematic transition in liquid crystals.\nFull phase separation, though, is much more demanding than self assembly: in full phase separation we eventually attain the (global) free energy minimum corresponding to the equilibrium state prescribed by the chosen thermodynamic variables (e.g. temperature and pressure). The notion of self assembly emphasises instead the propensity of the constituents to aggregate, to form structures at intermediate scales, structures which can often be only very locally stable, and whose existence may rely not only on the structure of the energy landscape but also on the kinetics of the constituents, such as their diffusion mechanism.\nIn the case of surfactants, we need to consider that these molecules are often very small—typically just a few nanometers in length. This small size means that thermal fluctuations play a significant role in their behavior, and their assembly is rapid due to their fast diffusion. For example, for an approximately spherical surfactant of size \\(R\\) in a viscous medium its diffusivity is given by the Stokes-Einstein relation (see )\n\n\\[\nD = \\frac{k_B T}{6\\pi \\eta R}\n\\]\n\nwhere \\(D\\) is the diffusion coefficient, \\(k_B\\) is Boltzmann’s constant, \\(T\\) is temperature, \\(\\eta\\) is the viscosity of the solvent, and \\(R\\) is the radius of the particle. Compared to a colloidal particle of \\(2 {\\rm \\mu m}\\), a single SDS surfactant molecule of approximately \\(2 {\\rm nm}\\) in size will be approximately 1000 times faster.\nThis leads to fast local equilibration and constant exchange between monomers and aggregates. As a result, the structures formed by surfactants are not static but exist in a dynamic equilibrium, with lifetimes and sizes that depend sensitively on temperature, concentration, and solvent conditions.\nDue to their amphiphilic nature, surfactants spontaneously organise into ordered structures without external guidance, but driven by forces such as hydrophobicity, described above.\nAs a result, surfactants minimize the system’s free energy by forming a variety of aggregates such as micelles, vesicles, or bilayers. The specific structure formed depends on the molecular geometry of the surfactant and the solution conditions. Self-assembly is in many cases reversible and fundamentally dynamic process, with aggregates constantly forming and dissociating in equilibrium with monomers in solution.\n\n\n17.2.1 Aggregation: general case\nSuppose we have a system where solute particles are dispersed in a solvent and tend to aggregate due to their mutual interactions. Suppose that we know the quantity \\(\\epsilon_N\\) representing the free energy change when a specific particle is taken from the bulk and added to an aggregate of size \\(N\\).\nCall \\(\\mu_1\\) and \\(\\mu_N\\) the chemical potential of isolated particles and aggregates of size \\(N\\) respectively. In equilibrium, they must be equal, with value \\(\\mu\\).\nLet’s focus on an aggregate of size \\(N\\). We can express \\(\\mu\\) in terms of\n\nthe interaction energy from being in the aggregate (\\(\\epsilon_N\\))\nthe (translational) entropy of the aggregate as a whole (\\(\\propto {\\text{ number of aggregates}}\\times \\ln {\\text{ number of aggregates}}\\))\n\nAssume an overall volume fraction of particles \\(\\phi\\). Call the volume fraction of particles in an aggregate with \\(N\\) particles \\(X_N\\), so that \\(\\sum_N X_N = \\phi\\). The numbers of aggregates of size \\(N\\) is then simply \\(X_N/N\\). This means that we can write the uniform chemical potential as\n\\[\\mu = \\epsilon_N + \\dfrac{k_B T}{N}\\ln{\\dfrac{X_N}{N}}\\]\nwe can rewrite this as\n\\[X_N = N \\exp{\\left( \\dfrac{N(\\mu-\\epsilon_N)}{k_BT}\\right)}\\]\nwe can eliminate \\(\\mu\\) by evaluating the expression for for \\(N=1\\) and plugging it back to get\n\\[X_N = N X_1^N \\exp{\\left( \\dfrac{N(\\epsilon_1-\\epsilon_N)}{k_BT}\\right)}\\]\nThe equation shows that one has a large fraction of the solutes in an aggregated state only if there is a free energy advantage at forming aggregates, i.e. \\(\\epsilon_1&gt;\\epsilon_N\\).\nThis means that knowning the form of \\(\\epsilon_N\\) allows us to predict the aggregation behaviour. For example, imagine we have an aggregate with \\(N\\) particles of total radius \\(r\\approx (Nv)^{1/3}\\) where \\(v\\) is the volume of a single particle. Then \\(\\epsilon_N\\) is the free energy per particle of the aggregate of size \\(N\\), \\(G_N=\\text{ bulk free energy}+\\text{surface free energy}\\). Assuming a surface tension \\(\\gamma\\) we can then write\n\\[\\epsilon_N = \\dfrac{G_N}{N} = \\epsilon_\\infty  +\\dfrac{1}{N}\\gamma r^2 =\\epsilon_\\infty +\\gamma\\left(\\dfrac{v^2}{N}\\right)^{1/3}\\]\nwhich is a monotonically decreasing function of \\(N\\). By defining \\(\\alpha k_B T = \\gamma v^{2/3}\\) we can extract a relation between \\(X_N\\) and \\(X_1\\) parametrised solely by \\(\\alpha\\), i.e.\n\\[\\text{number of aggregates of size N per unit volume} = \\dfrac{X_N}{N}\\sim (X_1 e^{\\alpha})^N\\]\nThis should be read as follows: if we have very few isolated particles at a certain thermodynamic condition, then \\(X_1 e^{\\alpha} &lt; 1\\) and the exponential factor \\((X_1 e^{\\alpha})^N\\) becomes vanishingly small for large \\(N\\), leaving us with very few large aggregates. On the contrary, as \\(X_1\\) approaches \\(e^{-\\alpha}\\) from below, we reach the critical point where \\(X_1 e^{\\alpha} = 1\\), and aggregates of all sizes become equally probable. Since \\(\\epsilon_N\\) is rapidly decreasing in \\(N\\), this means that above a critical value of overall packing fraction \\(\\phi\\), the system cannot remain in a homogeneous state. Instead, it undergoes phase separation into a dilute phase of isolated monomers (with \\(X_1\\) pinned at \\(e^{-\\alpha}\\)) in coexistence with a dense phase consisting of one or very few aggregates of very large (effectively infinite) size.\nThe volume fraction \\(\\phi\\) at which this occurs is called critical aggregation concentration, or CAC.\n\n\n17.2.2 Aggregation: the surfactant case\nFor amphiphilic molecules like surfactants, the free energy change \\(\\epsilon_N\\) associated with adding a molecule to an aggregate is not a monotonically decreasing function of \\(N\\). Instead, \\(\\epsilon_N\\) typically exhibits a minimum at a characteristic aggregation number \\(N^*\\). This reflects the fact that aggregates (such as micelles) of a particular size are thermodynamically favored: too-small aggregates cannot sufficiently shield the hydrophobic tails from the solvent, while too-large aggregates become energetically unfavorable due to packing constraints or headgroup repulsion. See Jones (2002) for a expanded discussion of this.\n\n\n\nSurfactants in solution: normally, this is done in water in the presence of an interface with air. Due to their nature as amphiphiles, the surfactants typically sit at the air-water interface in a dynamical, equilibrium process that exchanges monomers between the bulk and the surface. The bulk surfactants, when the concentration is larger than a critical value, also self-assemble into micellar structures, which are at equilibrium with the isolated single surfactants.\n\n\nMathematically, this means that the distribution \\(X_N\\) of aggregates as a function of \\(N\\) is sharply peaked around \\(N^*\\), leading to a well-defined aggregate size in solution. The equilibrium is then characterized by a coexistence of monomers and aggregates of size \\(N^*\\), with very few intermediate-sized clusters. This is in contrast to the general case discussed above, where the aggregate size distribution can be broad or even diverge near a phase separation threshold.\nThis behavior underlies the concept of the critical micelle concentration (CMC): below the CMC, almost all surfactant molecules are present as monomers; above the CMC, additional surfactant molecules predominantly form micelles of size \\(N^*\\), while the monomer concentration remains nearly constant.\nAbove the critical micellar concentration surfactants self-assemble in solution spontaneously into larger structures. (In the following we will consider aqueous solutions, although the arguments also apply to other polar or non-polar (organic) solvents.) This allows the hydrophobic parts to crowd together while being ‘shielded’ by the hydrophilic heads. The density of the hydrophobic cores is very similar to the density of fluid hydro-carbons and the random arrangements of the chains resemble closely a fluid structure.\nThe surfactant assemblies are not held together by chemical bonds, but only by weak interactions ( \\(\\lesssim k B T\\) ). Their existence and properties are thus determined by a delicate balance between different effects, such as the transfer of hydrophobic chains into the core, interactions between the head group and the entropy of mixing. Small changes in control parameters, for example temperature, salt concentration or pH , thus have large effects on the characteristics of the surfactant aggregates. Nevertheless, for given conditions, they have very well-defined properties (shape, size etc.).\n\n\n17.2.3 Shape of surfactant assemblies\nSurfactants spontaneously self-assemble into a variety of different structures. We use packing considerations to understand and predict the shape of surfactant aggregates, leveraging what we have learned on colloids, polymers and liquid crystals.\nWe construct a geometric model where a surfactant molecule is described using the following parameters:\n\noptimal headgroup area \\(a_{0}\\): As discussed in the previous section, this depends on a delicate balance of forces and is thus not only controlled by the chemistry of the surfactant molecule, but also depends on different control parameters of the solution, such as salt concentration, pH or temperature.\nvolume \\(v\\) of the hydrophobic part: The hydrophobic part usually consists of hydrocarbon chains and for saturated hydrocarbons the volume \\(v\\) can be approximated by \\(v \\approx(27.4+26.9 \\mathrm{n}) \\times 10^{-3} \\mathrm{~nm}^{3}\\) where n is the number of carbon atoms.\ncritical chain length \\(\\boldsymbol{l}_{\\boldsymbol{c}}\\) : The maximum effective length of the hydrophobic chains is called the critical chain length \\(l_{c}\\), which has to be shorter than the fully extended molecular length of the chain \\(l_{\\max }\\). For saturated hydrocarbons the critical length can be estimated using \\(l_{c} \\leq l_{\\max } \\approx(0.154+0.1265 \\mathrm{n}) \\mathrm{nm}\\). The critical chain length heavily depends on the detailed chemical structure of the molecule, for example on the presence of double bonds or branching, as well as the temperature.\n\nThe structure which will be adopted is determined by a balance between entropy, which favours small aggregates, and energy considerations: A certain shape or size might only be possible by imposing a headgroup area \\(a&gt;a_{0}\\), which is energetically not favourable. We will now establish the criteria for the different shapes.\n\n\n\nDifferent types of self assembled structures: (A) spherical micelles, (B) cylindrical micelles and (C) bilayers.\n\n\nA. Spherical micelle For a spherical micelle of radius \\(R\\) with aggregation number \\(N\\) , the total volume and surface area are given by\n\\[\n\\begin{gathered}\nN v=\\frac{4 \\pi}{3} R^{3} \\\\\nN a_{0}=4 \\pi R^{2}\\end{gathered}\\]\nTherefore $$=&lt;\n$$\nwhere we used the fact that the radius \\(R\\) cannot be larger than the critical chain length \\(\\mathrm{l}_{\\mathrm{c}}\\). We thus obtain for the critical packing parameter \\(P\\)\n\\[\nP=\\frac{v}{a_{0} l_{c}}&lt;\\frac{1}{3}\n\\]\nB. Cylindrical micelles\nFor a cylindrical micelle the total volume and surface area are given by \\[N v=\\pi R^{2} L\\]\nand\n\\[N a_{0}=2 \\pi R L\\]\nTherefore \\(\\dfrac{v}{a_{0}}=\\dfrac{R}{2}&lt;\\dfrac{l_{c}}{2}\\) again using \\(R&lt;l_{C}\\).\nWe thus obtain for the critical packing parameter \\(P=\\dfrac{v}{a_{0} l_{c}}\\) that \\[\\dfrac{1}{3}&lt;P&lt;\\dfrac{1}{2}\\]\nBelow the lower limit spherical micelles are formed.\nBelow the lower limit spherical micelles are formed.\nC. Bilayers\nFor a bilayer (with separation \\(D\\) between the layers) the total volume and surface area are given by\n\\[\n\\begin{aligned}\n& N v=A D \\\\\n& N a_{0}=2 A \\\\\n& \\frac{v}{a_{0}}=\\frac{D}{2}&lt;l_{c} \\quad\\left(\\text { using } D&lt;2 l_{c}\\right)\n\\end{aligned}\n\\]\nWe thus obtain for the critical packing parameter \\(P=\\dfrac{v}{a_{0} l_{c}}\\) that \\[\\dfrac{1}{2}&lt;P&lt;1\\]",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#a-simple-on-lattice-model-for-micelle-formation",
    "href": "soft-matter/soft-matter_surfactants.html#a-simple-on-lattice-model-for-micelle-formation",
    "title": "17  Surfactants",
    "section": "17.3 A simple on-lattice model for micelle formation",
    "text": "17.3 A simple on-lattice model for micelle formation\nAggregation of surfactants has been studied in various ways, including numerical simulations. One can employ extremely realistic, all-atom molecular dynamics to glean the microscopic details or construct more coarse-grained, statistical mechanics models using for example on lattice interactions.\nHere below you can find a Javascript implementation of a lattice-gas-like model where surfactants are represented by chains of 3 sites on a lattice, with one site for the solvophilic head and two for the solvophobic tail. Each lattice site is occupied by either an amphiphile segment or a solvent molecule.\nThere are only nearest-neighbor interactions and the hamiltonian is simply\n\\[\nH=n_{\\mathrm{HH}} E_{\\mathrm{HH}}+n_{\\mathrm{TS}} E_{\\mathrm{TS}}+n_{\\mathrm{HS}} E_{\\mathrm{HS}}+\\sum_{\\mathrm{i}} E_{\\mathrm{c}}\n\\]\nwhere \\(n_{\\mathrm{HH}}, n_{\\mathrm{TS}}\\) and \\(n_{\\mathrm{HS}}\\) are the total number of head-head, tail-solvent and head-solvent bonds, \\(E_{\\mathrm{HH}}, E_{\\mathrm{Ts}}\\) and \\(E_{\\mathrm{HS}}\\) are the head-head, tail-solvent and head-solvent interaction energies and \\(E_c\\) is the energy associated with the conformation of the \\(i\\) th molecule. The model has been discussed in detail in Care (1987).\nThe model is simplified by setting the head-head interaction to zero, the tail-solvent interaction hydrophobic \\(E_{\\rm TS}&gt;0\\), the head-solvent hydrophilic \\(E_{\\rm HS}&lt;0\\) and the chain completely flexible, \\(E_c^i=0\\).\n\nviewof temperature = Inputs.range([0.01, 2], {step: 0.05, label: \"Temperature (kT)\", value: 0.5})\n\n\n\n\n\n\n\nviewof numChains = Inputs.range([3, 300], {step: 10, label: \"Number of Chains\", value: 100})\n\n\n\n\n\n\n\n\nCode\n// This is ObservableJS code  \n// You can run at observableehq.com or convert it to an equivalent (and faster?) Python version if you like\nviewof simulation = {\n    // --- Parameters ---\n    const width = 40, height = 40;\n    const chainLength = 3;\n    const chainsToCreate = numChains;\n    const T = temperature;\n    // const numChains = 10;\n    const E_TS = 1.0;     // tail-solvent energy (solvophobic)\n    const E_HS = -1.5;    // head-solvent energy (solvophilic)\n    // const T = 0.1;        // temperature (kT units)\n\n    // --- Initialize grid and chains ---\n    let grid = Array.from({length: width}, () =&gt; Array(height).fill(null));\n    let chains = [];\n\n    // Periodic boundary helper\n    function mod(n, m) { return ((n % m) + m) % m; }\n\n    function getNeighbors(x, y) {\n        return [\n            {x: mod(x - 1, width), y: y},\n            {x: mod(x + 1, width), y: y},\n            {x: x, y: mod(y - 1, height)},\n            {x: x, y: mod(y + 1, height)}\n        ];\n    }\n\n    function placeChains() {\n        for (let id = 0; id &lt; numChains; id++) {\n            for (let tries = 0; tries &lt; 100; tries++) {\n                let x = Math.floor(Math.random() * width);\n                let y = Math.floor(Math.random() * height);\n                if (grid[x][y]) continue;\n\n                let chain = [{x, y, type: 'head'}];\n                grid[x][y] = {id, type: 'head'};\n                let ok = true;\n\n                for (let i = 1; i &lt; chainLength; i++) {\n                    let last = chain[chain.length - 1];\n                    let nbs = getNeighbors(last.x, last.y).filter(p =&gt; !grid[p.x][p.y]);\n                    if (nbs.length === 0) { ok = false; break; }\n                    let next = nbs[Math.floor(Math.random() * nbs.length)];\n                    chain.push({...next, type: 'tail'});\n                    grid[next.x][next.y] = {id, type: 'tail'};\n                }\n\n                if (ok) { chains.push({id, segments: chain}); break; }\n                else { chain.forEach(p =&gt; grid[p.x][p.y] = null); }\n            }\n        }\n    }\n\n    function energy(seg) {\n        let solventNbs = getNeighbors(seg.x, seg.y).filter(p =&gt; !grid[p.x][p.y]).length;\n        return seg.type === 'head' ? solventNbs * E_HS : solventNbs * E_TS;\n    }\n\n    function attemptMove(chain) {\n        const forward = Math.random() &lt; 0.5;\n        const tail = forward ? chain.segments[0] : chain.segments.at(-1);\n        const head = forward ? chain.segments.at(-1) : chain.segments[0];\n        const options = getNeighbors(head.x, head.y).filter(p =&gt; !grid[p.x][p.y]);\n\n        if (options.length === 0) return;\n        const next = options[Math.floor(Math.random() * options.length)];\n\n        const dE_old = energy(tail);\n        grid[tail.x][tail.y] = null;\n        const dE_new = energy({...next, type: tail.type});\n        grid[tail.x][tail.y] = {id: chain.id, type: tail.type};\n\n        const deltaU = dE_new - dE_old;\n        const accept = deltaU &lt; 0 || Math.random() &lt; Math.exp(-deltaU / T);\n\n        if (accept) {\n            grid[tail.x][tail.y] = null;\n            grid[next.x][next.y] = {id: chain.id, type: tail.type};\n            if (forward) {\n                chain.segments.shift();\n                chain.segments.push({...next, type: tail.type});\n            } else {\n                chain.segments.pop();\n                chain.segments.unshift({...next, type: tail.type});\n            }\n        }\n    }\n\n    // --- Visualization ---\n    const svg = d3.create(\"svg\")\n        .attr(\"viewBox\", `0 0 ${width} ${height}`)\n        .style(\"width\", \"400px\")\n        .style(\"height\", \"400px\")\n        .style(\"border\", \"1px solid #ccc\");\n\n    const g = svg.append(\"g\");\n\n    function draw() {\n        const data = chains.flatMap(c =&gt; c.segments);\n        g.selectAll(\"circle\")\n            .data(data, d =&gt; `${d.x}-${d.y}`)\n            .join(\"circle\")\n            .attr(\"cx\", d =&gt; d.x + 0.5)\n            .attr(\"cy\", d =&gt; d.y + 0.5)\n            .attr(\"r\", 0.45)\n            .attr(\"fill\", d =&gt; d.type === \"head\" ? \"blue\" : \"red\");\n    }\n\n    // --- Main Loop ---\n    placeChains();\n    draw();\n\n    let running = true;\n    const button = html`&lt;button&gt;⏸ Pause&lt;/button&gt;`;\n    button.onclick = () =&gt; {\n        running = !running;\n        button.textContent = running ? \"⏸ Pause\" : \"▶ Resume\";\n    };\n\n    (async () =&gt; {\n        while (true) {\n            if (running) {\n                for (let i = 0; i &lt; chains.length; i++) {\n                    const c = chains[Math.floor(Math.random() * chains.length)];\n                    attemptMove(c);\n                }\n                draw();\n            }\n            await new Promise(r =&gt; setTimeout(r, 50));\n        }\n    })();\n\n    return html`&lt;div&gt;${button}&lt;br&gt;${svg.node()}&lt;/div&gt;`;\n}",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#references",
    "href": "soft-matter/soft-matter_surfactants.html#references",
    "title": "17  Surfactants",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCare, Christopher M. 1987. “Cluster Size Distribution in a Monte Carlo Simulation of the Micellar Phase of an Amphiphile and Solvent Mixture.” Journal of the Chemical Society, Faraday Transactions 1: Physical Chemistry in Condensed Phases 83 (9): 2905–12. https://pubs.rsc.org/en/content/articlehtml/1987/f1/f19878302905.\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.\n\n\nWilding, Nigel B., and Francesco Turci. 2025. “Origin of the Inverse Temperature Dependence of Hydrophobic Attraction.” Phys. Rev. Res. 7 (June): L022079. https://doi.org/10.1103/66lz-1yw9.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html",
    "href": "soft-matter/soft-matter_glasses.html",
    "title": "18  Arrested states",
    "section": "",
    "text": "18.1 Energy landscapes\nIn these chapters we have been considering systems implicitly (or explicitly) in thermal equilibrium with a surrounding environment. This is typically some kind of dispersion medium (a solvent). But we have also been more demanding: we have required that the dispersed phase (the colloids, the polymers the anisotropic particles of liquid crystals) have explored exhaustively their free energy options and that they are truly in some macrostate corresponding to a global, stable thermal equilibrium state.\nEquilibrium signifies time reversibility.\nMany fluids, whether simple liquids like argon or complex liquids such as colloids and polymers, can be rapidly cooled (quenched) to temperatures well below their equilibrium freezing point without crystallization occurring on experimental timescales. In thermodynamic terms this is interpreted as a failure of the system to reach its true equilibrium (minimum free energy) state, namely the ordered crystalline phase. At these low temperatures, the dynamics slows down and the large scale structures remain disordered as the parent fluids. This slow dynamics is however characterised by a continuous drift away from equilibrium, with all of the characteristics of the systems (slowly) evolving during time as the system ages.\nThese relaxation dynamics are examples of nonequilibrium processes: the system spontaneously relaxes under the competing constraints of its kinetic rules (typically, diffusion mechanisms) and from the shape of its free energy profile.\nIn this chapter we are going to consider two paradigmatic cases of such nonequilibrium dynamics\nWhat is distinctive of both gels and glasses is that they are both examples of amorphous materials that behave like solids: they lack long-range atomic order yet resist deformation like crystalline solids. Indeed one can calculate properties that are distinctive of solids for both gels and glasses, including:\nThese features highlight that rigidity and solidity do not require crystalline order; amorphous materials like gels and glasses can be mechanically solid while remaining structurally disordered. Indeed, the border between solidity and fluidity has been questioned in recent research work, emphasizing how this depends on observational timescales and the notion of metastability, see Sausset, Biroli, and Kurchan (2010).\nThe free energy landscape is a conceptual framework used to describe the multitude of possible configurations (microstates) of a system and their associated free energies. Each point in this high-dimensional landscape corresponds to a particular arrangement of all the particles in the system, and the height at that point represents the free energy of that configuration. In particular, the landscapes is characterised by the presence of free energy minima (valleys) which correspond to stable or metastable states—configurations where the system tends to reside and local maxima (barrier) between the valleys, representing the so-called transition states.\nVarious minima can be clustered together when their energies are relatively close, i.e. when the barriers that separate them are of the order of the energy from thermal fluctuations \\(k_B T\\). These clusters of minima are known as (meta)-basins and are very important for amorphous systems: the same system at the same temperature can display similar macroscopic characteristic not because these reflect the properties of one particular minimum, but because they are the result of local averaging within a given metabasin.\nIn the context of supercooled liquids and glasses, the landscape is rugged, with many local minima separated by high barriers. At high temperatures, the system can easily hop between minima (exploring many configurations), but as the temperature decreases, it becomes trapped in deeper minima, leading to slow dynamics and eventual dynamical arrest (glass formation).\nSimilarly, in the case of gels, the free energy landscape is also rugged, but the system becomes arrested due to the formation of a percolating network of reversible bonds. Instead of being trapped in deep minima solely by energetic barriers (as in glasses), the system’s dynamics are constrained by the connectivity of the network. The system can only relax if enough bonds break and reform to allow large-scale rearrangements, which becomes increasingly unlikely as the network spans the system. Thus, the arrested state in gels is associated with the system being confined within a region of the landscape corresponding to networked, mechanically stable configurations, separated from other regions by high barriers related to breaking the network connectivity.\nThe free energy landscape is helpful as it suggests ways to enumerate distinct conformations of a disordered system: the various metabasins can be in principle enumerated. Their number \\(N_{\\rm minima}\\) allows us to define a configurational entropy:\n\\[\nS_{\\rm conf} = k_B \\ln N_{\\rm minima}\n\\]\nHow is this different from the total entropy? In principle, the total entropy \\(S\\) can be obtained from thermodynamic relations, for example as an integral over the pressure:\n\\[\nS(T) = S(T_0) + \\int_{T_0}^{T} \\frac{1}{T'} \\left( \\frac{\\partial P}{\\partial T'} \\right)_V dV\n\\]\nor, more commonly for liquids and glasses at constant volume,\n\\[\nS(T) = S(T_0) + \\int_{T_0}^{T} \\frac{C_V(T')}{T'} dT'\n\\]\nwhere \\(C_V\\) is the heat capacity at constant volume and \\(T_0\\) is a reference temperature. For a fluid like a gas, only the total entropy is well defined. Even for a liquid at high temperature this is the case, because there are no meaningful metabasins in the free energy profile.\nIt is only as we decrease the temperature further that the metabasins can be defined (via coarse-graining). These reference conformations can be used to split the total entropy into two contributions\n\\[S = S_{\\rm conf}+S_{\\rm vib}\\]\nwhere \\(S_{\\rm vib}\\) is the vibrational entropy around the reference conformations, and is mainly due to thermal fluctuations. In this sense, we are describing the thermodynamics of a disordered system in a way closer to what we would do for a crystal, where the reference conformations are provided by the crystalline packings (e.g. FCC, HCP, BCC etc.).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#sec-energy-landscapes",
    "href": "soft-matter/soft-matter_glasses.html#sec-energy-landscapes",
    "title": "18  Arrested states",
    "section": "",
    "text": "The free energy landscape for supercooled liquids, adapted from Royall et al. (2018).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#glasses",
    "href": "soft-matter/soft-matter_glasses.html#glasses",
    "title": "18  Arrested states",
    "section": "18.2 Glasses",
    "text": "18.2 Glasses\n\n18.2.1 Glass formation\nThere exist numerous types of glassformers: these include atomic glasses (such as silicate glasses and metallic glasses), molecular glasses (organic molecules, sugar glasses), polymer glasses (amorphous polymers), colloidal glasses (dense suspensions of colloidal particles). Each type is characterized by the nature and scale of its constituents, the interactions that frustrate crystallization, and its physical properties. The table below summarizes some representative examples:\n\n\n\n\n\n\n\n\nGlassformer\nScale of Constituents\nProperties\n\n\n\n\nSilicate glass\nAtomic (Si, O atoms)\nStrong, transparent, high melting point\n\n\nMetallic glass\nAtomic (metal atoms)\nHigh strength, corrosion resistant, ductile\n\n\nPolymer glass\nMacromolecular (polymers)\nFlexible, low density, tunable glass transition\n\n\nColloidal glass\nMesoscopic (colloids, ~nm–μm)\nOpaque, tunable rheology, soft solid-like\n\n\nMolecular glass\nMolecular (organic molecules)\nLow melting point, fragile, optical uses\n\n\nSugar glass\nMolecular (sucrose, glucose)\nBrittle, water soluble, low thermal stability\n\n\nChalcogenide glass\nAtomic (S, Se, Te atoms)\nInfrared transparency, phase-change memory\n\n\n\nAll of them are amorphous, in the sense that they are disordered and do not posses long range positional or orientational order. all of them are different, but they are often formed following a similar protocol.\nHow are glasses formed? The figure below illustrates the standard route. First of all, one selects systems for which crystal formation (via nucleation and growth) is hindered (or in other words, frustrated): this can be because of the overall composition (silicate glasses, i.e. window glasses, have many components) or because of competing interactions (binary mixtures may have interactions between the A and B components that favour mixing without crystallisation).\nThen, for a chosen composition, one cools down the liquid across the temperature where in principle (if given enough time) the system would crystallise (dubbed \\(T_m\\)). The liquid becomes a so-called supercooled liquid which is a state of local equilibrium that has access to many disordered basins, but not the crystaline one (see energy landscape picture above).\n\n\n\nStandard picture of glass formation: a (local) equilibrium state (the supercooled liquid) is cooled at a certain cooling rate \\(R\\). The cooling rate is fast enough to avoid crystallization (freezing) at \\(T_m\\). At the glass transition temperature \\(T_g\\) the structural relaxation time of the system exceeds the observation timescale and the system falls off equilibrium. The experimental glass transition temperature is not unique and depends on the protocol: fast cooling rate lead to higher glass transition temperatures.\n\n\nAs we keep on cooling the supercooled liquid to lower and lower temperature, the molecules or constituents move more and more slowly, so that the diffusivity is progressively reduced. This slowing down is typically characterized in terms of autocorrelation functions of the density.\nA common way to quantify this is through the (self-part) intermediate scattering function \\(F_s(q, t)\\), which measures how density fluctuations at a given wavevector \\(q\\) decay over time:\n\n\nThe intermediate scattering function \\(F_s(q, t)\\) can be decomposed into two contributions:\n\nSelf part \\(F_s^{\\text{self}}(q, t)\\): Measures the correlation of each particle with its own initial position. It captures single-particle dynamics (how far a particle moves from where it started).\nCollective part \\(F_s^{\\text{coll}}(q, t)\\): Measures correlations between different particles, reflecting how density fluctuations evolve collectively. The collective part is given by \\[\n  F_s^{\\text{coll}}(q, t) = \\frac{1}{N} \\left\\langle \\sum_{j,k=1}^N e^{i \\mathbf{q} \\cdot [\\mathbf{r}_j(t) - \\mathbf{r}_k(0)]} \\right\\rangle\n  \\] In glassy systems, the self part is sensitive to particle mobility (e.g., caging and hopping), while the collective part describes how groups of particles rearrange together. Both are important for understanding relaxation and dynamical arrest.\n\n\n\\[\nF_s^{\\text{self}}(q, t) = \\left\\langle \\frac{1}{N} \\sum_{j=1}^N e^{i \\mathbf{q} \\cdot [\\mathbf{r}_j(t) - \\mathbf{r}_j(0)]} \\right\\rangle\n\\]\n\nAt high temperatures, \\(F_s(q, t)\\) decays rapidly, indicating fast relaxation. As temperature decreases, the decay becomes much slower, often exhibiting a two-step relaxation: a rapid initial drop (the “\\(\\beta\\)-relaxation”) followed by a long plateau and then a slow final decay (the “\\(\\alpha\\)-relaxation”). The time at which \\(F_s(q, t)\\) decays to a certain fraction (e.g., \\(1/e\\)) defines the structural relaxation time \\(\\tau_\\alpha\\), which represents the typical time for a particle to move over a lengthscale \\(\\sim 2\\pi/q\\).\n\n\n\nIntermediate scattering functions for molecular dynamics simulations of a standard glassformer (a coarse grained metallic binary mixture) for various reduced temperatures. From Coslovich, Ozawa, and Kob (2018). One can identify the initial relaxation (\\(\\beta\\)) followed by a long plateau which is itself followed by the complete relaxation (\\(\\alpha\\)). Notice the logarithmic scale of the horizontal axis (in reduced time units).\n\n\nAs the supercooled liquid gets colder and colder, the relaxation time increases rapidly, eventually of many orders of magnitude. At some experimentally determined temperature the relaxation time is larger than the observation time, i.e.\n\\[\\tau_{\\alpha}&gt;t_{\\rm obs}\\]\nThis means that it is impossible to take time averages to estimate thermodynamic properties of the system, because the system does not satisfy any longer the minimal requirement of the ergodic hypothesis. This failure of ergodicity means that the system effectively falls out of equilibrium, because it does not explore all relevant microstates within the available time. The system has become a glass. The temperature at which this clearly non-ordinary (and non-thermodynamic!) transition occurs is called the experimental glass transition temperature \\(T_g\\).\nA key feature of the glass transition is that there is no unique glass transition temperature \\(T_g\\): the transition temperature itself depends on how fast we are cooling the liquid down. It is a protocol dependent property. We can understand this by referring back to the energy landscape: at high temperatures, the system has enough thermal energy to explore many minima in the landscape, hopping over barriers with ease. As the temperature decreases, the barriers become increasingly difficult to cross within the available observation time. If the cooling is slow, the system can equilibrate and find deeper minima, resulting in a lower \\(T_g\\). If the cooling is fast, the system becomes trapped in higher-energy, shallower minima, and \\(T_g\\) is higher. Therefore, the glass transition is not a sharp thermodynamic phase transition, but a kinetic phenomenon determined by the interplay between the system’s relaxation time and the timescale of the experimental protocol.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#viscosity-and-relaxation-times",
    "href": "soft-matter/soft-matter_glasses.html#viscosity-and-relaxation-times",
    "title": "18  Arrested states",
    "section": "18.3 Viscosity and relaxation times",
    "text": "18.3 Viscosity and relaxation times\nThe structural relaxation time extracted from correlation functions such as the intermediate scattering function is proportional to a macroscopic property, the viscosity\n\\[\\tau_\\alpha \\propto \\eta\\]\nand hence the viscosity at the glass transition increases rapidly (diverges) like \\(\\tau_\\alpha\\).\n\n\n\n\n\n\nNoteRelationship between viscosity and structural relaxation time\n\n\n\n\n\nThe proportionality between viscosity \\(\\eta\\) and the structural relaxation time \\(\\tau_\\alpha\\) can be understood using linear response theory and the Green-Kubo relations.\nThe viscosity is given by the Green-Kubo formula as an integral of the stress autocorrelation function: \\[\n\\eta = \\frac{1}{k_B T V} \\int_0^\\infty \\langle \\sigma_{xy}(0) \\sigma_{xy}(t) \\rangle dt\n\\] where \\(\\sigma_{xy}\\) is the off-diagonal component of the stress tensor.\nIn supercooled liquids and glasses, the decay of the stress autocorrelation function is governed by the same slow structural relaxation processes that control \\(\\tau_\\alpha\\). Thus, the integral is dominated by timescales of order \\(\\tau_\\alpha\\), leading to: \\[\n\\eta \\sim G_\\infty \\tau_\\alpha\n\\] where \\(G_\\infty\\) is the instantaneous (high-frequency) shear modulus.\nThis proportionality holds in the regime where the relaxation is dominated by structural rearrangements (i.e., near the glass transition), and is supported by both experiments and simulations. Therefore, the dramatic increase in viscosity as the glass transition is approached directly reflects the growth of the structural relaxation time.\n\n\n\n\n\n\nViscosity as a function of the inverse temperature scaled by the experimental glass transition temperature (Angell plot). One observes an increase of several orders of magnitude between the high temperature and the low temperature regime. Also one can distinguish different classes of glassformers, conventionally termed as strong and fragile. Adapted from Debenedetti and Stillinger (2001).\n\n\nThis dramatic increase in relaxation time is a hallmark of glassy dynamics and underlies the kinetic arrest observed in glasses.\nPresently, we do not have a single unified theory of the glass transition: there is no universal law capable to predict the viscosity (or the relaxation times) of glassy systems from specific microscopic parameters. To date, the origin of the glass transition remains one of the major unsolved problems condensed matter physics.\nFirst steps are typically phenomenological: glasses are classified as strong or fragile based on how their viscosity (or relaxation time) increases as temperature decreases toward the glass transition temperature \\(T_g\\). We have seen earlier in Section 10.2 that activated process are often governed by what are called Arrhenius laws, with a rate \\(R\\propto \\exp\\left[-\\Delta E/k_B T\\right]\\) where \\(\\Delta E\\) is an energy barrier. Viscosity curves can be fitted by empirical models inspired the activated picture:\n\nStrong glasses (e.g., silica, SiO\\(_2\\)) follow an Arrhenius law : \\[\n  \\eta(T) = \\eta_0 \\exp\\left(\\frac{E_A}{k_B T}\\right)\n  \\] where \\(E_A\\) is an activation energy. The plot of \\(\\log \\eta\\) vs \\(1/T\\) is a straight line.\nFragile glasses (e.g., o-terphenyl, many polymers) show super-Arrhenius behavior, often described by the Vogel-Fulcher-Tammann (VFT) equation: \\[\n  \\eta(T) = \\eta_0 \\exp\\left(\\frac{B}{T - T_0}\\right)\n  \\] where \\(B\\) and \\(T_0\\) are empirical parameters. The plot of \\(\\log \\eta\\) vs \\(1/T\\) is highly curved.\n\nThe distinction is visualized in the so-called Angell plot, where strong glasses show nearly linear behavior, while fragile glasses show a dramatic upturn as \\(T \\to T_g\\).\nThese models are suggestive of a thermodynamic origin of the glass transition. Indeed, it is possible to think of the emerging barrier \\(E_A\\) and \\(B\\) as the result of some free energy expression from the sampling of the energy landscapes in various basins.\n\n18.3.1 Connection between VFT and configurational entropy: the Adam-Gibbs model\nA key theoretical link between the dramatic slowdown of dynamics (as described by the VFT law) and the underlying thermodynamics is provided by the Adam-Gibbs model, details in Bouchaud and Biroli (2004).\nThis model proposes that the structural relaxation time \\(\\tau_\\alpha\\) is controlled by the configurational entropy \\(S_{\\rm conf}\\), which counts the number of distinct amorphous basins available to the system.\nThe key idea is the existence of cooperatively rearranging regions (CRRs): to relax, particles need to move together to explore a new metabasin in the energy landscape.\nThe sketch of the theory is the following: we split the system into \\(M_{CRR}\\) independent cooperatively rearranging regions of size \\(n_{CRR}\\) particles each. The total number of particles is \\(N = M_{CRR} n_{CRR}\\).\nWe assume that the total configurational entropy is additive and that each CRR contributes a constant amount \\(s_{conf}\\), irrespective of its size\n\\(S_{conf} = M_{CRR} s^* = \\dfrac{N}{n_{CRR}} s^*\\)\nRearranging gives the size of a CRR as a function of the configurational entropy per particle \\(S_{conf}/N\\): \\[n_{CRR}(T) \\propto \\dfrac{1}{S_{conf}(T)/N}\\]\nWe then assume that the activation free energy \\(\\Delta G\\) required for a CRR to rearrange is proportional to its size:\n\\[\\Delta G(T) \\propto n_{CRR}(T)\\]\nBy assuming activated dynamics, the Adam-Gibbs relation follows: \\[\n\\tau_\\alpha(T) = \\tau_0 \\exp\\left(\\frac{A}{T S_{\\rm conf}(T)}\\right)\n\\] where \\(A\\) is a constant. As temperature decreases, \\(S_{\\rm conf}\\) drops, leading to a rapid increase in \\(\\tau_\\alpha\\). If \\(S_{\\rm conf}\\) vanishes at a finite temperature \\(T_K\\) (the so-called Kauzmann temperature), the relaxation time diverges, reproducing the VFT form: \\[\n\\tau_\\alpha(T) \\sim \\exp\\left(\\frac{B}{T - T_0}\\right)\n\\] with \\(T_0 \\approx T_K\\). Thus, the Adam-Gibbs model provides a thermodynamic interpretation of the VFT law, connecting the kinetic slowdown to the loss of configurational entropy as the glass transition is approached.\n\n\n18.3.2 Alternative perspective: Dynamical facilitation and the parabolic law\nWhile thermodynamic models like Adam-Gibbs relate the glass transition to configurational entropy, a radical alternative approach is the dynamical facilitation theory, see Chandler and Garrahan (2010) for more details. This framework emphasizes that glassy slowdown arises from the dynamics themselves, rather than underlying thermodynamic changes.\nHere the central observation is that the dynamics of all glassy systems is highly hetogeneous in space and time: some regions are very mobile, while others are essentially frozen. This dynamic heterogeneity is a hallmark of glassy dynamics. It corresponds to a dynamical coexistence between fast-relaxing and slow relaxing region that can be formalised as a dynamical phase transition.\nIn dynamical facilitation, mobility is sparse at low temperatures: regions of the system can only relax if they are adjacent to already mobile regions—mobility facilitates further mobility. This leads to hierarchical, cooperative dynamics without invoking a thermodynamic singularity.\nA schematic illustration:\n\nAt high \\(T\\), mobile regions are abundant and relaxation is fast.\nAs \\(T\\) decreases, mobile regions become rare, and relaxation requires the creation and propagation of mobility, which is a rare event.\nThe relaxation time grows rapidly due to the need for cooperative rearrangements.\n\nThis scenario predicts a parabolic law for the relaxation time:\n\\[\n\\log \\tau_\\alpha(T) \\sim J^2 \\left( \\frac{1}{T} - \\frac{1}{T_0} \\right)^2\n\\]\nwhere \\(J\\) is an energy scale and \\(T_0\\) is an onset temperature. Unlike the VFT law, the parabolic law does not diverge at finite \\(T\\) but still captures the super-Arrhenius growth of relaxation times.\nInterestingly, both the two perspective fit the viscosity data well within their regimes of validity (and recent research suggests that close to the glass transition temperature the microscopic mechanisms resemble dynamical facilitation).\nHere below is a the visual representation of the time evolution of the magnitude of the displacement field in a model of glass governed by dynamical facilitation, see Hasyim and Mandadapu (2024) for more details.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#physical-gels",
    "href": "soft-matter/soft-matter_glasses.html#physical-gels",
    "title": "18  Arrested states",
    "section": "18.4 Physical gels",
    "text": "18.4 Physical gels\nThe word gel signifies many different things to different scientific communities. In the context of soft matter physics, a physical gel is typically understood as a system in which the constituent particles or polymers are connected via reversible, non-covalent bonds to form a percolating network that spans the entire sample. This network imparts solid-like mechanical properties to the material, even though the underlying structure remains disordered and fluid-like on a microscopic scale.\nPhysical gels differ from chemical gels, where the network is formed by irreversible covalent bonds. In physical gels, the bonds can break and reform dynamically, have an energy of order \\(1 k_BT\\), allowing the system to respond to external stresses and sometimes to self-heal. In this sense they are reversible systems and often are known as thermoreversible gels. reversible linking is typically achieved by the formation of dense local regions (e.g. microscystalline or glassy) that act as cross-links to form a spanning network.\nExamples include gelatin desserts, agarose gels, and colloidal suspensions such as yoghurt, paint, inks where attractive interactions lead to network formation, see Zaccarelli (2007) for a comprehensive review.\nThe transition from a fluid to a gel state is often associated with the appearance of a system-spanning cluster, which can be described using concepts from percolation theory. The mechanical rigidity of the gel arises when this cluster forms, leading to a dramatic increase in viscosity and the emergence of an elastic response.\n\n\n\n\n\n\nNotePercolation\n\n\n\n\n\nPercolation is the study of an apparently innocent mathematical problem: take a square grid if \\(L\\times L\\) sites and randomly label \\(N\\) of them; how does the likelihood of forming a cluster that spans across the grid (i.e. percolates) depend on the probability \\(p=N/L^2\\) of having a labelled square?\nThe problem is interesting for its various applications across domains of science as diverse as networks formation, transport, material science, epidemiology and ecology. It is amenable to an analytical treatment and can be solved via direct simulations. Percolation is a classic example of a phase transition and critical phenomena. The probability \\(p_c\\) at which a spanning cluster first appears is called the percolation threshold. For a large 2D square lattice, \\(p_c \\approx 0.5927\\). Below \\(p_c\\), only small, disconnected clusters exist; above \\(p_c\\), a giant connected component spans the system. The transitions is continuous, i.e. second order.\nAnalytically, percolation is tractable and exhibits universal critical exponents near \\(p_c\\), making it a cornerstone of statistical physics and network theory.\nYou can play with percolation on a squared lattice with the code below.\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table illustrates that various values of the critical percolation probability are known for various lattices, either exactly or via simulation. Notice that this non-universal feature, the critical \\(p_c\\), decreases with increasing dimensionality: at higher dimensions, the connectivity is naturally higher and so a comparatively smaller fraction of the system is require in order to find a percolating path.\n\n\n\n\n\n\n\n\n\n\nLattice Type\nDimension\nCoordination \\(z\\)\n\\(p_c^{site}\\)\n\\(p_c^{bond}\\)\n\n\n\n\nSquare\n2\n4\n0.592746\n0.5\n\n\nTriangular\n2\n6\n0.5\n0.347296\n\n\nHoneycomb\n2\n3\n0.697043\n0.652703\n\n\nKagome\n2\n4\n0.6527\n0.5244\n\n\nUnion Jack\n2\n8\n0.379\n0.429\n\n\nCubic\n3\n6\n0.3116\n0.2488\n\n\nFCC\n3\n12\n0.199\n0.120\n\n\nBCC\n3\n8\n0.245\n0.180\n\n\nDiamond\n3\n4\n0.43\n0.389\n\n\nHypercubic (4D)\n4\n8\n0.197\n0.160\n\n\nHypercubic (5D)\n5\n10\n0.141\n0.118\n\n\nHypercubic (6D)\n6\n12\n0.109\n0.094\n\n\nHypercubic (7D)\n7\n14\n0.091\n0.079\n\n\nHypercubic (8D)\n8\n16\n0.078\n0.069\n\n\n\nInstead, other properties of percolation are universal: these are the critical exponents of observables on the lattice,and do not depend on the lattice detail, such as the probability to find a site in a percolating cluster \\(P_{\\infty}(p)\\propto (p-p_c)^{\\beta}\\)\n\n\n\nDimension \\(d\\)\n\\(\\beta\\) (Percolation)\n\n\n\n\n2\n5/36 ≈ 0.1389\n\n\n3\n≈ 0.41\n\n\n4\n≈ 0.65\n\n\n5\n≈ 0.81\n\n\n6\n≈ 0.96\n\n\n≥6 (Mean-field)\n1\n\n\n\n\n\n\nIn the case of physical gels, percolation is a required ingredient for mechanical stability: in the absence of a large spanning percolating network, it is not possible for the gel to sustain external stresses or even self-generated stresses such as its own weight under gravity.\n\n18.4.1 Colloid-polymer mixtures as an example\nWhen the colloids are significantly larger than the polymers, the resulting depletion interaction is attractive and very short ranged: it is a so-called sticky interaction, ideally suited to form robust physical bonds where, the colloids cluster in a nonequilibrium, branched, dense phase.\nThe resulting phase diagrams different significantly for the ones of simple liquids (e.g., Lennard-Jones interactions): the liquid-gas coexistence curve (the binodal) becomes metastable.\n\n\n\nPhase diagrams of colloid-polymer mixtures for different values of the size ratio parameter \\(q=\\sigma_{\\rm polymer}/\\sigma_{\\rm colloid}\\). Left: very small size ratio make the gas liquid binodal (enclosing the yellow fluid-fluid region) metastable to gas-solid phase separation. Centre: as we increase the size ratio, the liquid pocket at high density starts developing. Right: for almost equal sizes the phase diagram is reminiscent of the one of simple fluids (e.g. Lennard-Jones) after noticing that the polymer reservoir packing fraction plays a role inverse to the temperature. Adapted from Dijkstra, Brader, and Evans (1999).)\n\n\nGiven the very short range of the interactions, models of colloid polymer mixtures can approximate the Asakura-Oosawa potential (see Section 14.2.5) using even simpler pair-wise interactions. For example, for Monte-Carlo simulations qand theoretical calculations one often uses the square-well model,\nThe square-well potential \\(U(r)\\) is defined as:\n\\[\nU(r) =\n\\begin{cases}\n\\infty & r &lt; \\sigma \\\\\n-\\epsilon & \\sigma \\leq r &lt; \\lambda \\sigma \\\\\n0 & r \\geq \\lambda \\sigma\n\\end{cases}\n\\]\nwhere \\(\\sigma\\) is the particle diameter (hard core), \\(\\epsilon\\) is the well depth (attractive strength), and \\(\\lambda\\) controls the range of the attraction.\n\n\n\n\n\n\n\n\n\nFor molecular dynamics one often employs the so-called Morse potential, which combines fast exponentially decaying tails and a repulsive core in a simple analytical form\n\\[\nU_{\\mathrm{Morse}}(r) = D \\left[ e^{-2\\alpha (r - r_0)} - 2 e^{-\\alpha (r - r_0)} \\right]\n\\]\nwhere \\(D\\) is the well depth, \\(\\alpha\\) controls the range (steepness) of the potential, and \\(r_0\\) is the equilibrium bond distance.\n\n\n\n\n\n\n\n\n\n\nPair correlations\nIn many situation, pair-wise correlation functions are sufficient to capture gel formation. Both the radial distribution function and the structure factor are able to describe gel formation, but they focus (and are accurate) in different regimes:\nThe radial distribution function \\(g(r)\\) has best statistics at short distances, capturing the features of the clusters that form the branched network.\n\n\n\nThe changes in the radial distribution function \\(g(r)\\) as time evolves and the frozen structure of a simulated colloidal gel emerges. The systems is at an overall dilute concentration, so clusters appear as very sharp peaks at short distances, and their structure in terms of nearest and second-nearest neighbour shells are reflected in the first few peaks. From Griffiths, Turci, and Royall (2017)\n\n\nIn gels near the percolation threshold, the structure factor \\(S(q)\\) exhibits scale-free (power-law) behavior at low \\(q\\):\n\\[\nS(q) \\sim q^{-D_f}\n\\]\nwhere \\(D_f\\) is the fractal dimension of the gel network. This power-law regime reflects the absence of a characteristic length scale in the structure—clusters are self-similar over a range of sizes.\n\nFor \\(q\\) much smaller than the inverse cluster size, \\(S(q)\\) flattens (finite-size effects).\nFor intermediate \\(q\\), \\(S(q) \\sim q^{-D_f}\\), indicating fractal geometry.\nFor large \\(q\\), \\(S(q)\\) reflects local (non-fractal) structure.\n\n\n\n\nThe same changes of the previous figures as seen in changes in the structure factor \\(S(q)\\) as time evolves and the frozen structure of a colloidal gel emerges. At small q, a characteristic power law behaviour emerges, while at higher q the microstructure changes are subtly capture by the reorganisation or splitting of the peaks.\n\n\nThe exponent \\(D_f\\) is related to the spatial scaling of mass with size in the fractal cluster: \\(M(R) \\sim R^{D_f}\\). Typical values for percolation clusters in 3D are \\(D_f \\approx 2.5\\).\nThus, by analyzing the low-\\(q\\) behavior of \\(S(q)\\), one can extract the fractal dimension and confirm scale-free, self-similar structure in gels.\n\n\n\n\n\n\nNoteMeasuring fractal dimensions: the box-counting algorithm\n\n\n\n\n\nFractal dimensions are in practice quite tricky to measure. A common method is the box-counting algorithm:\n\nOverlay a grid of boxes of size \\(\\ell\\) over the structure (e.g., a cluster or network).\nCount the number \\(N(\\ell)\\) of boxes that contain any part of the structure.\nRepeat for different box sizes \\(\\ell\\).\nPlot \\(\\log N(\\ell)\\) versus \\(\\log(1/\\ell)\\). For a fractal, this plot is linear over some range, and the slope gives the fractal dimension \\(D_f\\): \\[\nN(\\ell) \\sim \\ell^{-D_f}\n\\]\n\nThis method is widely used for experimental images and simulation data to estimate the fractal dimension of clusters, aggregates, or networks.\nBelow, illustrate this using a known fractal, the Sierpinski carpet, whose fractal dimensions is exactly \\(D_f = \\log{8}/\\log{3}\\approx 1.8929\\)\n\n# Example: Box-counting fractal dimension for the Sierpinski carpet\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sierpinski_carpet(n):\n    \"\"\"Generate a Sierpinski carpet of order n as a 2D boolean array.\"\"\"\n    size = 3**n\n    carpet = np.ones((size, size), dtype=bool)\n    for i in range(n):\n        step = 3**i\n        for x in range(0, size, 3*step):\n            for y in range(0, size, 3*step):\n                carpet[x+step:x+2*step, y+step:y+2*step] = False\n    return carpet\n\ndef box_count(img, box_sizes):\n    \"\"\"Count number of boxes containing any part of the structure for each box size using histogramdd.\"\"\"\n    N = []\n    coords = np.argwhere(img)\n    for box in box_sizes:\n        bins = [img.shape[0] // box, img.shape[1] // box]\n        # Assign each coordinate to a box\n        hist, _ = np.histogramdd(coords, bins=bins, range=[[0, img.shape[0]], [0, img.shape[1]]])\n        N.append(np.count_nonzero(hist))\n    return np.array(N)\n\n# Generate Sierpinski carpet\norder = 5\ncarpet = sierpinski_carpet(order)\n\n# Box sizes (powers of 3)\nbox_sizes = [3**i for i in range(order, 0, -1)]\nN_boxes = box_count(carpet, box_sizes)\n\n# Plot Sierpinski carpet\nplt.figure(figsize=(4,4))\nplt.imshow(carpet, cmap='binary')\nplt.title('Sierpinski carpet (order %d)' % order)\nplt.axis('off')\nplt.show()\n\n# Box-counting plot\nplt.figure(figsize=(5,4))\nplt.plot(np.log(1/np.array(box_sizes)), np.log(N_boxes), 'o-', label='Box counting')\n# Linear fit for slope (fractal dimension)\nslope, intercept = np.polyfit(np.log(1/np.array(box_sizes)), np.log(N_boxes), 1)\nplt.plot(np.log(1/np.array(box_sizes)), slope*np.log(1/np.array(box_sizes))+intercept, '--', label=f'Fit: $D_f$={slope:.4f}')\nplt.xlabel(r'$\\log(1/\\ell)$')\nplt.ylabel(r'$\\log N(\\ell)$')\nplt.title('Box-counting for Sierpinski carpet')\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.4.2 Arrested spinodal scenario for colloidal gels\nBut how do colloidal gels come about? A viable scenario is the so-called arrested spinodal decomposition scenario. Here, a system is rapidly quenched into a region of its phase diagram where it would normally phase separate into two distinct phases (e.g., a dense and a dilute phase) via spinodal decomposition. However, before the phase separation can complete, the dynamics of the dense regions slow down dramatically—often due to glassy or jamming behavior—leading to a dynamically arrested, bicontinuous structure.\nThis process is relevant for colloid-polymer mixtures, protein solutions, and some polymer blends. The resulting gels are characterized by a network-like structure that reflects the early stages of spinodal decomposition, “frozen in” by the arrest of particle motion.\nFor the arrested spinodal scenario to hold, the system evolves following these steps:\n\nThe system is quenched inside the spinodal region, where spontaneous fluctuations grow.\nDomains of different densities form, but the dense domains become dynamically arrested before macroscopic phase separation completes.\nThe final structure is a bicontinuous, percolating network with solid-like mechanical properties.\n\n\n\n\nThe arrested spinodal scenario mapped on the phase diagram of the dispersed particles (e.g. colloids), adapted from Zaccarelli (2007). We distinguish various lines: the liquid-gas coexistence (binodal) in red, the spinodal line (where phase separation is unstable) black dashed, the percolation line (across which a percolating cluster can be detected), the glass transition line (a dynamical line, where the relaxation time of the system exceeds a conventional threshold, e.g. 100 seconds). If a system is cooled rapidly down from the fluid to the coexistence region, two scenarios are possible: the fluid spontaneously phase separates into two equilibrium phases (liquid and gas, orange horizontal line); or, the cooling is so rapid that proper equilibrium phases cannot be formed and the systems separates into a disordered arrested dense phase and a vapor phase (green horizontal line). This second scenario corresponds to physical gel formation via spinodal decomposition.\n\n\nFor more details, see Zaccarelli (2007).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBouchaud, Jean-Philippe, and Giulio Biroli. 2004. “On the Adam-Gibbs-Kirkpatrick-Thirumalai-Wolynes Scenario for the Viscosity Increase in Glasses.” The Journal of Chemical Physics 121 (15): 7347–54. https://bris.on.worldcat.org/oclc/110636005.\n\n\nChandler, David, and Juan P Garrahan. 2010. “Dynamics on the Way to Forming Glass: Bubbles in Space-Time.” Annual Review of Physical Chemistry 61 (1): 191–217. https://bris.on.worldcat.org/oclc/4761088692.\n\n\nCoslovich, Daniele, Misaki Ozawa, and Walter Kob. 2018. “Dynamic and Thermodynamic Crossover Scenarios in the Kob-Andersen Mixture: Insights from Multi-CPU and Multi-GPU Simulations.” The European Physical Journal E 41: 1–11. https://bris.on.worldcat.org/oclc/7634416384.\n\n\nDebenedetti, Pablo G, and Frank H Stillinger. 2001. “Supercooled Liquids and the Glass Transition.” Nature 410 (6825): 259–67. https://www.nature.com/articles/35065704.\n\n\nDijkstra, Marjolein, Joseph M Brader, and Robert Evans. 1999. “Phase Behaviour and Structure of Model Colloid-Polymer Mixtures.” Journal of Physics: Condensed Matter 11 (50): 10079. https://bris.on.worldcat.org/oclc/4843512718.\n\n\nGriffiths, Samuel, Francesco Turci, and C Patrick Royall. 2017. “Local Structure of Percolating Gels at Very Low Volume Fractions.” The Journal of Chemical Physics 146 (1). https://pubs.aip.org/aip/jcp/article/146/1/014501/313344/Local-structure-of-percolating-gels-at-very-low.\n\n\nHasyim, Muhammad R, and Kranthi K Mandadapu. 2024. “Emergent Facilitation and Glassy Dynamics in Supercooled Liquids.” Proceedings of the National Academy of Sciences 121 (23): e2322592121. https://www.pnas.org/doi/abs/10.1073/pnas.2322592121.\n\n\nRoyall, C Patrick, Francesco Turci, Soichi Tatsumi, John Russo, and Joshua Robinson. 2018. “The Race to the Bottom: Approaching the Ideal Glass?” Journal of Physics: Condensed Matter 30 (36): 363001. https://bris.on.worldcat.org/oclc/1098708850.\n\n\nSausset, F, G Biroli, and J Kurchan. 2010. “Do Solids Flow?” Journal of Statistical Physics 140: 718–27. https://bris.on.worldcat.org/oclc/5649060652.\n\n\nZaccarelli, Emanuela. 2007. “Colloidal Gels: Equilibrium and Non-Equilibrium Routes.” Journal of Physics: Condensed Matter 19 (32): 323101. https://doi.org/10.1088/0953-8984/19/32/323101.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html",
    "href": "soft-matter/soft-matter_active.html",
    "title": "19  Active matter",
    "section": "",
    "text": "19.1 Beyond thermal systems\nAll the system we have considered up to now are composed of naturally occurring or synthetic molecules (or atoms) which interact via conservative forces (potentials) and are subject to thermal fluctuations. Essentially, energy is either stored in the system as potential energy or exchanged with the environment as heat, leading to equilibrium dynamics governed by the laws of thermodynamics and statistical mechanics. In these systems, the random motion of particles arises from thermal noise, and the system eventually relaxes to a Boltzmann distribution.\nHowever, many systems in nature and technology are driven out of equilibrium by continuous energy consumption at the microscopic scale. These are known as active matter systems. Here, each constituent (such as a bacterium, a synthetic microswimmer, or a molecular motor) converts energy from its surroundings into a variety of mechanisms:\nThis is what is typically performed by many living organisms: by consuming energy locally (hence dissipating heat) they perform some function. Here we focus on of the most fundamental kins of functions which is motion. A bacterium uses energy to propel itself into space, explore it and eventually interact with the environment, including other bacteria.\nWe could, at this stage, say that these interactions and the behaviour emerging from them are inherently biological and governed solely by mechanisms that transcend the physical correlations (behavioural science). As physicists, we do not negate the existence of such dimensions (especially when communication and sensing are involved) but aim to run a different research programme: we want to gauge the level of biological surprise1, as nicely put by Andrea Cavagna, a complex system physicist.\nTo do so we need to understand how systems that dissipate energy locally to produce self-propelled motion can lead to the emergence of non-trivial phase behaviour and how this differs from systems that are either at thermal equilibrium (e.g. colloidal fluids) or are slowly relaxing towards it (e.g. glasses and gels).\nThe field of research that studies such systems is called physics of active matter and in this chapter we will describe some reference model systems and main results.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#beyond-thermal-systems",
    "href": "soft-matter/soft-matter_active.html#beyond-thermal-systems",
    "title": "19  Active matter",
    "section": "",
    "text": "motion\nsensing\nprocessing\ngrowth and deformation",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#life-inspired-motion-run-and-tumble",
    "href": "soft-matter/soft-matter_active.html#life-inspired-motion-run-and-tumble",
    "title": "19  Active matter",
    "section": "19.2 Life-inspired motion: run and tumble",
    "text": "19.2 Life-inspired motion: run and tumble\nThe run-and-tumble model describes the motion of active particles, such as bacteria, that alternate between two types of movement: “runs” (straight-line motion) and “tumbles” (random reorientation). This model captures the behavior of microorganisms like E. coli.\n\n\n\n\n\n\nFigure 19.1: Swimming Escherichia coli bacteria with fluorescently-labeled flagellar filaments, showing individual run-and-tumble events with flagellar filaments unbundling, cells turning, and flagellar filaments rejoining the bundle. The details of the experimental procedure are given in Turner, Ryu, and Berg (2000).\n\n\n\n\n\nCode\nviewof simulation = {\n    const n = 100, W = 1000, H = 300, dt = 0.5, v0 = 1.5;\n\n    const tumbleSlider = Inputs.range([0.01, 1.0\n    ], {step:0.01, value:0.1, label:\"Tumble rate λ\"});\n    const button = html`&lt;button&gt;Pause&lt;/button&gt;`;\n    let running = true;\n    button.onclick = () =&gt; {\n        running = !running;\n        button.textContent = running ? \"Pause\" : \"Resume\";\n    };\n\n    let state = Array.from({length: n}, () =&gt; ({\n        x: Math.random() * W,\n        y: Math.random() * H,\n        theta: Math.random() * 2 * Math.PI\n    }));\n\n    // Each particle gets a trace array\n    let traces = Array.from({length: n}, () =&gt; []);\n\n    // Reset traces when tumble rate changes\n    let lastLambda = tumbleSlider.value;\n    tumbleSlider.addEventListener(\"input\", () =&gt; {\n        traces = Array.from({length: n}, () =&gt; []);\n        lastLambda = tumbleSlider.value;\n    });\n\n    function step(state, λ) {\n        return state.map((p, i) =&gt; {\n            if (Math.random() &lt; λ * dt) p.theta = Math.random() * 2 * Math.PI;\n\n            // Predict next position\n            let nx = p.x + v0 * Math.cos(p.theta) * dt;\n            let ny = p.y + v0 * Math.sin(p.theta) * dt;\n\n            // Bounce at left/right walls\n            if (nx &lt; 0) {\n                p.x = 0;\n                p.theta = Math.PI - Math.random() * Math.PI; // random angle pointing right\n            } else if (nx &gt; W) {\n                p.x = W;\n                p.theta = Math.PI + Math.random() * Math.PI; // random angle pointing left\n            } else {\n                p.x = nx;\n            }\n\n            // Bounce at top/bottom walls\n            if (ny &lt; 0) {\n                p.y = 0;\n                p.theta = (Math.random() * Math.PI); // random angle pointing down\n            } else if (ny &gt; H) {\n                p.y = H;\n                p.theta = Math.PI + (Math.random() * Math.PI); // random angle pointing up\n            } else {\n                p.y = ny;\n            }\n\n            // Add current position to trace\n            traces[i].push([p.x, p.y]);\n            // Limit trace length for performance\n            if (traces[i].length &gt; 200) traces[i].shift();\n            return p;\n        });\n    }\n\n    const container = html`&lt;div&gt;&lt;/div&gt;`;\n    container.append(tumbleSlider, button);\n    const svg = d3.select(container).append(\"svg\")\n        .attr(\"width\", W)\n        .attr(\"height\", H)\n        .style(\"background\", \"#f8f8f8\");\n\n    while (true) {\n        const λ = tumbleSlider.value;\n        if (running) state = step(state, λ);\n\n        svg.selectAll(\"*\").remove();\n\n        // Draw traces\n        traces.forEach(trace =&gt; {\n            if (trace.length &gt; 1) {\n                svg.append(\"path\")\n                    .attr(\"d\", d3.line()(trace))\n                    .attr(\"stroke\", \"#90caf9\")\n                    .attr(\"stroke-width\", 1)\n                    .attr(\"fill\", \"none\");\n            }\n        });\n\n        // Draw particles\n        svg.selectAll(\"circle\")\n            .data(state)\n            .enter().append(\"circle\")\n            .attr(\"cx\", d =&gt; d.x)\n            .attr(\"cy\", d =&gt; d.y)\n            .attr(\"r\", 3)\n            .attr(\"fill\", \"#1976d2\");\n\n        svg.selectAll(\"line\")\n            .data(state)\n            .enter().append(\"line\")\n            .attr(\"x1\", d =&gt; d.x)\n            .attr(\"y1\", d =&gt; d.y)\n            .attr(\"x2\", d =&gt; d.x + 10 * Math.cos(d.theta))\n            .attr(\"y2\", d =&gt; d.y + 10 * Math.sin(d.theta))\n            .attr(\"stroke\", \"#1976d2\")\n            .attr(\"stroke-width\", 1.2);\n\n        yield container;\n        await Promises.delay(16);\n    }\n}\n\n\n\n\n\n\n\n\n\nFigure 19.2: Non-interacting run and tumble particles.\n\n\n\n\n\nThe model is designed to encode a minimal set of ingredients that make the characteristic trajectories of the bacteria quite different from an ordinary random walk. We can formulate this kind of model via a simple algorithm.\n\nRun Phase: During a run, the particle moves in a straight line with constant velocity \\(v_0\\): \\[\n\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + v_0 \\hat{\\mathbf{n}}(t) \\Delta t\n\\]\nwhere:\n\n\\(\\mathbf{r}(t)\\) is the position of the particle at time \\(t\\),\n\\(v_0\\) is the constant speed,\n\\(\\hat{\\mathbf{n}}(t)\\) is the unit vector indicating the direction of motion.\n\nTumble Phase: During a tumble, the particle randomly reorients. The new direction \\(\\hat{\\mathbf{n}}(t)\\) is chosen from a uniform distribution over the unit sphere (in 3D) or circle (in 2D).\n\nWe can imagine a schematic algorithm: - Initialize the particle’s position \\(\\mathbf{r}(0)\\) and direction \\(\\hat{\\mathbf{n}}(0)\\). - For each time step \\(\\Delta t\\): - With probability \\(\\lambda \\Delta t\\), perform a tumble (randomize \\(\\hat{\\mathbf{n}}\\)). - Otherwise, update the position using the run equation. - Repeat for the desired simulation duration.\nHere, \\(\\lambda\\) is the tumble rate, which determines the frequency of reorientation events.\nThe mean squared displacement (MSD) of run-and-tumble particles exhibits a characteristic three-phase behavior:\n\nBallistic regime (short times):\nAt very short times, before the first tumble occurs, particles move in straight lines at constant speed. The MSD grows quadratically with time: \\[\n\\langle \\Delta r^2(t) \\rangle \\sim v_0^2 t^2\n\\]\nDiffusive regime (long times):\nAt times much longer than the average run time (\\(t \\gg 1/\\lambda\\)), the direction of motion has been randomized many times, and the motion becomes diffusive: \\[\n\\langle \\Delta r^2(t) \\rangle \\sim 2 D_{\\text{eff}} t\n\\] where \\(D_{\\text{eff}} = \\frac{v_0^2}{2\\lambda}\\) in 2D.\n\nAt intermediate timescales, the MSD transitions smoothly from ballistic to diffusive behavior. This crossover is a hallmark of persistent random walks like run-and-tumble dynamics.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#coloured-noise",
    "href": "soft-matter/soft-matter_active.html#coloured-noise",
    "title": "19  Active matter",
    "section": "19.3 Coloured noise",
    "text": "19.3 Coloured noise\nColoured noise introduces temporal correlations into the random forces acting on a particle, unlike white noise, which is uncorrelated. This is often modeled using an Ornstein-Uhlenbeck process for the noise term.\nIn the context of active matter, coloured noise can be used to describe the dynamics of active particles, where the noise term \\(\\boldsymbol{\\eta}(t)\\) evolves as: \\[\n\\frac{d\\boldsymbol{\\eta}(t)}{dt} = -\\frac{\\boldsymbol{\\eta}(t)}{\\tau_c} + \\sqrt{\\frac{2D_c}{\\tau_c}} \\boldsymbol{\\xi}(t),\n\\]\nwhere:\n\n\\(\\tau_c\\) is the correlation time of the noise,\n\\(D_c\\) is the noise strength,\n\\(\\boldsymbol{\\xi}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nThe particle’s velocity \\(\\mathbf{v}(t)\\) is then given by: \\[\n\\mathbf{v}(t) = v_0 \\hat{\\mathbf{n}}(t) + \\boldsymbol{\\eta}(t),\n\\] where \\(\\hat{\\mathbf{n}}(t)\\) is the direction of self-propulsion.\nThe coloured noise description of active matter is a generalisation of the run and tumble dynamics. The run phase corresponds to the persistence of \\(\\hat{\\mathbf{n}}(t)\\) over time, governed by the correlation time \\(\\tau_c\\), whereas the tumble phase is analogous to a rapid decorrelation of \\(\\hat{\\mathbf{n}}(t)\\), which can be modeled by resetting \\(\\boldsymbol{\\eta}(t)\\) or introducing a large noise term.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#active-brownian-particle-and-motility-induced-phase-separation",
    "href": "soft-matter/soft-matter_active.html#active-brownian-particle-and-motility-induced-phase-separation",
    "title": "19  Active matter",
    "section": "19.4 Active Brownian particle and motility-induced phase separation",
    "text": "19.4 Active Brownian particle and motility-induced phase separation\nActive Brownian particles (ABPs) are a minimal model for self-propelled colloids, such as Janus particles, which move due to chemical reactions at their surfaces. For example, a colloid half-coated with platinum can catalyze the decomposition of hydrogen peroxide in solution, generating local gradients that propel the particle forward.\nThe ABP model captures the essential physics of these systems. Each particle moves with a constant speed in a direction that undergoes rotational diffusion. This leads to persistent motion at short times and diffusive behavior at long times, similar to the run-and-tumble model but with continuous reorientation.\nThe dynamics of an ABP can be described by the following equations:\n\nTranslational Motion: The position \\(\\mathbf{r}(t)\\) of the particle evolves as: \\[\n\\frac{d\\mathbf{r}(t)}{dt} = v_0 \\hat{\\mathbf{n}}(t) + \\sqrt{2D_t} \\boldsymbol{\\xi}(t),\n\\] where:\n\n\\(v_0\\) is the self-propulsion speed,\n\\(\\hat{\\mathbf{n}}(t)\\) is the unit vector indicating the particle’s orientation,\n\\(D_t\\) is the translational diffusion coefficient,\n\\(\\boldsymbol{\\xi}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nRotational Motion: The orientation \\(\\hat{\\mathbf{n}}(t)\\) undergoes rotational diffusion, described by: \\[\n\\frac{d\\hat{\\mathbf{n}}(t)}{dt} = \\sqrt{2D_r} \\boldsymbol{\\eta}(t),\n\\] where:\n\n\\(D_r\\) is the rotational diffusion coefficient,\n\\(\\boldsymbol{\\eta}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nIn 2D, the orientation \\(\\hat{\\mathbf{n}}(t)\\) can be expressed in terms of an angle \\(\\theta(t)\\): \\[\n\\hat{\\mathbf{n}}(t) = (\\cos\\theta(t), \\sin\\theta(t)),\n\\] and the rotational dynamics reduce to: \\[\n\\frac{d\\theta(t)}{dt} = \\sqrt{2D_r} \\eta_\\theta(t),\n\\] where \\(\\eta_\\theta(t)\\) is a scalar Gaussian white noise term.\n\nWe can analyse this a little more in detail by considering the characteristic features of the displacements of (dilute or noninteracting) active Brownian particles.\n\n19.4.1 Mean squared displacement of Active Brownian Particles\nIndeed, it can be shown (see below) that the mean squared displacement for active Brownian particles displays three regimes: \\[\n\\operatorname{MSD}(\\tau) = \\left[4 D_T + 2 v^2 \\tau_R \\right] \\tau + 2 v^2 \\tau_R^2 \\left( e^{-\\tau / \\tau_R} - 1 \\right)\n\\]\nwhere: - \\(D_{\\mathrm{eff}} = \\frac{v_0^2}{2 D_r}\\) is the effective long-time diffusion coefficient,\nThis clearly shows that there are three main regimes:\n\na very short time diffusive regime \\(MSD(\\tau) = \\propto 4D_T\\tau\\)\nan intermediate regime around the reorientation time \\(\\tau_R\\): \\[\n  \\operatorname{MSD}(\\tau) = 4 D_{\\mathrm{T}} \\tau + 2 v^2 \\tau^2\n  \\] leading to super-diffusive (ballistic) motion\na final regime where the reorientation has taken place and where a new diffusive regime is reached where the MSD is proportional to time, i.e. \\(\\operatorname{MSD}(\\tau)=\\left[4 D_{\\mathrm{T}}+2 v^2 \\tau_{\\mathrm{R}}\\right] \\tau\\) but with a new diffusion constant.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe derive the MSD expression for 2d Active Brownian particles.\nWe first integrate the velocity\n\\[\n\\mathbf{r}(\\tau) - \\mathbf{r}(0) = v \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds + \\sqrt{2 D_T} \\int_0^\\tau \\boldsymbol{\\eta}(s) ds.\n\\]\nSince \\(\\boldsymbol{\\eta}(t)\\) is independent from \\(\\hat{\\mathbf{n}}(t)\\),\n\\[\n\\langle \\Delta r^2(\\tau) \\rangle = v^2 \\left\\langle \\left| \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds \\right|^2 \\right\\rangle + 4 D_T \\tau,\n\\]\nusing \\(\\langle |\\int \\boldsymbol{\\eta}(s) ds|^2 \\rangle = 2 d D_T \\tau\\) with dimension \\(d=2\\).\nWe first need to evaluate the orientation correlation integral.\nWrite the first term as a double integral:\n\\[\n\\left\\langle \\left| \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds \\right|^2 \\right\\rangle = \\int_0^\\tau ds \\int_0^\\tau ds' \\langle \\hat{\\mathbf{n}}(s) \\cdot \\hat{\\mathbf{n}}(s') \\rangle.\n\\]\nFor 2D rotational diffusion, the orientation correlation is exponential:\n\\[\n\\langle \\hat{\\mathbf{n}}(s) \\cdot \\hat{\\mathbf{n}}(s') \\rangle = e^{-|s - s'|/\\tau_R}.\n\\]\nWe calculate the following double integral\n\\[\nI(\\tau) = \\int_0^\\tau ds \\int_0^\\tau ds' e^{-|s - s'|/\\tau_R}.\n\\]\nUse symmetry: the integrand depends only on \\(|s - s'|\\). Express as\n\\[\nI(\\tau) = 2 \\int_0^\\tau ds \\int_0^s ds' e^{-(s - s')/\\tau_R} = 2 \\int_0^\\tau ds \\int_0^s du\\, e^{-u/\\tau_R},\n\\]\nwhere we set \\(u = s - s'\\).\nIntegrate over \\(u\\):\n\\[\n\\int_0^s e^{-u/\\tau_R} du = \\tau_R \\left(1 - e^{-s/\\tau_R}\\right).\n\\]\nSo\n\\[\nI(\\tau) = 2 \\tau_R \\int_0^\\tau \\left(1 - e^{-s/\\tau_R}\\right) ds = 2 \\tau_R \\left[ \\tau - \\int_0^\\tau e^{-s/\\tau_R} ds \\right].\n\\]\nBy integrating the exponential we get\n\\[\n\\int_0^\\tau e^{-s/\\tau_R} ds = \\tau_R \\left(1 - e^{-\\tau/\\tau_R}\\right).\n\\]\nHence\n\\[\nI(\\tau) = 2 \\tau_R \\left[ \\tau - \\tau_R \\left(1 - e^{-\\tau/\\tau_R} \\right) \\right] = 2 \\tau_R \\tau - 2 \\tau_R^2 \\left(1 - e^{-\\tau/\\tau_R} \\right).\n\\]\nPutting it all together,\n\\[\n\\langle \\Delta r^2(\\tau) \\rangle = v^2 I(\\tau) + 4 D_T \\tau = v^2 \\left[ 2 \\tau_R \\tau - 2 \\tau_R^2 (1 - e^{-\\tau/\\tau_R}) \\right] + 4 D_T \\tau,\n\\]\nor equivalently\n\\[\n\\boxed{\n\\langle \\Delta r^2(\\tau) \\rangle = \\left(4 D_T + 2 v^2 \\tau_R \\right) \\tau + 2 v^2 \\tau_R^2 \\left( e^{-\\tau/\\tau_R} - 1 \\right).\n}\n\\]\n\n\n\n\n\\(\\tau_p = 1/D_r\\) is the persistence time.\n\nShort times (\\(t \\ll \\tau_p\\)):\nThe motion is ballistic: \\[\n\\langle \\Delta r^2(t) \\rangle \\approx v_0^2 t^2\n\\]\nLong times (\\(t \\gg \\tau_p\\)):\nThe motion is diffusive: \\[\n\\langle \\Delta r^2(t) \\rangle \\approx 4 D_{\\mathrm{eff}} t\n\\]\nThis crossover from ballistic to diffusive behavior is a hallmark of persistent random walks such as ABPs.\n\n\n\n\n\n\n\n\n19.4.2 Interacting ABPs and motility induced phase separation\nWhen the active Brownian particles are actually interacting with each other, they can give rise to an exceptional nonequilibrium phenomenon of self organisation called motility-induced phase separation (MIPS).\n\n\n\n\n\n\nFigure 19.3: Motility induced phase separation in a 3D system of active Brownian particles. As the persistence of the motion increases (i.e. the rotational diffusion decreases), the system phase separates into a dense and a dilute phase, akin to a liquid-gas phase separation in equilibrium systems. Adapted from Turci and Wilding PRL 2021\n\n\n\nThe idea is to consider purely repulsive particles (i.e hard spheres) obeying the ABP dynamics. This means that, in equilibrium (i.e. in the absence of self-propulsion) no liquid-gas phase separation is possible (see Section 14.3.1).\nHowever, as we reduce the rotational diffusion, the persistent motion becomes more important, the system becomes more out of equilibrium and new physics comes to play.\nIn particular, the collision between particles are no longer leading to quick decorrelation: on the contrary, head-to-head collisions between the particles mean that there is a finite residence time for a pair of particles to stay in each other neighborhoods. This effect is amplified by multiple collisions, leading to many-body caging effects that promote density heterogeneities in the fluid.\nThe result is striking: for sufficiently low rotational diffusions, the fluid of active Brownian particles spontaneously phase separates into a dilute and a dense phase, akin to the gas and liquid phase of equilibrium systems. Before reaching this regime, the fluid also displays a critical like phenomenology, with enhanced fluctuations terminating a critical point.\nThis has been examined in detail in computer simulations, both in two dimensional systems and in three dimensional systems (where the physics is even richer and presents parallels with the situation of colloid-polymer mixtures due to the very short range nature of the effective interactions between active particles).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurner, Linda, William S Ryu, and Howard C Berg. 2000. “Real-Time Imaging of Fluorescent Flagellar Filaments.” Journal of Bacteriology 182 (10): 2793–2801.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#footnotes",
    "href": "soft-matter/soft-matter_active.html#footnotes",
    "title": "19  Active matter",
    "section": "",
    "text": "Andrea Cavagna, a complex system physicist, describes “biological surprise” as the extent to which observed behaviour cannot be explained by physical interactions alone.↩︎",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/slides.html",
    "href": "soft-matter/slides.html",
    "title": "Complex Disordered Systems: Slides",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#introduction",
    "href": "soft-matter/slides.html#introduction",
    "title": "Complex Disordered Systems: Slides",
    "section": "",
    "text": "Introduction to Disordered Systems [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#colloids",
    "href": "soft-matter/slides.html#colloids",
    "title": "Complex Disordered Systems: Slides",
    "section": "Colloids",
    "text": "Colloids\n\nInteractions [PDF] [HTML]\nPhase behaviour [PDF] [HTML]\nStatic and dynamic correlations [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#polymers",
    "href": "soft-matter/slides.html#polymers",
    "title": "Complex Disordered Systems: Slides",
    "section": "Polymers",
    "text": "Polymers\n\nDefinitions and models [PDF] [HTML]\nPolymers and solvents [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#liquid-crystals",
    "href": "soft-matter/slides.html#liquid-crystals",
    "title": "Complex Disordered Systems: Slides",
    "section": "Liquid Crystals",
    "text": "Liquid Crystals\n\nAnisotropy and orientational order [PDF] [HTML]\nMaier-Saupe Theory, Lebwohl-Lasher Model, Frank Elasticity, Topological Defects [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#surfactants",
    "href": "soft-matter/slides.html#surfactants",
    "title": "Complex Disordered Systems: Slides",
    "section": "Surfactants",
    "text": "Surfactants\n\nAggregation and micelles [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/slides.html#glasses",
    "href": "soft-matter/slides.html#glasses",
    "title": "Complex Disordered Systems: Slides",
    "section": "Glasses",
    "text": "Glasses\n\nEquilibration and glassiness [PDF] [HTML]",
    "crumbs": [
      "Complex disordered systems",
      "Lecture slides"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html",
    "href": "soft-matter/soft-matter_problems.html",
    "title": "Complex disordered systems: Problems",
    "section": "",
    "text": "Colloids",
    "crumbs": [
      "Complex disordered systems",
      "Problems"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#colloids",
    "href": "soft-matter/soft-matter_problems.html#colloids",
    "title": "Complex disordered systems: Problems",
    "section": "",
    "text": "Interaction of two charged colloidal spheres\n\nAdapted from J Dhont, K, Kang (1996) An Introduction to Dynamics of Colloids. Elsevier.\n\nConsider a colloidal particle of total charge \\(Q\\) located at the origin, immersed in a solvent of permittivity \\(\\varepsilon = \\varepsilon_0 \\varepsilon_r\\). The solvent contains a symmetric electrolyte with two mobile ionic species:\n\nCations of charge \\(+ze\\) and bulk concentration \\(n_0\\),\nAnions of charge \\(-ze\\) and bulk concentration \\(n_0\\).\n\n\nShow that The mean electrostatic potential \\(\\phi(\\mathbf{r})\\) satisfies Poisson’s equation: \\[\n\\nabla^2 \\phi(\\mathbf{r}) = -\\frac{1}{\\varepsilon} \\left[ z e\\, n_+(\\mathbf{r}) - z e\\, n_-(\\mathbf{r}) + Q \\delta(\\mathbf{r}) \\right].\n\\]\nAssuming the ions form an ideal gas in thermal equilibrium, their local densities follow Boltzmann distributions: \\[\nn_+(\\mathbf{r}) = n_0 \\exp[-\\beta z e\\, \\phi(\\mathbf{r})], \\qquad\nn_-(\\mathbf{r}) = n_0 \\exp[+\\beta z e\\, \\phi(\\mathbf{r})], \\] where \\(\\beta = 1/(k_B T)\\).\nUse these expressions to obtain the (simplified) nonlinear Poisson–Boltzmann equation : \\[\n\\nabla^2 \\phi(\\mathbf{r}) = \\frac{2 z e n_0}{\\varepsilon} \\sinh\\big(\\beta z e\\, \\phi(\\mathbf{r})\\big) - \\frac{Q}{\\varepsilon}\\delta(\\mathbf{r}).\n\\]\nFind physical conditions to linearise the equation.\nSolve the resulting linear equation for spherical symmetry to obtain the screened Coulomb (Debye–Hückel) potential: \\[\n\\phi(r) = \\frac{Q}{4\\pi \\varepsilon} \\frac{e^{-\\kappa r}}{r},\n\\qquad \\kappa^2 = \\frac{2 n_0 z^2 e^2}{\\varepsilon k_B T}.\n\\]\n\nThe parameter \\(\\kappa^{-1}\\) is the Debye screening length, and the solution shows that ionic screening converts the long-range Coulomb potential into a short-range Yukawa potential.\nThe following real to Fourier spcace mappings may be useful\n\n\n\nReal space\nFourier space\n\n\n\n\n\\(f(\\mathbf{r})\\)\n\\(f(\\mathbf{k})\\)\n\n\n\\(\\nabla^2 f(\\mathbf{r})\\)\n\\(-k^2 f(\\mathbf{k})\\)\n\n\n\\(\\delta(\\mathbf{r})\\)\n\\(1\\)\n\n\n\\(\\dfrac{e^{-\\kappa r}}{4\\pi r}\\)\n\\(\\dfrac{1}{k^2 + \\kappa^2}\\)\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe start by considering a single colloidal particle of total charge \\(Q\\) located at the origin, immersed in a solvent characterized by a permittivity \\(\\varepsilon = \\varepsilon_0 \\varepsilon_r\\). The solvent contains a symmetric electrolyte composed of two mobile ionic species: cations of charge \\(+ze\\) and bulk concentration \\(n_0\\), and anions of charge \\(-ze\\) with the same bulk concentration \\(n_0\\). The overall system is globally electroneutral in the bulk, so the net ionic charge far from the colloid is zero. The presence of the charged colloid generates an electrostatic potential \\(\\phi(\\mathbf{r})\\) in the surrounding medium.\nThe mean electrostatic potential satisfies Poisson’s equation, which relates the divergence of the electric field to the local charge density. In this system, the total charge density consists of the fixed colloidal charge, represented by \\(Q \\delta(\\mathbf{r})\\), and the contribution from the mobile ions, which depends on their local concentrations. This gives \\[\n\\nabla^2 \\phi(\\mathbf{r}) = -\\frac{1}{\\varepsilon} \\left[ z e\\, n_+(\\mathbf{r}) - z e\\, n_-(\\mathbf{r}) + Q \\delta(\\mathbf{r}) \\right].\n\\]\nTo relate the ionic densities to the electrostatic potential, we assume the ions behave as an ideal gas in thermal equilibrium. Each ion experiences a potential energy proportional to its charge times the local potential, \\(U_\\pm(\\mathbf{r}) = \\pm z e \\phi(\\mathbf{r})\\). In equilibrium, the number density of each species follows the Boltzmann distribution, so the cation and anion densities are \\[\nn_+(\\mathbf{r}) = n_0 \\exp[-\\beta z e\\, \\phi(\\mathbf{r})], \\qquad n_-(\\mathbf{r}) = n_0 \\exp[+\\beta z e\\, \\phi(\\mathbf{r})],\n\\] where \\(\\beta = 1/(k_B T)\\) is the inverse thermal energy. This captures the intuitive fact that cations are repelled by positive potentials and attracted to negative potentials, while anions behave oppositely.\nSubstituting these expressions for \\(n_+\\) and \\(n_-\\) into Poisson’s equation yields \\[\n\\nabla^2 \\phi(\\mathbf{r}) = -\\frac{1}{\\varepsilon} \\Big[ z e n_0 e^{-\\beta z e \\phi} - z e n_0 e^{\\beta z e \\phi} + Q \\delta(\\mathbf{r}) \\Big].\n\\] The difference of exponentials can be written as twice a hyperbolic sine, leading to the nonlinear Poisson–Boltzmann equation \\[\n\\nabla^2 \\phi(\\mathbf{r}) = \\frac{2 z e n_0}{\\varepsilon} \\sinh\\big(\\beta z e\\, \\phi(\\mathbf{r})\\big) - \\frac{Q}{\\varepsilon}\\delta(\\mathbf{r}).\n\\] This equation describes how the potential generated by the colloid is screened by the surrounding ions. The nonlinearity arises because the ionic response depends exponentially on the potential itself.\nThe equation can be linearized under the condition that the potential is small compared to the thermal voltage, specifically when \\(|\\beta z e \\phi(\\mathbf{r})| \\ll 1\\). In this limit, the hyperbolic sine can be approximated by its argument\n\\[\\sinh(\\beta z e \\phi) \\approx \\beta z e \\phi.\\]\nThe Poisson–Boltzmann equation then reduces to a linear differential equation\n\\[\n\\nabla^2 \\phi(\\mathbf{r}) - \\kappa^2 \\phi(\\mathbf{r}) = -\\frac{Q}{\\varepsilon} \\delta(\\mathbf{r}),\n\\]\nwhere the inverse Debye length \\(\\kappa\\) is defined by\n\\[\n\\kappa^2 = \\frac{2 n_0 z^2 e^2}{\\varepsilon k_B T}.\n\\]\nThis linearized equation is known as the Debye–Hückel equation and describes the potential around a point charge in a screening medium.\nThis can be solved in Fourier space using the relations:\n\n\n\nReal space\nFourier space\n\n\n\n\n\\(f(\\mathbf{r})\\)\n\\(f(\\mathbf{k})\\)\n\n\n\\(\\nabla^2 f(\\mathbf{r})\\)\n\\(-k^2 f(\\mathbf{k})\\)\n\n\n\\(\\delta(\\mathbf{r})\\)\n\\(1\\)\n\n\n\\(\\dfrac{e^{-\\kappa r}}{4\\pi r}\\)\n\\(\\dfrac{1}{k^2 + \\kappa^2}\\)\n\n\n\nThis gives\n\\[\n\\phi(r) = \\frac{Q}{4\\pi \\varepsilon} \\frac{e^{-\\kappa r}}{r}.\n\\] At short distances (\\(r \\ll \\kappa^{-1}\\)), the potential reduces to the familiar Coulomb form, while at larger distances it decays exponentially due to ionic screening. The characteristic decay length \\(\\kappa^{-1}\\) is the Debye screening length, which increases with decreasing ionic strength and temperature. This solution illustrates how the electrolyte effectively screens the electrostatic interactions between colloidal particles, converting the long-range Coulomb potential into a short-range interaction that governs the stability and dynamics of colloidal suspensions.\nNote. In the linearised regime, the Poisson-Boltzmann equation reads\n\\[\n\\nabla^2 \\phi-\\kappa^2 \\phi=-\\frac{Q}{\\varepsilon} \\delta(\\mathbf{r})\n\\]\nA similar equation exists in field theory for a massive scalar field \\(\\Phi(\\mathbf{r})\\): it is the Klein-Gordon equation in Euclidean space\n\\[\n\\left(-\\nabla^2+m^2\\right) \\Phi(\\mathbf{r})=J(\\mathbf{r})\n\\]\nIt is easy to identify the field the mass and the source with the respective static potential field, the Debye screening parameter and the point charge. The Yukawa potential is then simply the propagator (the Green’s function).\n\n\n\n\n\nSecond virial coefficient for depletion interactions\nConsider a dilute suspension of spherical colloidal particles of radius \\(R_c\\) in a solution of small ideal polymer coils of radius \\(R_p \\ll R_c\\). The polymers are non-adsorbing and behave as an ideal gas outside the colloids. The colloids interact via the Asakura–Oosawa depletion potential \\(U_{\\rm AO}(r)\\):\n\\[\nU_{\\rm AO}(r) =\n\\begin{cases}\n\\infty, & r &lt; 2 R_c \\\\[2mm]\n-\\Pi\\, V_{\\rm overlap}(r), & 2 R_c \\le r \\le 2 R_c + 2 R_p \\\\[1mm]\n0, & r &gt; 2 R_c + 2 R_p\n\\end{cases}\n\\]\nwhere \\(r\\) is the center-to-center distance, \\(\\Pi = n_p k_B T\\) is the so called osmotic pressure of the polymer solution, and \\(V_{\\rm overlap}(r)\\) is the overlap volume of the depletion zones, given by\n\\[\nV_{\\text {overlap }}(r)=\\frac{\\pi\\left(4 R_{\\mathrm{eff}}+r\\right)\\left(2 R_{\\mathrm{eff}}-r\\right)^2}{12},\n\\]\nwith \\(R_{eff} = R_c+R_p\\).\n\nWrite the expression for the second virial coefficient \\(B_2\\) in terms of the interaction potential:\n\\[\nB_2 = -2 \\pi \\int_0^\\infty \\left( e^{-\\beta U_{\\rm AO}(r)} - 1 \\right) r^2 dr\n\\]\nAssuming the polymer-induced attraction is weak (\\(\\beta |U_{\\rm AO}(r)| \\ll 1\\)) and that the polymers are much smaller than the colloids, expand the exponential to first order and simplify the integral.\nExpress the resulting \\(B_2\\) in terms of \\(R_c\\), \\(R_p\\), and the polymer osmotic pressure \\(\\Pi\\).\nDiscuss qualitatively how \\(B_2\\) depends on polymer concentration and size, and explain what a negative \\(B_2\\) implies for colloidal aggregation.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe second virial coefficient \\(B_2\\) measures the effective two-body contribution to the pressure of a dilute colloidal suspension, following the virial expansion. It is defined for a pairwise potential \\(U(r)\\) as:\n\\[\nB_2 = -2\\pi \\int_0^\\infty \\left( e^{-\\beta U(r)} - 1 \\right) r^2 dr.\n\\]\nHere, \\(U(r) = U_{\\mathrm{ AO}}(r)\\) is the Asakura–Oosawa depletion potential.\n\nRewrite \\(B_2\\)\nThe AO potential has three regions:\n\nHard-core region (\\(r &lt; 2 R_c\\)), where \\(U = \\infty \\rightarrow e^{-\\beta U} = 0\\). The integrand becomes \\(-1\\), giving contribution:\n\n\\[\nB_2^{\\mathrm HC} = -2\\pi \\int_0^{2R_c} (-1) r^2 dr = 2\\pi \\int_0^{2R_c} r^2 dr = \\frac{16\\pi}{3} R_c^3,\n\\]\nwhich is just the hard-sphere contribution.\n\nDepletion region (\\(2R_c \\le r \\le 2R_c + 2 R_p\\)), where \\(U(r) = -\\Pi V_{\\mathrm overlap}(r)\\).\nNo interaction (\\(r &gt; 2R_c + 2 R_p\\)), where \\(U = 0 \\rightarrow e^{-\\beta U} - 1 = 0\\).\n\n\nThe overlap is written as (lecture notes) \\[\nV_{\\text {overlap }}(r)=\\frac{4 \\pi}{3} R_d^3\\left[1-\\frac{3}{4} \\frac{r}{R_d}+\\frac{1}{16}\\left(\\frac{r}{R_d}\\right)^3\\right], \\quad r \\leq R_d=2R_c+2R_p\n\\]\nand can be shown to be equivalent to\n\\[\nV_{\\text {overlap }}(r)=\\frac{\\pi\\left(4 R_{\\mathrm{eff}}+r\\right)\\left(2 R_{\\mathrm{eff}}-r\\right)^2}{12}\n\\]\nSo the integral reduces to contributions from \\(r &lt; 2R_c\\) (hard core) and \\(2R_c \\le r \\le 2R_c + 2R_p\\) (depletion).\n\nLinearize the exponential. For weak polymer-induced attraction, \\(\\beta |U| = \\beta \\Pi V_{\\mathrm overlap} \\ll 1\\), we can expand:\n\n\\[\ne^{-\\beta U(r)} \\approx 1 - \\beta U(r) = 1 + \\beta \\Pi V_{\\mathrm overlap}(r).\n\\]\nThen the integrand in the depletion region becomes:\n\\[\n-(e^{-\\beta U} - 1) \\approx -\\beta \\Pi V_{\\mathrm overlap}(r).\n\\]\nSo the contribution to \\(B_2\\) from the depletion region is:\n\\[\nB_2^{\\mathrm dep} = -2\\pi \\int_{2R_c}^{2R_c + 2R_p} \\big( e^{-\\beta U} - 1 \\big) r^2 dr \\approx -2\\pi \\beta \\Pi \\int_{2R_c}^{2R_c + 2R_p} V_{\\mathrm overlap}(r) r^2 dr.\n\\]\n\nExpress \\(B_2\\) in terms of \\(R_c\\), \\(R_p\\), \\(\\Pi\\)\n\nCombining the hard-core and depletion contributions, the second virial coefficient is\n\\[\nB_2 = \\frac{16\\pi}{3} R_c^3 - 2\\pi \\beta \\Pi \\int_{2R_c}^{2R_c + 2R_p} V_{\\mathrm{overlap}}(r)\\, r^2 dr\n\\]\nWe then calculate the second integral we just need to choose how to represent the overlap. We use the second expression above and leverage the fact that \\(R_p \\ll R_c\\) and that then the integral runs over \\(r=2R_C+x\\) with \\(x\\in[0,2R_p]\\), so with \\(x\\) clearly subleading\nSo, then\n\\[\n4R_{\\mathrm eff}+r = 6 R_c+4 R_p+x\n\\]\nand\n\\[\n2R_{\\mathrm eff}-r = 2R_c+2R_p-2R_c-x= 2R_p-x\n\\]\nSo together\n\\[\nV_{\\mathrm overlap}(x) = \\dfrac{\\pi(6R_c+4R_p+x)(2R_p-x)^2}{12}\n\\]\ndropping subleading contributions leads to \\[\nV_{\\mathrm overlap}(x) \\approx \\dfrac{6\\pi R_c(2R_p-x)^2}{12}\n\\]\nThen, we rewrite\n\\[\\int_{2R_c}^{2R_c + 2R_p} V_{\\mathrm{overlap}}(r)\\, r^2 dr\\]\nas \\[\\int_{0}^{2R_p} V_{\\mathrm{overlap}}(x)\\, (2R_c+x)^2  dx\\]\nand approximate as \\[ \\approx (2 R_c)^2 \\int_{0}^{2R_p } V_{\\mathrm{overlap}}(x)dx\\]\nand then\n\\[\nB_2\\approx  \\frac{16\\pi}{3} R_c^3  -2 \\pi \\beta \\Pi\\left(2 R_c\\right)^2 \\int_0^{2  R_p} \\frac{\\pi}{2} R_c\\left(2 R_p-x\\right)^2 d x\n\\]\nSo we finally integrate\n\\[\n\\int_0^{2 R_p}\\left(2 R_p-x\\right)^2 d x\n\\]\nvia a change of variable \\(y=2 R_p-x\\)\n\\[\n\\int_0^{2 R_p}\\left(2 R_p-x\\right)^2 d x=\\int_{2 R_p}^0 y^2(-d y)=\\int_0^{2 R_p} y^2 d y=\\frac{\\left(2 R_p\\right)^3}{3}=\\frac{8 R_p^3}{3}\n\\]\nwhich leads to\n\\[\nB_2 \\approx\\frac{16\\pi}{3} R_c^3 -\\frac{32 \\pi^2}{3} \\beta \\Pi R_c^3 R_p^3\n\\]\nwhere \\(R_c\\) is the colloid radius, \\(R_p\\) is the polymer radius, \\(\\Pi\\) is the osmotic pressure of the polymer solution, and \\(V_{\\mathrm{overlap}}(r)\\) is the overlap volume between two colloids at separation \\(r\\). This formula shows how \\(B_2\\) depends explicitly on \\(R_c\\), \\(R_p\\), and \\(\\Pi\\).\n\nPhysical interpretation\n\n\n\\(B_2\\) decreases with increasing polymer concentration (via \\(\\Pi\\)) and polymer size (\\(R_p\\)).\nWhen the depletion attraction is strong enough, \\(B_2\\) can become negative, which signals that attractive interactions dominate over repulsion.\nNegative \\(B_2\\) implies a tendency for colloidal aggregation or even phase separation, because the effective interaction favuors close approach of colloids.\n\n\n\n\n\n\nColloids and sedimentation\nA particle suspended in a liquid can be defined as colloidal if the kinetic energy of its thermally-driven Brownian motion is large enough to prevent it from settling completely to the bottom of the container under the influence of gravity.\nA second definition of a colloid is a particle whose gravitational Peclet number Pe is significantly smaller than 1. (The gravitational Peclet number is the ratio of the time taken by a particle to diffuse a distance equal to its radius to the time taken by it to sediment the same distance under gravity.)\nShow that these two definitions of a colloid are essentially equivalent.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo show the equivalence between these two definitions, we need to establish the relationship between thermal energy dominating over gravitational settling and the gravitational Peclet number being much less than 1.\nFor the first definiton, a spherical particle of radius \\(a\\) and density \\(\\rho_p\\) in a fluid of density \\(\\rho_f\\), the gravitational force is: \\[F_g = \\frac{4\\pi a^3}{3}(\\rho_p - \\rho_f)g\\]\nThe gravitational potential energy difference over a distance equal to the particle radius is: \\[U_g = F_g \\cdot a = \\frac{4\\pi a^4}{3}(\\rho_p - \\rho_f)g\\]\nFor thermal motion to dominate, we require: \\[k_B T \\gg U_g = \\frac{4\\pi a^4}{3}(\\rho_p - \\rho_f)g\\]\nFor th second definition, the gravitational Peclet number is defined as: \\[\\text{Pe} = \\frac{t_{\\text{diffusion}}}{t_{\\text{sedimentation}}}\\]\nFor diffusion over distance \\(a\\): \\[t_{\\text{diffusion}} = \\frac{a^2}{D}\\] where \\(D\\) is the diffusion coefficient given by the Einstein relation: \\(D = \\frac{k_B T}{6\\pi\\eta a}\\).\nFor sedimentation over distance \\(a\\) under Stokes drag: \\[t_{\\text{sedimentation}} = \\frac{a}{v_s}\\] where the sedimentation velocity is: \\(v_s = \\frac{F_g}{6\\pi\\eta a} = \\frac{2a^2(\\rho_p - \\rho_f)g}{9\\eta}\\).\nTherefore: \\[\\text{Pe} = \\frac{a^2/D}{a/v_s} = \\frac{a v_s}{D} = \\frac{a \\cdot \\frac{2a^2(\\rho_p - \\rho_f)g}{9\\eta}}{\\frac{k_B T}{6\\pi\\eta a}}\\]\nSimplifying: \\[\\text{Pe} = \\frac{2a^3(\\rho_p - \\rho_f)g \\cdot 6\\pi\\eta a}{9\\eta \\cdot k_B T} = \\frac{4\\pi a^4(\\rho_p - \\rho_f)g}{3k_B T}\\]\nFrom the first definition, colloidal behavior requires: \\[k_B T \\gg \\frac{4\\pi a^4}{3}(\\rho_p - \\rho_f)g\\]\nThis is equivalent to: \\[\\frac{4\\pi a^4(\\rho_p - \\rho_f)g}{3k_B T} \\ll 1\\]\nBut this is exactly the condition \\(\\text{Pe} \\ll 1\\) from the second definition.\nTherefore, the two definitions are mathematically equivalent: thermal energy dominance over gravitational settling occurs precisely when the gravitational Peclet number is much smaller than unity. Both conditions ensure that Brownian motion prevents complete sedimentation, which is the hallmark of colloidal behavior.\n\n\n\n\n\nHard Rods and Cavity Function\nConsider a 1-dimensional (1D) system of Hard Rods of length \\(\\sigma\\). The number density is \\(\\rho\\), and the equation of state is exactly known as (Tonks equation): \\(Z_{\\rm comp} = \\frac{1}{1 - \\rho \\sigma}\\).\nThe potential is: \\[\nu(x) =\n\\begin{cases}\n\\infty, & x &lt; \\sigma \\\\\n0, & x \\ge \\sigma\n\\end{cases}\n\\]\nThe pair distribution function at contact, \\(g(\\sigma^+)\\), is known to be directly related to the equation of state by the 1D virial theorem (see lecture notes): \\[\nZ = 1 + \\rho \\sigma g(\\sigma^+)\n\\]\n\nCalculate \\(g(\\sigma^+)\\) for the 1D hard rod system using the Tonks equation of state and the 1D virial theorem.\nDetermine the value of the Boltzmann factor, \\(e^{-\\beta u(\\sigma)}\\), at contact.\nDetermine the value of the cavity Function, \\(y(\\sigma)= e^{\\beta U(r)}g(r)\\), at contact.\nExplain the physical meaning of \\(y(\\sigma)\\). How does it differ from \\(g(\\sigma^+)\\) in its interpretation?\nDerive the full \\(g(r)\\) by maximising the (Shannon) entropy\\[\nS[p]=-\\int_0^{\\infty} p(l) \\ln p(l) d l\n\\] for the probability to find a gap of length \\(l\\). Assume that the gaps are independent for simplicity and see that the \\(g(r)\\) you obtain matches the previous assumptions.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nCalculate \\(g(\\sigma^+)\\)\n\nWe use the Tonks equation and the virial theorem: \\[\nZ_{\\mathrm comp} = \\frac{1}{1 - \\rho \\sigma}\n\\]\n\\[\nZ_{\\mathrm comp} = 1 + \\rho \\sigma g(\\sigma^+)\n\\] Setting the two expressions for \\(Z\\) equal: \\[\n1 + \\rho \\sigma g(\\sigma^+) = \\frac{1}{1 - \\rho \\sigma}\n\\] Isolating \\(g(\\sigma^+)\\): \\[\n\\rho \\sigma g(\\sigma^+) = \\frac{1}{1 - \\rho \\sigma} - 1\n\\] Combine the right side using a common denominator: \\[\n\\rho \\sigma g(\\sigma^+) = \\frac{1 - (1 - \\rho \\sigma)}{1 - \\rho \\sigma} = \\frac{\\rho \\sigma}{1 - \\rho \\sigma}\n\\] Dividing by \\(\\rho \\sigma\\): \\[\ng(\\sigma^+) = \\frac{1}{1 - \\rho \\sigma}\n\\] This is the exact, analytic expression for the contact value for 1D hard rods.\n\nDetermine \\(e^{-\\beta u(\\sigma)}\\)\n\nThe Boltzmann factor \\(e^{-\\beta u(x)}\\) at contact \\(x = \\sigma\\) is determined by the potential \\(u(x)\\): \\[\ne^{-\\beta u(\\sigma)} = e^{-\\beta (0)} = 1\n\\]\nSince the potential is zero for \\(x \\ge \\sigma\\), the particles are not interacting via the direct core repulsion precisely at \\(\\sigma^+\\).\n\nDetermine \\(y(\\sigma)\\)\n\nThe cavity function \\(y(x)\\) is related to \\(g(x)\\) and the Boltzmann factor: \\[\ng(x) = e^{-\\beta u(x)} y(x)\n\\] At contact, \\(x = \\sigma\\): \\[\ng(\\sigma^+) = e^{-\\beta u(\\sigma)} y(\\sigma)\n\\] Substituting the results from (1) and (2): \\[\n\\frac{1}{1 - \\rho \\sigma} = (1) \\cdot y(\\sigma)\n\\] Therefore, the cavity function at contact is: \\[\ny(\\sigma) = \\frac{1}{1 - \\rho \\sigma}\n\\] In this 1D case, \\(g(\\sigma^+)\\) and \\(y(\\sigma)\\) are equal.\n\nExplain the meaning of \\(y(\\sigma)\\)\n\nLet’s think of the direct correlation function\nIt is useful to think about the Pecus Yevick apprximation for the direct correlation function\n\\[c(r) = (1-e^{+\\beta u(r)}) (h(r)+1)\\]\n(see lecture notes).\nThis can be rewritten as\n\\[c(r) = (1-e^{+\\beta u(r)}) g(r) = (1-e^{+\\beta u(r)}) y(r)e^{-\\beta u(r)}\\]\nand then\n\\[c(r) = (e-{\\beta u(r)}-1) y(r)= f(r) y(r)\\]\nwhere we used the Mayer f-function. Both \\(f(r)\\) and \\(y(r)\\) are continuous and differentiable, making solving the Ornstein Zernicke problem a tractable problem. In particular, \\(y(r)\\) allows us to remove the discontinuities cause by the potential at short distances and focus on the correlations produced outside of the core.\nThe \\(g(r)\\) is the total probability of finding a particle exactly one rod length (\\(\\sigma\\)) away from another particle, given the direct physical repulsion (\\(\\sigma^-\\) region) and the indirect correlations (the rest of the fluid). It is an experimental, measurable quantity directly linked to pressure via the virial theorem.\nThe cavity function \\(y(r)\\) is the probability of finding a particle at distance \\(\\sigma\\) if the direct, instantaneous repulsive interaction \\(u(x)\\) were suddenly removed (i.e., if \\(u(\\sigma)\\) was momentarily zero). It represents the correlation due only to the structure induced by the surrounding fluid.\nWe can reread the PY appoximation in this light\n\\[c(r) = g_{\\mathrm total}- g_{\\mathrm indirect}\\]\nWe assume that there is a potential of mean force \\(w(r)\\) generating the correlations, such that\n\\[g(r) = g_{\\mathrm total} (r)= e^{-beta w(r)}\\]\nand then we decide to approximate the indirect but by removing the interparticle potential from the potential of mean force\n\\[ g_{\\mathrm indirect} = e^{-\\beta (w(r)=u(r))}\\]\nThen the PY approximation with the \\(c(r) =f(r) y(r)\\) comes about.\n\nDerivation \n\nHard rods cannot overlap, so: \\[\n   g(r)=0\\quad(r&lt;\\sigma).\n\\]\nLet the free gap between rods be\n\\[l = r-\\sigma \\ge 0\\].\nWe use the maximum entropy principle to find \\(p(l)\\).\nWe want to maximise\n\\[\nS[p]=-\\int_0^{\\infty} p(l) \\ln p(l) d l\n\\]\nsubject to\n\\[\n\\int_0^{\\infty} p(l) d l=1, \\quad \\int_0^{\\infty} l p(l) d l=\\lambda\n\\]\nwhere the mean free gap is\n\\[\\lambda=\\frac{1-\\rho\\sigma}{\\rho}\\]\nVia Lagrange multipliers\n\\[\n\\delta\\left[-\\int p \\ln p d l+\\alpha\\left(\\int p d l-1\\right)+\\gamma\\left(\\int l p d l-\\lambda\\right)\\right]=0\n\\]\nand then\n\\[\np(l)=\\frac{1}{\\lambda} e^{-l/\\lambda}\n\\]\nand \\[\n\\gamma=-\\frac{1}{\\lambda}, \\quad \\alpha=1-\\ln \\lambda\n\\]\nWe then convert from gap probability to pair correlation.\nThe pair correlation is the probability to find the particle as some position \\(r\\) scaled by the density: \\[\ng(r)=\\frac{p(r-\\sigma)}{\\rho}.\n\\]\nSubstitute \\(p(l)\\) and \\(\\lambda\\):\n\\[\ng(r)=\\frac{1}{(1-\\rho\\sigma)}\\exp\\left[-\\frac{\\rho(r-\\sigma)}{1-\\rho\\sigma}\\right].\n\\]\nChecks: \\[\ng(\\sigma^+) = \\frac{1}{1-\\rho\\sigma}.\n\\]\nMatches the 1D hard-rod equation of state: \\[\nP = \\frac{\\rho k_BT}{1-\\rho\\sigma}.\n\\]",
    "crumbs": [
      "Complex disordered systems",
      "Problems"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#polymers",
    "href": "soft-matter/soft-matter_problems.html#polymers",
    "title": "Complex disordered systems: Problems",
    "section": "Polymers",
    "text": "Polymers\n\nPolymer in theta solvent\nConsider a polymeric molecule initially a solvent at temperature \\(T_1=\\Theta\\) (i.e. at the so-called theta temperature of the solvent). It is composed of \\(N=10^5\\) identical monomers of size \\(b=0.2 \\text{nm}\\).\n\nEstimate its root-mean-square end-to-end distance.\nProvide a second estimate at a second temperature \\(T_2\\gg \\Theta\\)\n\n\n\nFlory exponent for a polymer globule\nConsider a single flexible polymer of \\(N\\) monomers, each of size \\(b\\), in a poor solvent (i.e., \\(T &lt; \\Theta\\)), so that the polymer collapses into a dense globule.\n\nThe entropic contribution for the chain can be approximated (see notes) with\n\n\\[\nS = \\frac{-3 k_B R^2}{2 N b^2}\n\\]\nwhere \\(R\\) is the radius of the globule (essentially proportional to the root means quared distance of the end to ened vector).\nTry to figure out why this leads to what is called as the “elastic” free energy of the chain.\n\nThe globule is dense. Write the interaction free energy using the virial expansion, noting that \\(\\rho=N/R^3\\) stopping at order three.\n\nWe state that the second virial drives the collapse (attraction) while the third virial is repulsive and prevents complete collapse.\n\nMinimise the total free energy to obtain a function \\(f(R)=0\\). Show that if we assume that \\(R\\) grows with N more slowly than in the theta conditions, the entropic part of the resulting function of R can beneglected.\nProceed with the approximation and extract the Flory exponent \\(\\nu\\) for the globule, defined by \\(R \\sim N^\\nu\\): how is it different for other solvent conditions?",
    "crumbs": [
      "Complex disordered systems",
      "Problems"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#liquid-crystals",
    "href": "soft-matter/soft-matter_problems.html#liquid-crystals",
    "title": "Complex disordered systems: Problems",
    "section": "Liquid crystals",
    "text": "Liquid crystals\n\nLinearising Maier-Saupe\nIn the Maier-Saupe mean-field theory of nematic liquid crystals, the orientation distribution function of rod-like molecules is given by\n\\[\np(\\theta)=\\frac{1}{Z}\\exp\\big(\\lambda P_2(\\cos\\theta)\\big),\\qquad\nZ=\\int_0^\\pi \\exp\\big(\\lambda P_2(\\cos\\theta)\\big)\\sin\\theta\\,d\\theta,\n\\] where \\(\\lambda=uS/(k_BT)\\) and \\[\nS=\\langle P_2(\\cos\\theta)\\rangle\n=\\frac{\\int_0^\\pi P_2(\\cos\\theta)p(\\theta)\\sin\\theta\\,d\\theta}{\\int_0^\\pi p(\\theta)\\sin\\theta\\,d\\theta}.\n\\]\nThis leads to the implicit (“self-consistent”) relation \\[\nS=\\frac{\\int_0^\\pi P_2(\\cos\\theta)e^{\\lambda P_2(\\cos\\theta)}\\sin\\theta\\,d\\theta}{\\int_0^\\pi e^{\\lambda P_2(\\cos\\theta)}\\sin\\theta\\,d\\theta}.\n\\]\nYou may use \\[\n\\int_0^\\pi P_2(\\cos\\theta)\\sin\\theta\\,d\\theta=0,\\qquad\n\\int_0^\\pi [P_2(\\cos\\theta)]^2\\sin\\theta\\,d\\theta=\\frac{2}{5} .\n\\]\n\nExplain why the above equation is implicit and why it means to linearise it near the isotropic phase.\nAssuming \\(\\lambda\\) is small, expand \\[\ne^{\\lambda P_2(\\cos\\theta)}\n\\] to first order in \\(\\lambda\\).\nUsing your expansion derive the linearised relation between \\(S\\) and \\(\\lambda\\).\nUsing \\(\\lambda=uS/(k_BT)\\), determine the value of \\(k_BT/u\\) at which a non-zero solution for \\(S\\) first appears.",
    "crumbs": [
      "Complex disordered systems",
      "Problems"
    ]
  }
]