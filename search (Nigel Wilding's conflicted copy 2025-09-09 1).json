[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "",
    "text": "Welcome to the Complex Disordered Matter Course!",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Overview",
    "text": "Overview\nThis course introduces your to the theoretical, computational and experimental aspects of the physics of complex disordered matter.\n\nComplex disordered matter is the study of wide range of systems like polymers, colloids, glasses, gels, and emulsions, which lack long-range order but exhibit intricate behaviour. Colloids, suspensions of microscopic particles in a fluid, are useful for studying disordered structures due to their observable dynamics. Similarly, polymer systems can form amorphous solids or glasses when densely packed or cooled, showing solid-like rigidity despite their disordered structure. These materials often undergo phase transitions, such as demixing and crystallisation, and near these transitions, they can display critical phenomena with extensive fluctuations and correlations.\nThese various are examples of soft matter. systems In soft matter systems, the interplay between disorder, softness, and phase behavior leads to rich physical phenomena, particularly near critical points where even small changes in external conditions can trigger large-scale reorganizations and universal behaviour. Glasses, for instance, exhibit slow relaxation and memory effects, while colloidal systems may crystallize, phase separate, or become jammed depending on particle interactions and concentration. Understanding such behaviors involves studying how microscopic interactions and thermal fluctuations influence macroscopic properties, especially in non-equilibrium conditions. Through techniques like scattering, microscopy, rheology, and simulation, one can explore how disordered soft materials respond to stress, age, or undergo transitions—insights that are vital for applications in materials design, biotechnology, and beyond.\nThis course is organized into three interconnected parts, each offering a distinct perspective on the study of complex disordered matter.\n\nPart 1: Unifying concepts (Nigel Wilding) introduces the theoretical framework for rationalising complex disordered matter which is grounded in statistical mechanics and thermodynamics. We emphasize the theory of phase transitions, thermal fluctuations, critical phenomena, and stochastic dynamics—providing the essential theoretical tools needed to describe and predict the behavior of soft and disordered systems.\n\nPart 2: Complex disordered matter (Francesco Turci) explores the phenomenology of key examples of complex disordered soft matter systems, including colloids, polymers, liquid crystals, glasses, gels, and active matter. These systems will be analyzed using the theoretical concepts introduced in Part 1, highlighting how disorder, interactions, and fluctuations shape their macroscopic behavior.\n\nPart 3: Experimental techniques (Adrian Barnes) focuses on the methods of microscopy, and scattering via x-rays, neutrons and light that are used to study complex disordered matter, offering insight into how their properties are measured and understood in real-world contexts.\n\nIn addition to theory and experiment, computer simulation plays a central role in soft matter research. This course includes a substantial coursework component consisting of two computational projects. These exercises will allow you to apply state-of-the-art simulation techniques to investigate the complex behavior of disordered systems, bridging theory and observation through hands-on exploration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#delivery-and-format",
    "href": "index.html#delivery-and-format",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Delivery and format",
    "text": "Delivery and format\n\nDetailed e-notes (accessible via Blackboard) can be viewed on a variety of devices. Pdf is also available.\nWe will give ‘traditional’ lectures (Tuesdays, Wednesdays, Fridays) in which we use slides to summarise and explain the lecture content. Questions are welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to the explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are problem classes (Thursdays) where you can try problem sheets and seek help. Lecturers may go over some problems with the class.\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#intended-learning-outcomes",
    "href": "index.html#intended-learning-outcomes",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Intended learning outcomes",
    "text": "Intended learning outcomes\nThe course will\n\nIntroduce you to the qualitative features of a range of complex and disordered systems and the experimental techniques used to study them.\nIntroduce you to a range of model systems and theoretical techniques used to elucidate the physics of complex disordered matter.\nProvide you with elementary computational tools to model complex disordered systems numerically and predict their properties.\nAllow you to apply your physics background to understand a variety of systems of inter-disciplinary relevance.\nConnect with the most recent advances in the research on complex disordered matter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contact-details",
    "href": "index.html#contact-details",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Contact details",
    "text": "Contact details\nThe course will be taught by\n\nProf Nigel B. Wilding (unit director): nigel.wilding@bristol.ac.uk\nDr Francesco Turci: F.Turci@bristol.ac.uk\nDr Adrian Barnes: a.c.barnes@bristol.ac.uk",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#questions-and-comments",
    "href": "index.html#questions-and-comments",
    "title": "Welcome to the Complex Disordered Matter Course!",
    "section": "Questions and comments",
    "text": "Questions and comments\nIf you have any questions about the course, please don’t hesitate to contact the relevant lecturer, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025/26. If you find any errors or mistakes or something which isn’t clear, please let us know by email, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "phase-transitions/literature.html",
    "href": "phase-transitions/literature.html",
    "title": "Literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course course is the absence of a wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking. To this end perhaps the most appropriate textbooks are:\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\nFor revision on thermodynamics and statistical mechanics\n\nF. Mandl: Statistical Physics.\n\nFor Stochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry",
    "crumbs": [
      "Unifying concepts",
      "Recommended texts"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html",
    "href": "phase-transitions/precourse-reading.html",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Ensembles and free energies\nComplex disordered systems are composed of an enormous number of interacting components—typically on the order of \\(\\sim 10^{23}\\). These interactions can lead to fascinating emergent behaviour, but they also render the systems analytically intractable; it is clearly impossible to solve Newton’s equations for such vast numbers of particles. To address this difficulty, we turn to Statistical Mechanics, which you first encountered in your second year. Statistical Mechanics provides the essential framework for connecting the microscopic behaviour of individual constituents with the macroscopic thermodynamic and dynamical properties of the system as a whole.\nIn this section, we will revisit and expand upon key concepts relevant to our discussion, with particular emphasis on the free energy—a central quantity that captures the balance between energy minimisation and entropy maximisation in determining the system’s equilibrium state. If any of these ideas feel unfamiliar, you may find it useful to revise the Statistical Mechanics material from your Year 2 Thermal Physics course notes.\nStatistical mechanics can be formulated in a variety of ensembles reflecting the relationship between the system and its environment. In what follows we summarise the formalism, focussing on the case of a particle fluid. Analogous equations apply to lattice spin models (see lectures and the book by Yeomans). Key ensembles are:",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "href": "phase-transitions/precourse-reading.html#ensembles-and-free-energies",
    "title": "Tools for understanding complex disordered matter",
    "section": "",
    "text": "Microcanonical ensemble\nApplies to a system of \\(N\\) particles (or spins) in a fixed volume \\(V\\) having adiabatic walls so that the internal energy \\(E\\) is constant. Denoted as constant-\\(NVE\\). Let \\(\\Omega\\) be the number of (micro)states having the prescribed energy:\n\\[\n\\Omega=\\sum_\\textrm{all states having energy E}\n\\]\nThermodynamically, the states favored in the canonical ensemble are those that maximise the entropy:\n\\[\nS=k_B\\ln \\Omega\\: .\n\\]\nwhere \\(k_B\\) is Boltzmann’s constant The microcanonical ensemble is useful for defining the entropy, but is little used in practice.\n\n\nCanonical ensemble\nApplies to a system of \\(N\\) particles in a fixed volume \\(V\\) and coupled to a heat bath at temperature \\(T\\). Denoted as constant-\\(NVT\\). A central quantity is the partition function\n\\[\nZ_{NVT}=\\sum_\\textrm{ all states i}e^{-\\beta E_i},~~~~~\\beta=1/(k_BT)\n\\tag{1}\\] which is a weighted sum over the states. The partition function provides the normalisation constant in the probability of finding the system in a given state \\(i\\).\n\\[\nP_i=\\frac{e^{-\\beta E_i}}{Z_{NVT}}.\n\\tag{2}\\]\nThe states favored in the canonical ensemble are those that minimise the free energy:\n\\[\nF_{NVT}=-\\beta^{-1}\\ln Z_{NVT}\\:.\n\\]\n\\(F_{NVT}\\) is known as the Helmholtz free energy. Thermodynamics also supplies a relation for the Helmholtz free energy:\n\\[\nF_{NVT}=E-TS\\:,\n\\] where \\(E\\) is the average internal energy. In minimising the free energy, the system strikes a compromise between low energy and high entropy. The temperature plays the role of arbiter, favouring high entropy at high \\(T\\), and low energy at low \\(T\\). The canonical ensemble is usually used to describe systems such as magnets, or a fluid held at constant volume. It is the ensemble we shall use most in this course.\n\n\nGrand canonical ensemble\nApplies to a system with a variable number of particle in a fixed volume \\(V\\) coupled to both a heat bath at temperature \\(T\\) and a particle reservoir with chemical potential \\(\\mu\\) (which is the field conjugate to \\(N\\)). Denoted as constant-\\(\\mu VT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty e^{\\beta\\mu N}Z_{NVT}\n\\] and a state probability analogous to Equation 2 holds. One can recast this in a form similar to Equation 1:\n\\[\nZ_{\\mu VT}=\\sum_{N=0}^\\infty\\:\\sum_{\\rm all~states~i}e^{-\\beta {\\cal H}_i},\n\\tag{3}\\] where \\({\\cal H}_i=E_i-\\mu N\\) is the form of the Hamiltonian in the grand canonical ensemble.\nStatistically, the states favored in the grand canonical ensemble are those that minimise the free energy:\n\\[\nF_{\\mu VT}=-\\beta^{-1}\\ln Z_{\\mu VT}\n\\] \\(F_{\\mu VT}\\) is known as the grand potential. It can also be derived from thermodynamics, from which one finds\n\\[\nF_{\\mu VT}=E-TS-\\mu N=-pV,\n\\] where \\(p\\) is the pressure.\nThe grand canonical ensemble is usually used to describe systems such as fluid connected to a particle reservoir. Sometimes for a magnet we consider the effects of an applied magnetic field, which is analogous to working in the grand canonical ensemble: the magnetic field (which is conjugate to the magnetisation) plays a similar role to the chemical potential in a fluid.\n\n\nIsothermal-isobaric ensemble\nApplies to a system with a fixed number of particles \\(N\\) that is coupled to a heat bath at temperature \\(T\\) and a reservoir that exerts a constant pressure \\(p\\) which allows the sample volume to fluctuate. Denoted as constant-\\(NpT\\).\nThe corresponding partition function is a weighted superset of the canonical one\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}\n\\] or \\[\nZ_{NVT}=\\int_0^\\infty dV\\:\\sum_{\\rm i}e^{-\\beta {\\cal H}_i},\n\\tag{4}\\] where \\({\\cal H}_i=E_i+pV\\) is the form of the Hamiltonian in the constant-\\(NpT\\) ensemble. Again a state probability analogous to Equation 2 holds.\nStatistically, the states favored in the costant-NpT ensemble are those that minimise the free energy:\n\\[\nF_{NpT}=-\\beta^{-1}\\ln Z_{NpT}\n\\] \\(F_{NpT}\\) is known as the Gibb’s free energy (often denoted \\(G\\)). It can also be derived from thermodynamics, from which one finds\n\\[\nF_{NpT}=E-TS+pV=\\mu N\n\\]\nThe constant-\\(NpT\\) ensemble is usually used to describe systems such as a fluid subject to a variable pressure, or a magnet coupled to a magnetic field \\(H\\). In the latter case the quantity \\(HM\\) plays the role of \\(pV\\) and\n\\[\nF_{NpT}=E-TS-MH\\:,\n\\] with \\(M\\) the total magnetisation.",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "href": "phase-transitions/precourse-reading.html#from-free-energies-to-observables",
    "title": "Tools for understanding complex disordered matter",
    "section": "From free energies to observables",
    "text": "From free energies to observables\nFree energies are not directly observable quantities. However, all physical observables can be expressed in terms of derivatives of the free energy. One can derive the appropriate relations either from Thermodynamics, or the corresponding statistical mechanics (Revise your year-2 Thermal Physics notes on this if necessary). As an example let us consider a fluid in the isothermal-isobaric ensemble for which the appropriate free energy is \\(F_{NpT}=E-TS+pV\\), and where the volume fluctuates in response to the prescribed pressure. We shall seek an expression for the average volume in terms of the free energy. First lets us take the thermodynamic route. Differentiating the free energy and applying the chain rule we have:\n\\[\ndF=dE-TdS-sdT+pdV+VdP\\:.\n\\] But from the first law of thermodynamics, \\(dE=TdS-pdV\\), so\n\\[\ndF=-SdT+Vdp\\:,\n\\] and rearranging yields \\[\nV=\\left(\\frac{\\partial F}{\\partial p}\\right)_T\\:.\n\\]\nWe can now show that this result is consistent with the definition of \\(F_{NpT}\\) in terms of the partition function. Write\n\\[\nZ_{NpT}=\\int_0^\\infty dV  e^{-\\beta p V}Z_{NVT}=\\int_0^\\infty dV\\sum_{all~states~i}e^{-\\beta (p V_i+E_i)}\n\\]\nThen\n\\[\n\\begin{align}\n\\left(\\frac{\\partial F}{\\partial p}\\right)_T\n&= -\\frac{1}{\\beta} \\left(\\frac{\\partial \\ln Z_{NpT}}{\\partial p}\\right)_T \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\frac{\\partial Z_{NpT}}{\\partial p} \\\\\n&= -\\frac{1}{\\beta} \\frac{1}{Z_{NpT}} \\int_0^\\infty dV \\int_{\\text{all states}} (-\\beta V) e^{-\\beta (p V + E)} \\\\\n&= \\langle V \\rangle_T \\,.\n\\end{align}\n\\]\nwhere in the last step we have used the fact that the probability of a state is defined to be \\(e^{-\\beta (p V_i+E_i)}/Z_{NpT}\\).\nExercise. Repeat these manipulations to find an expression for the mean particle number \\(N\\) in the grand canonical ensemble\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn the grand canonical ensemble (GCE), the relevant free energy is\n\\[\nF_{\\mu VT} = E - TS - \\mu N\n\\]\nFrom the first law of thermodynamics changes in the internal energy are given by:\n\\[\ndE = TdS - PdV + \\mu dN=TdS+\\mu dN\n\\] where we have used the fact that \\(V\\) is fixed in the GCE, so \\(dV=0\\).\nDifferentiating \\(F_{\\mu VT}\\):\n\\[\ndF_{\\mu VT} = dE - TdS - SdT - \\mu dN - N d\\mu\n= -S dT - N d\\mu\n\\] where for the last equality we have substitued for \\(dE\\) from above.\nThus \\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V} = -N\n\\quad \\Rightarrow \\quad\n\\langle N \\rangle = -\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_{T, V}\n\\]\nNow consider the statistical mechanics route to calculate \\(\\langle N\\rangle\\):\n\\[\nZ_{\\mu V T} = \\sum_{N=0}^{\\infty} \\sum_{\\text{states}} e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nThe grand potential (now written as \\(F_{\\mu VT}\\)) is:\n\\[\nF_{\\mu VT} = -k_B T \\ln Z_{\\mu V T}\n\\]\nWe now differentiate:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\left( \\frac{1}{Z} \\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} \\right)\n\\]\nFrom the partition function\n\\[\n\\frac{\\partial Z_{\\mu V T}}{\\partial \\mu} = \\sum_{N=0}^\\infty \\sum_{\\text{states}} \\left( \\beta N \\right) e^{-\\beta (E_{N,i} - \\mu N)}\n\\]\nSubstitute:\n\\[\n\\left( \\frac{\\partial F_{\\mu VT}}{\\partial \\mu} \\right)_T = -k_B T \\cdot \\beta \\cdot \\frac{1}{Z_{\\mu V T}} \\sum_{N=0}^\\infty \\sum_{\\text{states}} N\\, e^{-\\beta (E_{N,i} - \\mu N)} = - \\langle N \\rangle\n\\]\nwhere in the last step we have used the fact that in the GCE the Boltzmann probability of a microstate is defined to be \\(e^{-\\beta (E_{N,i}-\\mu N)}/Z_{\\mu VT}\\).",
    "crumbs": [
      "Unifying concepts",
      "Precourse reading and revision"
    ]
  },
  {
    "objectID": "phase-transitions/introduction.html",
    "href": "phase-transitions/introduction.html",
    "title": "1  Introduction to phase behaviour and enhanced fluctuations",
    "section": "",
    "text": "A phase transition can be defined as a macroscopic rearrangment of the internal constituents of a system in response to a change in the thermodynamic conditions to which they are subject. A wide variety of physical systems undergo such transitions. Understanding the properties of phase transitions is fundamental to the study of soft and complex matter, as these systems often exhibit rich and subtle transformations between different states of organization. Whether in colloidal suspensions, polymer blends, liquid crystals, or biological materials, phase transitions underpin a wide range of physical behaviours, from self-assembly and pattern formation to critical phenomena and dynamical arrest. By analysing how macroscopic phases emerge from microscopic interactions and external conditions, one gains crucial insight into the principles that govern structure, stability, and functionality in these intricate systems. As such, an understanding of phase transitions not only enriches theoretical understanding but also informs practical applications across materials science, biophysics, and nanotechnology. For these reasons we will devote a large proportion of this course to the study of phase transitions.\nTwo classic examples of systems displaying phase transitions are the ferromagnet and fluid systems. For the magnet, a key observable is the magnetisation defined as the magnetic moment per spin, given by \\(m=M/N\\), with \\(N\\) the number of spins. \\(m\\) can be positive or negative, dependent on whether the spins are aligned ‘up’ or ‘down’. As the temperature of a ferromagnet is increased, its net magnetisation \\(|m|\\) is observed to decrease smoothly, until at a certain temperature known as the critical temperature, \\(T_c\\), it vanishes altogether (see left part of Figure 1.1). We define the magnetisation to be the order parameter of this phase transition.\nOne can also envisage applying a magnetic field \\(H\\) to the system which, depending on its sign (i.e. whether it is aligned (positive) or anti-aligned (negative) relative to the magnetisation axis), favours up or down spin states respectively, as shown schematically in Figure 1.1 (right part). Changing the sign of the magnetic field \\(H\\) for \\(T&lt;T_c\\) leads to a phase transition chacterised by a discontinuous jump in \\(m\\). We shall explore this behaviour in more detail in section 6.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of a simple magnet (schematic). Left: magnetisation as a function of temperature for zero applied magnetic field, \\(H=0\\). Right: Applying a magnetic field that is aligned or antialigned with the direction of the magnetisation leads to a phase transition. The \\(H=0\\) axis at \\(T&lt;T_c\\) is the coexistence curve for which positive and negative magnetisations are equally likely.\n\n\n\nSimilarly, a change of state from liquid to gas can be induced in a fluid system (though not in an ideal gas) simply by raising the temperature. Typically the liquid-vapour transition is abrupt, reflecting the large number density difference between the states either side of the transition. However the abruptness of this transition can be reduced by applying pressure. At one particular pressure and temperature the discontinuity in the density difference between the two states vanishes and the two phases coalesce. These conditions of pressure and temperature serve to locate the critical point for the fluid. We define the density difference \\(\\rho_{liq}-\\rho_{vap}\\) to be the order parameter for the liquid-gas phase transition. We shall meet order parameters for other, more complex, systems in Section 5.5,\n\n\n\n\n\n\nFigure 1.2: Phase diagram of a simple fluid (schematic)\n\n\n\nIn the vicinity of a critical point, a system displays a host of remarkable behaviors known as critical phenomena. Chief among these is the divergence of thermal response functions—such as specific heat, compressibility, or magnetic susceptibility—which signal an enhanced sensitivity to external perturbations. These singularities arise from the emergence of large-scale cooperative interactions among the system’s microscopic constituents, as measured by a diverging correlation length (see Chapter 2). One visually striking manifestation of this is critical opalescence, particularly observed in fluids like CO\\(_2\\). As carbon dioxide nears its critical temperature and pressure, the distinction between its liquid and gas phases vanishes, giving rise to huge fluctuations in density. These fluctuations scatter visible light, rendering the fluid milky or opalescent. This scattering effect directly reflects the long-range correlations developing within the fluid. The movie below illustrates the effect as the critical temperature of CO\\(_2\\) is approached from above. Note the appearence of a liquid-vapour interface (meniscus) as the system enters the two-phase region.\n\nThe recalcitrant problem posed by the critical region is how best to incorporate such collective effects within the framework of a rigorous mathematical theory that affords both physical insight and quantitative explanation of the observed phenomena. This matter has been (and still is!) the subject of intense theoretical activity.\nThe importance of the critical point stems largely from the fact that many of the phenomena observed in its vicinity are believed to be common to a whole range of apparently quite disparate physical systems. Systems such as liquid mixtures, superconductors, liquid crystals, ferromagnets, antiferromagnets and molecular crystals may display identical behaviour near criticality. This observation implies a profound underlying similarity among physical systems at criticality, regardless of many aspects of their distinctive microscopic nature. These ideas have found formal expression in the so-called ‘universality hypothesis’ which, since its inception in the 1970s, has enjoyed considerable success.\nIn the next few lectures, principal aspects of the contemporary theoretical viewpoint of phase transitions and critical phenomena will be reviewed. Mean field theories of phase transitions will be discussed and their inadequacies in the critical region will be exposed. The phenomenology of the critical region will we described including power laws, critical exponents and their relationship to scaling phenomena. These will be set within the context of the powerful renormalisation group technique. The notion of universality as a phenomenological hypothesis will be introduced and its implications for real and model systems will be explored. Finally, the utility of finite-size scaling methods for computer studies of critical phenomena will be discussed, culminating in the introduction of a specific technique suitable for exposing universality in model systems. Thereafter we will consider some foundational concepts in the dynamics of complex disorderd matters. We shall look at the processes by which one phase transform into another and introduce differential equations that allow us to deal with the inherent stochasticity of thermal systems. The wider applicability of these unifying concepts to complex disordered systems such as colloids, polymers, liquid crystals and glasses will be covered in part 2 of the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to phase behaviour</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html",
    "href": "phase-transitions/background.html",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "2.1 Observables and expectation values\nIn seeking to describe phase transition and critical phenomena, it is useful to have a quantitative measure of the difference between the phases: this is the role of the order parameter, \\(Q\\). In the case of the fluid, the order parameter is taken as the difference between the densities of the liquid and vapour phases. In the ferromagnet it is taken as the magnetisation. As its name suggest, the order parameter serves as a measure of the kind of orderliness that sets in when the temperature is cooled below a critical temperature.\nOur first task is to give some feeling for the principles which underlie the ordering process. Referring back to ?sec-canonical, the probability \\(p_a\\) that a physical system at temperature \\(T\\) will have a particular microscopic arrangement (alternatively referred to as a ‘configuration’ or ‘state’), labelled \\(a\\), of energy \\(E_a\\) is\n\\[\np_a=\\frac{1}{Z}e^{-E_a/k_BT}\n\\tag{2.1}\\]\nThe prefactor \\(Z^{-1}\\) is the partition function: since the system must always have some specific arrangement, the sum of the probabilities \\(p_a\\) must be unity, implying that\n\\[\nZ=\\sum_ae^{-E_a/k_BT}\n\\tag{2.2}\\] where the sum extends over all possible microscopic arrangements.\nThese equations assume that physical system evolves rapidly (on the timescale of typical observations) amongst all its allowed arrangements, sampling them with the probabilities Equation 2.1 the expectation value of any physical observable \\(O\\) will thus be given by averaging \\(O\\) over all the arrangements \\(a\\), weighting each contribution by the appropriate probability:\n\\[\\overline {O}=\\frac{1}{Z}\\sum_a O_a e^{-E_a/k_BT}\n\\tag{2.3}\\]\nSums like Equation 2.3 are not easily evaluated. Nevertheless, some important insights follow painlessly. Consider the case where the observable of interest is the order parameter, or more specifically the magnetisation of a ferromagnet.\n\\[\nQ=\\frac{1}{Z}\\sum_a Q_a e^{-E_a/k_BT}\n\\tag{2.4}\\]\nIt is clear from Equation 2.1 that at very low temperature the system will be overwhelmingly likely to be found in its minimum energy arrangements (ground states). For the ferromagnet, these are the fully ordered spin arrangements having magnetisation \\(+1\\), or \\(-1\\).\nNow consider the high temperature limit. The enhanced weight that the fully ordered arrangement carries in the sum of Equation 2.4 by virtue of its low energy, is now no longer sufficient to offset the fact that arrangements in which \\(Q_a\\) has some intermediate value, though each carry a smaller weight, are vastly greater in number. A little thought shows that the arrangements which have essentially zero magnetisation (equal populations of up and down spins) are by far the most numerous. At high temperature, these disordered arrangements dominate the sum in Equation 2.4 and the order parameter is zero.\nThe competition between energy-of-arrangements weighting (or simply ‘energy’) and the ‘number of arrangements’ weighting (or ‘entropy’) is then the key principle at work here. The distinctive feature of a system with a critical point is that, in the course of this competition, the system is forced to choose amongst a number of macroscopically different sets of microscopic arrangements.\nFinally in this section, we note that the probabilistic (statistical mechanics) approach to thermal systems outlined above is completely compatible with classical thermodynamics. Specifically, the bridge between the two disciplines is provided by the following equation\n\\[\nF=-k_BT \\ln Z\n\\tag{2.5}\\]\nwhere \\(F\\) is the “Helmholtz free energy”. All thermodynamic observables, for example the order parameter \\(Q\\), and response functions such as the specific heat or magnetic susceptibility are obtainable as appropriate derivatives of the free energy. For instance, utilizing Equation 2.2, one can readily verify (try it as an exercise!) that the average internal energy is given by\n\\[\\overline{E}=-\\frac{\\partial \\ln Z}{\\partial \\beta},\\]\nwhere \\(\\beta=(k_BT)^{-1}\\).\nThe relationship between other thermodynamic quantities and derivatives of the free energy are given in fig. Figure 2.1",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#observables-and-expectation-values",
    "href": "phase-transitions/background.html#observables-and-expectation-values",
    "title": "2  Key concepts for phase transitions",
    "section": "",
    "text": "Figure 2.1: Relationships between the partition function and thermodynamic observables",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/background.html#sec-correlations",
    "href": "phase-transitions/background.html#sec-correlations",
    "title": "2  Key concepts for phase transitions",
    "section": "2.2 Correlations",
    "text": "2.2 Correlations\n\n2.2.1 Spatial correlations\nThe two-point connected correlation function measures how fluctuations at two spatial points are statistically related. For a scalar field \\(\\phi(\\vec{R})\\), which could represent eg. the local magnetisation \\(m\\) in a magnet at position vector \\(\\vec{R}\\), or the local particle number density \\(\\rho\\) in a fluid, it is defined as:\n\\[\nC(r) = \\langle \\phi(\\vec{R}) \\phi(\\vec{R} + \\vec{r}) \\rangle - \\langle \\phi(\\vec{R}) \\rangle^2,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes an ensemble or spatial average over all \\(\\vec{R}\\), and \\(r = |\\vec{r}|\\) is the spatial separation between the two points.\n\\(C(r)\\) quantifies the spatial extent over which field values are correlated and in homogeneous and isotropic systems, it depends only on the separation \\(r\\).\nIf \\(C(r)\\) decays quickly, we say that correlations are short-ranged. Typically this occurs well away from criticality and takes the form of exponential decay\n\\[\n  C(r) \\sim e^{-r/\\xi}\n  \\] where the correlation length \\(\\xi\\) is the characteristic scale over which correlations decay.\nNear a critical point \\(C(r)\\) decays more slowly - in a power-law fashion - and correlations are long-ranged.\n\\[\n  C(r) \\sim r^{-(d - 2 + \\eta)}\n  \\] where \\(d\\) is the spatial dimension and \\(\\eta\\) is a critical exponent.\nIn isotropic fluids and particle systems, a closely related and more directly measurable quantity (particularly in simulations) is the radial distribution function \\(g(r)\\), which describes how particle density varies as a function of distance from a reference particle. For such systems, the two-point correlation function of the number density field \\(\\rho(\\vec{r})\\) is related to \\(g(r)\\) as follows:\n\\[\ng(r) = 1+\\frac{C(r)}{\\rho^2},\n\\] where \\(\\rho\\) is the average number density. This relation shows that \\(g(r)\\) encodes the same spatial correlations as \\(C(r)\\), but in a form that is more natural for discrete particle systems. Note that by definition \\(g(r)\\to 1\\) in the absence of correlations ie. when \\(C(r)=0\\). This is typically the case for \\(r\\gg\\xi\\).\nExperimentally one doesn’t typically have direct access to \\(C(r)\\), but rather its Fourier transform known as the structure factor\n\\[\nS(k) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r),\n\\] where \\(k\\) is the scattering wavevector.\nIn equilibrium:\n\nFor short-range correlations (finite \\(\\xi\\)), \\(S(k)\\) typically has a Lorentzian form: \\[\nS(k) \\sim \\frac{1}{k^2 + \\xi^{-2}}.\n\\]\nAt criticality (where \\(\\xi \\to \\infty\\)), \\(S(k)\\) follows a power law: \\[\nS(k) \\sim k^{-2 + \\eta}.\n\\]\n\nThis relation enables the extraction of \\(\\xi\\) from experimental or simulation data, especially via scattering techniques.\n\n\n2.2.2 Temporal correlations\nConsider a thermodynamic variable \\(x\\) with zero mean that fluctuates over time. Examples include the local magnetization in a magnetic system or the local density in a fluid. Here, \\(x\\) represents a deviation from the average value — a fluctuation.\nWe’re interested in how such fluctuations are correlated over time when the system is in thermal equilibrium. For instance, if \\(x\\) is positive at some time \\(t\\), it’s more likely to remain positive shortly after.\nThese temporal correlations are characterized by the two-time correlation function (also known as an auto-correlation function):\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle\n\\]\nIn equilibrium, the correlation function must be independent of the starting time \\(\\tau\\). Therefore, we define:\n\\[\n\\langle x(\\tau) x(\\tau + t) \\rangle = M_{xx}(t)\n\\]\nThat is, \\(M_{xx}(t)\\) depends only on the time difference \\(t\\).\nWe typically expect \\(M_{xx}(t)\\) to decay exponentially over a characteristic correlation time \\(t_c\\):\n\\[\nM_{xx}(t) \\sim \\exp(-t / t_c)\n\\]\n\n\n\n\n\n\nFigure 2.2: Sketch of \\(M_{xx}(t)\\) against \\(t\\)\n\n\n\nThis exponential decay reflects how the memory of fluctuations fades with time.\nNow consider two different fluctuating variables, \\(x\\) and \\(y\\) (e.g., local magnetizations at different positions). Their cross-correlation function is defined as:\n\\[\n\\langle x(\\tau) y(\\tau + t) \\rangle = M_{xy}(t)\n\\]\nThis defines the elements of a dynamic correlation matrix, of which \\(M_{xx}(t)\\) is the diagonal.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background concepts</span>"
    ]
  },
  {
    "objectID": "phase-transitions/approach-to-criticality.html",
    "href": "phase-transitions/approach-to-criticality.html",
    "title": "3  The approach to criticality",
    "section": "",
    "text": "It is a matter of experimental fact that the approach to criticality in a given system is characterized by the divergence of various thermodynamic observables. Let us remain with the archetypal example of a critical system, the ferromagnet, whose critical temperature will be denoted as \\(T_c\\). For temperatures close to \\(T_c\\), the magnetic response functions (the magnetic susceptibility \\(\\chi\\) and the specific heat) are found to be singular functions, diverging as a power of the reduced (dimensionless) temperature \\(t \\equiv\n(T-T_c)/T_c\\):-\n\\[\n\\chi \\equiv \\frac{\\partial M}{\\partial H}\\propto t^{-\\gamma} ~~~~ (H=0)\n\\tag{3.1}\\]\n(where \\(M=mN\\)), \\[\nC_H \\equiv \\frac{\\partial E}{\\partial T}\\propto t^{-\\alpha} ~~~~ (H=\\textrm{ constant})\n\\tag{3.2}\\]\nAnother key quantity is the correlation length \\(\\xi\\), which measures the distance over which fluctuations of the magnetic moments are correlated. This is observed to diverge near the critical point with an exponent \\(\\nu\\).\n\\[\n\\xi \\propto t^{-\\nu} ~~~~ (T &gt; T_c,\\: H=0)\n\\tag{3.3}\\]\nSimilar power law behaviour is found for the order parameter \\(Q\\) (in this case the magnetisation) which vanishes in a singular fashion (it has infinite gradient) as the critical point is is approached as a function of temperature:\n\\[\nm \\propto t^{\\beta} ~~~~ (T &lt; T_c,\\: H=0)\n\\tag{3.4}\\] (here the symbol \\(\\beta\\), is not to be confused with \\(\\beta=1/k_BT\\)– this unfortunately is the standard notation.)\nFinally, as a function of magnetic field:\n\\[m \\propto h^{1/\\delta} ~~~~ (T = T_c,\\: H&gt;0) . \\tag{3.5}\\] with \\(h=(H-H_c)/H_c\\), the reduced magnetic field.\nAs examples, the behaviour of the magnetisation and correlation length are plotted in Figure 3.1 as a function of \\(t\\).\n\n\n\n\n\n\nFigure 3.1: Singular behaviour of the correlation length and order parameter in the vicinity of the critical point as a function of the reduced temperature \\(t\\).\n\n\n\nThe quantities \\(\\gamma, \\alpha, \\nu, \\beta\\) in the above equations are known as critical exponents. They serve to control the rate at which the various thermodynamic quantities change on the approach to criticality.\nRemarkably, the form of singular behaviour observed at criticality for the example ferromagnet also occurs in qualitatively quite different systems such as the fluid. All that is required to obtain the corresponding power law relationships for the fluid is to substitute the analogous thermodynamic quantities in to the above equations. Accordingly the magnetisation order parameter is replaced by the density difference \\(\\rho_{liq}-\\rho_{gas}\\) while the susceptibility is replaced by the isothermal compressibility and the specific heat capacity at constant field is replaced by the specific heat capacity at constant volume. The approach to criticality in a variety of qualitatively quite different systems can therefore be expressed in terms of a set of critical exponents describing the power law behaviour for that system (see the book by Yeomans for examples).\nEven more remarkable is the experimental observation that the values of the critical exponents for a whole range of fluids and magnets (and indeed many other systems with critical points) are identical. This is the phenomenon of universality. It implies a deep underlying physical similarity between ostensibly disparate critical systems. The principal aim of theories of critical point phenomena is to provide a sound theoretical basis for the existence of power law behaviour, the factors governing the observed values of critical exponents and the universality phenomenon. Ultimately this basis is provided by the Renormalisation Group (RG) theory, for which K.G. Wilson was awarded the Nobel Prize in Physics in 1982.\nMore about the scientists mentioned in this chapter:\nKenneth Wilson",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The approach to criticality</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html",
    "href": "phase-transitions/Ising-model.html",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "4.1 The 2D Ising model\nIn order to probe the properties of the critical region, it is common to appeal to simplified model systems whose behaviour parallels that of real materials. The sophistication of any particular model depends on the properties of the system it is supposed to represent. The simplest model to exhibit critical phenomena is the two-dimensional Ising model of a ferromagnet. Actual physical realizations of 2-d magnetic systems do exist in the form of layered ferromagnets such as K\\(_2\\)CoF\\(_4\\), so the 2-d Ising model is of more than just technical relevance.\nThe 2-d spin-\\(\\frac{1}{2}\\) Ising model envisages a regular arrangement of magnetic moments or ‘spins’ on an infinite plane. Each spin can take two values, \\(+1\\) (‘up’ spins) or \\(-1\\) (‘down’ spins) and is assumed to interact with its nearest neighbours according to the Hamiltonian\n\\[\n{\\cal H}_I=-J\\sum_{&lt;ij&gt;}s_is_j - H\\sum_i s_i\n\\tag{4.1}\\]\nwhere \\(J&gt;0\\) measures the strength of the coupling between spins and the sum extends over nearest neighbour spins \\(s_i\\) and \\(s_j\\), i.e it is a sum of the bonds of the lattice. \\(H\\) is a magnetic field term which can be positive or negative (although for the time being we will set it equal to zero). The order parameter is simply the average magnetisation:\n\\[m=\\frac{1}{N} \\langle \\sum_i s_i \\rangle\\:,\\] where \\(\\langle\\cdot\\rangle\\) means an average over configurations.\nThe fact that the Ising model displays a phase transition was argued in Chapter 2. Thus at low temperatures for which there is little thermal disorder, there is a preponderance of aligned spins and hence a net spontaneous magnetic moment (ie. the system is ferromagnetic). As the temperature is raised, thermal disorder increases until at a certain temperature \\(T_c\\), entropy drives the system through a continuous phase transition to a disordered spin arrangement with zero net magnetisation (ie. the system is paramagnetic). These trends are visible in configurational snapshots from computer simulations of the 2D Ising model (see Figure 4.1). Although each spin interacts only with its nearest neighbours, the phase transition occurs due to cooperative effects among a large number of spins. In the neighbourhood of the transition temperature these cooperative effects engender fluctuations that can extend over all length-scales from the lattice spacing up to the correlation length.\nAn interactive Monte Carlo simulation of the Ising model demonstrates the phenomenology, By altering the temperature you will be able to observe for yourself how the spin arrangements change as one traverses the critical region. Pay particular attention to the configurations near the critical point. They have very interesting properties. We will return to them later!\nAlthough the 2-d Ising model may appear at first sight to be an excessively simplistic portrayal of a real magnetic system, critical point universality implies that many physical observables such as critical exponents are not materially influenced by the actual nature of the microscopic interactions. The Ising model therefore provides a simple, yet quantitatively accurate representation of the critical properties of a whole range of real magnetic (and indeed fluid) systems. This universal feature of the model is largely responsible for its ubiquity in the field of critical phenomena. We shall explore these ideas in more detail later in the course.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "href": "phase-transitions/Ising-model.html#the-2d-ising-model",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "",
    "text": "\\(T=1.2T_c\\)\n\n\n\n\n\n\n\n\\(T=T_c\\)\n\n\n\n\n\n\n\n\\(T=0.95T_c\\)\n\n\n\n\n\n\nFigure 4.1: Configurations of the 2d Ising model. The patterns depict typical arrangements of the spins (white=+1, black=−1) generated in a computer simulation of the Ising model on a square lattice of \\(N=512\\) sites, at temperatures (from left to right) of \\(T= 1.2T_c\\), \\(T=T_c\\), and \\(T=0.95T_c\\). In each case only a portion of the system containing \\(128\\) sites in shown. The typical island size is a measure of the correlation length \\(\\xi\\): the excess of black over white (below \\(T_c\\) is a measure of the order parameter.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "href": "phase-transitions/Ising-model.html#exact-solutions-the-one-dimensional-ising-chain",
    "title": "4  The Ising model: the prototype model for a phase transition",
    "section": "4.2 Exact solutions: the one dimensional Ising chain",
    "text": "4.2 Exact solutions: the one dimensional Ising chain\nOne might well ask why the 2D Ising model is the simplest model to exhibit a phase transition. What about the one-dimensional Ising model (ie. spins on a line)? In fact in one dimension, the Ising model can be solved exactly. It turns out that the system is paramagnetic for all \\(T&gt;0\\), so there is no phase transition at any finite temperature. To see this, consider the ground state of the system in zero external field. This will have all spins aligned the same way (say up), and hence be ferromagnetic. Now consider a configuration with a various “domain walls” dividing spin up and spin down regions:\n\n\n\n\n\n\nFigure 4.2: (a) Schematic of an Ising chain at \\(T=0\\). (b) At a small finite temperature the chain is split into domains of spins ordered in the same direction. Domains are separated by notional domain “walls”, which cost energy \\(\\Delta=2J\\). Periodic boundary conditions are assumed.\n\n\n\nInstead of considering the underlying spin configurations, we shall describe the system in terms of the statistics of its domain walls. The energy cost of a wall is \\(\\Delta = 2J\\), independent of position. Domain walls can occupy the bonds of the lattice, of which there are \\(N-1\\). Moreover, the walls are noninteracting, except that you cannot have two of them on the same bond. (Check through these ideas if you are unsure.)\nIn this representation, the partition function involves a count over all possible domain wall arrangements. Since the domain walls are non interacting (eg it doesn’t cost energy for one to move along the chain) we can calculate \\(Z\\) by considering the partition function associated with a single domain wall being present or absent on some given bond, and then simply raise to the power of the number of bonds:\n\\[Z=Z_1^{N-1}\\]\nwhere\n\\[Z_1=e^{\\beta J} + e^{\\beta (J-\\Delta)}=e^{\\beta J}(1+e^{-\\beta\\Delta})\\] is the domain wall partition function for a single bond and represent the sum over the two possible states: domain wall absent or present. Then the free energy per bond of the system is\n\\[\\beta f\\equiv \\beta F/(N-1)=-\\ln Z_1=-\\beta J-\\ln(1+e^{-\\beta\\Delta})\\]\nThe first term on the RHS is simply the energy per spin of the ferromagnetic (ordered) phase, while the second term arises from the free energy of domain walls. Clearly for any finite temperature (ie. for \\(\\beta&lt;\\infty\\)), this second term is finite and negative. Hence the free energy will always be lowered by having a finite concentration of domain walls in the system. Since these domain walls disorder the system, leading to a zero average magnetisation, the 1D system is paramagnetic for all finite temperatures. Exercise: Explain why this argument works only in 1D.\nThe animation below lets you see qualitatively how the typical number of domain walls varies with temperature. If you’d lke to explore more quantitatively, a python code performing a Monte Carlo simulation is available. You will learn about Monte Carlo simulation in the coursework and in later parts of the course.\n\n\nShow python code\n#Monte Carlo simulation of the 1d Ising chain with periodic bounary conditions\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Slider\n\n# Number of spins\nN = 20 \n\n# Initialize spins (+1 or -1)\nspins = np.random.choice([-1, 1], size=N)\n\n# Initial temperature\nT = 2.0\n\n# Set up figure and axis for the spins\nfig, ax = plt.subplots(figsize=(10, 2))\nplt.subplots_adjust(bottom=0.25)  # make room for slider\nax.set_xlim(-0.5, N - 0.5)\nax.set_ylim(-1, 1)\nax.axis('off')\n\n# Create text objects for each spin\ntexts = []\nfor i in range(N):\n    arrow = '↑' if spins[i] == 1 else '↓'\n    t = ax.text(i, 0, arrow, fontsize=24, ha='center', va='center')\n    texts.append(t)\n\ndef update(frame):\n    \"\"\"Perform Metropolis updates over all spins, then refresh display.\"\"\"\n    global spins, T\n    for _ in range(N):\n        i = np.random.randint(N)\n        left = spins[(i - 1) % N]\n        right = spins[(i + 1) % N]\n        deltaE = 2 * spins[i] * (left + right)\n        # Metropolis criterion ensures configurations appear with the correct Boltzmann probability\n        if deltaE &lt; 0 or np.random.rand() &lt; np.exp(-deltaE / T):\n            spins[i] *= -1\n    # Update arrows on screen\n    for idx, t in enumerate(texts):\n        t.set_text('↑' if spins[idx] == 1 else '↓')\n    return texts\n\n# Create the animation with caching disabled and blit turned off\nani = FuncAnimation(\n    fig,\n    update,\n    interval=200,\n    blit=False,\n    cache_frame_data=False\n)\n\n# Add a temperature slider\nax_T = plt.axes([0.2, 0.1, 0.6, 0.03], facecolor='lightgray')\nslider_T = Slider(ax_T, 'Temperature T', 0.1, 5.0, valinit=T)\n\ndef on_T_change(val):\n    \"\"\"Callback to update T when the slider changes.\"\"\"\n    global T\n    T = val\n\nslider_T.on_changed(on_T_change)\n\n# Show the plot (ani is kept in scope so it won't be deleted)\nplt.show()\n\n\n\n\n\n Temperature T =\n\n2.0\n\n \n\n\n4.2.1 More general 1D spins systems: transfer matrix method\nGenerally speaking one-dimensional systems lend themselves to a degree of analytic tractability not found in most higher dimensional models. Indeed for the case of a 1-d assembly of \\(N\\) spins each having \\(m\\) discrete energy states, and in the presence of a magnetic field, it is possible to reduce the evaluation of the partition function to the calculation of the eigenvalues of a matrix–the so called transfer matrix.\nLet us start by assuming that the assembly has cyclic boundary conditions, then the total energy of configuration \\(\\{s\\}\\) is\n\\[\\begin{aligned}\nH(\\{s\\})=&-\\sum_{i=1}^N (Js_is_{i+1}+Hs_i)\\\\\n\\:=&-\\sum_{i=1}^N (Js_is_{i+1}+H(s_i+s_{i+1})/2)\\\\\n\\:=&\\sum_{i=1}^N E(s_i,s_{i+1})\n\\end{aligned}\\]\nwhere we have defined \\(E(s_i,s_{i+1})=-Js_is_{i+1}-H(s_i+s_{i+1})/2\\).\nNow the partition function may be written\n\\[\\begin{aligned}\nZ_N =& \\sum_{\\{s\\}}\\exp\\left(-\\beta H(\\{s\\})\\right)\\nonumber \\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta[E(s_1,s_2)+E(s_2,s_3)+....E(s_N,s_1)]\\right) \\nonumber\\\\\n=&\\sum_{\\{s\\}}\\exp\\left(-\\beta E(s_1,s_2)\\right)\\exp\\left(-\\beta E(s_2,s_3)\\right)....\\exp\\left(-\\beta E(s_N,s_1)\\right) \\nonumber\\\\\n=&\\sum_{i,j,...,l=1}^m V_{ij}V_{jk}...V_{li}\n\\end{aligned} \\tag{4.2}\\]\nwhere the \\(V_{ij}=\\exp(-\\beta E_{ij})\\) are elements of an \\(m \\times m\\) matrix \\({\\bf V}\\), known as the transfer matrix (\\(i,j,k\\) etc are dummy indices that run over the matrix elements). You should see that the sum over the product of matrix elements picks up all the terms in the partition function and therefore Equation 4.2 is an alternative way of writing the partition function.\nThe reason it is useful to transform to a matrix representation is that it transpires that the sum over the product of matrix elements in Equation 4.2 is simply just the trace of \\({\\bf V}^N\\) (check this yourself for a short periodic chain), given by the sum of its eigenvalues:-\n\\[Z_N=\\lambda_1^N+\\lambda_2^N+...\\lambda_m^N\\] For very large \\(N\\), this expression simplifies further because the largest eigenvalue \\(\\lambda_1\\) dominates the behaviour since \\((\\lambda_2/\\lambda_1)^N\\) vanishes as \\(N\\rightarrow \\infty\\). Consequently in the thermodynamic limit one may put \\(Z_N=\\lambda_1^N\\) and the problem reduces to identifying the largest eigenvalue of the transfer matrix.\nSpecializing to the case of the simple Ising model in the presence of an applied field \\(H\\), the transfer matrix takes the form\n\\[{\\bf V}(H)=\\left(\n\\begin{array}{cc}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{array} \\right)\\]\nThis matrix has two eigenvalues which can be readily calculated in the usual fashion as the roots of the characteristic polynomial \\(|{\\bf V}-\\lambda{\\bf I}|\\). They are\n\\[\\lambda_{\\pm}=e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\\]\nHence the free energy per spin \\(f=-k_BT\\ln \\lambda_+\\) is\n\\[f=-k_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\\]\nThe Ising model in 2D can also be solved exactly, as was done by Lars Onsager in 1940. The solution is extremely complicated and is regarded as one of the pinnacles of statistical mechanics. In 3D no exact solution is known.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Ising model</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html",
    "href": "phase-transitions/mean-field-theory.html",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "",
    "text": "5.1 Mean field solution of the Ising model\nOf the wide variety of models of interest to the critical point theorist, the majority have shown themselves intractable to direct analytic (pen and paper) assault. In a very limited number of instances models have been solved exactly, yielding the phase coexistence parameters, critical exponents and the critical temperature. The 2-d spin-\\(\\frac{1}{2}\\) Ising model is certainly the most celebrated such example, its principal critical exponents are found to be \\(\\beta=\\frac{1}{8}, \\nu=1, \\gamma=\\frac{7}{4}\\). Its critical temperature is \\(-2J/\\ln(\\sqrt{2}-1)\\approx 2.269J\\). Unfortunately such solutions rarely afford deep insight to the general framework of criticality although they do act as an invaluable test-bed for new and existing theories.\nThe inability to solve many models exactly often means that one must resort to approximations. One such approximation scheme is mean field theory.\nLet us look for a mean field expression for the free energy of the Ising model whose Hamiltonian is given in Equation 4.1 . Write\n\\[s_i=\\langle s_i\\rangle+(s_i-\\langle s_i\\rangle)=m+(s_i-m)=m+\\delta s_i\\]\nThen \\[\\begin{aligned}\n{\\cal H}_I=&-J\\sum_{&lt;i,j&gt;}[m+(s_i-m)][m+(s_j-m)]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{&lt;i,j&gt;}[m^2+m(s_i-m)+m(s_j-m)+\\delta s_i\\delta s_j]-H\\sum_i s_i\\nonumber\\\\\n=&-J\\sum_{i}(qms_i-qm^2/2)-H\\sum_i s_i-J\\sum_{&lt;i,j&gt;}\\delta s_i\\delta s_j\n\\end{aligned} \\tag{5.1}\\] where in the last line we have used the fact that when for each site \\(i\\) we perform the sum \\(\\sum_{&lt;i,j&gt;}\\) over bonds of a quantity which is independent of \\(s_j\\), then the result is just the number of bonds per site times that quantity. Since the number of bonds on a lattice of \\(N\\) sites of coordination \\(q\\) is \\(Nq/2\\) (because each bond is shared between two sites), there are therefore \\(q/2\\) bonds per site.\nNow the mean field approximation is to ignore the last term in the last line of Equation 5.1 giving the configurational energy as\n\\[\n{\\cal H}_{mf}=-\\sum_{i}H_{mf}s_i+NqJm^2/2\n\\] with \\(H_{mf}\\equiv qJm+ H\\) the “mean field” seen by spin \\(s_i\\). As all the spins are decoupled (independent) in this approximation we can write down the partition function, which follows by taking the partition function for a single spin (by summing the Boltzmann factor for \\(s_i=\\pm 1\\)) and raising to the power \\(N\\) to find\n\\[\nZ=e^{-\\beta qJm^2N/2}[2\\cosh(\\beta(qJm+H))]^N\n\\]\nThe free energy follows as\n\\[F(m)=NJqm^2/2-Nk_BT\\ln[2\\cosh(\\beta (qJm+H)]\\:.\\]\nand the magnetisation as\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\tanh(\\beta(qJm+H))\n\\]\nTo find \\(m(H,T)\\), we must numerically solve this last equation self consistently.\nNote that we can obtain \\(m\\) in a different way. Consider some arbitary spin, \\(s_i\\) say. Then this spin has an energy \\({\\cal H}_{mf}(s_i)\\). Considering this energy for both cases \\(s_i=\\pm 1\\) and the probability \\(p(s_i)=e^{-\\beta{\\cal H}_{mf}(s_i)}/Z\\) of each, we have that\n\\[\\langle s_i\\rangle=\\sum_{s_i=\\pm 1}s_ip(s_i)\\] but for consistancy, \\(\\langle s_i\\rangle=m\\). Thus\n\\[\n\\begin{aligned}\nm & = \\sum_{s_i=\\pm 1}s_ip(s_i)\\nonumber\\\\\n\\: & = \\frac{e^{\\beta(qJm+H)}-e^{\\beta(qJm+H)}} {e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\\nonumber\\\\\n\\: & = \\tanh(\\beta(qJm+H))\n\\end{aligned} \\tag{5.2}\\] as before.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-breaking",
    "href": "phase-transitions/mean-field-theory.html#sec-breaking",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.2 Spontaneous symmetry breaking",
    "text": "5.2 Spontaneous symmetry breaking\n\n\n\n\n\n\nFigure 5.1: Schematic of the form of the free energy for a critical, subcritical and supercritical temperature\n\n\n\nThis mean field analysis reveals what is happening in the Ising model near the critical temperature \\(T_c\\). Figure 5.1 shows sketches for \\(\\beta F(m)/N\\) as a function of temperature, where for f simplicity we restrict attention to \\(H=0\\). In this case \\(F(m)\\) is symmetric in \\(m\\), Moreover, at high \\(T\\), the entropy dominates and there is a single minimum in \\(F(m)\\) at \\(m=0\\). As \\(T\\) is lowered, there comes a point (\\(T=T_c=qJ/k_B\\)) where the curvature of \\(F(m)\\) at the origin changes sign; precisely at this point\n\\[\\frac{\\partial^2 F}{\\partial m^2}=0.\\] At lower temperature, there are instead two minima at nonzero \\(m=\\pm m^\\star\\), where the equilibrium magnetisation \\(m^\\star\\) is the positive root (calculated explicitly below) of\n\\[m^\\star=\\tanh(\\beta Jqm^\\star)= \\tanh(\\frac{m^\\star T_c}{T})\\] The point \\(m=0\\) which remains a root of this equation, is clearly an unstable point for \\(T&lt;T_c\\) (since \\(F\\) has a maximum there).\nThis is an example of spontaneous symmetry breaking. In the absence of an external field, the Hamiltonian (and therefore the free energy) is symmetric under \\(m\\to -m\\). Accordingly, one might expect the actual state of the system to also show this symmetry. This is true at high temperature, but spontaneously breaks down at low ones. Instead there are a pair of ferromagnetic states (spins mostly up, or spins mostly down) which – by symmetry– have the same free energy, lower than the unmagnetized state.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#phase-diagram",
    "href": "phase-transitions/mean-field-theory.html#phase-diagram",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.3 Phase diagram",
    "text": "5.3 Phase diagram\nThe resulting zero-field magnetisation curve \\(m(T,H=0)\\) looks like Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: Phase diagram of a simple magnet in the \\(m\\)-\\(T\\) plane.\n\n\n\nThis shows the sudden change of behaviour at \\(T_c\\) (phase transition). For \\(T&lt;T_c\\) it is arbitrary which of the two roots \\(\\pm m^\\star\\) is chosen; typically it will be different in different parts of the sample (giving macroscopic “magnetic domains”). But this behaviour with temperature is qualitatively modified by the presence of a field \\(H\\), however small. In that case, there is always a slight magnetization, even far above \\(T_c\\) and the curves becomes smoothed out, as shown. There is no doubt which root will be chosen, and no sudden change of the behaviour (no phase transition). Spontaneous symmetry breaking does not occur, because the symmetry is already broken by \\(H\\). (The curve \\(F(m)\\) is lopsided, rather than symmetrical about \\(m=0\\).)\nOn the other hand, if we sit below \\(T_c\\) in a positive field (say) and gradually reduce \\(H\\) through zero so that it becomes negative, there is a very sudden change of behaviour at \\(h=0\\): the equilibrium state jumps discontinuously from \\(m=m^\\star\\) to \\(m=-m^\\star\\).\n\n\n\n\n\n\nFigure 5.3: Phase diagram of a simple magnet in the \\(H\\)-\\(T\\) plane.\n\n\n\nThis is called a first order phase transition as opposed to the “second order” or continuous transition that occurs at \\(T_c\\) in zero field. The definitions are:\nFirst order transition: magnetisation (or similar order parameter) depends discontinuously on a field variable (such as \\(h\\) or \\(T\\)).\nContinuous transition: Change of functional form, but no discontinuity in \\(m\\); typically, however, \\((\\partial m/\\partial T)_h\\) (or similar) is either discontinuous, or diverges with an integrable singularity.\nIn this terminology, we can say that the phase diagram of the magnet in the \\(H,T\\) plane shows a line of first order phase transitions, terminating at a continuous transition, which is the critical point.\n\n\n\n\n\n\nAside on Quantum Criticality\n\n\n\n\n\nIn some magnetic systems such as \\(CePd_2Si_2\\), one can, by applying pressure or altering the chemical composition, depress the critical temperature all the way to abolute zero! This may seem counterintuitive, after all at \\(T=0\\) one should expect perfect ordering, not the large fluctuations that accompany criticality. It turns out that the source of the fluctuations that drive the system critical is zero point motion associated with the Heisenberg uncertainty principle. Quantum criticality is a matter of ongoing active research, and open questions concern the nature of the phase diagrams and the relationship to superconductivity. Although the subject goes beyond the scope of this course, there is an accessible article here if you want to learn more.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "href": "phase-transitions/mean-field-theory.html#a-closer-look-critical-exponents",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.4 A closer look: critical exponents",
    "text": "5.4 A closer look: critical exponents\nLet us now see how we can calculate critical exponents within the mean field approximation.\n\n5.4.1 Zero H solution and the order parameter exponent\nIn zero field\n\\[m=\\tanh(\\frac{mT_c}{T})\\] where \\(T_c=qJ/k_B\\) is the critical temperature at which \\(m\\) first goes to zero.\nWe look for a solution where \\(m\\) is small (\\(\\ll 1\\)). Expanding the tanh function and replacing \\(\\beta=(k_BT)^{-1}\\) yields\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +O(m^5)\\:.\\] Then \\(m=0\\) is one solution. The other solution is given by\n\\[m^2=3\\left(\\frac{T}{T_c} \\right)^3\\left(\\frac{T_c}{T} -1\\right)\\]\nNow, considering temperatures close to \\(T_c\\) to guarantee small \\(m\\), and employing the reduced temperature \\(t=(T-T_c)/T_c\\), one finds\n\\[m^2\\simeq -3t\\]\nHence\n\\[\\begin{aligned}\nm= 0  &    ~~~\\textrm{for } T&gt;T_c \\:\\:\\:  \\textrm{ since~otherwise~{\\it m}~imaginary}\\nonumber\\\\\nm= \\pm\\sqrt{-3t} & ~~\\textrm{ for}  \\:\\:\\: T&lt;T_c ~~\\textrm{ real}\n\\end{aligned}  \\tag{5.3}\\] This result implies that (within the mean field approximation) the critical exponent \\(\\beta=1/2\\).\n\n\n5.4.2 Finite (but small) field solution: the susceptibility exponent\nIn a finite, but small field we can expand Equation 5.2 thus:\n\\[m=\\frac{mT_c}{T}-\\frac{1}{3}\\left(\\frac{mT_c}{T} \\right)^3 +\\frac{H}{kT}\\]\nConsider now the isothermal susceptibility\n\\[\n\\begin{aligned}\n\\chi  \\equiv & \\left(\\frac{\\partial m}{\\partial H}\\right)_T\\\\\n      =     & \\frac{T_c}{T}\\chi - \\left(\\frac{T_c}{T}\\right)^3 \\chi m^2 + \\frac{1}{k_BT}  \n\\end{aligned}\n\\]\nThen\n\\[\\chi \\left[ 1-\\frac{T_c}{T} +\\left(\\frac{T_c}{T}\\right)^3m^2  \\right]=\\frac{1}{k_BT}\\]\nHence near \\(T_c\\)\n\\[\\chi=\\frac{1}{k_BT_c}\\left(\\frac{1}{t+m^2}\\right)\\]\nThen using the results of Equation 5.3\n\\[\n\\begin{aligned}\n\\chi= (k_BT_ct)^{-1} & \\textrm{ for} ~~~ T&gt; T_c \\\\\n\\chi= (-2k_BT_ct)^{-1} & \\textrm{ for}  ~~~T \\le T_c\n\\end{aligned}\n\\]\nwhere one has to take the non-zero value for \\(m\\) below \\(T_c\\) to ensure +ve \\(\\chi\\), i.e. thermodynamic stability. This result implies that (within the mean field approximation) the critical exponent \\(\\gamma=1\\).\nThe schematic behaviour of the Ising order parameter and susceptibility are shown in Figure 5.5 (a) and (b) respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mean field behaviour of the Ising magnetisation (schematic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Mean field behaviour of the Ising susceptibility (schematic)\n\n\n\n\n\n\n\nFigure 5.4",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "href": "phase-transitions/mean-field-theory.html#sec-landau-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.5 Landau theory",
    "text": "5.5 Landau theory\nLandau theory is a slightly more general type of mean field theory than that discussed in the previous subsection because it is not based on a particular microscopic model. Its starting point is the Helmholtz free energy, which Landau asserted can be written in terms of power series expansion of the order parameter \\(\\phi\\):\n\\[\nF_(\\phi)=\\sum_{i=0}^{\\infty}a_i\\phi^i\n\\] The equilibrium value of \\(\\rho\\) is that which minimises the Landau free energy.\n\n\n\n\n\n\nA note on order parameters\n\n\n\n\n\nWe have already seen examples of these in earlier sections, e.g., for the liquid-gas transition this was \\[\n\\rho_{liq} - \\rho_{gas}: \\quad \\textrm{difference in density of two coexisting phases},\n\\] while for the Ising magnet it is the magnetisation \\(m\\). Both quantities vanish at the critical point. These are examples of scalar order parameters – a single number is required to represent the degree of order (\\(n = 1\\)).\nIn the absence of a symmetry-breaking field, the Landau free-energy density \\(f_L\\) must have symmetry \\(f_L(-\\phi) = f_L(\\phi)\\) (Ising case).\nFor some other systems, \\(n\\) component vectors are required in order to represent the order:\n\\[\n\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, \\dots, \\phi_n)\n\\]\nThen \\(f_L(\\boldsymbol{\\phi})\\) should be symmetric under \\(O(n)\\) rotations in \\(n\\)-component \\(\\phi\\)-space.\nThe table below lists examples of order parameters for various physical systems.\n\n\n\n\n\n\n\n\nPhysical System\nOrder Parameter \\(\\varphi\\)\nSymmetry Group\n\n\n\n\nUniaxial (Ising) ferromagnet\nMagnetisation per spin, \\(m\\)\n\\(O(1)\\)\n\n\nFluid (liquid-gas)\nDensity difference, \\(\\rho - \\rho_c\\)\n\\(O(1)\\)\n\n\nLiquid mixtures\nConcentration difference, \\(c - c_c\\)\n\\(O(1)\\)\n\n\nBinary (AB) alloy (e.g., \\(\\beta\\)-brass)\nConcentration of one of the species, \\(c\\)\n\\(O(1)\\)\n\n\nIsotropic (vector) ferromagnet\n\\(n\\)-component magnetisation, \\(\\mathbf{m} = (m_1, m_2, \\dots, m_n)\\)\n\\(O(n)\\)\n\n\n\n\\(n = 2\\): xy model\n\\(O(2)\\)\n\n\n\n\\(n = 3\\): Heisenberg model\n\\(O(3)\\)\n\n\nSuperfluid He\\(^4\\)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nSuperconductor (s-wave)\nMacroscopic condensate wavefunction, \\(\\Psi\\)\n\\(O(2)\\)\n\n\nNematic liquid crystal\nOrientational order, \\(\\langle P_2(\\cos \\theta)\\rangle\\)\n\n\n\nSmectic A liquid crystal\n1-dimensional periodic density\n\n\n\nCrystal\n3-dimensional periodic density\n\n\n\n\nNotes:\n\nIn superfluid \\(^4He\\) the order parameter is\n\n\\[\n\\Psi = |\\Psi| e^{i\\theta},\n\\]\nthe complex wavefunction of the macroscopic condensate. Both the amplitude \\(|\\Psi|\\) and phase \\(\\theta\\) must be specified, so this corresponds to \\(n = 2\\).\nSuperconductors also correspond to \\(n = 2\\).\n\nIn a nematic liquid crystal, the orientational order parameter is\n\n\\[\n\\langle P_2(\\cos \\theta) \\rangle \\equiv \\frac{1}{2}\\langle 3\\cos^2 \\theta - 1\\rangle,\n\\]\nwhere \\(\\theta\\) is the angle a molecule makes with the average direction of the long axes of the molecules (known as the director \\(\\hat{n}\\)). Rotational symmetry is broken. For the case of an \\(n\\) component vector, the free energy should be a function of:\n\\[\n\\phi^2 \\equiv |\\boldsymbol{\\phi}|^2 = \\phi_1^2 + \\phi_2^2 + \\dots + \\phi_n^2 = \\sum_{i=1}^n \\phi_i^2\n\\] in the absence of a symmetry breaking field. Rotational symmetry is incorporated into the theory.\n\n\n\n\n\n\n\n\n\n\n\n(a) Schematic of the isotropic liquid phase of a system of elongated molcules.\n\n\n\n\n\n\n\n\n\n\n\n(b) Schematic of the nematic liquid phase of a system elongated molcules. This phase has uniaxial ordering.\n\n\n\n\n\n\n\nFigure 5.5: Isotropic and uniaxially ordered (nematic) phases of liquid crystal molecules.\n\n\n\n\n\n\nTo exemplify the approach, let us specialise to the case of a ferromagnet where \\(\\phi=m\\), the magnetisation and write the Landau free energy as\n\\[\nF(m)=F_0+a_2m^2+a_4m^4\n\\tag{5.4}\\]\nHere only the terms compatible with the order parameter symmetry are included in the expansion and we truncate the series at the 4th power because this is all that is necessary to yield the essential phenomenology. On symmetry grounds, the free energy of a ferromagnet should be invariant under a reversal of the sign of the magnetisation. Terms linear and cubic in \\(m\\) are not invariant under \\(m\\to -m\\), and so do not feature.\nOne can understand how the Landau free energy can give rise to a critical point and coexistence values of the magnetisation, by plotting \\(F(m)\\) for various values of \\(a_2\\) with \\(a_4\\) assumed positive (which ensures that the magnetisation remains bounded). This is shown in the following movie:\n\n\nThe situation is qualitatively similar to that discussed in Section 5.2. Thermodynamics tells us that the system adopts the state of lowest free energy. From the movie, we see that for \\(a_2&gt;0\\), the system will have \\(m=0\\), i.e. will be in the disordered (or paramagnetic) phase. For \\(a_2&lt;0\\), the minimum in the free energy occurs at a finite value of \\(m\\), indicating that the ordered (ferromagnetic) phase is the stable one. In fact, the physical (up-down) spin symmetry built into \\(F\\) indicates that there are two equivalent stable states at \\(m=\\pm m^\\star\\). \\(a_2=0\\) corresponds to the critical point which marks the border between the ordered and disordered phases. Note that it is an inflexion point, so has \\(\\frac{d^2F}{dm^2}=0\\).\nClearly \\(a_2\\) controls the deviation from the critical temperature, and accordingly we may write\n\\[a_2=\\tilde{a_2} t\\] where \\(t\\) is the reduced temperature. Thus we see that the trajectory of the minima as a function of \\(a_2&lt;0\\) in the above movie effective traces out the coexistence curve in the \\(m-T\\) plane.\nWe can now attempt to calculate critical exponents. Restricting ourselves first to the magnetisation exponent \\(\\beta\\) defined by \\(m=t^\\beta\\), we first find the equilibrium magnetisation, corresponding to the minimum of the Landau free energy:\n\\[\n\\frac{dF}{dm}=2\\tilde{a_2} tm+4a_4m^3=0\n\\tag{5.5}\\]\nwhich implies\n\\[m\\propto (-t)^{1/2},\\] so \\(\\beta=1/2\\), which is again a mean field result.\nLikewise we can calculate the effect of a small field \\(H\\) if we sit at the critical temperature \\(T_c\\). Since \\(a_2=0\\), we have\n\\[F(m)=F_0+a_4m^4-Hm\\]\n\\[\\frac{\\partial F}{\\partial m}=0 \\Rightarrow m(H,T_c)=\\left(\\frac{H}{4a_4}\\right)^{1/3}\\]\nor\n\\[H \\sim m^\\delta ~~~~~ \\delta=3\\] which defines a second critical exponent.\nNote that at the critical point, a small applied field causes a very big increase in magnetisation; formally, \\((\\partial m/\\partial H)_T\\) is infinite at \\(T=T_c\\).\nA third critical exponent can be defined from the magnetic susceptibility at zero field\n\\[\\chi=\\left(\\frac{\\partial m}{\\partial H}\\right)_{T,V} \\sim |T-T_c|^{-\\gamma}\\]\nExercise: Show that the Landau expansion predicts \\(\\gamma=1\\).\nFinally we define a fourth critical exponent via the variation of the heat capacity (per site or per unit volume) \\(C_H\\), in fixed external field \\(H=0\\):\n\\[C_H \\sim |T-T_c|^{-\\alpha}\\]\nBy convention, \\(\\alpha\\) is defined to be positive for systems where there is a divergence of the heat capacity at the critical point (very often the case). The heat capacity can be calculated from\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\]\nFrom the minimization over \\(m\\) @#eq-minimize one finds (exercise: check this) \\[\n\\begin{aligned}\nF = & 0 ~~~~T&gt;T_c\\nonumber\\\\\nF = & -a_2^2/4a_4 ~~~~ T &lt; T_c\n\\end{aligned}\n\\]\nUsing the fact that \\(a_2\\) varies linearly with \\(T\\), we have\n\\[\n\\begin{aligned}\nC_H =& 0 ~~~~ T\\to T_c^+\\nonumber\\\\\nC_H =& \\frac{T\\tilde a_2^2}{2a_4} ~~~~ T \\to T_c^-\\:,\n\\end{aligned}\n\\]\nwhich is actually a step discontinuity in specific heat. Since for positive \\(\\alpha\\) the heat capacity is divergent, and for negative \\(\\alpha\\) it is continuous, this behaviour formally corresponds to \\(\\alpha=0\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "href": "phase-transitions/mean-field-theory.html#shortcomings-of-mean-field-theory",
    "title": "5  Mean field theory and perturbation schemes",
    "section": "5.6 Shortcomings of mean field theory",
    "text": "5.6 Shortcomings of mean field theory\nWhile mean field theories provide a useful route to understanding qualitatively the phenomenology of phase transitions, in real ferromagnets, as well as in more sophisticated theories, the critical exponents are not the simple fraction and integers found here. This failure of mean field theory to predict the correct exponents is of course traceable to their neglect of correlations. In later sections we shall start to take the first steps to including the effects of long range correlations.\n\n\n\nComparison of true Ising critical exponents with their mean field theory predictions in a number of dimensions.\n\n\n\\(\\:\\)\nMean Field\n\\(d=1\\)\n\\(d=2\\)\n\\(d=3\\)\n\n\nCritical temperature \\(k_BT/qJ\\)\n\\(1\\)\n\\(0\\)\n\\(0.5673\\)\n\\(0.754\\)\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\frac{1}{2}\\)\n-\n\\(\\frac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(1\\)\n\\(\\infty\\)\n\\(\\frac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(\\frac{1}{2}\\)\n\\(\\infty\\)\n\\(1\\)\n\\(0.63\\pm 0.001\\)",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mean field theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html",
    "href": "phase-transitions/scaling.html",
    "title": "7  The Static Scaling Hypothesis",
    "section": "",
    "text": "7.1 Experimental Verification of Scaling\nHistorically, the first step towards properly elucidating near-critical behaviour was taken with the static scaling hypothesis. This is essentially a plausible conjecture concerning the origin of power law behaviour which appears to be consistent with observed phenomena. According to the hypothesis, the basis for power law behaviour (and associated scale invariance or “scaling”) in near-critical systems is expressed in the claim that: in the neighbourhood of a critical point, the basic thermodynamic functions (most notably the free energy) are generalized homogeneous functions of their variables. For such functions one can always deduce a scaling law such that by an appropriate change of scale, the dependence on two variables (e.g. the temperature and applied field) can be reduced to dependence on one new variable. This claim may be warranted by the following general argument.\nA function of two variables \\(g(u,v)\\) is called a generalized homogeneous function if it has the property\n\\[g(\\lambda^au,\\lambda^bv)=\\lambda g(u,v)\\] for all \\(\\lambda\\), where the parameters \\(a\\) and \\(b\\) (known as scaling parameters) are constants. An example of such a function is \\(g(u,v)=u^3+v^2\\) with \\(a=1/3, b=1/2\\).\nNow, the arbitrary scale factor \\(\\lambda\\) can be redefined without loss of generality as \\(\\lambda^a=u^{-1}\\) giving\n\\[g(u,v)=u^{1/a}g(1,\\frac{v}{u^{b/a}})\\] A corresponding relation is obtained by choosing the rescaling to be \\(\\lambda^b=v^{-1}\\).\n\\[\\label{eq:sca2}\ng(u,v)=v^{1/b}g(\\frac{u}{v^{a/b}},1)\\]\nThis equation demonstrates that \\(g(u,v)\\) indeed satisfies a simple power law in \\(\\it{one}\\) variable, subject to the constraint that \\(u/v^{a/b}\\) is a constant. It should be stressed, however, that such a scaling relation specifies neither the function \\(g\\) nor the parameters \\(a\\) and \\(b\\).\nNow, the static scaling hypothesis asserts that in the critical region, the free energy \\(F\\) is a generalized homogeneous function of the (reduced) thermodynamic fields \\(t=(T-T_c)/T_c\\) and \\(h=(H-H_c)\\). Remaining with the example ferromagnet, the following scaling assumption can then be made:\n\\[F(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h) \\:.\n\\label{eq:scagibbs}\\]\nWithout loss of generality, we can set \\(\\lambda^a=t^{-1}\\), implying \\(\\lambda=t^{-1/a}\\) and \\(\\lambda^b=t^{-b/a}\\).\nThen \\[F(t,h)=t^{1/a}F(1,t^{-b/a}h)\\] where our choice of \\(\\lambda\\) ensures that \\(F\\) on the rhs is now a function of a single variable \\(t^{-b/a}h\\).\nNow, as stated in Chapter 2, the free energy provides the route to all thermodynamic functions of interest. An expression for the magnetisation can be obtained simply by taking the field derivative of \\(F\\) (cf. Figure 2.1)\n\\[m(t,h)=-t^{(1-b)/a}m(1,t^{-b/a}h)\n\\tag{7.1}\\]\nIn zero applied field \\(h=0\\), this reduces to\n\\[m(t,0)=(-t)^{(1-b)/a}m(1,0)\\] where the r.h.s. is a power law in \\(t\\). Equation 3.4 then allows identification of the exponent \\(\\beta\\) in terms of the scaling parameters \\(a\\) and \\(b\\).\n\\[\\beta=\\frac{1-b}{a}\\]\nBy taking further appropriate derivatives of the free energy, other relations between scaling parameters and critical exponents may be deduced. Such calculations (Exercise: try to derive them) yield the results \\(\\delta =\nb/(1-b)\\),\\(\\gamma = (2b-1)/a\\), and \\(\\alpha =(2a-1)/a\\) . Relationships between the critical exponents themselves can be obtained trivially by eliminating the scaling parameters from these equations. The principal results (known as “scaling laws”) are:- \\[\n\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\n\\end{aligned}\n\\]\nThus, provided all critical exponents can be expressed in terms of the scaling parameters \\(a\\) and \\(b\\), then only two critical exponents need be specified, for all others to be deduced. Of course these scaling laws are also expected to hold for the appropriate thermodynamic functions of analogous systems such as the liquid-gas critical point.\nThe validity of the scaling hypothesis finds startling verification in experiment. To facilitate contact with experimental data for real systems, consider again Equation 7.1. Eliminating the scaling parameters \\(a\\) and \\(b\\) in favour of the exponents \\(\\beta\\) and \\(\\delta\\) gives\n\\[\n\\frac{m(t,h)}{t^{\\beta}}=m(1,\\frac{h}{t^{\\beta\\delta}})\n\\] where the RHS of this last equation can be regarded as a function of the single scaled variable \\(\\tilde{H} \\equiv t^{-\\beta\\delta} h(t,M)\\).\nFor some particular magnetic system, one can perform an experiment in which one measures \\(m\\) vs \\(h\\) for various fixed temperatures. This allows one to draw a set of isotherms, i.e. \\(m-h\\) curves of constant \\(t\\). These can be used to demonstrate scaling by plotting the data against the scaling variables \\(M=t^{-\\beta}m(t,h)\\) and \\(\\tilde{H}=t^{-\\beta\\delta}h(t,M)\\). Under this scale transformation, it is found that all isotherms (for \\(t\\) close to zero) coincide to within experimental error. Reassuringly, similar results are found using the scaled equation of state of simple fluid systems such as He\\(^3\\) or Xe.\nIn summary, the static scaling hypothesis is remarkably successful in providing a foundation for the observation of power laws and scaling phenomena. However, it furnishes little or no guidance regarding the role of co-operative phenomena at the critical point. In particular it provides no means for calculating the values of the critical exponents appropriate to given model systems.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "href": "phase-transitions/scaling.html#experimental-verification-of-scaling",
    "title": "7  The Static Scaling Hypothesis",
    "section": "",
    "text": "Figure 7.1: Magnetisation of CrBr\\(_3\\) in the critical region plotted in scaled form (see text). From Ho and Lister (1969).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/scaling.html#sec-compsim",
    "href": "phase-transitions/scaling.html#sec-compsim",
    "title": "7  The Static Scaling Hypothesis",
    "section": "7.2 Computer simulation",
    "text": "7.2 Computer simulation\nIn seeking to employ simulation to obtain estimates of bulk critical point properties (such as the location of a critical point and the values of its associated exponents), one is immediately confronted with a difficulty. The problem is that simulations are necessarily restricted to dealing with systems of finite-size and cannot therefore accommodate the truly long ranged fluctuations that characterize the near-critical regime. As a consequence, the critical singularities in \\(C_v\\), order parameter, etc. appear rounded and shifted in a simulation study. Figure 7.2 shows a schematic example for the susceptibility of a magnet.\n\n\n\n\n\n\nFigure 7.2: Schematic of the near-critical temperature dependence of the magnet susceptibility in a finite-sized system.\n\n\n\nThus the position of the peak in a response function (such as \\(C_v\\)) measured for a finite-sized system does not provide an accurate estimate of the critical temperature. Although the degree of rounding and shifting reduces with system size, it is often the case, that computational constraints prevent access to the largest system sizes which would provide accurate estimates of critical parameters. To help deal with this difficulty, finite-size scaling (FSS) methods have been developed to allow extraction of bulk critical properties from simulations of finite size. FSS will be discussed in Section 8.7",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The static scaling hypothesis</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html",
    "href": "phase-transitions/rg.html",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "8.1 The critical point: A many length scale problem\nOwing the the absence of a wholly appropriate textbook for the material covered in this section, I have supplied more detailed notes than used in other parts of the unit.\nThe critical region is characterised by correlated microstructure on \\(\\underline{all}\\) length-scales up to and including the correlation length. Such a profusion of degrees of freedom can only be accurately characterized by a very large number of variables. Mean field theories and approximation schemes fail in the critical region because they at best incorporate interactions among only a few spins, while neglecting correlations over larger distances. Similarly, the scaling hypothesis fails to provide more than a qualitative insight into the nature of criticality because it focuses on only one length-scale, namely the correlation length itself. Evidently a fuller understanding of the critical region may only be attained by taking account of the existence of structure on all length-scales. Such a scheme is provided by the renormalisation group method, which stands today as the cornerstone of the modern theory of critical phenomena.\nA near critical system can be characterized by three important length scales, namely\nThe authentic critical region is defined by a window condition:\n\\[L_\\textrm{ max} \\gg \\xi \\gg L_\\textrm{ min}\\]\nThe physics of this regime is hard to tackle by analytic theory because it is characterized by configurational structure on all scales between \\(L_\\textrm{ min}\\) and \\(\\xi\\) (in fact it turns out that the near critical configurational patterns are fractal-like, cf. ?fig-ssb. Moreover different length scales are correlated with one another, giving rise to a profusion of coupled variables in any theoretical description. The window regime is also not easily accessed by computer simulation because it entails studying very large system sizes \\(L_\\textrm{\nmax}\\), often requiring considerable computing resources.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "href": "phase-transitions/rg.html#the-critical-point-a-many-length-scale-problem",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "",
    "text": "The correlation length, \\(\\xi\\), ie the size of correlated microstructure.\nMinimum length scale \\(L_\\textrm{ min}\\), i.e. the smallest length in the microscopics of the problem, e.g. lattice spacing of a magnet or the particle size in a fluid.\nMacroscopic size \\(L_{max}\\) eg. size of the system.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-rgmethod",
    "href": "phase-transitions/rg.html#sec-rgmethod",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.2 Methodology of the RG",
    "text": "8.2 Methodology of the RG\nThe central idea of the renormalisation group (RG) method is a stepwise elimination of the degrees of freedom of the system on successively larger length-scales. To achieve this one introduces a fourth length scale \\(L\\). In contrast to the other three, which characterize the system itself, \\(L\\) characterises the description of the system. It may be thought of as typifying the size of the smallest resolvable detail in a description of the system’s microstructure.\nConsider the Ising model arrangements displayed in Figure 4.1. These pictures contain all the details of each configuration shown: the resolution length \\(L\\) in this case has its smallest possible value, coinciding with the lattice spacing i.e. \\(L=L_{\\min}\\). In the present context, the most detailed description is not the most useful: the essential signals with which we are concerned are hidden in a noise of relevant detail. A clue to eliminating this noise lies in the nature of the correlation length, i.e. the size of the largest droplets. The explicit form of the small scale microstructure is irrelevant to the behaviour of \\(\\xi\\). The small scale microstructure is the noise. To eliminate it, we simply select a larger value of the resolution length (or ‘coarse-graining’ length) \\(L\\).\nThere are many ways of implementing this coarse-graining procedure. We adopt a simple strategy in which we divide our sample into blocks of side \\(L\\), each of which contains \\(L^d\\) sites, with \\(d\\) the space dimensions . The centres of the blocks define a lattice of points indexed by \\(I=1,2,N/L^d\\). We associate with each block lattice point centre, \\(I\\), a coarse-grained or block variable \\(S_I(L)\\) defined as the spatial average of the local variables it contains:\n\\[\nS_I(L)=L^{-d}\\sum_i^Is_i\n\\tag{8.1}\\] where the sum extends over the \\(L^d\\) sites in the block \\(I\\). The set of coarse grained coordinates \\(\\{S(L)\\}\\) are the basic ingredients of a picture of the system having spatial resolution of order \\(L\\).\nThe coarse graining operation is easily implemented on a computer. In so doing one is faced with the fact that while the underlying Ising spins can only take two possible values, the block variables \\(S_I(L)\\) have \\(L^d+1\\) possible values. Accordingly in displaying the consequences of the blocking procedure, we need a more elaborate colour convention than that used in Figure 4.1. We will associate with each block a shade of grey drawn from a spectrum ranging from black to white.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(aiv)\n\n\n\n\n\n\n\n(biv)\n\n\n\n\n\n\nFigure 8.1: See text for details\n\n\n\nThe results of coarse-graining configurations typical of three different temperatures are shown in Figure 8.1 and Figure 8.2. Two auxiliary operations are implicit in these results. The first operation is a length scaling: the lattice spacing on each blocked lattice has been scaled to the same size as that of the original lattice, making possible the display of correspondingly larger portions of the physical system. The second operation is a variable scaling: loosely speaking, we have adjusted the scale (‘contrast’) of the block variable so as to match the spectrum of block variable values to the spectrum of shades at our disposal.\nConsider first a system marginally above its critical point at a temperature \\(T\\) chosen so that the correlation length \\(\\xi\\) is approximately 6 lattice spacing units. A typical arrangement (without coarse-graining) is shown in Figure 8.1(ai). The succeeding figures, Figure 8.1(aii) and Figure 8.1(aiii), show the result of coarse-graining with block sizes \\(L=4\\) and \\(L=8\\), respectively. A clear trend is apparent. The coarse-graining amplifies the consequences of the small deviation of \\(T\\) from \\(T_c\\). As \\(L\\) is increased, the ratio of the size of the largest configurational features (\\(\\xi\\)) to the size of the smallest (\\(L\\)) is reduced. The ratio \\(\\xi/L\\) provides a natural measure of how ‘critical’ is a configuration. Thus the coarse-graining operation generates a representation of the system that is effectively less critical the larger the coarse-graining length. The limit point of this trend is the effectively fully disordered arrangement shown in Figure 8.1(aiii) and in an alternative form in Figure 8.1(aiv), which shows the limiting distribution of the coarse grained variables, averaged over many realizations of the underlying configurations: the distribution is a Gaussian which is narrow (even more so the larger the \\(L\\) value) and centred on zero. This limit is easily understood. When the system is viewed on a scaled \\(L\\) larger than \\(\\xi\\), the correlated microstructure is no longer explicitly apparent; each coarse-grained variable is essentially independent of the others.\nA similar trend is apparent below the critical point. Figure 8.1(bi) show a typical arrangement at a temperature \\(T&lt;T_c\\) such that again \\(\\xi\\) is approximately \\(6\\) lattice spacings. Coarse-graining with \\(L=4\\) and \\(L=8\\) again generates representations which are effectively less critical (Figure 8.1(bii) and (biii)). This time the coarse-graining smoothes out the microstructure which makes the order incomplete. The limit point of this procedure is a homogeneously ordered arrangement in which the block variables have a random (Gaussian) distribution centred on the order parameter (Figure 8.1(biv)).\nConsider now the situation at the critical point. Figure 8.2(ai) shows a typical arrangement; Figure 8.2(aii) and (aiii) show the results of coarse-graining with \\(L=4\\) and \\(L=8\\) respectively. Since the \\(\\xi\\) is as large as the system itself the coarse graining does not produce less critical representations of the physical system: each of the figures displays structure over all length scales between the lower limit set by \\(L\\) and the upper limit set by the size of the display itself. A limiting trend is nevertheless apparent. Although the \\(L=4\\) pattern is qualitatively quite different from the pattern of the local variables, the \\(L=4\\) and \\(L=8\\) patterns display qualitatively similar features. These similarities are more profound than is immediately apparent. A statistical analysis of the spectrum of \\(L=4\\) configurations (generated as the local variables evolve in time) shows (Figure 8.2(iv)) that it is almost identical to that of the \\(L=8\\) configurations (given the block variable scaling). The implication of this limiting behaviour is clear: the patterns formed by the ordering variable at criticality look the same (in a statistical sense) when viewed on all sufficiently large length scales.\n\n\n\n\n\n\n\n\n(ai)\n\n\n\n\n\n\n\n(bi)\n\n\n\n\n\n\n\n\n\n(aii)\n\n\n\n\n\n\n\n(bii)\n\n\n\n\n\n\n\n\n\n(aiii)\n\n\n\n\n\n\n\n(biii)\n\n\n\n\n\n\n\n\n\n(iv)\n\n\n\n\n\n\nFigure 8.2: See text for details\n\n\n\nLet us summarize. Under the coarse-graining operation there is an evolution or flow of the system’s configuration spectrum. The flow tends to a limit, or fixed point, such that the pattern spectrum does not change under further coarse-graining. These scale-invariant limits have a trivial character for \\(T&gt;T_c\\), (a perfectly disordered arrangement) and \\(T&lt; T_c\\), (a perfectly ordered arrangement). The hallmark of the critical point is the existence of a scale-invariant limit which is neither fully ordered nor fully disordered but which possesses structure on all length scales. A nice illustration of critical point scale invariance in the Ising model can be viewed here.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#universality-and-scaling",
    "href": "phase-transitions/rg.html#universality-and-scaling",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.3 Universality and Scaling",
    "text": "8.3 Universality and Scaling\nEquipped with the coarse-graining technique, we now address the universality phenomenon. We aim to understand how it is that systems that are different microscopically can nevertheless display critical point behaviour which (in certain respects) is quantitatively identical.\nTo obtain initial insight we introduce a spin-1 Ising model in which the spins take on three values (\\(s_i=1,0,-1\\)), in contrast to the two values (\\(s_i=1,-1\\)) of the spin-1/2 Ising model. The two models have properties which are different: for example, \\(T_c\\) for the three-state model is some \\(30\\%\\) lower than that of the two-state model (for the same coupling \\(J\\)). However, there is abundant evidence that the two models have the same universal properties.\nLet us explore what is the same and what is different in the configurations of the two models at criticality. The configurations of the local variables \\(s_i\\) are clearly qualitatively different for the two models. Now consider the coarse-grained configurations (with \\(L=4\\) and \\(L=8\\) respectively) for the three-state model at the critical point. We have already seen that the coarse-graining operation bears the configuration spectrum of the critical two-state Ising model to a non-trivial scale-invariant limit. It is scarcely surprising that the same is true for the three-state model. What is remarkable is that the two limits are the same! This is expressed in Figure 8.2(iv), which shows the near coincidence of the distribution of block variables (grey-levels) for the two different coarse-graining lengths. Thus the coarse-graining erases the physical differences apparent in configurations where the local behaviour is resolvable, and exposes a profound configurational similarity.\n\n8.3.1 Fluid-magnet universality\nLet us now turn to fluid-magnet universality. In a magnet, the relevant configurations are those formed by the coarse-grained magnetisation (the magnetic moment averaged over a block of side \\(L\\)). In a fluid, the relevant configurations are those of the coarse-grained density (the mass averaged over a block if side \\(L\\)) or more precisely, its fluctuation from its macroscopic average (Figure 8.3). The patterns in the latter (bubbles of liquid or vapour) may be matched to pattern in the former (microdomains of the magnetisation), given appropriate scaling operations to camouflage the differences between the length scales and the differences between the variable scales.\n\n\n\n\n\n\nFigure 8.3: Schematic representation of the coarse graining operation via which the universal properties of fluids and magnets may be exposed.\n\n\n\nThe results is illustrated in Figure 8.4.\n\n\n\n\n\n\n\n\n2D critical Ising model and 2d critical Lennard-Jones fluid at small lengthscales\n\n\n\n\n\n\n\n\n\nSame models as above, but viewed at large lengthscales\n\n\n\n\n\n\nFigure 8.4: Snapshot configurations of the 2D critical Ising model (left) and the 2D critical Lennard-Jones fluid (right). When viewed on sufficiently large length scales the configurational patterns appear universal and self similar.\n\n\n\nA movie in which we progressively zoom out shows how the loss of microscopic details reveals the large lengthscale universal features.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#near-critical-scaling",
    "href": "phase-transitions/rg.html#near-critical-scaling",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.4 Near critical scaling",
    "text": "8.4 Near critical scaling\nThe similarity of coarse-grained configurations of different systems is not restricted to the critical temperature itself. Suppose we have a two state spin model and a three state spin model each somewhat above their critical points at reduced temperature \\(t\\). The two systems will have somewhat different correlation lengths, \\(\\xi_1\\) and \\(\\xi_2\\) say. Suppose however, we choose coarse-graining lengths \\(L_1\\) for \\(L_2\\) for the two models such that \\(\\xi_1/L_1=\\xi_2/L_2\\). We adjust the scales of the block variables (our grey level control) so that the typical variable value is the same for the two systems. We adjust the length scale of the systems (stretch or shrink our snapshots) so that the sizes of the minimum-length-scale structure (set by \\(L_1\\) and \\(L_2\\)) looks the same for each system. Precisely what they look like depends upon our choice of \\(\\xi/L\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-unipics",
    "href": "phase-transitions/rg.html#sec-unipics",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.5 Universality classes",
    "text": "8.5 Universality classes\nCoarse graining does not erase all differences between the physical properties of critical systems. Differences in the space dimension \\(d\\) of two critical systems will lead to different universal properties such as critical exponents. Thus, for instance, the critical exponents of the 2D magnet, match those of the 2d fluid, but they are different to those of 3d magnets and fluids.\n\n\n\n\n\n\n\n\n\n\\(d=2\\)\n\\(d=3\\)\n\n\n\n\nCritical temperature\n0.5673\n0.75\n\n\nOrder parameter exponent \\(\\beta\\)\n\\(\\tfrac{1}{8}\\)\n\\(0.325 \\pm 0.001\\)\n\n\nSusceptibility exponent \\(\\gamma\\)\n\\(\\tfrac{7}{4}\\)\n\\(1.24 \\pm 0.001\\)\n\n\nCorrelation length exponent \\(\\nu\\)\n\\(1\\)\n\\(0.63 \\pm 0.001\\)\n\n\n\nIn fact the space dimension is one of a small set of qualitative features of a critical system which are sufficiently deep-seated to survive coarse graining and which together serve to define the system’s universal behaviour, or universality class. The constituents of this set are not all identifiable a priori. They include the number of components \\(n\\) of the order parameter. Up to now, we have only considered order parameters which are scalar (for a fluid the density, for a magnet the magnetisation), for which \\(n=1\\). In some ferromagnets, the order parameter may have components along two axes, or three axes, implying a vector order parameter, with \\(n=2\\) (the solcalled XY model) or \\(n=3\\) (Heisenberg model), respectively. It is clear that the order-parameter \\(n\\)-value will be reflected in the nature of the coarse-grained configurations, and thus in the universal observables they imply.\nA third important feature which can change the universality class of a critical system is the range of the interaction potential between its constituent particles. Clearly for the Ising model, interactions between spins are inherently nearest neighbour in character. Most fluids interact via dispersion forces (such as the Lennard-Jones potential) which is also short ranged owing to the \\(r^{-6}\\) attractive interaction. However some systems have much longer ranged interactions. Notable here are systems of charged particles which interact via a Coulomb potential. The long ranged nature of the Coulomb potential (which decays like \\(r^{-1}\\)) means that charged systems often do not have the same critical exponents as the Ising model and fluid.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#critical-exponents",
    "href": "phase-transitions/rg.html#critical-exponents",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.6 Critical exponents",
    "text": "8.6 Critical exponents\nWe consider now how the critical exponents, may be computed via the coarse-graining procedure. In what follows we will refer only to the behaviour of a single typical coarse grained variable, which we shall denote \\(S(L)\\). We suppose that \\(t\\) is sufficiently small that \\(\\xi \\gg\nL_\\textrm{ min}\\). Universality and scaling may be expressed in the claim that, for any \\(L\\) and \\(t\\), scale factors \\(a(L)\\) and \\(b(L)\\) may be found such that the probability distribution \\(p(S(L),t)\\) can be written in the form\n\\[\np(S(L),t)=b(L)\\tilde{p}(b(L)S(L),a(L)t)\n\\tag{8.2}\\] where \\(\\tilde{p}\\) is a function unique to a universality class. The role of the scale factors \\(a\\) and \\(b\\) is to absorb the basic non-universal scales identified in Section 8.2. The critical exponents are implicit in the \\(L\\)-dependence of these scale factors. Specifically one finds:\n\\[\n\\begin{aligned}\na(L) & =a_0L^{1/\\nu} \\\\\nb(L) & =b_0L^{\\beta/\\nu}\n\\end{aligned}\n\\tag{8.3}\\] where the amplitudes \\(a_0\\) and \\(b_0\\) are system specific (non-universal) constants.\nThese results state that the critical exponents (in the form \\(1/\\nu\\) and \\(\\beta/\\nu\\)) characterize the ways in which the configuration spectrum evolves under coarse-graining. Consider, first the exponent ratio \\(\\beta/\\nu\\). Precisely at the critical point, there is only one way in which the coarse-grained configurations change with \\(L\\): the overall scale of the coarse-grained variable (the black-white contrast in our grey scale representation) is eroded with increasing \\(L\\). Thus the configurations of coarse-graining length \\(L_1\\) match those of a larger coarse-graining length \\(L_2\\) only if the variable scale in the latter configurations is amplified. The required amplification follows from Equation 8.2 and and Equation 8.3: it is\n\\[\n\\frac{b(L_2)}{b(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{\\beta/\\nu}\\:.\n\\] The exponent ratio \\(\\beta/\\nu\\) thus controls the rate at which the scale of the ordering variable decays with increasing coarse-graining length.\nConsider now the exponent \\(1/\\nu\\). For small but non-zero reduced temperature (large but finite \\(\\xi\\)) there is second way in which the configuration spectrum evolves with \\(L\\). As noted previously, coarse graining reduces the ratio of correlation length to coarse-graining length, and results in configurations with a less critical appearance. More precisely, we see from Equation 8.2 that increasing the coarse graining length from \\(L_1\\) to \\(L_2\\) while keeping the reduced temperature constant has the same effect on the configuration spectrum as keeping coarse-graining length constant which amplifying the reduced temperature \\(t\\) by a factor\n\\[\n\\frac{a(L_2)}{a(L_1)}=\\left(\\frac{L_2}{L_1}\\right)^{1/\\nu}\\:.\n\\] One may think of the combination \\(a(L)t\\) as a measure of the effective reduced temperature of the physical system viewed with resolution length \\(L\\). The exponent \\(1/\\nu\\) controls the rate at which the effective reduced temperature flows with increasing coarse-graining length.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#sec-fss",
    "href": "phase-transitions/rg.html#sec-fss",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.7 Finite-size scaling",
    "text": "8.7 Finite-size scaling\nWe can exploit the fact that the scale factors \\(a(L)\\) and \\(b(L)\\) depend on critical exponents to estimate the values of these exponents using computer simulation. Consider the average of the block variable \\(S(L)\\). Consideration of Equation 8.1 shows that this is non other than the value of the order parameter \\(Q\\), measured over a block of side \\(L\\). Thus from the definition of an average\n\\[\nQ(L,t)=\\bar {S}(L,t)=\\int S(L)p(S(L),t)dS(L)\n\\] where \\(p(S(L))\\) is the probability distribution of \\(S(L)\\).\nMaking use of the representation of Equation 8.2, we then have that\n\\[Q\n(L,t) = \\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)dS(L)\n\\]\nTo integrate this we need to change the integration variable from \\(S(L)\\) to \\(b(L)S(L)\\). We have \\(d(b(L)S(L))=b(L)dS(L)\\) since \\(b(L)\\) does not fluctuate. Thus \\[\n\\begin{aligned}\nQ(L,t)  & =  b^{-1}(L)\\int b(L)S(L)\\tilde{p}(b(L)S(L),a(L)t)d(b(L)S(L))\\nonumber\\\\\n        & =  b^{-1}(L)f(a(L)t)\\nonumber\\\\\n       & =  b_0L^{-\\beta/\\nu}f(a_0L^{1/\\nu}t)\n\\end{aligned}\n\\]\nwhere \\(f\\) is a universal function (defined as the first moment of \\(\\tilde{p}(x,y)\\) with respect to \\(y\\)).\nThe above results provide a method for determining the critical exponent ratios \\(\\beta/\\nu\\) and \\(1/\\nu\\) via computer simulations of near critical systems. For instance, at the critical point (\\(t=0\\)) and for finite block size, \\(Q(L,0)\\) will not be zero (the \\(T\\) at which Q vanishes for finite \\(L\\) is above the true \\(T_c\\), cf. Section 7.2. However, we know that its value must vanish in the limit of infinite \\(L\\); it does so like\n\\[Q(L,0)=b_0L^{-\\beta/\\nu}f(0)\\equiv Q_0L^{-\\beta/\\nu}\\]\nThus by studying the critical point \\(L\\) dependence of \\(Q\\) we can estimate \\(\\beta/\\nu\\). A similar approach in which we study two block sizes \\(L\\), and tune \\(t\\) separately in each case so that the results for \\(QL^{\\beta/\\nu}\\) are identical provides information on the value of \\(1/\\nu\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#summary-of-main-points",
    "href": "phase-transitions/rg.html#summary-of-main-points",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.8 Summary of main points",
    "text": "8.8 Summary of main points\nAs this is quite a long chapter let us summarise the main points:\n\nLimitations of conventional theories: Mean field theories and the scaling hypothesis are insufficient in the critical region due to their neglect of correlations across all relevant length scales.\nCritical region characteristics: Near-critical systems exhibit correlated microstructure on all length scales up to the correlation length. The complexity of this structure makes both analytical and computational study challenging.\nRelevant length scales:\n\nCorrelation length (\\(\\xi\\))\nMinimum microscopic scale (\\(L_\\textrm{min}\\))\nMacroscopic system size (\\(L_\\textrm{max}\\))\n\nWindow Condition for criticality: The true critical regime satisfies \\(L_\\textrm{max} \\gg \\xi \\gg L_\\textrm{min}\\), encompassing a broad range of scales where complex, often fractal-like, structures are present.\nRenormalisation Group (RG) methodology: RG involves the stepwise elimination of degrees of freedom by coarse-graining the system over increasing length scales. A fourth scale, \\(L\\), represents the resolution at which the system is described.\nEffect of coarse-graining: Coarse-graining changes the effective reduced temperature, captured by the relation \\(a(L)t\\), where \\(a(L)\\) scales with \\(L\\) as \\(L^{1/\\nu}\\). This helps describe how critical configurations evolve with resolution.\nUniversality:\n\nCoarse-graining reveals that microscopically different systems can exhibit identical critical behavior when observed at large scales.\nThe concept of universality explains why disparate systems, such as magnets and fluids, can show the same critical exponents and scaling laws.\nCritical behavior depends primarily on general features like dimensionality and symmetry, rather than microscopic details.\n\nFinite-Size scaling:\n\nThe average block variable \\(Q(L,t)\\) is related to block size \\(L\\) and reduced temperature \\(t\\) through scaling relations. Computer simulations exploit this to extract scaling functions and the values of critical exponents.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "href": "phase-transitions/rg.html#addendum-the-effective-coupling-viewpoint-of-the-renormalization-group-non-examinable",
    "title": "8  Universality and the Renormalisation Group Theory of Critical Phenomena",
    "section": "8.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)",
    "text": "8.9 Addendum: The effective coupling viewpoint of the renormalization group (non examinable)\n\n\n\n\n\n\nNotes for those interested in a different perspective on RG theory.\n\n\n\n\n\nLet us begin by returning to our fundamental Equation 2.1, which we rewrite as\n\\[p = Z^{-1}e^{-{\\cal H}}\\] where \\({\\cal H}\\equiv E/k_BT\\).\nThe first step is then to imagine that we generate, by a computer simulation procedure for example, a sequence of configurations with relative probability \\(\\exp(-{\\cal H})\\). We next adopt some coarse-graining procedure which produces from these original configurations a set of coarse-grained configurations. We then ask the question: what is the energy function \\({\\cal H}^\\prime\\) of the coarse-grained variables which would produce these coarse-grained configurations with the correct relative probability \\(\\exp(-{\\cal H}^\\prime)\\)? Clearly the form of \\({\\cal H}^\\prime\\) depends on the form of \\({\\cal H}\\) thus we can write symbolically\n\\[{\\cal H}^\\prime=R({\\cal H})\\]\nThe operation \\(R\\), which defines the coarse-grained configurational energy in terms of the microscopic configurational energy function is known as a renormalisation group transformation (RGT). What it does is to replace a hard problem by a less hard problem. Specifically, suppose that our system is near a critical point and that we wish to calculate its large-distance properties. If we address this task by utilizing the configurational energy and appealing to the basic machinery of statistical mechanics set out in Equation 2.1 and Equation 2.2, the problem is hard. It is hard because the system has fluctuations on all the (many) length scales intermediate between the correlation length \\(\\xi\\) and the minimum length scale \\(L_\\textrm{min}\\).\nHowever, the task may instead be addressed by tackling the coarse-grained system described by the energy \\({\\cal H}^\\prime\\). The large-distance properties of this system are the same as the large-distance properties of the physical system, since coarse-graining operation preserves large-scale configurational structure. In this representation the problem is a little easier: while the \\(\\xi\\) associated with \\({\\cal H}^\\prime\\) is the same as the \\(\\xi\\) associated with \\({\\cal H}\\), the minimum length scale of \\({\\cal H}^\\prime\\) is bigger than that of \\({\\cal H}\\). Thus the statistical mechanics of \\({\\cal H}^\\prime\\) poses a not-quite-so-many-length-scale problem, a problem which is effectively a little less critical and is thus a little easier to solve. The benefits accruing from this procedure may be amplified by repeating it. Repeated application of \\(R\\) will eventually result in a coarse- grained energy function describing configurations in which the \\(\\xi\\) is no bigger than the minimum length scale. The associated system is far from criticality and its properties may be reliably computed by any of a wide variety of approximation schemes. These properties are the desired large-distance properties of the physical system. As explicit reference to fluctuations of a given scale is eliminated by coarse-graining, their effects are carried forward implicitly in the parameters of the coarse-grained energy.\nIn order to setup the framework for a simple illustrative example, let is return to the lattice Ising model for which the energy function depended only on the product of nearest neighbour spins. The coefficient of this product in the energy is the exchange coupling, \\(J\\). In principle, however, other kinds of interactions are also allowed; for example, we may have a product of second neighbour spins with strength \\(J_2\\) or, perhaps, a product of four spins (at sites forming a square whose side is the lattice spacing), with strength \\(J_3\\). Such interactions in a real magnet have their origin in the quantum mechanics of the atoms and electrons and clearly depend upon the details of the system. For generality therefore we will allow a family of exchange couplings \\(J_1\\),\\(J_2\\),\\(J_3,\\dots\\), or \\(J_a, a =\n1,2,\\dots\\) In reduced units, the equivalent coupling strengths are \\(K_a =J_a/k_BT\\). Their values determine uniquely the energy for any given configuration.\n\nWe note that it is not only useful to allow for arbitrary kinds of interactions: if we wish to repeat the transformation several (indeed many) times, it is also necessary because even if we start with only the nearest neighbour coupling in \\({\\cal H}\\) the transformation will in general produce others in \\({\\cal H}^\\prime\\).\n\nNow consider the coarse-graining procedure. Let us suppose that this procedure takes the form of a ‘majority rule’ operation in which the new spins are assigned values \\(+1\\) or \\(-1\\) according to the signs of the magnetic moments of the blocks with which they are associated. The new energy function \\({\\cal H}^\\prime\\) will be expressible in terms of some new coupling strengths \\(K^\\prime\\) describing the interactions amongst the new spin variables (and thus, in effect, the interactions between blocks of the original spin variables). The RGT simply states that these new couplings depend on the old couplings: \\(K_1^\\prime\\) is some function \\(f_1\\) of all the original couplings, and generally\n\\[K^\\prime_a=f_a(K_1,K_2,\\dots) =f_a({\\bf K}),~~~~ a= 1, 2,\\dots\n\\tag{8.4}\\] where K is shorthand for the set \\(K_1, K_2,\\dots\\)\n\n8.9.1 A simple example\nThis example illustrates how one can perform the RG transformation Equation 9.1 directly, without recourse to a ‘sequence of typical configurations’. The calculation involves a very crude approximation which has the advantage that it simplifies the subsequent analysis.\n\n\n\n\n\n\nFigure 8.5: Coarse graining by decimation. The spins on the original lattice are divided into two sets \\(\\{s^\\prime\\}\\) and \\(\\{\\tilde{s}\\}\\). The \\(\\{s^\\prime\\}\\) spins occupy a lattice whose spacing is twice that of the original. The effective coupling interaction between the \\(\\{s^\\prime\\}\\) spins is obtained by performing the configurational average over the \\(\\{\\tilde{s}\\}\\)\n\n\n\nConsider an Ising model in two dimensions, with only nearest neighbour interactions as shown in Figure 9.1. We have divided the spins into two sets, the spins \\(\\{s^\\prime\\}\\) form a square lattice of spacing \\(2\\), the others being denoted by \\(\\{\\tilde{s}\\}\\). One then defines an effective energy function \\({\\cal H^\\prime}\\) for the \\(s^\\prime\\) spins by performing an average over all the possible arrangements of the \\(\\tilde{s}\\) spins\n\\[\n\\exp(-{\\cal H}^\\prime)=\\sum_{\\{\\tilde {s}\\}} \\exp(-{\\cal H}).\n\\tag{8.5}\\]\nThis particular coarse-graining scheme is called ‘decimation’ because a certain fraction (not necessarily one-tenth!) of spins on the lattice is eliminated. This formulation of a new energy function realizes two basic aims of the RG method: the long-distance physics of the ‘original’ system, described by \\({\\cal H}\\), is contained in that of the ‘new’ system, described by \\({\\cal H}^\\prime\\) (indeed the partition functions are the same as one can see by summing both sides over \\(s^\\prime\\)) and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing (‘minimum length scale’) has been reduced by a factor of \\(1/2\\) (the ratio of the lattice spacings of the two systems). We must now face the question of how to perform the configuration sum in Equation 9.2. This cannot in general be done exactly, so we must resort to some approximation scheme. The particular approximation which we invoke is the high temperature series expansion. In its simplest mathematical form, since \\({\\cal H}\\) contains a factor \\(1/k_BT\\), it involves the expansion of \\(\\exp(-{\\cal H})\\) as a power series:\n\\[\\exp(-{\\cal H}/k_BT)=1-{\\cal H}/k_BT +\\frac{1}{2!}({\\cal H}/k_BT)^2+.....\\]\nWe substitute this expansion into the right hand side of Equation 9.2 and proceed to look for terms which depend on the \\(s^\\prime\\) spins after the sum over the possible arrangements of the \\(\\tilde{s}\\) spins is performed. This sum extends over all the possible (\\(\\pm 1\\)) values of all the \\(\\tilde{s}\\) spins. The first term (the 1) in the expansion of the exponential is clearly independent of the values of the \\(s^\\prime\\) spins. The second term (\\({\\cal H}\\)) is a function of the \\(s^\\prime\\) spins, but gives zero when the sum over the \\(s^\\prime\\) spins is performed because only a single factor of any \\(s^\\prime\\) ever appears, and \\(+ 1 - 1 = 0\\). The third term (\\({\\cal H}^2/2\\)) does contribute. If one writes out explicitly the form of \\({\\cal H}^2/2\\) one finds terms of the form \\(K^2s_1^\\prime\\tilde{s}\\tilde{s}s_2^\\prime=K^2s_1^\\prime s_2^\\prime\\), where \\(s_1^\\prime\\) and \\(s_2^\\prime\\) denote two spins at nearest neighbour sites on the lattice of \\(s^\\prime\\) spins and \\(\\tilde{s}\\) is the spin (in the other set) which lies between them. Now, in the corresponding expansion of the left hand side of Equation 9.2, we find terms of the form \\(K^\\prime s_1^\\prime s_2^\\prime\\), where \\(K^\\prime\\) is the nearest neighbour coupling for the \\(s^\\prime\\) spins. We conclude (with a little more thought than we detail here) that\n\\[\nK^\\prime=K^2\n\\tag{8.6}\\]\nOf course many other terms and couplings are generated by the higher orders of the high temperature expansion and it is necssary to include these if one wishes reliable values for the critical temperature and exponents, However, our aim here is to use this simple calculation to illustrate the RG method. Let us therefore close our eyes, forget about the higher order terms and show how the RGT Equation 9.3 can be used to obtain information on the phase transition.\n\n\n\n\n\n\nFigure 8.6: Coupling flow under the decimation transformation described in the text.\n\n\n\nThe first point to note is that that mathematically Equation 9.3 has the fixed point \\(K^*= 1\\); if \\(K= 1\\) then the new effective coupling \\(K^\\prime\\) has the same value \\(1\\). Further, if \\(K\\) is just larger than \\(1\\), then \\(K^\\prime\\) is larger than \\(K\\), i.e. further away from \\(1\\). Similarly, if \\(K\\) is less than \\(1\\), \\(K^\\prime\\) is less than \\(K\\). We say that the fixed point is unstable: the flow of couplings under repeated iteration of Equation 9.3 is away from the fixed point, as illustrated in Figure 9.2. The physical significance of this is as follows: suppose that the original system is at its critical point so that the ratio of \\(\\xi\\) to lattice spacing is infinite. After one application of the decimation transformation, the effective lattice spacing has increased by a factor of two, but this ratio remains infinite; the new system is therefore also at its critical point. Within the approximations inherent in Equation 9.3, the original system is an Ising model with nearest neighbour coupling \\(K\\) and the new system is an Ising model with nearest neighbour coupling \\(K^\\prime\\). If these two systems are going to be at a common critically, we must identify \\(K^\\prime=\nK\\). The fixed point \\(K^*= 1\\) is therefore a candidate for the critical point \\(K_c\\), where the phase transition occurs. This interpretation is reinforced by considering the case where the original system is close to, but not at, criticality. Then \\(\\xi\\) is finite and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing is reduced by a factor of two. This instability of a fixed point to deviations of \\(K\\) from \\(K^*\\) is a further necessary condition for its interpretation as a critical point of the system. In summary then we make the prediction\n\\[\nK_c=J/k_BT_c=1\n\\tag{8.7}\\]\nWe can obtain further information about the behaviour of the system close to its critical point. In order to do so, we rewrite the transformation (Equation 9.3) in terms of the deviation of the coupling from its fixed point value. A Taylor expansion of the function \\(K^\\prime=K^2\\) yields\n\\[\n\\begin{aligned}\nK^\\prime =& (K^*)^2 +(K-K^*)\\left.\\frac{\\partial K^\\prime}{\\partial K}\\right|_{K=K^*}+\\frac{1}{2}(K-K^*)^2\\left.\\frac{\\partial^2 K^\\prime}{\\partial K^2}\\right|_{K=K^*}+\\ldots\\nonumber\\\\\nK^\\prime - K^* =& 2 (K - K^*)+ (K - K^*)^2\n\\end{aligned}\n\\]\nwhere in the second line we have used the fact that the first derivative evaluates to \\(2K^*=2\\) and \\((K^*)^2=K^*\\).\nFor a system sufficiently close to its critical temperature the final term can be neglected. The deviation of the coupling from its fixed point (critical) value is thus bigger for the new system than it is for the old by a factor of two. This means that the reduced temperature is also bigger by a factor of two:\n\\[t^\\prime= 2t\\]\nBut \\(\\xi\\) (in units of the appropriate lattice spacing) is smaller by a factor of \\(1/2\\):\n\\[\\xi^\\prime= \\xi/2\\]\nThus, when we double \\(t\\), we halve \\(\\xi\\), implying that\n\\[\\xi\\propto t^{-1}\\]\nfor \\(T\\) close to \\(T_c\\). Thus we see that the RGT predicts scaling behaviour with calculable critical exponents. In this simple calculation we estimate the critical exponent \\(\\nu=1\\) for the square lattice Ising model. This prediction is actually in agreement with the exactly established value. The agreement is fortuitous- the prediction in Eq. refeq:Kc for \\(K_c\\), is larger than the exactly established value by a factor of more than two. In order to obtain reliable estimates more sophisticated and systematic methods must be used.\nThe crude approximation in the calculation above produced a transformation, Equation 9.3, involving only the nearest neighbour coupling, with the subsequent advantages of simple algebra. We pay a penalty for this simplicity in two ways: the results obtained for critical properties are in rather poor agreement with accepted values, and we gain no insight into the origin of universality.\n\n\n8.9.2 Universality and scaling\nIn order to expose how universality can arise, we should from the start allow for several different kinds of coupling \\(J_a\\), and show how the systems with different \\(J_a\\) can have the same critical behaviour.\n\n\n\n\n\n\nFigure 8.7: General flow in coupling space\n\n\n\nFigure 9.3 is a representation of the space of all coupling strengths \\(K_a\\) in the energy function \\({\\cal H}/k_BT\\). This is of course actually a space of infinite dimension, but representing three of these, as we have done, enables us to illustrate all the important aspects. First let us be clear what the points in this space represent. Suppose we have some magnetic material which is described by a given set of exchange constants \\(J_1,J_2,J_3.....\\) As the temperature \\(T\\) varies, the coupling strengths \\(K_a=J_a/k_BT\\) trace out a straight line, or ray, from the origin of the space in the direction (\\(J_1,J_2,J_3 ....\\) ). Points on this ray close to the origin represent this magnet at high temperatures, and conversely points far from the origin represent the magnet at low temperatures. The critical point of the magnet is represented by a specific point on this ray, \\(K_a=\nJ_a/k_BT, a= 1,2,\\dots\\) The set of critical points on all of the possible rays forms a surface, the critical surface. Formally, it is defined by the set of all possible models (of the Ising type) which have infinite \\(\\xi\\). It is shown schematically as the shaded surface in Figure 9.3. (In the figure it is a two-dimensional surface; more generally it has one dimension less than the full coupling constant space, dividing all models into high and low temperature phases.)\nOur immediate goal then is to understand how the RGT can explain why different physical systems near this critical surface have the same behaviour. Let us turn now to the schematic representation of the RG flow in Figure 9.3. Suppose we start with a physical system, with coupling strengths \\(K_a,  a= 1,2, \\dots\\). What the RGT does is generate a new point in the figure, at the coupling strengths \\(K_a^{(1)}=f_a({\\bf K})\\); these are the couplings appearing in the effective energy function describing the coarse-grained system. If we repeat the transformation, the new energy function has coupling strengths \\(K_a^{(2)}=f_a({\\bf K})\\). Thus repeated application of the transformation generates a flow of points in the figure: \\({\\bf K\\to\nK^{(1)}\\to\\dots\\to K^{(n)}}\\) where the superscript (\\(n\\)) labels the effective couplings after \\(n\\) coarse-graining steps. if the change in coarse-graining scale is \\(b\\) (\\(&gt; 1\\)) at each step, the total change in coarse-graining scale is \\(b^n\\) after \\(n\\) steps. In the process, therefore, the ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor of \\(b^{-n}\\). The dots in Figure 9.3 identify three lines of RG flow starting from three systems differing only in their temperature. (The flow lines are schematic but display the essential features revealed in detailed calculations.)\nConsider first the red dots which start from the nearest neighbour Ising model at its critical point. The ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor b at each step, but, since it starts infinite, it remains infinite after any finite number of steps. In this case we can in principle generate an unbounded number of dots, \\({\\bf K^{(1)}, K^{(2)},\\dots,K^{(n)}}\\), all of which lie in the critical surface. The simplest behaviour of such a sequence as \\(n\\) increases is to tend to a limit, \\(K^*\\), say. In such a case\n\\[K^*_a=f_a(K^*)~~~~ a= 1,2 .....\\]\nThis point \\({\\bf K^*} \\equiv K_1^*, K_2^*, \\dots\\) is therefore a fixed point which lies in the critical surface.\nBy contrast, consider the same magnet as before, now at temperature \\(T\\) just greater than \\(T_c\\), its couplings \\(K_a\\), will be close to the first red dot (in fact they will be slightly smaller) and so will the effective couplings \\(K_a^{(1)},K_2^{(2)},\\dots\\) of the corresponding coarse-grained systems. The new flow will therefore appear initially to follow the red dots towards the same fixed point. However, the flow must eventually move away from the fixed point because each coarse-graining now produces a model further from criticality. The resulting flow is represented schematically by one set of black dots. The other set of black dots shows the expected flow starting from the same magnet slightly below its critical temperature.\nWe are now in a position to understand both universality and scaling within this framework. We will suppose that there exists a single fixed point in the critical surface which sucks in all flows starting from a point in that surface. Then any system at its critical point will exhibit large-length scale physics (large-block spin behaviour) described by the single set of fixed point coupling constants. The uniqueness of this limiting set of coupling constants is the essence of critical point universality. It is, of course, the algebraic counterpart of the unique limiting spectrum of coarse-grained configurations, discussed in Section 8.5. Similarly the scale-invariance of the critical point configuration spectrum (viewed on large enough length scales) is expressed in the invariance of the couplings under iteration of the transformation (after a number of iterations large enough to secure convergence to the fixed point).\nTo understand the behaviour of systems near but not precisely at critically we must make a further assumption (again widely justified by explicit studies). The flow line stemming from any such system will, we have argued, be borne towards the fixed point before ultimately deviating from it after a number of iterations large enough to expose the system’s noncritical character. We assume that (as indicated schematically in the streams of red and blue lines in Figure 9.3 the deviations lie along a single line through the fixed point, the direction followed along this line differing according to the sign of the temperature deviation \\(T-T_c\\). Since any two sets of coupling constants on the line (on the same side of the fixed point) are related by a suitable coarse-graining operation, this picture implies that the large-length-scale physics of all near- critical systems differs only in the matter of a length scale. This is the essence of near-critical point universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Universality and renormalization group theory</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html",
    "href": "phase-transitions/nucleation-and-growth.html",
    "title": "8  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "",
    "text": "8.1 Introduction to nucleation\nIn previous discussions, we considered first-order phase transitions but deferred a detailed analysis of the dynamical mechanism by which a system evolves from one phase to another. We now address this question explicitly.\nConsider again the Ising model at a temperature \\(T &lt; T_c\\), where the system is initially prepared in the majority spin-up phase at zero external field, \\(H = 0\\). We now examine the system’s response to the application of a small negative external field \\(H &lt; 0\\), which lowers the free energy of the spin-down phase relative to the spin-up phase.\nDespite the global free energy favoring the spin-down phase, the system does not undergo an instantaneous transition. This delay is a consequence of the free energy barrier associated with nucleating a region of the stable phase within the metastable one, as introduced in Section 5.2. The dynamical pathway of the phase transition proceeds via nucleation of localized regions—referred to as droplets—of the stable (spin-down) phase embedded within the metastable (spin-up) background. Once nucleated, these droplets may grow over time and ultimately coalesce to transform the system to the stable phase.\nThe nucleation of a droplet of the stable phase of size \\(n\\) spins entails a competition between bulk and interfacial contributions to the free energy. The bulk free energy gain is linear in \\(n\\), given by \\(-nH\\), due to the alignment of spins with the external field. However, this gain is offset by an interfacial free energy cost arising from broken bonds at the boundary between phases. For the Ising model, each broken bond contributes an energy cost of \\(+2J\\), so the total interfacial energy scales with the perimeter (in 2D) or surface area (in 3D) of the droplet. This interfacial contribution is referred to as the surface tension, and constitutes a true free energy cost: it includes not only the energetic penalty from broken bonds but also an entropic contribution due to the configurational degrees of freedom associated with the droplet shape.\nThe resulting competition between the extensive free energy gain and the sub-extensive interfacial cost leads to a free energy barrier for droplet formation. Only fluctuations that produce a droplet larger than a critical size \\(n_c\\) will grow; smaller droplets will shrink. This framework is formalized in classical nucleation theory, which provides a quantitative description of the nucleation rate, critical droplet size, and the associated activation energy barrier.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nucleation, growth and spinoidal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#classical-nucleation-theory-homogeneous-nucleation",
    "href": "phase-transitions/nucleation-and-growth.html#classical-nucleation-theory-homogeneous-nucleation",
    "title": "9  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "9.2 Classical Nucleation Theory: Homogeneous Nucleation",
    "text": "9.2 Classical Nucleation Theory: Homogeneous Nucleation\nWe now present the framework of classical nucleation theory (CNT) for the case of homogeneous nucleation, in which the nucleation of the stable phase occurs spontaneously and uniformly throughout the bulk of the metastable phase, without the aid of impurities, defects, or surfaces.\nLet us consider a droplet of the stable (spin-down) phase of radius \\(R\\) embedded within the metastable (spin-up) background. The total change in free energy \\(\\Delta F(R)\\) associated with forming such a droplet consists of two competing contributions:\n\nBulk free energy gain: The interior of the droplet consists of \\(V \\sim R^d\\) spins aligned with the external field \\(H &lt; 0\\), leading to a volume free energy change\n\\[\n\\Delta F_{\\text{bulk}}(R) = -|\\Delta f| \\, R^d,\n\\]\nwhere \\(|\\Delta f| \\propto |H|\\) is the free energy density difference between the metastable and stable phases, and \\(d\\) is the spatial dimensionality of the system.\nInterfacial free energy cost: The boundary between the two phases has a surface area scaling as \\(R^{d-1}\\), and incurs a free energy cost proportional to the surface tension \\(\\sigma\\):\n\\[\n\\Delta F_{\\text{surface}}(R) = \\sigma \\, S_d \\, R^{d-1},\n\\]\nwhere \\(S_d\\) is a geometrical factor (e.g., \\(S_2 = 2\\pi\\) in 2D and \\(S_3 = 4\\pi\\) in 3D).\n\nThe total free energy change is therefore given by\n\\[\n\\Delta F(R) = \\sigma \\, S_d \\, R^{d-1} - |\\Delta f| \\, V_d \\, R^d,\n\\]\nwhere \\(V_d\\) is another dimension-dependent constant. This expression exhibits a characteristic maximum at a critical droplet radius \\(R_c\\), obtained by extremizing \\(\\Delta F(R)\\) with respect to \\(R\\):\n\\[\n\\frac{d \\Delta F}{dR} = 0 \\quad \\Rightarrow \\quad R_c = \\frac{(d-1)\\sigma S_d}{d |\\Delta f| V_d}.\n\\]\n\n\n\n\n\n\nFigure 9.1: Free energy barrier \\(\\Delta F(r)\\) for nucleation of a spherical droplet as a function of radius \\(R\\) (schematic)\n\n\n\nThe corresponding free energy barrier for nucleation is\n\\[\n\\Delta F_c = \\Delta F(R_c) = \\frac{(d-1)^{d-1}}{d^d} \\cdot \\frac{(S_d)^d \\, \\sigma^d}{(|\\Delta f|)^{d-1} \\, (V_d)^{d-1}}.\n\\]\nThis barrier must be surmounted by thermal fluctuations in order for a critical nucleus to form and grow. The nucleation rate per unit volume is given (in the Arrhenius approximation) by\n\\[\nI \\sim I_0 \\exp\\left( -\\frac{\\Delta F_c}{k_B T} \\right),\n\\]\nwhere \\(I_0\\) is a prefactor determined by microscopic kinetics, and \\(k_B\\) is Boltzmann’s constant.\n\n9.2.1 Interpretation and Scaling Behavior\nSeveral key features emerge from this analysis:\n\nBarrier scaling: The nucleation barrier \\(\\Delta F_c \\sim \\sigma^d / |\\Delta f|^{d-1}\\) diverges as \\(H \\to 0\\), reflecting the increasing stability of the metastable phase near the coexistence point.\nCritical radius: The critical droplet size \\(R_c \\sim \\sigma / |\\Delta f|\\) also diverges as \\(|\\Delta f| \\to 0\\), indicating that larger fluctuations are required to initiate nucleation close to the coexistence line.\nDimensional dependence: Both \\(\\Delta F_c\\) and \\(R_c\\) exhibit strong dependence on the spatial dimension \\(d\\), with nucleation becoming increasingly suppressed in higher dimensions due to the dominance of interfacial cost.\n\nIn summary, homogeneous nucleation in a first-order transition is governed by a delicate balance between surface tension and bulk free energy gain. Only droplets exceeding a critical size can overcome the barrier and initiate a transition. This sets an intrinsic timescale for the dynamics of phase transformation, which can become extremely long near coexistence due to the exponentially small nucleation rate.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Nucleation, growth and spinoidal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "href": "phase-transitions/nucleation-and-growth.html#domain-growth",
    "title": "8  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "8.3 Domain growth",
    "text": "8.3 Domain growth\nFollowing successful nucleation of a supercritical droplet, the system enters a regime where the global transformation is driven by the deterministic growth of domains of the stable phase. Such domain growth involves more atoms or molecules attaching to these nuclei, causing them to expand into larger structures (e.g., growing crystals or droplets).\nGrowth rate depends on factors like temperature, concentration, and the availability of building blocks.\nThe shape and structure of the final phase often depend on how growth occurs (e.g., slow growth may form perfect crystals; rapid growth may be irregular).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nucleation, growth and spinoidal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "href": "phase-transitions/nucleation-and-growth.html#spinodal-decomposition",
    "title": "8  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "8.4 Spinodal decomposition",
    "text": "8.4 Spinodal decomposition\nPreviously we have considered a scenario in which we move our system to a statepoint just inside the coexistence region so that the original phase remains metastable. We then wait for a fluctuation that yields a critical nucleus and subsequent growth.\nNow imagine that rather than positioning the system just inside the coexistence region we move immediately to a state point well inside the coexistence region (cf. Figure 8.2) such that there is no metastable minimum in the free energy. Then there is no nucleation and growth, rather the system is unstable and immediately starts to phase separate at all points in the system. This so called spinodal decomposition.\n\n\n\n\n\n\nFigure 8.2: Schematic phase doagram showing the regions in which one observes nucleation and growth, or spinodal decomposition\n\n\n\nThe nature of spinodal decomposition and late-time domain growth depends sensitively on whether the order parameter is conserved or not.\n\n\n8.4.1 Non-Conserved Order Parameter Dynamics (Model A)\nIn systems with a non-conserved order parameter, such as the Ising model with single spin flip (so called Glauber) dynamics, the order parameter can relax locally without constraint. This leads to curvature-driven motion of interfaces.\nThe typical domain size \\(L(t)\\) grows algebraically with time:\n\\[\nL(t) \\sim t^{1/2},\n\\]\ncorresponding to a dynamic exponent \\(z = 2\\). The growth is driven by reduction of the total interfacial area—regions of high curvature (small domains) shrink and are absorbed by larger, flatter ones.\nLet is aassume that domain walls move with velocity proportional to curvature: \\[\n  v \\sim \\frac{1}{L}\n  \\] Then with the typical domain size being \\(L(t)\\) it follows that\n\\[\n  \\frac{dL}{dt} \\sim \\frac{1}{L}\n  \\]\nIntegrating both sides yields the \\(t^{1/2}\\) domain growth law.\nIt turns out that the detailed evolution of the coarse-grained order parameter field \\(\\phi(\\mathbf{r}, t)\\) is governed by the Allen–Cahn equation:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = - \\frac{\\delta F[\\phi]}{\\delta \\phi},\n\\]\nwhere \\(F[\\phi]\\) is a coarse-grained Ginzburg–Landau free energy functional:\n\\[\nF[\\phi] = \\int d^d x \\left[ \\frac{1}{2} (\\nabla \\phi)^2 + V(\\phi) \\right],\n\\]\nwith \\(V(\\phi)\\) typically a double-well potential such as \\(V(\\phi) = \\frac{1}{4}(\\phi^2 - 1)^2\\).\n\n\n\n8.4.2 Conserved Order Parameter Dynamics (Model B)\nFor systems with a conserved order parameter, such as phase separation in binary alloys or the Ising model with spin-swap (so called ‘Kawasaki’) dynamics, the order parameter (e.g., composition or particle number) must be conserved locally. This imposes a diffusive constraint on the dynamics.\nThe domain size again grows algebraically, but with a different exponent:\n\\[\nL(t) \\sim t^{1/3},\n\\]\ncorresponding to a dynamic exponent \\(z = 3\\).\nA chemical potential difference drives diffusion and is given by \\[\n\\Delta \\mu \\sim \\frac{\\sigma}{L}\n\\] (again due to curvature, where \\(\\sigma\\) is surface tension)\nNow the flux is proportional to the chemical potential gradient (Fick’s law): \\[\n\\text{Flux} \\sim -\\nabla \\mu \\sim \\frac{\\Delta \\mu}{L} \\sim \\frac{\\sigma}{L^2}\n\\]\nand the rate of change of domain size is proportional to this flux: \\[\n\\frac{dL}{dt} \\sim \\frac{1}{L^2}\n\\quad\\Rightarrow\\quad\nL(t) \\sim t^{1/3}\n\\]\nIt turns out that the detailed dynamics are described by the Cahn–Hilliard equation, a continuity equation of the form:\n\\[\n\\frac{\\partial \\phi}{\\partial t} = \\nabla^2 \\left( \\frac{\\delta F[\\phi]}{\\delta \\phi} \\right),\n\\]\nreflecting that the order parameter can only evolve via diffusion of its conjugate chemical potential. This leads to the slow transport of material across domains and a more sluggish coarsening process compared to the non-conserved case.\n\n\n8.4.3 Schematic of Domain Growth in 2D Ising model\nHere are schematic illustrations of domain growth for:\n\nNon-conserved dynamics (Model A): Domains coarsen rapidly, with smoother and larger regions due to free relaxation of the order parameter.\nConserved dynamics (Model B): Coarsening is slower and domains are more intricate, reflecting the constraint of local conservation.\n\n\n\n\n\n\n\n\n\nNon-Conserved Dynamics (Model A)\n\n\n\n\n\n\n\nConserved Dynamics (Model B)\n\n\n\n\n\n\nFigure 8.3: Schematic illustrations of domain morphology resulting from spinodal decomposition or late-time domain growth for non-conserved and conserved dynamics.\n\n\n\n\n\n8.4.4 Dynamics Scaling Hypothesis\nAt late times, both conserved and non-conserved systems exhibit dynamic scaling: the statistical properties of the domain morphology become self-similar under rescaling of lengths by \\(L(t)\\).\nFor example, the equal-time two-point correlation function satisfies\n\\[\nC(r, t) = f\\left(\\frac{r}{L(t)}\\right),\n\\]\nwhere \\(f(x)\\) is a time-independent scaling function. Plots of \\(C(r, t)\\) collapse when plotted as a function of \\(r/L(t)\\).\nThe structure factor \\(S(k, t)\\), which is the Fourier transform of the correlation function, is experimentally accessible eg via X-ray or neutron scattering, and also obeys dynamic scaling:\n\\[\nS(k, t) = \\int d^d r \\, e^{-i \\vec{k} \\cdot \\vec{r}} \\, C(r, t).\n\\]\nSubstituting the scaling form of \\(C(r, t)\\) into this expression, and changing variables to \\(\\vec{u} = \\vec{r}/L(t)\\), gives:\n\\[\nS(k, t) = L(t)^d \\int d^d u \\, e^{-i \\vec{k} \\cdot L(t) \\vec{u}} \\, f(u) = L(t)^d \\, g(kL(t)),\n\\]\nwith \\(g(x)\\) a universal scaling function dependent on the dynamical class and dimensionality.\n\n\n8.4.5 Summary of Growth Laws\n\n\n\n\n\n\n\n\n\n\nDynamics Type\nConservation\nEquation Type\nGrowth Law\nDynamic Exponent\n\n\n\n\nModel A (e.g. Glauber)\nNo\nAllen–Cahn\n\\(L(t) \\sim t^{1/2}\\)\n\\(z = 2\\)\n\n\nModel B (e.g. Kawasaki)\nYes\nCahn–Hilliard\n\\(L(t) \\sim t^{1/3}\\)\n\\(z = 3\\)\n\n\n\n\n\nRemarks: The domain growth exponents \\(1/z\\) are robust under many conditions, but can be modified in the presence of disorder, long-range interactions, or hydrodynamic effects.\n\n\nIn both the model A and model B cases, the system coarsens until it reaches equilibrium, characterized by a uniform macroscopic phase and the complete elimination of interfaces.\n\n\nThe approach to equilibrium is algebraically slow (described by power laws) due to the scale-free nature of domain dynamics.\n\nMore about the scientists mentioned in this chapter:\nJohn Cahn",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nucleation, growth and spinoidal decomposition</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html",
    "href": "phase-transitions/master-equation-and-diffusion.html",
    "title": "11  Introduction to stochastic processes",
    "section": "",
    "text": "11.1 The Master Equation\nMany natural phenomena are stochastic — they involve randomness in their evolution over time.\nIn classical physics, this randomness may arise from our lack of knowledge about microscopic details. For example, in a gas, we do not know the precise positions and velocities of each particle, so their collisions with the container walls appear random.\nIn quantum mechanics, stochasticity is even more intrinsic. The fundamental objects are probability amplitudes, and outcomes are inherently probabilistic.\nTo describe these systems effectively, we use a coarse-grained probabilistic description, which tracks the likelihood of different outcomes rather than precise trajectories. One of the key tools in this approach is the master equation.\nConsider a system in microstate \\(i\\) with energy \\(E_i\\). It can transition to neighboring microstates \\(j\\), where the energy difference \\(|E_j - E_i|\\) is small (within \\(\\delta E\\)).\nLet \\(\\nu_{ij}\\) be the rate at which the system jumps from state \\(i\\) to state \\(j\\). Over an infinitesimal time interval \\(dt\\), the probability \\(p_i\\) changes as:\n\\[\ndp_i = \\left[ -p_i \\sum_j  \\nu_{ij} + \\sum_j \\nu_{ji} p_j \\right] dt\n\\]\nThis expression contains two terms:\nThe master equation becomes:\n\\[\n\\frac{dp_i}{dt} = -\\sum_j \\nu_{ij} p_i + \\sum_j \\nu_{ji} p_j\n\\]\nThis is a linear first-order differential equation for the vector of probabilities \\(\\{p_i\\}\\).\nAlternatively, in matrix form:\n\\[\n\\frac{d\\mathbf{p}}{dt} = W \\mathbf{p}\n\\]\nwhere \\(W\\) is the rate matrix with entries:\nThis structure ensures probability conservation: the total probability \\(\\sum_i p_i = 1\\) remains constant in time.\nThe master equation is first order in time and does not have time reversal symmetry so describes an irreversible process. This irreversibility arises from the coarse-graining process that throws away information about underlying microphysics which is described by Newton’s equations and which are time reversible. Only by doing so is the entropy allowed to increase which is required by the second law of thermodynamics for an irreversible process. Consequently the increase of entropy is linked to our knowledge about the system rather than anything it is doing internally in a manner that may appear dubious. Can it be possible that macroscopic and reproducible phenomena such as heat flow depend on how we handle information? Perhaps yes since the division between work and heat is somewhat arbitrary. Were we able to track all the particle positions there would be no need to talk about heat energy or heat flow.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#the-master-equation",
    "title": "11  Introduction to stochastic processes",
    "section": "",
    "text": "A loss term: the system leaves state \\(i\\) at rate \\(\\nu_{ij}\\),\nA gain term: the system arrives in state \\(i\\) from other states \\(j\\) at rate \\(\\nu_{ji}\\).\n\n\n\n\n\n\n\n\n\\(W_{ij} = \\nu_{ji}\\) for \\(i \\neq j\\),\n\\(W_{ii} = -\\sum_{j \\neq i} \\nu_{ij}\\).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "href": "phase-transitions/master-equation-and-diffusion.html#from-the-master-equation-to-the-diffusion-equation",
    "title": "11  Introduction to stochastic processes",
    "section": "11.2 From the Master Equation to the Diffusion Equation",
    "text": "11.2 From the Master Equation to the Diffusion Equation\nNow consider the situation where the state index \\(i\\) corresponds to a position in space, \\(x_i = i a\\), ie a one-dimensional lattice with lattice spacing \\(a\\), and transitions only occur between neighboring lattice sites.\nWe assume:\n\nTransition rates are symmetric: \\(\\nu_{i, i+1} = \\nu_{i, i-1} = \\nu\\),\nThe spacing \\(a\\to 0\\) in the continuum limit.\n\nThe master equation becomes a finite-difference equation:\n\\[\n\\begin{aligned}\n\\frac{dp_i}{dt} =  &\\sum_j\\nu_{ij}(p_j-p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i-1}-p_i) + \\nu(p_{i+1} - p_i)\\\\\n\\frac{dp_i}{dt} =  &\\nu(p_{i+1} + p_{i-1} - 2p_i)\n\\end{aligned}\n\\]\nWe now define a continuous variable \\(x = i a\\), and a probability density \\(p(x, t)\\) such that \\(p_i(t) \\approx p(x, t)\\).\nUsing Taylor expansions:\n\\[\np(x \\pm a, t) = p(x, t) \\pm a \\frac{\\partial p}{\\partial x} + \\frac{a^2}{2} \\frac{\\partial^2 p}{\\partial x^2} + \\cdots\n\\]\nSubstituting into the master equation gives:\n\\[\n\\frac{\\partial p}{\\partial t} =\n\\nu a^2 \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nDefining the diffusion constant \\(D = \\nu a^2\\), we obtain the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2}\n\\]\nThe diffusion equation describes the evolution of the probability density of a particle with diffusion constant (sometimes called diffusivity) given by \\(D = \\nu a^2\\). The dimensions of \\(D\\) are \\([\\text{length}]^2/[\\text{time}]\\). Typically, after expansion, we set the lattice spacing \\(a\\) to 1. Additionally, the diffusion equation can describe many non-interacting diffusing particles. In this case, we replace \\(p\\) with \\(\\rho\\), representing the density or concentration of particles, and use the normalization \\(\\int dx \\, \\rho = M\\), where \\(M\\) is the number of particles.\nThe diffusion equation, much like the master equation from which it originates, explicitly violates time-reversal symmetry, thus permitting entropy to increase.\nThe solution of the diffusion equation for an initial condition where the particle is initially localized at the origin (formally, \\(p(x,0) = \\delta(x)\\)) is a Gaussian:\n\\[\np(x,t) = (4 \\pi D t)^{-1/2} \\exp\\left[-\\frac{x^2}{4 D t}\\right]\n\\]\nWe explicitly see the arrow of time by examining this Gaussian solution at various times \\(t\\). As \\(t\\) increases, the Gaussian “bell-shaped” curve spreads out. Its width grows according to \\(\\langle x^2 \\rangle^{1/2} \\sim t^{1/2}\\). This is known as “diffusive scaling,” and it implies that, after time \\(t\\), a particle will typically be found at a distance roughly proportional to \\(t^{1/2}\\) from its starting point. Conversely, exploring a region of size \\(L\\) typically requires a time of order \\(O(L^2)\\).\nThe evolution of the solution to the 1d diffusion equation in a spatial region \\(x=[0,1]\\) as a function of times are shown in the movie below. The diffusion constant is \\(D=0.01\\). The movie corresponds to a particle initialised at \\(x=0.5\\). One sees how the probability density spreads out over the range as time increases. This can be used to model the diffusion of particles down a concentration gradient as you will see in the next part of the course.\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport os\n\n# Parameters\nL = 1.0        # Length of the domain\nT = 1.0        # Total time \nnx = 400       # Number of spatial points\nnt = 2000      # Number of time steps\nD = 0.1        # Diffusion coefficient\n\n# Discretization\ndx = L / (nx - 1)\ndt = T / nt\n\n# Stability condition auto-adjust\nif D * dt / dx**2 &gt; 0.5:\n    print(\"Adjusting dt and nt to satisfy stability condition...\")\n    dt = 0.4 * dx**2 / D\n    nt = int(T / dt)\n    dt = T / nt  # Recalculate dt exactly\n\nnt = int(nt / 50)  # Artificial slowdown for animation\n\nprint(f\"Using dt = {dt:.4e}, nt = {nt}\")\n\n# Initialize x and u\nx = np.linspace(0, L, nx)\n\n# Initial condition: smooth narrow Gaussian\nsigma = 0.01\nu = np.exp(-(x - L/2)**2 / (2 * sigma**2))\n\n# Normalize initial condition\nu /= np.sum(u) * dx\n\n# Setup figure\nplt.rcParams.update({\n    \"text.usetex\": True,\n    \"font.family\": \"serif\"\n})\n\nfig, ax = plt.subplots()\nline, = ax.plot(x, u)\n\n# Compute peak height for setting y-axis\npeak_height = 1 / (np.sqrt(2 * np.pi) * sigma)\nax.set_ylim(0, peak_height * 1.05)\n\nax.set_xlabel(r'$x$', fontsize=20)\nax.set_ylabel(r'$p(x,t)$', fontsize=20)\n\nax.tick_params(axis='both', which='major', labelsize=14)\n\n# Tiny time counter text\ntime_text = ax.text(0.85, 0.05, '', transform=ax.transAxes, fontsize=10, verticalalignment='bottom')\n\n\n# Function to update the plot\ndef update(frame):\n    global u\n    unew = np.copy(u)\n    unew[1:-1] = u[1:-1] + D * dt / dx**2 * (u[2:] - 2*u[1:-1] + u[:-2])\n    u = unew\n\n    # Normalize at every step\n    u /= np.sum(u) * dx\n\n    line.set_ydata(u)\n    current_time = frame * dt\n    time_text.set_text(r'$t=%.4f$' % current_time)\n    return line, time_text\n\nani = animation.FuncAnimation(fig, update, frames=nt, interval=100, blit=True)\n\n\n# Save the animation as a movie\nWriter = animation.writers['ffmpeg']\nwriter = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nani.save(\"../Movies/diffusion_evolution.mp4\", writer=writer)\n\n\nplt.show()",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "href": "phase-transitions/master-equation-and-diffusion.html#consequences-of-time-reversal-symmetry",
    "title": "11  Introduction to stochastic processes",
    "section": "11.3 Consequences of time reversal symmetry",
    "text": "11.3 Consequences of time reversal symmetry\nAs we have seen by introducing a type of coarse graining, the master equation violates time reversal symmetry of the underlying Newtonian dynamics. Remarkably however, the fact that the underlying microphysics is actually time reversal symmetric has several deep consequences which survive the coarse graining procedure These results are some of the cornerstones of nonequilibrium thermodynamics\n\n11.3.1 Detailed balance\nRecall from year 2 Statistical Mechanics that for a system in equilibrium, the principle of equal a-priori probabilities of microstates holds. Therefore\n\\[\n\\nu_{ij} p_i^{eq} = \\nu_{ji} p_j^{eq}\n\\]\nHence, on average, the actual rate of quantum jumps from \\(i\\) to \\(j\\) (the left-hand side) is the same as from \\(j\\) to \\(i\\). This is a stronger statement than the master equation, which asserts only that there is overall balance between the rate of jumping into and out of state \\(i\\) in equilibrium. The result above is known as the principle of detailed balance.\nThis principle is powerful because it applies not only to individual states but also to any grouping of states.\nExercise: Show that for two groups of states, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\nu_{AB} p_A^{eq} = \\nu_{BA} p_B^{eq}\n\\]\nHence, detailed balance arguments can be extended to subsystems within a large isolated system, and even to systems that are not isolated. However, in such cases, the principle is far from obvious, because once states are grouped together:\n\\[\n\\nu_{AB} \\ne \\nu_{BA}, \\quad p_A \\ne p_B\n\\]\n(This can be easily demonstrated, for example, by considering two groups that contain different numbers of states with similar energies.) Nonetheless, the detailed balance relation holds, in equilibrium, in the general form above.\n\n\n11.3.2 Computer simulation\nIn computer simulation, good results will be obtained if one accurately follows the microscopic equations of motion. This is the molecular dynamics (MD) method which we now outline.\nMolecular dynamics\nMolecular Dynamics (MD) involves a system of classical particles interacting through specified interparticle forces. The motion of these particles is determined by numerically integrating Newton’s equations of motion. In MD simulations, averages of state variables are obtained as time averages over trajectories in phase space. Typically, the forces acting between particles are conservative, ensuring that the total energy \\(E\\) remains constant. This conservation implies that the motion is restricted to a \\((2dN - 1)\\)-dimensional surface in phase space, denoted by \\(\\Gamma(E)\\).\nA central aspect of MD is the averaging of observables. For a given observable \\(A\\), its average is computed as the time average along the trajectory. Mathematically, this is expressed as:\n\\[\n\\left\\langle A(\\{ \\mathbf{p}_i \\}, \\{ \\mathbf{r}_i \\}) \\right\\rangle = \\frac{1}{\\tau} \\int_{t_0}^{t_0+\\tau} dt\\, A(\\{ \\mathbf{p}_i(t) \\}, \\{ \\mathbf{r}_i(t) \\})\n\\]\nThis formula represents the integral of the observable over a time interval \\(\\tau\\), normalized by the length of that interval.\nThe practical steps of an MD simulation start with generating an initial random configuration of particle positions \\(\\{ \\mathbf{r}_i \\}\\) and momenta \\(\\{ \\mathbf{p}_i \\}\\). The system’s equations of motion are then iteratively solved using a suitable algorithm to allow it to reach equilibrium. After equilibration, a production run is performed over many time steps to collect meaningful data. Finally, relevant averages, such as pressure or kinetic energy, are calculated from the collected data.\nMonte Carlo\nHowever, to obtain the equilibrium properties of the system, it may be much faster to use a dynamics which is nothing like the actual equations of motion.\nAt first sight, this looks very dangerous; however, if one can prove that in the required equilibrium distribution, the artificial dynamics obey the principle of detailed balance, then it is (almost) guaranteed that the steady state found by simulation is the true equilibrium state.\nThe best known example is the Monte Carlo method, in which the dynamical algorithm consists of random jumps. The jump rates \\(\\nu_{AB}\\) for all pairs of states \\((A, B)\\) take the form:\n\\[\n\\nu_{AB} = \\nu_0 \\quad \\text{if } E_B \\le E_A\n\\]\n\\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)} \\quad \\text{if } E_B \\ge E_A\n\\]\nwhere \\(\\nu_0\\) is a constant.\nExercise: Show that this gives the canonical distribution in steady state.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo show that this jump rate rule gives the canonical distribution in steady state, we assume the system reaches a steady-state probability distribution \\(P_A\\) for state \\(A\\) and apply the condition of detailed balance.\nDetailed balance requires: \\[\nP_A \\nu_{AB} = P_B \\nu_{BA}\n\\]\nAssume the canonical distribution: \\[\nP_A = \\frac{1}{Z} e^{-\\beta E_A}, \\quad P_B = \\frac{1}{Z} e^{-\\beta E_B}\n\\]\nNow consider two cases for \\(\\nu_{AB}\\) and \\(\\nu_{BA}\\):\nCase 1: \\(E_B \\leq E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0, \\quad \\nu_{BA} = \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nSubstitute into the detailed balance condition: \\[\nP_A \\nu_0 = P_B \\nu_0 e^{-\\beta (E_A - E_B)}\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A = P_B e^{-\\beta (E_A - E_B)}\n\\]\nUse canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} = \\frac{1}{Z} e^{-\\beta E_B} e^{-\\beta (E_A - E_B)} = \\frac{1}{Z} e^{-\\beta E_A}\n\\]\nVerified.\nCase 2: \\(E_B &gt; E_A\\)\nThen: \\[\n\\nu_{AB} = \\nu_0 e^{-\\beta (E_B - E_A)}, \\quad \\nu_{BA} = \\nu_0\n\\]\nSubstitute: \\[\nP_A \\nu_0 e^{-\\beta (E_B - E_A)} = P_B \\nu_0\n\\]\nCancel \\(\\nu_0\\): \\[\nP_A e^{-\\beta (E_B - E_A)} = P_B\n\\]\nAgain using canonical form: \\[\n\\frac{1}{Z} e^{-\\beta E_A} e^{-\\beta (E_B - E_A)} = \\frac{1}{Z} e^{-\\beta E_B} = P_B\n\\]\nVerified.\nHence, in both cases the detailed balance condition is satisfied with the canonical distribution, and the steady state is indeed the canonical distribution.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to stochastic processes</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html",
    "title": "11  The Langevin Approach",
    "section": "",
    "text": "11.1 The Random Walk and the Langevin equation\nThe concept of a random walk and its continuum limit – diffusion – introduced in the previous chapter, expresses the time evolution of the probability distribution \\(p(x, t)\\) for a particle’s position \\(x\\) by the diffusion equation:\n\\[\n\\frac{\\partial p}{\\partial t} = D \\frac{\\partial^2 p}{\\partial x^2},\n\\]\nwhich is a standard example of a so called Fokker-Planck equation, which is second-order in space and first-order in time.\nIn contrast, the Langevin equation provides a stochastic differential equation for the particle’s trajectory \\(x(t)\\). To understand it, consider the random hopping motion of a particle on a 1d lattice over a small time increment \\(\\Delta t\\):\n\\[\nx(t + \\Delta t) = x(t) + \\Delta x(t)\n\\]\nHere, \\(\\Delta x(t)\\) is a random displacement. If the lattice spacing is \\(a\\), we define the step statistics as:\n\\[\n\\Delta x(t) =\n\\begin{cases}\n+a & \\text{with probability } \\nu \\Delta t \\\\\n-a & \\text{with probability } \\nu \\Delta t \\\\\n0 & \\text{with probability } 1 - 2\\nu \\Delta t\n\\end{cases}\n\\]\nThis defines a discrete-time, discrete-space random walk. The average and variance of the step are easily derived using binomial statistics:\nThe steps \\(\\Delta x(t)\\) are uncorrelated across time.\nTo take the continuum limit, we let both \\(a \\to 0\\) and \\(\\Delta t \\to 0\\) in such a way that:\n\\[\na \\propto \\sqrt{\\Delta t}\n\\]\nIn this limit, we obtain the Langevin equation:\n\\[\n\\dot{x}(t) = \\eta(t)\n\\]\nwhere \\(\\eta(t)\\equiv\\frac{\\Delta x(t)}{\\Delta t}\\) is a stochastic noise satisfying:\n\\[\n\\langle \\eta(t) \\rangle = 0\n\\]\n\\[\n\\langle \\eta(t) \\eta(t') \\rangle = \\Gamma \\delta(t - t')\n\\]\nThis \\(\\eta(t)\\) is known as white noise — it has zero mean and is uncorrelated at different times. \\(\\Gamma\\) measures its amplitude.\nThe Langevin equation tells us that the velocity \\(\\dot{x}(t)\\) is purely driven by noise. We can formally integrate it:\n\\[\nx(t) - x_0 = \\int_0^t \\eta(t')\\, dt'\n\\]\nTaking ensemble averages:\nComparing this with the diffusion equation result, we identify:\n\\[\n\\Gamma = 2D\n\\]\nHence, the Langevin description yields the same physical behavior — not just the mean-square displacement but also the full probability distribution \\(p(x, t)\\) — as the diffusion (Fokker-Planck) equation. This equivalence arises from the fact that the integral of many small, independent random steps leads to a Gaussian distribution, in agreement with the solution of the diffusion equation.\nFor more details, see: Stochastic Processes in Physics and Chemistry by N.G. van Kampen (North Holland, 1981).",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#the-random-walk-and-the-langevin-equation",
    "title": "11  The Langevin Approach",
    "section": "",
    "text": "Mean: \\(\\langle \\Delta x \\rangle = 0\\)\nVariance: \\(\\langle (\\Delta x)^2 \\rangle = 2 a^2 \\nu \\Delta t = 2D \\Delta t\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean displacement: \\[\n\\langle x(t) - x_0 \\rangle = 0\n\\]\nMean square displacement: \\[\n\\langle [x(t) - x_0]^2 \\rangle = \\int_0^t \\int_0^t \\langle \\eta(t') \\eta(t'') \\rangle\\, dt'\\, dt'' = \\Gamma \\int_0^t dt' = \\Gamma t\n\\]",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "href": "phase-transitions/Brownian-and-Langevin-dynamics.html#brownian-motion",
    "title": "11  The Langevin Approach",
    "section": "11.2 Brownian Motion",
    "text": "11.2 Brownian Motion\nLet us now examine Brownian motion, originally observed as the erratic motion of colloidal particles suspended in a fluid. These particles undergo constant collisions with surrounding (smaller) fluid molecules, which results in seemingly random movement.\nFrom a coarse-grained perspective — where we do not track each individual collision — this appears as motion under random forces. This statistical treatment introduces irreversibility at the macroscopic level, even though the underlying molecular dynamics are reversible.\nThe Langevin equation provides a way to model this behavior. For a particle of mass \\(m\\) in one dimension, Langevin proposed the equation:\n\\[\nm \\ddot{x} = -\\gamma \\dot{x} + f(t)\n\\]\nHere:\n\n\\(-\\gamma \\dot{x}\\) is a frictional damping force, where \\(\\gamma\\) is the damping coefficient.\n\\(f(t)\\) is a random force due to molecular collisions.\n\n\nOften, the mobility is defined as \\(\\mu = 1/\\gamma\\) — note that this is unrelated to chemical potential.\n\n\n11.2.1 Noise Properties\nIn principle the random forces are correlated in time since the molecular collisions which cause them are correlated and have some definite duration.\nLet us assume that there is some correlation time \\(t_c\\) over which \\(\\langle f(t_1) f(t_2) \\rangle = g(t_1 - t_2)\\) decays rapidly as shown in the sketch below:\n\n\n\n\n\n\nFigure 11.1: Sketch of \\(g(t1−t2)\\) against \\(|t1− t2|\\)\n\n\n\nThen as long as we consider timescales \\(\\gg t_c\\) we can safely replace \\(g(t_1 - t_2)\\) by a delta function. Thus we can make the approximation of white noise\n\\[\n\\langle f(t) \\rangle = 0\n\\]\n\\[\n\\langle f(t_1) f(t_2) \\rangle = \\Gamma \\delta(t_1 - t_2)\n\\]\n\n\n11.2.2 Solving the Langevin Equation (velocity)\nLet’s set \\(m = 1\\) for simplicity and solve the equation:\n\\[\n\\dot{v} + \\gamma v = f(t)\n\\]\nWe apply an integrating factor:\n\\[\n\\frac{d}{dt} \\left[ v e^{\\gamma t} \\right] = e^{\\gamma t} f(t)\n\\]\nIntegrating both sides:\n\\[\nv(t) = v_0 e^{-\\gamma t} + \\int_0^t e^{-\\gamma (t - t')} f(t')\\, dt'\n\\]\nTaking the average:\n\\[\n\\langle v(t) \\rangle = v_0 e^{-\\gamma t}\n\\]\nThus:\n\nAt short times: (\\(\\gamma t \\ll 1\\)): \\(\\langle v \\rangle \\approx v_0\\) ie. friction is negligible.\nAt long times: (\\(\\gamma t \\gg 1\\)): \\(\\langle v \\rangle \\to 0\\) ie. the system loses memory of the initial velocity.\n\n\n\n11.2.3 Mean-square velocity\nWe now compute (do it as an exercise):\n\\[\n\\langle v(t)^2 \\rangle = v_0^2 e^{-2\\gamma t} + \\Gamma \\int_0^t e^{-2\\gamma (t - t')} dt' = v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma} \\left(1 - e^{-2\\gamma t} \\right)\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n&\\text{From the solution for }v(t)\\text{ (with }m=1\\text{):}\\quad\nv(t)=v_0 e^{-\\gamma t}+\\int_0^t e^{-\\gamma (t-t')}f(t')\\,dt'.\\\\[6pt]\n&\\Rightarrow\\;\nv(t)^2\n= v_0^2 e^{-2\\gamma t}\n+ 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} f(t')\\,dt'\n+ \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')} f(t')f(t'')\\,dt'\\,dt''.\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\langle v(t)^2\\rangle\n&= v_0^2 e^{-2\\gamma t}\n+ 2 v_0 e^{-\\gamma t}\\int_0^t e^{-\\gamma (t-t')} \\underbrace{\\langle f(t')\\rangle}_{=\\,0}\\,dt'\\\\\n&\\quad + \\int_0^t\\!\\!\\int_0^t e^{-\\gamma(2t-t'-t'')}\n\\underbrace{\\langle f(t')f(t'')\\rangle}_{=\\,\\Gamma\\,\\delta(t'-t'')}\\,dt'\\,dt''\\\\[6pt]\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma \\int_0^t e^{-2\\gamma (t-t')}\\,dt'\\\\[6pt]\n&= v_0^2 e^{-2\\gamma t}\n+ \\Gamma\\left[\\,-\\frac{1}{2\\gamma}e^{-2\\gamma (t-t')}\\,\\right]_{t'=0}^{t'=t}\\\\[6pt]\n&= v_0^2 e^{-2\\gamma t} + \\frac{\\Gamma}{2\\gamma}\\bigl(1-e^{-2\\gamma t}\\bigr).\n\\end{aligned}\n\\]\n\\[\n\\boxed{\\;\\langle v(t)^2\\rangle\n= v_0^2 e^{-2\\gamma t} + \\dfrac{\\Gamma}{2\\gamma}\\left(1-e^{-2\\gamma t}\\right)\\;}\n\\]\n\n\n\nImplying that at\n\nShort times: \\(\\langle v^2 \\rangle \\approx v_0^2\\)\nLong times: \\(\\langle v^2 \\rangle \\to \\Gamma / (2\\gamma)\\)\n\nAt equilibrium, the equipartition theorem gives:\n\\[\n\\frac{1}{2} m \\langle v^2 \\rangle = \\frac{1}{2} k_B T\n\\]\nUsing this to identify \\(\\Gamma\\):\n\\[\n\\Gamma = 2 \\gamma k_B T\n\\]\nThis important result relates the noise strength to the damping and temperature — they have the same microscopic origin (molecular collisions).\n\n\n11.2.4 Mean-square displacement\nWe now integrate \\(v(t)\\) again to get position \\(x(t)\\) (with \\(m = 1\\)):\nUsing the result above and substituting \\(\\Gamma = 2\\gamma k_B T\\), we find:\n\\[\n\\langle [x(t) - x_0]^2 \\rangle = \\frac{(v_0^2 - k_B T)}{\\gamma^2} (1 - e^{-\\gamma t})^2 + \\frac{2 k_B T}{\\gamma} \\left[ t - \\frac{1 - e^{-\\gamma t}}{\\gamma} \\right]\n\\]\nLimiting behaviours:\n\nShort times: (\\(\\gamma t \\ll 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx v_0^2 t^2\n\\]\n(correspinding to ballistic motion)\nLong time (\\(\\gamma t \\gg 1\\)):\n\\[\n\\langle [x(t) - x_0]^2 \\rangle \\approx \\frac{2 k_B T}{\\gamma} t\n\\]\n(corresponding to diffusive motion)\n\n\nThe effective diffusion constant is:\n\\[\nD = \\frac{k_B T}{\\gamma}\n\\]\nThis is the Einstein relation, connecting the rate of diffusion to temperature and damping. It is useful as it allows an explicit expression for the diffusion constant if one knows \\(\\gamma\\). A famous example is a sphere: the equation for fluid flow past a moving sphere may be solved and yields \\(\\gamma=6\\pi\\eta a\\) where \\(a\\) is the radius of the sphere and here \\(\\eta\\) is the fluid viscosity. This gives\n\\[\nD=\\frac{6\\pi\\eta a}{kT}\n\\] which is the Stokes-Einstein formula for the diffusion constant of a colloidal particle.\n\n\n11.2.5 External Forces and Mobility\nNow consider a charged particle with charge \\(q\\) under an external electric field \\(E\\). The Langevin equation becomes:\n\\[\nm \\dot{v} = -\\gamma v + qE\n\\]\nAt long times, the particle reaches a steady drift velocity:\n\\[\n\\langle v \\rangle = \\frac{qE}{\\gamma} = \\frac{qED}{k_B T}\n\\]\nDefining the mobility \\(\\mu\\) by \\(\\langle v \\rangle = \\mu qE\\), we get the Nernst-Einstein relation:\n\\[\n\\mu = \\frac{D}{k_B T}\n\\]\nThis relation connects the response of a system to an external perturbation (mobility) with its internal fluctuations (diffusivity).\n\n\n11.2.6 Molecular Dynamics simulation of Brownian motion for a colloid particle in a liquid suspension\n\n\n\nShow python code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom numba import njit\n\n# Parameters\nn_fluid = 300\nbox_size = 20.0\nn_steps = 10000\nsigma_f = 1.0\nsigma_c = 10.0\nepsilon = 0.05\nmass_f = 1.0\nmass_c = 2.0\ndt = 1e-5\ndt_e = 1e-5\n\n# Echo the parameter values\nprint(\"Simulation Parameters:\")\nprint(f\"n_fluid = {n_fluid}\")\nprint(f\"box_size = {box_size}\")\nprint(f\"n_steps = {n_steps}\")\nprint(f\"sigma_f = {sigma_f}\")\nprint(f\"sigma_c = {sigma_c}\")\nprint(f\"epsilon = {epsilon}\")\nprint(f\"mass_f = {mass_f}\")\nprint(f\"mass_c = {mass_c}\")\nprint(f\"dt = {dt}\")\n \n # Derived quantities\n\nsigma_f6 = sigma_f ** 6\nsigma_f12 = sigma_f **12\nsigma_cf = 0.5 * (sigma_f + sigma_c)\nsigma_cf6 = sigma_cf ** 6\nsigma_cf12 = sigma_cf ** 12\n\nnp.random.seed(42)\n\n# Safe initialization to avoid overlaps with colloid and other fluid particles\ndef initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos, min_dist_factor=0.85):\n    min_dist_ff = min_dist_factor * sigma_f\n    min_dist_cf = min_dist_factor * 0.5 * (sigma_f + sigma_c)\n    positions = []\n    max_attempts = 20000\n\n    for _ in range(n_fluid):\n        for attempt in range(max_attempts):\n            trial = np.random.rand(2) * box_size\n            too_close = False\n\n            # Check distance to colloid center\n            if np.linalg.norm(trial - colloid_pos[0]) &lt; min_dist_cf:\n                too_close = True\n\n            # Check distances to already placed fluid particles\n            for existing in positions:\n                if np.linalg.norm(trial - existing) &lt; min_dist_ff:\n                    too_close = True\n                    break\n\n            if not too_close:\n                positions.append(trial)\n                break\n        else:\n            raise RuntimeError(\"Failed to place a fluid particle without overlap after many attempts.\")\n    \n    return np.array(positions)\n\ncolloid_pos = np.array([[box_size / 2, box_size / 2]])\nfluid_pos = initialize_fluid_positions(n_fluid, box_size, sigma_f, sigma_c, colloid_pos)\nfluid_vel = (np.random.rand(n_fluid, 2) - 0.5)\ncolloid_vel = np.zeros((1, 2))\n\n@njit\ndef compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid):\n    forces_f = np.zeros_like(fluid_pos)\n    force_c = np.zeros_like(colloid_pos)\n\n    for i in range(n_fluid):\n        # Fluid-colloid interaction\n        rij = fluid_pos[i] - colloid_pos[0]\n        rij -= box_size * np.round(rij / box_size)\n        dist2 = np.dot(rij, rij)\n        if dist2 &lt; (2.5 ** 2) * ((sigma_cf6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n            r2 = dist2\n            r6 = r2 ** 3\n            r12 = r6 ** 2\n            fmag = 48 * epsilon * ((sigma_cf12 / r12) - 0.5 * (sigma_cf6 / r6)) / r2\n            fvec = fmag * rij\n            forces_f[i] += fvec\n            force_c[0] -= fvec\n\n        for j in range(i + 1, n_fluid):\n            rij = fluid_pos[i] - fluid_pos[j]\n            rij -= box_size * np.round(rij / box_size)\n            dist2 = np.dot(rij, rij)\n            if dist2 &lt; (2.5 ** 2) * ((sigma_f6 ** (1/6)) ** 2) and dist2 &gt; 1e-10:\n                r2 = dist2\n                r6 = r2 ** 3\n                r12 = r6 ** 2\n                fmag = 48 * epsilon * ((sigma_f12 / r12) - 0.5 * (sigma_f6 / r6)) / r2\n                fvec = fmag * rij\n                forces_f[i] += fvec\n                forces_f[j] -= fvec\n\n    return forces_f, force_c\n\n    fluid_history = []\ncolloid_history = []\nforces_f, force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n\nn_equilibration = 2000  # Number of steps to equilibrate before tracking\n\n# Equilibration phase (no history recorded)\n\nfor step in range(n_equilibration):\n    fluid_pos += fluid_vel * dt_e + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt_e + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt_e\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt_e\n    forces_f = new_forces_f\n    force_c = new_force_c\n    # if step &lt; 10:  # Log only the first few steps\n    #     force_mag = np.linalg.norm(force_c[0])\n    #     print(f\"Step {step:3d} | Colloid Pos: {colloid_pos[0]} | Vel: {colloid_vel[0]} | |F|: {force_mag:.4e}\")\n\n# Production phase (history recorded)\n\nfor step in range(n_steps):\n    fluid_pos += fluid_vel * dt + 0.5 * forces_f / mass_f * dt**2\n    colloid_pos += colloid_vel * dt + 0.5 * force_c / mass_c * dt**2\n    fluid_pos %= box_size\n    colloid_pos %= box_size\n    new_forces_f, new_force_c = compute_forces_numba(fluid_pos, colloid_pos, sigma_cf6, sigma_cf12, sigma_f6, sigma_f12, epsilon, box_size, n_fluid)\n    fluid_vel += 0.5 * (forces_f + new_forces_f) / mass_f * dt\n    colloid_vel += 0.5 * (force_c + new_force_c) / mass_c * dt\n    forces_f = new_forces_f\n    force_c = new_force_c\n    if step % 10 == 0:\n        fluid_history.append(fluid_pos.copy())\n        colloid_history.append(colloid_pos.copy())\n\nfig, ax = plt.subplots()\n# Calculate figure and plot scale parameters\nfig_width_inch = fig.get_size_inches()[0]\ndpi = fig.dpi\naxis_length_pt = fig_width_inch * dpi\nmarker_scale = 0.1  # Scale factor for visibility\nfluid_marker_size = (marker_scale * axis_length_pt / box_size) ** 2\ncolloid_marker_size = 0.7*(marker_scale * sigma_c / sigma_f * axis_length_pt / box_size) ** 2\nfluid_scatter = ax.scatter([], [], s=fluid_marker_size, c='blue')\ncolloid_scatter = ax.scatter([], [], s=colloid_marker_size, c='red')\ntrajectory, = ax.plot([], [], 'r--', linewidth=1, alpha=0.5)\nax.set_xlim(0, box_size)\nax.set_ylim(0, box_size)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_aspect('equal')\ncolloid_traj = []\n\ndef init():\n    empty_offsets = np.empty((0, 2))\n    fluid_scatter.set_offsets(empty_offsets)\n    colloid_scatter.set_offsets(empty_offsets)\n    trajectory.set_data([], [])\n    return fluid_scatter, colloid_scatter, trajectory\n\ndef update(frame):\n    fluid_scatter.set_offsets(fluid_history[frame])\n    colloid_scatter.set_offsets(colloid_history[frame])\n    colloid_traj.append(colloid_history[frame][0])\n    traj_array = np.array(colloid_traj)\n    trajectory.set_data(traj_array[:, 0], traj_array[:, 1])\n    return fluid_scatter, colloid_scatter, trajectory\n\nani = animation.FuncAnimation(fig, update, frames=len(fluid_history), init_func=init, blit=True, interval=20)\nani.save(\"brownian_colloid.mp4\", writer=\"ffmpeg\", fps=30)\nprint(\"Simulation complete. Video saved as 'brownian_colloid.mp4'.\")\n\n\nMore about the scientists mentioned in this chapter:\nPaul Langevin\nRobert Brown\nAlbert Einstein\nWalther Nernst\nGeorge Stokes",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Brownian and Langevin dynamics</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html",
    "href": "phase-transitions/dynamics-of-fluctuations.html",
    "title": "12  Dynamics of Fluctuations",
    "section": "",
    "text": "12.1 Linear Response Theory and the Fluctuation-Dissipation Theorem\nNow suppose we gently perturb the system. For example, we might apply a small thermodynamic force \\(f_x\\) that couples to a fluctuating variable \\(x\\) — like a weak magnetic field \\(h\\) acting on a local magnetization.\n(Formally, this means adding a perturbation \\(-f_x x\\) to the Hamiltonian.)\nA typical experimental protocol applies the perturbation from \\(t = -\\infty\\) and switches it off at \\(t = 0\\). For \\(t &gt; 0\\), the average response of another variable \\(y\\) is observed to decay:\n\\[\n\\langle y(t) \\rangle_f = R_{yx}(t) \\, f_x\n\\]\nHere, \\(R_{yx}(t)\\) is the response function describing how \\(y\\) responds to a small force applied to \\(x\\) at earlier times (for \\(t \\geq 0\\)).\nThe key idea is this: If the perturbation is small enough, its effects are indistinguishable from those of a spontaneous fluctuation. So, the decay of the response function should mirror the decay of the correlation function of naturally occurring fluctuations.\nThis is the essence of the fluctuation-dissipation theorem:\n\\[\nk_B T \\, R_{yx}(t) = M_{yx}(t)\n\\]\nThis powerful result says that the system’s response to a small disturbance is directly related to the correlation of fluctuations in thermal equilibrium.\nThe factor \\(k_B T\\) ensures both sides of the equation have the same physical dimensions.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dynamics of fluctuations (non-examinable)</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html#linear-response-theory-and-the-fluctuation-dissipation-theorem",
    "href": "phase-transitions/dynamics-of-fluctuations.html#linear-response-theory-and-the-fluctuation-dissipation-theorem",
    "title": "12  Dynamics of Fluctuations",
    "section": "",
    "text": "Figure 12.1: Eﬀect of perturbation on a quantity which is zero in equilibrium\n\n\n\n\n\n\n\n\n\nNote: We skip the full proof, which requires formal machinery from classical mechanics (e.g., Poisson brackets) or quantum mechanics (density matrices). For more, see the final chapter of Chandler.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dynamics of fluctuations (non-examinable)</span>"
    ]
  },
  {
    "objectID": "phase-transitions/dynamics-of-fluctuations.html#onsagers-theorem",
    "href": "phase-transitions/dynamics-of-fluctuations.html#onsagers-theorem",
    "title": "12  Dynamics of Fluctuations",
    "section": "12.2 Onsager’s Theorem",
    "text": "12.2 Onsager’s Theorem\nNow, recall from the previous section that the correlation matrix is symmetric:\n\\[\nM_{xy}(t) = M_{yx}(t)\n\\]\nCombining this with the fluctuation-dissipation theorem gives:\n\\[\nR_{xy}(t) = R_{yx}(t)\n\\]\nThis result is known as Onsager’s reciprocal relation. It states that the response of variable \\(x\\) to a force acting on \\(y\\) is the same as the response of \\(y\\) to a force acting on \\(x\\) — provided the system is in equilibrium.\nThis is a deep and subtle consequence of microscopic reversibility, and Onsager’s real contribution was realizing such a connection could exist. (Onsager also solved the 2D Ising model analytically and won the Nobel Prize in Chemistry in 1968.)\n\n\n\n\n\n\n\n\n\n\n\n(a) Lars Onsager\n\n\n\n\n\n\n\n\n\n\n\n(b) Onsager’s gravestone and one-up-man-ship with colleague John Kirkwood\n\n\n\n\n\n\n\nFigure 12.2: Norwegian Physicist and Nobel Laureate, Lars Onsager",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dynamics of fluctuations (non-examinable)</span>"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html",
    "href": "phase-transitions/problems.html",
    "title": "Unifying concepts: Problems",
    "section": "",
    "text": "Although you should try all of these questions, some of them are deliberately quite challenging. If you don’t get very far with some, don’t worry. We’ll be going over them in problems classes, so you can just regard them as worked examples.\n\n1. Existence of a phase transition in \\(d=2\\).\nIn lectures it was argued that no long ranged order occurs at finite-temperatures in a one dimensional system because of the presence of domain walls. Were macroscopic domain walls to exist in two dimensions at finite temperature, they would similarly destroy long ranged order and prevent a phase transition. By calculating the free energy of a 2D domain wall for an Ising lattice, show that domain walls do not in fact exist for sufficiently low \\(T\\).\n(Hint: Model the domain wall as a non-reversing \\(N\\)-step random walk on the lattice and find an expression for its energy and -from the number of random walk configurations- its entropy.)\n\n\n\n2. Correlation Length\nFor a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small (as occurs at low \\(T\\))?\n\n\n\n3. A model fluid\nThe van der Waals (vdW) equation of state is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nUse a graph-plotting program to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of \\(P\\) vs \\(V\\) on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\n4. Mean field theory of the Ising model heat capacity\nUsing results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle\n  E\\rangle/\\partial T\\) has the behaviour\n\\[\\begin{aligned}\nC_H=& 0 ~~~~ T&gt;T_c\\\\\nC_H=& 3Nk_B/2 ~~~~ T\\le T_c\n\\end{aligned}\\]\n\n\n\n5. Magnetisation and fluctuations\nA system of spins on a lattice, has, in the absence of an applied field, a Hamiltonian \\({\\cal H}\\). In the presence of a field \\(h\\) the Hamiltonian becomes \\[\n\\tilde {\\cal H}={\\cal H}-hM\n\\] where \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\n\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\n\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\n6. Spin-1 Ising model\nA set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j + h\\sum_i s_i\\]\nFind the partition function and hence show that in the mean field approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\n7. Transfer Matrix.\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\n8. Landau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\n9. Scaling equation of state\nConsider a Landau expression for the free energy of a magnetic system having magnetisation \\(m\\):\n\\[\nF=F_0+\\tilde{a}_2tm^2+a_4m^4-Hm\\:,\n\\] where \\(t=T-T_c\\) and \\(H\\) is an applied magnetic field; \\(\\tilde{a}_2\\) and \\(a_4\\) are positive constants and \\(F_0\\) is a constant background term.\nShow that the equation of state for the model is\n\\[\nH=2\\tilde{a}_2tm+4a_4m^3\\:.\n\\]\nUse the near-critical power law behaviour of \\(m\\) to show that the equation of state may be written in the scaling form\n\\[\n\\frac{H}{m^\\delta}=g\\left(\\frac{t}{m^{1/\\beta}}\\right)\\:,\n\\] and find the (mean field) values of the critical exponents \\(\\delta\\) and \\(\\beta\\).\nDeduce that \\(g(x)=x+1\\) up to a choice of scale for \\(\\tilde{a}_2\\) and \\(a_4\\).\n\n\n\n10. Scaling laws\nUsing the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\\]\nHence derive the scaling laws among the critical exponents:\n\\[\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\\]\n(Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 9: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\n\n\n\n11. Classical nucleation theory\nA supercooled liquid metal is undergoing solidification. According to classical nucleation theory, the Gibbs free energy change \\(\\Delta G\\) for forming a spherical solid nucleus of radius \\(r\\) in the liquid is given by:\n\\[\n\\Delta G(r) = \\frac{4}{3}\\pi r^3 \\Delta G_v + 4\\pi r^2 \\gamma\n\\] where \\(\\Delta G_v &lt; 0\\) is the free energy change per unit volume due to the phase change, and \\(\\gamma &gt; 0\\) is the interfacial energy between the solid and liquid phases.\n(a) Derive the expression for the critical radius \\(r^*\\) at which the nucleus becomes stable and begins to grow.\n(b) Show that the critical energy barrier for nucleation \\(\\Delta G^*\\) is given by:\n\\[\n\\Delta G^* = \\frac{16\\pi \\gamma^3}{3 (\\Delta G_v)^2}\n\\]\n(c) Explain qualitatively how the degree of undercooling \\(\\Delta T\\) affects the rate of nucleation. You may use the fact that \\(\\Delta G_v \\propto \\Delta T\\) to support your answer.\n\n\n\n12. Colloidal diffusion\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + e^{-\\gamma t} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{kT}{M} + \\left( u_0^2 - \\frac{kT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\n\n13. Einstein’s expression for the diffusion coefficient\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle [X(t) - X(0)]^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nUse the fact that \\(X(t) - X(0)\n= \\int_{0}^{t} u(t')\\,\\mathrm{d}t'\\) to show that the Einstein relation may be written as\n\\[\n\\gamma = \\frac{1}{\\mu} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\n\n14. Life in one dimension\nA particle lives on the sites of a one-dimensional lattice. At any instant it has probability \\(\\alpha\\) per unit time that it will hop to the site on its right and probability \\(\\alpha\\) per unit time of hopping to the site on its left.\nWrite down the master equation for the set of probabilities \\(p_n(t)\\) of finding the particle at the \\(n^{\\text{th}}\\) site, where \\(-\\infty &lt; n &lt; \\infty\\).\nSolve the master equation for the \\(p_n\\), subject to the initial condition that the particle was at the site \\(n = 0\\) at time \\(t = 0\\). Hence obtain the mean position \\(\\langle n \\rangle\\) and root mean square deviation from the mean, both as functions of time.\nHint: The second part of the question is most easily done by introducing the generating function\n\\[\nF(z, t) = \\sum_{n=-\\infty}^{\\infty} p_n(t) z^n.\n\\]\n\n\n\n15. Master equation\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\n\n16. Detailed balance\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\n\n17. Jump processes\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump from state 1 to 2 and 2 to 3 but not directly from 1 to 3. Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the rate matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html",
    "href": "phase-transitions/Solutions_to_problems.html",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "",
    "text": "1. Existence of a phase transition in \\(d=2\\).\nHere we present outline solutions to the problems.\nConsider the simplest elementary excitation that will destroy long range order in the 2d system: a domain wall of \\(N\\) segments which divides an Ising system of \\(L\\times L\\) spins into a spin up and a spin down part.\nThe associated energy cost is \\(2JN \\equiv \\Delta E\\).\nTo evaluate the entropy gain due to a domain wall in the system we have to estimate \\(\\Omega\\) the number of possible paths for the domain wall. If we start at the left hand side then there are \\(L\\) starting positions. At each step the domain wall can move to the right, move up or move down. This implies that the number of domain walls is approximately\n\\[\n\\Omega\\approx L3^N\n\\] Hence the entropy gain is:\n\\[\n\\Delta S=Nk_B\\ln 3+k_B\\ln L\\approx Nk_B\\ln 3  \n\\]\nAccordingly, the change in the free energy associated with inserting such a domain wall into an ordered system is\n\\[\n\\Delta F=\\Delta E-T\\Delta S= N(2J-k_BT\\ln 3)\n\\]\nFor small enough \\(T &lt;2J/(k_B\\ln 3)\\), the free energy change is positive. Thus the ordered phase is free energetically stable against formation of a wall. Accordingly there will be a non zero value for \\(T_c\\) in two dimensions.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#existence-of-a-phase-transition-in-d2.",
    "href": "phase-transitions/Solutions_to_problems.html#existence-of-a-phase-transition-in-d2.",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "",
    "text": "Figure 1: An \\(N\\)-step domain wall in an Ising lattice.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#correlation-length",
    "href": "phase-transitions/Solutions_to_problems.html#correlation-length",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "2. Correlation Length",
    "text": "2. Correlation Length\nDenote by \\(m\\) the number of domain walls between sites \\(i\\) and \\(j\\). Then \\(s_is_j=1\\) for \\(m\\) even, and \\(s_is_j=-1\\) for \\(m\\) odd.\nHence\n\\[\n\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\n\\] with \\(p_m\\) the probability of finding \\(m\\) domain walls between them.\nNow \\(p_m\\) is given by the binomial distribution, with the probability of a single domain wall at each bond given by\n\\[\np=\\frac{e^{-2J/k_BT}}{1+e^{-2J/k_BT}}\n\\] and the probability of no wall is \\(1-p\\). Now, in the regime where \\(T\\) is small, \\(p\\) is very small, and there will be few domain walls between sites \\(i\\) and \\(j\\). If additionally, \\(R_{ij}=|i-j|a\\) is large, it tranpires that the binomial distribution assumes the limiting form of a Poissonian distribution (revise this if necessary). Thus\n\\[\np_m=\\frac{\\bar{m}^me^{-\\bar {m}}}{m!}\n\\]\nwhere \\(\\bar{m}=p|j-i|=pR_{ij}/a\\) . Then\n\\[\\begin{eqnarray*}\n\\langle s_i s_j\\rangle &=& e^{-\\bar {m}}\\sum_m\\frac{(-1)^m\\bar{m}^m} {m!}\\approx e^{-2\\bar{m}}\\\\\n                       &=& e^{-2pR_{ij}/a}\\\\\n                       &=& e^{-R_{ij}/\\xi}\n\\end{eqnarray*}\\] with \\(\\xi=a/2p\\), the correlation length.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#a-model-fluid",
    "href": "phase-transitions/Solutions_to_problems.html#a-model-fluid",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "3. A model fluid",
    "text": "3. A model fluid\nThe van der Waals (vdW) equation of state (See Sec 4.4.1 of the book by Yeomans) is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\n\\left(P+\\frac{a}{V^2}\\right)(V-b)=Nk_BT\n\\] where \\(a\\) and \\(b\\) are constants chosen to describe a specific substance and \\(N\\) is Avogadro’s number. Hence\n\\[\nP=\\frac{Nk_BT}{V-b}-\\frac{a}{V^2}\n\\tag{1}\\]\n\\[\n\\Rightarrow \\frac{\\partial P}{\\partial V}=\\frac{-Nk_BT}{(V-b)^2}+\\frac{2a}{V^3}\n\\]\n\\[\n\\Rightarrow \\frac{\\partial^2 P}{\\partial V^2}=\\frac{2Nk_BT}{(V-b)^3}-\\frac{6a}{V^4}\n\\]\nNow at criticality (ie. a continuous transition).\n\\[\n\\left(\\frac{\\partial P}{\\partial V}\\right)_T=\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\n\\]\nThus \\[\\begin{eqnarray*}\n\\frac{Nk_BT}{(V_c-b)^2} &=& \\frac{2a}{V_c^3}\\\\\n\\frac{2Nk_BT}{(V_c-b)^3} &=& \\frac{6a}{V_c^4}\n\\end{eqnarray*}\\] solving for \\(V_c\\) and \\(Nk_BT_c\\) yields\n\\[\\begin{eqnarray*}\nV_c &=& 3b\\\\\nNk_BT_c &=& \\frac{8a}{27b}\n\\end{eqnarray*}\\]\nSubstituting these two results into Equation 1 yields\n\\[\nP_c=\\frac{a}{27b^2}\n\\]\nNow let \\(P=P_c p, V=V_cv, T=T_ct\\) in the vdW eqn. (Note that in this context \\(t\\) is not the reduced temperature).\n\\[\n\\left(P_cp+\\frac{a}{(V_cv)^2}\\right)(V_cv-b)=N_Ak_BT_ct\n\\]\nSubstituting in for \\(V_c, N_Ak_BT_c\\) and \\(P_c\\)\n\\[\\begin{eqnarray*}\n\\left(p\\frac{a}{27b^2}+\\frac{a}{9b^2v^2}\\right)\\left(3bv-b\\right)&=&\\frac{8a}{27b}t\n\\Rightarrow\\left(p+\\frac{3}{v^2}\\right)\\left(v-\\frac{1}{3}\\right)&=&\\frac{8}{3}t\n\\end{eqnarray*}\\]\nThis expression for the equation of state in terms of reduced variables is useful because reference to the system specific parameters \\(a\\) and \\(b\\) has vanished. In this form the equation is therefore universal.\nPlotting \\(P/P_c\\) vs \\(V/V_c\\) for isotherms (values of \\(t\\)) and focussing on the region close to the critical point, one finds\n\n\n\n\n\n\nFigure 2: Isotherms of \\(p\\) versus \\(v\\) for various \\(t\\) spanning the critical temperatures\n\n\n\n\n\n\n\n\n\nFigure 3: (a) \\(\\frac{\\partial p}{\\partial v}\\) for \\(T=T_c\\). (b) \\(\\frac{\\partial^2 p}{\\partial v^2}\\) for \\(T=T_c\\).\n\n\n\nPlotting \\((\\frac{\\partial p}{\\partial v})_{t=1}\\) and \\((\\frac{\\partial^2 p}{\\partial\nv^2})_{t=1}\\), we see that there is indeed a point of inflexion on the critical isotherm, at \\(v=1\\), this is the critical point (ie. a continuous phase transition), Figure 3 .\nSubcritical isotherms (first order phase transition) exhibit a so called van-der Waals loop.\nTo find the compressibility critical exponent \\(\\gamma\\), we recall that\n\\[\n\\kappa_T=\\frac{-1}{V}\\left(\\frac{\\partial V}{\\partial P}\\right)_T=\\frac{-1}{p_cv}\\left(\\frac{\\partial v}{\\partial p}\\right)_t\\propto \\tilde{t}^{-\\gamma}\n\\] with \\(\\tilde{t}=(T-T_c)/T_c\\) small.\nNow from the reduced equation of state\n\\[\n\\frac{\\partial p}{\\partial v}=\\frac{-8t}{3(v-1/3)^2}+\\frac{6}{v^3}\n\\] setting \\(t=\\tilde{t}+1\\) and \\(v=1\\) gives \\(\\frac{\\partial p}{\\partial v} =-6\\tilde{t}\\), ie the compressibility diverges\n\\[\n\\kappa_T\\propto \\tilde{t}^{-1}\n\\] ie. \\(\\gamma=1\\), which is the same as the mean field result which we derived in another context of the magnetic susceptibility.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#mean-field-theory-of-the-ising-model-heat-capacity",
    "href": "phase-transitions/Solutions_to_problems.html#mean-field-theory-of-the-ising-model-heat-capacity",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "4. Mean field theory of the Ising model heat capacity",
    "text": "4. Mean field theory of the Ising model heat capacity\nWe insert into the expression for the mean Ising energy\n\\[\n\\langle E \\rangle =-J\\sum_{&lt;i,j&gt;}\\langle s_is_j\\rangle\\:,\n\\] the simplest mean field approximation \\(\\langle s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Recalling the behaviour of the order parameter for small \\(t\\), that the number of bonds \\(=qN/2\\), and the mean field value of \\(T_c=qJ/k_B\\), we have for \\(T&lt;T_c\\)\n\\[\\begin{eqnarray*}\n\\langle E \\rangle &=& \\frac{-NqJm^2}{2}\n\\:                &=& \\frac{3NqJt}{2}\n\\:                &=& \\frac{3Nk_B(T-T_c)}{2}\n\\end{eqnarray*}\\] while \\(\\langle E \\rangle= {\\rm constant}\\) for \\(T&gt;T_c\\).\nHence differentiating, we find \\[\\begin{eqnarray*}\nC_H&=& 0 &~~~~~T&gt;T_c\\\\\nC_H&=& 3Nk_B/2 & ~~~~~T\\le T_c\n\\end{eqnarray*}\\]\nThis independence of the heat capacity on \\(t\\) corresponds to a critical exponent \\(\\alpha=0\\)",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#magnetisation-and-fluctuations",
    "href": "phase-transitions/Solutions_to_problems.html#magnetisation-and-fluctuations",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "5. Magnetisation and fluctuations",
    "text": "5. Magnetisation and fluctuations\nThe free energy is\n\\[\nF=-k_BT\\ln Z\n\\] with the partition function\n\\[\nZ=\\sum_{{s}}\\exp[-({\\cal H}-hM)/k_BT]\n\\]\nThus \\[\\begin{eqnarray*}\n-\\left(\\frac{\\partial F}{\\partial h}\\right)_T &=& k_BT\\frac{1}{Z}\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\\\\n\\: &=&\\frac{1}{Z}\\sum_{{s}}M \\exp[-({\\cal H}-hM)/k_BT]\\\\\n   &=& \\langle M\\rangle\n\\end{eqnarray*}\\] where we have used the definition of the average of an observable given in lectures.\nNow\n\\[\\begin{eqnarray*}\n\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T &=& -k_BT\\left[\\frac{1}{Z}\\left(\\frac{\\partial^2 Z}{\\partial h^2}\\right)_T-\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\frac{1}{Z^2}\\left(\\frac{\\partial Z}{\\partial h}\\right)_T\\right]\\\\\n\\: &=&\\frac{-1}{k_BT}\\left[\\frac{1}{Z}\\sum_{{s}}M^2 \\exp[-({\\cal H}-hM)/k_BT]-\\langle M\\rangle^2\\right]\\\\\n\\:   &=& \\frac{-1}{k_BT}\\left[\\langle M^2\\rangle-\\langle M\\rangle^2\\right]\n\\end{eqnarray*}\\]\nYou should recognise the terms in square brackets as the variance of the magnetisation distribution.\nThus the susceptibility is \\[\n\\chi_H\\equiv\\frac{\\partial \\langle M\\rangle}{\\partial h}=\\frac{1}{k_BT}\\left[\\langle M^2\\rangle-\\langle M\\rangle^2\\right]\n\\]\nIncidently, this is known as the fluctuation-dissipation theorem. It is a neat result, because it allows you to calculate the response to a perturbation from equilibrium, without actually perturbing the system! Instead one merely looks at the form of the equilibrium fluctuations. It is used extensively in computer simulations.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#spin-1-ising-model",
    "href": "phase-transitions/Solutions_to_problems.html#spin-1-ising-model",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "6. Spin-1 Ising model",
    "text": "6. Spin-1 Ising model\nAs in lectures, the mean field Hamiltonian for a single spin is\n\\[\n{\\cal H}(s_0)=-s_0\\left(qJm + H\\right)\n\\] The probability of finding this spin with value \\(s_0\\) is \\[\\begin{eqnarray*}\np(s_0) &=& \\frac{e^{-\\beta{\\cal H}(s_0)}} {\\sum_{s_0=0,\\pm 1}e^{-\\beta{\\cal H}(s_0)}}\\\\\n&=&\\frac{e^{\\beta s_0(qJm+H)}}{1+e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\n\\end{eqnarray*}\\]\nNow for consistency \\(\\langle s_0\\rangle=m\\), so \\[\\begin{eqnarray*}\nm &=& \\sum_{s_0=0,\\pm 1}s_0p(s_0)\\\\\n\\:&=& \\frac{0+e^{\\beta(qJm+H)}-e^{\\beta(qJm+H)}} {e^0+e^{\\beta(qJm+H)}+e^{-\\beta(qJm+H)}}\\\\\n\\:&=&  \\frac{2\\sinh[\\beta(Jqm+h)]}{1+2\\cosh[\\beta(Jqm+h)]}\n\\end{eqnarray*}\\] To get the critical temperature, we can solve this graphically. One plots the RHS as a function of \\(m\\), for various \\(\\beta\\). On the same graph one plots the curve \\(y=m\\) (representing the LHS). \\(T_c\\) is the highest \\(T\\) for which the two curves intersect.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#transfer-matrix",
    "href": "phase-transitions/Solutions_to_problems.html#transfer-matrix",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "7. Transfer Matrix",
    "text": "7. Transfer Matrix\nThe transfer matrix is a list of the possible interactions of a pair of spins with one another and with a magnetic field. For a 1d spin-1/2 system it takes the form: \\[\n{\\bf V}(H)=\\begin{bmatrix}\ne^{\\beta(J+H)} & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}\n\\end{bmatrix}\n\\] We need to find the eigenvalues, so we solve the characteristic equation det(\\({\\bf V}-\\lambda {\\bf I})=0\\), i.e.\n\\[\n{\\bf V}(H)=\\begin{vmatrix}\ne^{\\beta(J+H)} -\\lambda & e^{-\\beta J} \\\\\ne^{-\\beta J}   & e^{\\beta(J-H)}-\\lambda\n\\end{vmatrix} =0\n\\]\nThen \\(\\lambda^2-(a+d)\\lambda+(ad-bc)=0\\). So\n\\[\\begin{eqnarray*}\n\\lambda_\\pm &=& \\frac{a+d\\pm\\sqrt{(a+d)^2-4(ad-bc)}}{2}\\\\\n\\lambda_{\\pm} &=& e^{\\beta J}\\cosh(\\beta H) \\pm \\frac{1}{2}\\sqrt{e^{2\\beta J}4\\cosh^2\\beta H-4(e^{2\\beta J}-e^{-2\\beta J})}\\\\\n\\lambda_{\\pm} &=& e^{\\beta J}\\cosh(\\beta H) \\pm \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}.\n\\end{eqnarray*}\\]\n(You’ll need the identity \\(\\cosh^2 x-\\sinh^2 x = 1\\)).\nFrom lectures, you should know that the partition function \\[\\begin{eqnarray*}\nZ={\\rm Tr}({\\bf V}^N)&=&\\lambda_+^N+\\lambda_-^N\\\\\n&\\approx & \\lambda_+^N \\hspace{5mm}{\\rm N~large}\n\\end{eqnarray*}\\] where \\(\\lambda_+\\) is the largest of the two evals.\nHence the free energy \\(F=-k_BT\\ln(Z)\\) can be written\n\\[\nF=-Nk_BT\\ln \\left[e^{\\beta J}\\cosh(\\beta H) + \\sqrt{e^{2\\beta J}\\sinh^2\\beta H+e^{-2\\beta J}}\\right].\n\\]\nNow the magnetisation per site is\n\\[\nm=-\\frac{1}{N}\\frac{\\partial F}{\\partial H}=\\frac{k_BT}{\\lambda_+}\\frac{\\partial \\lambda_+}{\\partial H}\n\\]\nYou can either be a hero here, or use a symbolic solution program like Maple or Wolfram Alpha. I did the latter to find the stated result.\n\\[\nm=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{(-4\\beta J)}}}\n\\]\nHence at zero \\(H\\), there is no spontaneous magnetisation at any \\(T\\).",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#landau-theory",
    "href": "phase-transitions/Solutions_to_problems.html#landau-theory",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "8. Landau theory",
    "text": "8. Landau theory\nIf this were an Ising model problem (ie a microscopic model) we could write down the partition function, get an explicit expression for the free energy and differentiate once (wrt \\(T\\)) to get the energy and again (wrt \\(T\\)) to get the heat capacity. But the starting point for Landau theory is the free energy itself, so we need another starting point, namely thermodynamics. The appropriate thermodynamic potential for the magnet is \\(F=U-TS-MH\\) with \\(U\\) the internal energy. Then\n\\[\\begin{eqnarray*}\ndF &=& dU-TdS-SdT-MdH-HdM\\\\\n\\: &=& TdS+HdM-TdS-SdT-MdH-HdM\\\\\n\\: &=& -SdT-MdH\n\\end{eqnarray*}\\] where we have used the first law for a magnet \\(dU=TdS+HdM\\),\nThus\n\\[\\begin{eqnarray*}\n\\left(\\frac{\\partial F}{\\partial T}\\right)_H &=& -S\\\\\n-T\\left(\\frac{\\partial^2 F}{\\partial T^2}\\right)_H &=&T\\frac{dS}{dT}=\\frac{dQ}{dT}\\\\ C_H=-T\\left(\\frac{\\partial^2 F}{\\partial T^2}\\right)_H\n\\end{eqnarray*}\\]\nwhere \\(C_H\\) is the specific heat at constant field and we have used the fact that \\(dS=dQ/T\\).\nNow from lectures, the equilibrium magnetisation in the Landau free energy is given by\n\\[\nm^2=\\frac{-a_2}{2a_4}\n\\] for \\(T&lt;T_c\\) and zero otherwise. Substituting this into the Landau free energy \\(F=F_0+a_2m^2+a_4m^4\\) gives\n\\[\\begin{eqnarray*}\nF = F_0 & ~~~~~ T&gt;T_c\\\\\nF = -a_2^2/4a_4 &~~~~~ T &lt; T_c\n\\end{eqnarray*}\\]\nUsing the fact that \\(a_2=\\tilde{a_2} t\\), with \\(t=(T-T_c)/T_c\\) and differentiating wrt \\(T\\) twice, to get the heat capacity, we find\n\\[\\begin{eqnarray*}\nC_H &=& 0 &~~~~~ T\\to T_c^+\\\\\nC_H &=& \\frac{T\\tilde a_2^2}{2a_4T_c^2} & ~~~~~~ T \\to T_c^-\\:,\n\\end{eqnarray*}\\] The jump discontinuity rather that a divergence in the specific heat at \\(T=T_c\\) formally corresponds to a critical exponent \\(\\alpha=0\\).\nTo get the susceptibility exponent, we add a magnetic field to the free energy\n\\[\nF(m)=F_0+a_2m^2+a_4m^4-Hm\n\\]\nThen the equilibrium magnetisation satisfies\n\\[\\begin{eqnarray*}\n\\frac{dF}{dm}&=&2\\tilde{a_2} tm+4a_4m^3-H=0\\\\\n\\Rightarrow H&=&2\\tilde{a_2} tm+4a_4m^3\\\\\n\\Rightarrow \\left(\\frac{\\partial H}{\\partial m}\\right )_T&=&2\\tilde{a_2} t+12a_4m^2\n\\end{eqnarray*}\\] Now using the results that \\(m^2=0\\) for \\(t&gt;0\\) and \\(m^2=-\\tilde{a_2}t/(2a_4)\\) for \\(t&lt;0\\), we have that in both cases\n\\[\n\\left(\\frac{\\partial H}{\\partial m}\\right )_T\\propto t\n\\]\nHence\n\\[\n\\left(\\frac{\\partial m}{\\partial H}\\right )_T\\propto t^{-1}\n\\] so \\(\\gamma=1\\).",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#scaling-laws",
    "href": "phase-transitions/Solutions_to_problems.html#scaling-laws",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "9. Scaling laws",
    "text": "9. Scaling laws\nFirst of all recall the definition of the critical exponents: \\[\\begin{eqnarray*}\nm      & \\propto & t^{\\beta};&~~~~~(h=0) \\\\\n\\chi_T & \\propto & t^{-\\gamma};&~~~~~(h=0) \\\\\nC_H & \\propto & t^{-\\alpha};&~~~~~(h=0) \\\\\nm & \\propto & h^{1/\\delta}.&~~~~~(t=0) \\\\\n\\end{eqnarray*}\\] The free energy in generalised homogeneous form is\n\\[\nF(\\lambda^a t,\\lambda^b h)=\\lambda F(t,h)\n\\]\nThe first of the scaling relations to be derived was covered in lectures: Let \\(\\lambda^a=1/t\\), so that \\(\\lambda=t^{-1/a}\\). Then\n\\[\\begin{eqnarray*}\nF(t,h)&=&t^{1/a}F(1,t^{-b/a}h)\\\\\nm(t,h)&=&-\\left(\\frac{\\partial F}{\\partial h}\\right)_t= -t^{(1-b)/a} \\left.\\frac{\\partial F(1,y)}{\\partial y}\\right|_{ht^{-b/a}}=t^{(1-b)/a}m(1,ht^{-b/a})\n\\end{eqnarray*}\\]\nso when \\(h=0\\), we have \\(m(1,t^{-b/a}h)=m(1,0)={\\rm const}\\) and hence we can identify \\(\\boxed{\\beta=(1-b)/a}\\).\nWe also have for the isothermal susceptibility\n\\[\n\\chi=\\left(\\frac{\\partial m}{\\partial\nh}\\right)_t=-t^{(1-2b)/a}\\left.\\frac{\\partial^2 F(1,y)}{\\partial y^2}\\right|_{ht^{-b/a}},\n\\] so taking again \\(h=0\\), we find \\(\\boxed{\\gamma=(2b-1)/a}\\).\nFor the specific heat at constant (zero) field, we have the definition: \\[\nC_H = \\left(\\frac{\\partial E}{\\partial T}\\right)_{h=0}=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\:,\n\\] where in the last step have used \\(E=-\\partial (\\beta F)/\\partial\\beta\\), with \\(\\beta=(k_BT)^{-1}\\) (see fig X). Alternatively one can use the thermodynamic derivation of this relation given in an earlier problem on Landau theory. Transforming from \\(T\\) to \\(t=(T-T_c)/T_c\\) and inserting the generalised homogeneous form for \\(F\\) gives:\n\\[\\begin{eqnarray*}\nC_H &=& -\\frac{T}{T_c^2}\\frac{\\partial^2}{\\partial t^2}[t^{1/a}F(1,t^{-b/a}h)]\\\\\nC_H &\\approx& -\\frac{1}{T_c}\\frac{\\partial^2}{\\partial t^2}[t^{1/a}F(1,t^{-b/a}h)]\\\\\nC_H &=& -\\frac{1}{T_c}\\frac{1}{a}(\\frac{1}{a}-1)t^{(1/a-2)}F(1,0)\n\\end{eqnarray*}\\] Here we have neglected all derivatives of \\(F\\) since they are multiplied by at least one power of \\(h\\) which is zero. Hence \\(\\boxed{\\alpha=2-1/a}\\).\nFinally, if we let \\(\\lambda^b=1/h\\), so that \\(\\lambda=h^{-1/b}\\) and consider the critical isotherm \\(t=0\\). Then\n\\[\\begin{eqnarray*}\nF(t,h)&=&h^{1/b}F(h^{-a/b}t,1)\\\\\n\\Rightarrow m(t,h)&=& \\frac{ta}{b}h^{(1-a-b)/b}\\left.\\frac{\\partial F(x,1)}{\\partial x}\\right|_{h^{-a/b}t}-\\frac{1}{b}h^{1/b-1}F(h^{-a/b}t,1).\n\\end{eqnarray*}\\] so when \\(t=0\\), we get \\(m(0,h)\\) and can identify \\(\\boxed{\\delta=b/(1-b)}\\).\nTo derive the relationships (``scaling laws’’) among the critical exponents, we eliminate \\(a\\) and \\(b\\) from the boxed scaling relations. Setting \\(a=(2-\\alpha)^{-1}\\) in the first scaling relation, we find \\(b=1-\\beta/(2-\\alpha)\\). Substituting this into the second scaling relation gives the second of the two scaling laws quoted in the notes. Substituting into the 4th scaling relation gives the first scaling law.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#colloidal-diffusion",
    "href": "phase-transitions/Solutions_to_problems.html#colloidal-diffusion",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "10. Colloidal diffusion",
    "text": "10. Colloidal diffusion\nWe start from the given formal solution of the Langevin equation for the velocity:\n\\[\nu(t) = u_0 e^{-\\gamma t} + e^{-\\gamma t} \\int_{0}^{t} e^{\\gamma t'} F(t') \\, dt'.\n\\]\nwith a goal to compute the mean square velocity \\(\\langle u^2(t)\\rangle\\).\nLet \\[\nA = u_0 e^{-\\gamma t},\n\\qquad\nB = e^{-\\gamma t} \\int_{0}^{t} e^{\\gamma t'} F(t') \\, dt'.\n\\] Then \\[\n\\langle u^2(t)\\rangle = \\langle (A + B)^2 \\rangle\n= \\langle A^2 \\rangle + 2\\langle A B \\rangle + \\langle B^2 \\rangle.\n\\]\n\n\\(\\langle A^2 \\rangle = u_0^2 e^{-2\\gamma t}.\\)\n\n\\(\\langle A B \\rangle = u_0 e^{-\\gamma t} \\, e^{-\\gamma t} \\int_0^t e^{\\gamma t'} \\langle F(t')\\rangle \\, dt' = 0\\)\n(since we assume \\(\\langle F(t)\\rangle = 0\\)).\n\nFor the noise term, assume \\[\\langle F(t)F(t')\\rangle = q \\,\\delta(t - t').\\]\nThen \\[\n\\langle B^2 \\rangle\n= e^{-2\\gamma t} \\int_{0}^{t}\\int_{0}^{t}\n  e^{\\gamma t'} e^{\\gamma t''} \\langle F(t')F(t'')\\rangle \\, dt' dt''\n= e^{-2\\gamma t}\\,q \\int_{0}^{t} e^{2\\gamma t'} \\, dt'\n= \\frac{q}{2\\gamma} \\bigl(1 - e^{-2\\gamma t}\\bigr).\n\\]\n\nPutting these together yields \\[\n\\boxed{\n\\langle u^2(t)\\rangle\n= u_0^2 e^{-2\\gamma t}\n\\;+\\;\n\\frac{q}{2\\gamma}\\,\\bigl(1 - e^{-2\\gamma t}\\bigr).\n}\n\\]\nNow relate the noise strength \\(q\\) to temperature\nAt long times \\(t\\to\\infty\\) the particle reaches thermal equilibrium, so by equipartition \\[\n\\tfrac12 M \\langle u^2 \\rangle_{\\infty}\n= \\tfrac12 kT\n\\;\\Longrightarrow\\;\n\\langle u^2 \\rangle_{\\infty}\n= \\frac{kT}{M}\n= \\frac{q}{2\\gamma}\n\\;\\Longrightarrow\\;\nq = 2\\gamma\\,\\frac{kT}{M}.\n\\]\nSubstituting back gives the full time‐dependent result:\n\\[\n\\boxed{\n\\langle u^2(t)\\rangle\n= \\frac{kT}{M}\n\\;+\\;\\Bigl(u_0^2 - \\frac{kT}{M}\\Bigr)\\,e^{-2\\gamma t}.\n}\n\\]",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html",
    "href": "phase-transitions/assignment_October2025.html",
    "title": "PHYSM0071: First coursework assignment",
    "section": "",
    "text": "1. Introduction and background\nIn this coursework assignment, you will explore the phenomena of spatial correlations and their relationship to phase behaviour in a simple lattice gas model. This model is a crude representation of a fluid in which particles can occupy the sites of a hypercubic lattice. The occupancy of a site \\(i\\) is specified by the variable \\(c_i=1\\) (occupied) or \\(c_i=0\\) vacant. The complete list of these occupancies \\(\\{c\\}\\) specifies a microstate. The instantaneous particle number density (fraction of occupied sites) is given by\n\\[\n\\rho=L^{-d}\\sum_i c_i\n\\]\nwhere \\(L\\) is the linear extent of the lattice and \\(d\\) its dimensionality.\nWithin the Grand Canonical ensemble (see 1.1.3 Grand canonical ensemble) the Hamiltonian of the lattice gas model is\n\\[{\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j - \\mu\\sum_ic_i\\]\nwhere \\(\\epsilon\\) is an attraction energy between a pair of particles on adjacent (nearest neighbouring) sites and \\(\\mu\\) is a field known as the chemical potential, which couples to the particle density which is assumed to fluctuate around a mean value controlled by the prescribed chemical potential. One can also fix the particle number to some prescribed value- the correspponding set of microstates then define the canonical ensemble (see 1.1.2 Canonical ensemble) of the lattice gas at that density.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "href": "phase-transitions/assignment_October2025.html#mapping-between-lattice-gas-and-ising-model",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.1 Mapping between lattice gas and Ising model",
    "text": "1.1 Mapping between lattice gas and Ising model\nThe lattice gas model is interesting because whilst being a plausible model for a fluid, it maps onto the Ising model. We say that they are isomorphic to one another. This isomorphism extends the applicability of the Ising model. To expose the mapping we write the grand partition function of the lattice gas:\n\\[ \\Xi=\\sum_\\textrm{ state}\\exp-\\beta{\\cal H}_{LG}=\\sum_{\\{c\\}}\\exp\\left[\\beta \\epsilon\\sum_{&lt;i,j&gt;}c_ic_j +\\beta\\mu\\sum_ic_i\\right] \\] where the sum is an unrestricted sum over the occupancies of the lattice sites.\nWe now change variables to\n\\[c_i=(1+s_i)/2; ~~~~ J=\\frac{\\epsilon}{4} ~~~~\nh=\\frac{\\epsilon q+2\\mu}{4},\n\\] where \\(q\\) is the lattice coordination number.\nOne finds (see assignment below),\n\\[{\\cal H}_{LG}={\\cal H}_\\textrm{ I} + \\textrm{ constant}\\] Since the last term does not depend on the configuration, it feeds through as an additive constant in the free energy; and since all observables feature as derivatives of the free energy, the constant has no physical implications.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#phase-diagram",
    "href": "phase-transitions/assignment_October2025.html#phase-diagram",
    "title": "PHYSM0071: First coursework assignment",
    "section": "1.2 Phase diagram",
    "text": "1.2 Phase diagram\nUsing these translation rules we can plot the phase diagram of the lattice gas in the plane of average number density and temperature.\n\n\n\n\n\n\nFigure 1.1: Phase diagram of the lattice gas model in the density-temperature plane.\n\n\n\nIn the \\(\\mu-T\\) plane there is a line of first order phase transitions terminating at a critical point which has density \\(\\rho_c=0.5\\). The first order line means that if \\(T&lt;T_c\\) and starting from the gas phase we smoothly increase the chemical potential through the coexistence value of \\(\\mu\\), the average number density of particles on our lattice \\(\\rho=N/L^d\\) jumps discontinuously from a low value $rho_$ to a high value \\(\\rho_\\textrm{liquid}\\). This is the gas-liquid phase transition. The values of \\(\\rho_\\textrm{gas}\\) and \\(\\rho_\\textrm{liquid}\\) merge at \\(T_c\\), the gas-liquid critical temperature. At higher temperatures, the distinction between the phases disappears.\n\n\n\n\n\n\n1.3 Aside on Real Fluids\n\n\n\n\n\nYou may wish to compare the phase diagram of the lattice gas mode with the results of (say) van der Waals equation (see recommended textbooks for the required phase diagram). The main difference is that the lattice gas has so-called “particle-hole” symmetry, \\(\\rho\\to 1-\\rho\\) (inherited from the up-down symmetry of the Ising model) which is not present for a real fluid. Accordingly, the phase diagram in a real fluid looks like a lopsided version of the above picture as shown in Figure 1.2. See here for some real experimental data showing the asymmetry of the coexistence curve in liquid metals.\n\n\n\n\n\n\nFigure 1.2: Schematic of the liquid-gas phase diagram in the \\(\\rho-T\\) plane for a realistic fluid .",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc",
    "href": "phase-transitions/assignment_October2025.html#grand-canonical-monte-carlo-gcmc",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.1 Grand Canonical Monte Carlo (GCMC)",
    "text": "2.1 Grand Canonical Monte Carlo (GCMC)\n\nSelect a random lattice site \\((i,j)\\).\nIf the site is empty, propose inserting a particle. If it is occupied, propose removing the particle.\nCompute the change in energy \\(\\Delta E\\) and particle number \\(\\Delta N = \\pm 1\\).\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta \\Phi &lt; 0, \\\\\ne^{-\\beta \\Delta \\Phi} & \\text{otherwise},\n\\end{cases}\n\\]\nwhere \\(\\Delta \\Phi = \\Delta E - \\mu \\Delta N\\) is the change in the grand potential.\nThis allows the system to explore states with varying particle numbers according to grand canonical statistics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "href": "phase-transitions/assignment_October2025.html#fixed-particle-number-case-canonical-ensemble",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.2 Fixed-Particle-Number Case (Canonical Ensemble)",
    "text": "2.2 Fixed-Particle-Number Case (Canonical Ensemble)\nIn contrast, we may for simplicity want to simulate with the total number of particles \\(N\\) fixed, as in the canonical ensemble. Then particle insertions and deletions are not allowed and the chemical potential term drops out of the Hamiltonian. We can use Kawasaki dynamics, which conserves particle number by moving particles between lattice sites:\n\nSelect two sites \\((i,j)\\) and \\((i',j')\\) at random.\nIf one site is occupied and the other is empty, propose exchanging the particle and vacancy.\nCompute the energy change \\(\\Delta E = E(C') - E(C)\\) associated with this proposed particle move.\nAccept the move with probability:\n\n\\[\nW(C \\to C') =\n\\begin{cases}\n1 & \\text{if } \\Delta E &lt; 0, \\\\\ne^{-\\beta \\Delta E} & \\text{otherwise}.\n\\end{cases}\n\\]\nThis approach ensures particle number is conserved while still allowing the system to explore equilibrium configurations at fixed \\(N\\).",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "href": "phase-transitions/assignment_October2025.html#observables-and-measurement",
    "title": "PHYSM0071: First coursework assignment",
    "section": "2.3 Observables and measurement",
    "text": "2.3 Observables and measurement\nA Monte Carlo sweep in either ensemble involves performing \\(L^2\\) update attempts on a lattice of size \\(L \\times L\\).\nThe expectation value of a macrovariable \\(O\\) (e.g., energy, particle density, correlation functions) is estimated by averaging over sampled configurations:\n\\[\n\\overline O \\approx \\frac{1}{{\\cal N}} \\sum_{n=0}^{\\cal N} O_n,\n\\]\nwhere \\(O_n\\) is the value measured in configuration \\(C_n\\), and \\({\\cal N}\\) is the total number of configurations generated.\nOther observables (those which are second derivatives of the free energy) such as the specific heat capacity can’t be calculated as simple averages. Instead they are calculated as a variance. The formula for the specific heat (see problem sheets) is\n\\[\nC=\\frac{1}{kT^2} \\left[\\overline{E^2} - \\overline{E}^2\\right] = \\frac{(\\Delta E)^2}{k_BT^2}\n\\]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#setup",
    "href": "phase-transitions/assignment_October2025.html#setup",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThis should be completed in the week prior to the release of the assignment to make sure that any technical problems are resolved.\n\nOn the PHYSM0071 Blackboard page open the Unit Information and Resources tab\nScroll down to Notable and open it (if off campus make sure you have the UoB VPN enabled)\nSelect the Jupyter Notebook (Legacy) notebook server option\nWhen the notebook has opened click the +Gitrepo button\nUnder enter Git Repository insert: https://github.com/nbwilding/Lattice-gas-coursework\nPress the “clone” button. This will download a notebook called lattice-gas.ipynb into Jupyter\nCheck that the program runs\nFamiliarise yourself with the main features of the program. Pay attention to how to change the temperature and number of lattice sites \\(N=L^2\\). Be aware that the program can take several minutes to run depending on the system size.\n\nThe assignment itself will be released on Monday 13th October 2025 on Blackboard via the Assessment, Submission and Feedback tab. The submission deadline is Monday 27th October 2025 at 09:30.\nPlease contact me (nigel.wilding@bristol.ac.uk) if you have trouble with the setup described above, detailing the problem you encountered.\nThe various elements of the assignment are set out below. Instructions are given in italics.",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "href": "phase-transitions/assignment_October2025.html#sec-isomorph",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.2 Isomorphism of the lattice gas and Ising model",
    "text": "3.2 Isomorphism of the lattice gas and Ising model\nBy referring to section 1.1 above, show that the Hamiltonian of the Lattice Gas model in the grand canonical ensemble\n\\[\n    {\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j -\\mu\\sum_ic_i\n\\] is transformed to that of the Ising model by means of the change of variable\n\\[\n    s_i=2c_i-1;~~~~ J=\\frac{\\epsilon}{4};~~~~\n    h=\\frac{\\epsilon q+2\\mu}{4}.\n\\] where \\(q\\) is the lattice coordination number (\\(q=4\\) in 2-dimensions and \\(q=6\\) in 3d).\n(Hint: Note that when doing sums over bonds \\(\\sum_{\\langle i,j\\rangle}\\) for a lattice of coordination \\(q\\) there are \\(q/2\\) bonds per site since each bond is shared between two sites.) [4 marks]\nGiven that the critical temperature of the 2d Ising model is \\(T_c\\approx 2.269\\: J/k_B\\), use the above mapping to find the value of the critical temperature of the 2d lattice gas model in units of \\(\\epsilon/k_B\\). [2 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "href": "phase-transitions/assignment_October2025.html#code-modification-tasks",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.3 Code modification tasks",
    "text": "3.3 Code modification tasks\nThe provided program simulates the 2D lattice gas model in the canonical ensemble (i.e., at a fixed particle number density \\(\\rho\\)), using the Kawasaki swap algorithm as described above. It allows the user to specify the number of lattice sites and the temperature for the simulation. The program computes the pair correlation function (also known as the radial distribution function) \\(g(r)\\) and the structure factor \\(S(k)\\) (refer to Chapter 3 for details).\nIn this implementation, the particle density is fixed at \\(\\rho = \\rho_c = 0.5\\). This choice ensures that the system approaches the critical point as the temperature is lowered within the supercritical regime, \\(T_c \\le T \\le 1.0\\). For convenience, the program uses dimensionless units by setting \\(\\epsilon = k_B = 1\\).\n\nReview the program thoroughly and gain a clear understanding of its functionality.\nModify the program to compute and output the radially averaged structure factor \\(S(|k|)\\). (Hint: When averaging, ensure proper normalization by accounting for the area of each radial bin.) [6 marks]\nAdd a function to calculate and print the dimensionless total energy and specific heat of the system. For the latter you will need the expression given in section 2.2, above.\n[5 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "href": "phase-transitions/assignment_October2025.html#computational-investigations-of-correlations-on-the-approach-to-criticality",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.4 Computational investigations of correlations on the approach to criticality",
    "text": "3.4 Computational investigations of correlations on the approach to criticality\n\nPerform simulations at various temperatures within the super range \\(T_c \\le T \\le 1.0\\), keeping the particle density fixed at \\(\\rho = 0.5\\). For each temperature, save the final configuration and describe how the overall structural characteristics evolve with temperature. [4 marks]\nPlot and compare the radial distribution function \\(g(r)\\) and the structure factor \\(S(k)\\), and its radially averaged form, at each temperature. [4 marks]\nInterpret the peak structure in the structure factor and discuss signs of clustering or ordering in these quantities. [3 marks]\nThe full width at half maximum (FWHM) of the radially averaged structure factor can be used to estimate the correlation length \\(\\xi\\) through \\(\\xi=1/FWHM\\). Plot the temperature dependence of \\(\\xi\\) and comment on its behaviour. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "href": "phase-transitions/assignment_October2025.html#temperature-and-system-size-dependence-of-the-specific-heat",
    "title": "PHYSM0071: First coursework assignment",
    "section": "3.5 Temperature and system size dependence of the specific heat",
    "text": "3.5 Temperature and system size dependence of the specific heat\nExamine how the specific heat capacity varies with dimensionless temperature over a broad range, including both subcritical (i.e. \\(T&lt;T_c\\)) and supercritical regimes. Describe the overall behavior of the specific heat across this temperature range. . How does the specific heat vary across the whole temperature range? [4 marks]\nRepeat the analysis for different system sizes. Compare the results and highlight any observed differences. Discuss possible explanations for these size-dependent effects. [4 marks]",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/assignment_October2025.html#your-report",
    "href": "phase-transitions/assignment_October2025.html#your-report",
    "title": "PHYSM0071: First coursework assignment",
    "section": "4.0 Your report",
    "text": "4.0 Your report\nYou should produce a short skeleton report (upto 4 sides of A4 including figures and in no less than 11 pt fontsize) focussing on and summarising the results of the above investigations and your comments/observations on the findings.\nAs well as a title and your name, the report should be laid out using the headings above:\n\nIsomorphism of the lattice gas and Ising model\nCode modifications. Include snippets of your modified code, highlighting the modifications in yellow. Do not include the whole code, just enough to see what you changed and where you did it.\n\nComputational investigations of correlations on the approach to criticality\nTemperature and system size dependence of the specific heat\n\nThere is no need for any other section headings ie. you don’t need abstract, introduction, references etc. Marks will be deducted for reports that exceed four sides (faces) of A4.\nSubmit your report in pdf format for grading via blackboard. The report will be marked out of 40, and will count for \\(25\\%\\) of the unit mark.\nLike any lab report, the marking focus will be on:\n\nDo your results look physically plausible?\nDid you describe the pertinent features?\nAre your descriptions clear?\nWhere appropriate, do you explain them correctly using appropriate concepts?",
    "crumbs": [
      "Unifying concepts",
      "First coursework assignment"
    ]
  },
  {
    "objectID": "phase-transitions/live-test.html",
    "href": "phase-transitions/live-test.html",
    "title": "Coding",
    "section": "",
    "text": "We provide you with a simple package able to run an Ising model simulation.\n\n\n\n\n\n\nYou can access directly the comfigurations as numpy arrays\n\n\n\n\n\n\nQuestion. Can you write a function to calculate the energy of a configuration?",
    "crumbs": [
      "Unifying concepts",
      "Coding"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_intro.html",
    "href": "soft-matter/soft-matter_intro.html",
    "title": "12  Entropy matters",
    "section": "",
    "text": "12.1 Systems and definitions\nMost of the matter around us does not simply fit within the idealised pictures of crystalline solids or simple liquids: examples include colloids, polymers, surfactants, liquid crystals, foams, gels, and biological materials such as proteins, DNA, and cell membranes.\nThis means that cellular life itself (the very constituents that make us) obeys to principles that go beyond the standard patters of conventional solid-state physics.\nThis branch of physics is called soft condensed matter physics, or macromolecular physics, or the physics of complex fluids. Specifically, sof matter refers to an area of condensed matter focused on systems that can be easily deformed.\nIn this course, we will emphasize the fact that many such systems are not crystalline: thermal noise and disordered configurations play a key role in their phase behavior, and hence we think of them as complex disordered systems.\nWhile we often think about problems in physics as a matter of energy minimisation, in soft-matter physics a key role is played by the fluctuations. Typically (but not exclusively) these are thermal fluctuations. This means that entropy and not only the energy from the interactions plays a key role.\nThis is because soft matter systems are typically composed of many microscopic constituents in contact with an environment. The appropriate description of the macroscopic state of such systems is therefore statistical and uses the language of statistical mechanics. The relevant energy, therefore, is the free energy of the statistical ensemble representative of the system under consideration. For example, in the canonical ensemble, this is the Helmholtz free energy\n\\[F = U-TS\\]\nwhere \\(U\\) is the internal energy, \\(T\\) the temperature and \\(S\\) is the entropy of the system. Therefore, in a broader sense, soft matter is the physics of those systems for which the internal energy and the entropy are on comparable scales.\nIn other words, fluctuations of the internal energy are on the same scale as thermal fluctuations:\n\\[\\Delta U \\sim k_BT \\]\nwhere \\(k_B\\) is the Bolztmann constant and \\(\\Delta U\\) indicates standard deviations from the average internal energy.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Entropy matters</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_intro.html#systems-and-definitions",
    "href": "soft-matter/soft-matter_intro.html#systems-and-definitions",
    "title": "12  Entropy matters",
    "section": "",
    "text": "12.1.1 Elementary constituents and energy scales\nSoft matter covers a wide spectrum of deformable systems. Each is constituted of many parts . Each is deformable because the interactions amongst such parts are weak compared to the perturbing forces (e.g. thermal fluctuations or mechanical loading).\nIn hard condensed matter, the elementary constituents are the atoms themselves, eventually with their subatomic particles. Between atoms, the scale of the interaction energies is in the 0.1 to 10 eV: for example the carbon-carbon covalent bond is approximately 3.6 eV.\nThe main units of soft matter are not atoms. They are instead themselves aggregates of atoms such as:\n\ncolloids, micrometer- to nanometer-sized particles dispersed in a fluid.\npolymers, macromolecules composed of long chains such as DNA, proteins, plastics\nsurfactants, macromolecules wuth polar head and tails that lead to the spontaneous formation of structures such as bilayers (e.g. the cellular membrane).\n\nAmongst such units, the dominating forces of soft matter are much weaker than in hard condensed matter:\n\nVan der Waals forces are of the order of 0.001-0.01 eV\nweak interactions such as hydrogen-bonds are typically in the 0.01-0.2 eV range\nthe thermal energy at room temperature is \\(k_B T \\approx 0.025 eV\\) (check it for yourself)\n\nAt the microscopic level, all these interactions have essentialy one source: the electrostatic force. However, this information is practically of no use when we want to understand how the units of soft matter come together to give rise to macroscopic properties of soft matter systems, such as their elastic properties, their viscosities, their plasticity. In fact, the emphasis on the atomistic details of the various units is fundamentally misleading: atomistically very different objects (e.g. colloids and micelles) can in fact share very similar macroscopic behaviours.\nTherefore, theories of soft matter leverage the concept of coarse-graining, rooted in the renormalisation group notions explored earlier. Coarse-graining means integrating out the unimportant degrees of freedom and only describing the units in terms of a few important parameters. For example, instead of taking a full atomistic representation of the DNA we may want to focus on the fact that structurally it is a long chain with specific bending energies: we may want to include the fact that it is formed by a double helix but we may not want to specifically construct every single atom in the sugar chain the forms the backbone. An example of DNA coarse garining is provided by the succesful model oxDNA (see picture below):\n\n\n\noxDNA model: (a) Base structure on one strand; (b) planarity of the bonding; (c) an example of the resulting double strand.\n\n\n\n\n\n\n\n\nCoarse graining\n\n\n\n\n\nCoarse graining is often motivated by intuition, experimental insights or a simple desire of simplification. It allows for a multi-scale description of the problem which permits to describe large systems and log time scales.\nNonetheless, coarse-graining can also be made mathematically rigorous. For example, rigorous coarse graining in statistical mechanics can be performed by projecting the dynamics onto a relevant and an irrelevant (fluctutating) part, in the so called Mori-Zwanzig formalism (Mori 1965).\n\n\n\n\n\n12.1.2 Classes of systems\nIn our exploration of soft matter we will focus on six main classes of systems which display different physics:\n\ncolloidal dispersions\npolymeric systems\nliquid crystals\nsurfactant aggregates\narrested systems\nactive matter\n\n\nColloidal dispersions\nColloidal dispersions are systems where small particles, typically in the nanometer to micrometer range, are dispersed in a continuous medium (a solvent). Prototypical colloids are spherical particles of various sizes (e.g. as those present in paint) but colloidal science has achieved a high degree of sophistication, with colloidal particles with various different shapes and interactions.\nColloids are often thought as big atoms: they exhibit Brownian motion, can form ordered structures (colloidal crystals), and display phase transitions similar to atomic systems. However, their larger size and slower dynamics make them ideal for studying phenomena that are difficult to observe in atomic systems.\n\n\nPolymeric systems\nPolymer physics is a field on its own. Polymers are macromolecules composed of repeating structural units (monomers) connected by covalent bonds. Their unique properties arise from their string-like structure and the interplay between entropic and energetic contributions.\nPolymers can be classified into two main categories:\n\nSynthetic polymers, including plastics (e.g., polyethylene, polystyrene) and synthetic rubbers.\nBiopolymers, such as polymers like DNA, RNA, and proteins.\n\n\nimport py3Dmol #install this with: pip install py3dmol\n\npdb_id = \"1XQ8\"  # Human micelle-bound alpha-synuclein, an analog of long polymers \n# Fetch the PDB structure from the RCSB PDB server\nview = py3Dmol.view(query='pdb:' + pdb_id)\n# Customize visualization (you can change the style or the color)\nview.setStyle({}, {'sphere': {'color': 'spectrum'}});\nview.setBackgroundColor('white')  # Set background color to white\nview.show()\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages.\n        \n\n3D Rendering of the bio-polymer synuclein as extracted from the RCSB Protein Data Bank (click to rotate)\n\n\nCompared to colloids, polymers have distiunctive characteristics common to all chain-like molecules, such as their topological constraints due to teh fact that two polymers cannot cross each other (a phenomenon known as polymer entanglement).\n\n\nLiquid crystals\nWhen we take soft matter units that are highly anisotropic (e.g. elongate in particular directions) thermal fluctuations and high packing lead to equilibrium states woth a degree of order that is intermediate between the complete disorder of a liquid and the long-ranged, three-dimensional order of a crystal.\nSuch states are referred to as liquid crystal and can be described successffully with continuum free energy theories that take into account the symmetries of the order parameters.\nComponents that form liquid crystals are called mesogens and include highly anisotropic organic macromolecules (as used in liquid crystal displays), rod-like molecules or polymeric aggregates, as well as disk-shaped molecules and particles(such as triphenylene and derivatives).\n\n\nSurfactant aggregates\nWhen two distinct fluid phases are put into contact, a free energy cost per unit area ensues: this is the surface tension. It is possible to control the tension by introducing molecules that sit at the interface between the two phases, called surfactants.\nHence, surfactants are molecules which contain chemical groups with different affinities (they are amphiphilic). A key example is the case of phospholipids, which posses both hydrophilic (water-preferring) heads and hyrdrophobic (water avoiding) tails. As surfactants sit at inetrface they are able to self-assemble and separate different fluid phases, forming equilibrium bilayers and vesicles that are ubiquitous in cell biology.\n\n\nArrested systems\nWe have stressed that the thermal energy is distinctive of soft matter systems. It would be then natural to assume that as we reduce the temperature, we should converge readily to the solid state physics of crystalline solids. In fact, on the way to low temperatures, the lack of long-range order of most soft-matter systems has important consequences: many such systems find themselves trapped in states that are not corresponding to the global energy minimum (i.e. the crystal) and instead display non trivial mechanisms of structural relaxation. These systems are disordered a bit like liquids, but share various mechanical properties, such as emergence rigidity, elasticity and plasticity.\nExamples of arrested systems include:\n\nGlasses, such as silica glass or metallic glasses, where the system is kinetically trapped in a disordered state.\nGels, which are networks of interconnected particles or polymers that span the entire system, providing rigidity despite being mostly liquid.\n\nThese systems are arrested as their relaxation towards equilibrium is so slow that is longer than any observable timescales. This makes them fundamentally out-of-equilibrium systems, escaping from an ordinary description in terms of equilibrium statistical mechanics.\n\n\nActive matter\nActive matter refers to systems composed of units that consume energy to generate motion or mechanical stresses. Unlike passive systems, active matter is inherently out of equilibrium due to the continuous energy input at the microscopic level. Examples include:\n\nBiological systems, such as bacterial colonies, cell tissues, and flocks of birds.\nSynthetic systems, like self-propelled colloids or active gels.\n\nThe study of active matter focuses on understanding how individual activity leads to emergent collective behaviors, such as swarming, clustering, or pattern formation, often described using hydrodynamic theories or agent-based models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMori, Hazime. 1965. “Transport, Collective Motion, and Brownian Motion.” Progress of Theoretical Physics 33 (3): 423–55. https://bris.on.worldcat.org/oclc/8091001741.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Entropy matters</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html",
    "href": "soft-matter/soft-matter_colloids.html",
    "title": "15  Colloids",
    "section": "",
    "text": "15.1 Kinds of colloids\nColloids are mixtures where one substance is dispersed throughout another. They consist of particles that are larger than typical molecules but small enough to remain suspended without settling. Examples include milk, fog, and paint.\nColloids can be classified based on the state of the dispersed phase and the dispersion medium. Depending on the particular mixture, one can obtain a wide variety of soft materials, with unique mechanical, optical, and thermal properties.\nClearly, colloidal materials form an incredibly diverse class, and surround us in our everyday lives. Here are some visual examples from the table above:\nThe IUPAC definition of colloids is based on the idea that the particles are dispersed in a medium, creating a subdivision at the colloidal scale: approximately 1nm to 1µm.\nThe small sizes of colloids means that they are constantly subject to the collisions with the atom/molecules/particle of the medium, triggered by thermal fluctuations. Due to this, the colloids undergo Brownian motion. For each particle, the amount of energy received from the medium is of the order of \\(k_B T\\) (the reference energy scale of thermal soft matter). This energy can be compared with the the potential energy to produce a dimensionless number, the gravitational Péclet number\n\\[{\\rm Pe}_g = \\dfrac{\\Delta m g R}{k_B T}\\]\nwhere \\(R\\) is the radius of the colloid respectively, \\(g\\) is the acceleration due to gravity, and \\(\\Delta m\\) is the buoyant mass, which for a spherical colloid is expressed as \\(\\Delta m = \\dfrac{4\\pi}{3}\\Delta\\rho R^3\\), with \\(\\Delta \\rho\\) being the density difference between the particle and the dispersion medium (the solvent). We have a colloidal suspension only when \\({\\rm Pe}_g \\lesssim 1\\), i.e. when Brownian motion is only marginally perturbed by the effects of gravity.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#kinds-of-colloids",
    "href": "soft-matter/soft-matter_colloids.html#kinds-of-colloids",
    "title": "15  Colloids",
    "section": "",
    "text": "Dispersion Phase\n\nDispersion Medium\n\n\n\n\n\nSolid\nLiquid\n\n\nSolid\nSolid suspension:\npigmented plastics,\nstained glass,\nruby glass, opal, pearl\nSol, colloidal suspension:\nmetal sol,\ntoothpaste, paint,\nink, clay slurries, mud\n\n\nLiquid\nSolid emulsion:\nbituminous road paving,\nice cream\nEmulsion:\nmilk, mayonnaise,\nbutter, pharmaceutical creams\n\n\nGas\nSolid foam:\nzeolites, expanded polystyrene,\n‘silica gel’\nFoam:\nfroths, soap foam,\nfire-extinguisher foam\n\n\n\n\n\n\n\nFirst row: opal, paint, smoke. Middle row: ice-cream (gelato), milk, fog. Bottom row: expanded polystyrene and foam. Source: unsplash.com",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#stability-of-colloids-and-colloid-colloid-interactions",
    "href": "soft-matter/soft-matter_colloids.html#stability-of-colloids-and-colloid-colloid-interactions",
    "title": "15  Colloids",
    "section": "15.2 Stability of colloids and colloid-colloid interactions",
    "text": "15.2 Stability of colloids and colloid-colloid interactions\nA colloidal dispersion is said to be stable if it is able to remain dispersed and Brownian for a long time (typically, significantly longer than the experimental observation time). Unstable colloids undergo aggregation or sedimentation due to the dominance of attractive forces or gravity.\nFor example, consider a colloidal suspension like milk. Milk is an emulsion where fat droplets are dispersed in water and stabilised by proteins. If lemon juice (an acid) is added, the dispersion medium (water) changes, the pH drops, and the emulsion is destabilised. This causes the proteins to coagulate, leading to the separation of curds (solid) and whey (liquid).\n\n\n\nCurds and whey resulting from the destabilisation of milk, a colloidal emulsion. (Wikimedia)\n\n\nIt is clear from this example that the nature of the dispersed phase and the dispersed medium ultimately determine the stability of colloidal dispersions. What ultimately matters for the stability is whether the colloids have a propensity to aggregate or not. This propensity is quantified in terms of colloidal interactions.\n\n15.2.1 Fundamental and effective forces\nAt the colloidal scales, only two kinds of fundamental forces are relevant: gravity and electro (and occasionally magneto) static forces. As we have seen above, when a system is truly colloidal, gravitational contributions are assumed to be small, so in effect for a non magnetic colloid (which is the vast majority of colloidal systems) only electrostatic forces are fundamentally important.\nHowever, colloidal dispersions consist of large particles carrying many charges both in the dispersed phase and the surrounding medium, arranged disorderly at the atomic scale. This makes a microscopic description of all charges and the resulting electrostatic fields not only unfeasible but also ineffective for understanding colloidal systems in terms of key characteristics such as particle size, density, and spatial distribution. The fundamental issue is the one of time-scales: the motion of colloids is much slower than the motion of individual ions or molecules. Over such longer timescales, many interactions at the microscopic atomistic scale take place and we can think of taking averages to extract macroscopic, effective interactions at the colloidal scale 1.\nThis is especially important due to quantum fluctuations: the uncertainty principle means that electron clouds around atoms are not fixed but exhibit intrinsic fluctuations in their charge distribution. Perturbative approaches allow us to capture the effective forces resulting from such fluctuations.\nThere are many examples of such effective forces (Lekkerkerker, Tuinier, and Vis 2024). One you may already know is the Van der Waals interaction. For colloids, we have additional relevant forces, such as the double layer interaction and the depletion interaction. We detail them here below.\n\n\n15.2.2 Van der Waals interaction\nThe London-van der Waals dispersion forces arise from the interaction between instantaneous dipoles in overall neutral atoms or molecules. We know from classical electrostatics that static dipoles interact via the dipole-dipole interaction, with a potential strength which decays like \\(1/r^3\\), where \\(r\\) is the separation between two dipoles.\nMore in general, neutral atom or molecules have electronic clouds that are fluctuating and not symmetrically distributed, creating a temporary dipole. Such instantaneous dipole induces a dipole in a neighboring atom or molecule by distorting its electron cloud.\nThe interaction energy between two dipoles \\(\\mu_1\\) and \\(\\mu_2\\) separated by a distance \\(r\\) is proportional to: \\[\nU \\propto -\\frac{\\mu_1 \\mu_2}{r^3}\n\\]\n\n\n\nThe dipole moments fluctuate due to quantum mechanical effects. The average interaction energy is derived using perturbation theory and is proportional to the polarisabilities \\(\\alpha_1\\) and \\(\\alpha_2\\) of the two particles:\n\n\\[\n    U(r) = -\\frac{C}{r^6}\n\\]\n\nwhere \\(C \\propto \\alpha_1 \\alpha_2\\) depends on the polarisabilities and ionization energies of the particles.\nThe resulting \\(r^{-6}\\) dependence is also referred to as the London dispersion force, which can be derived within quantum-mechanical perturbation theory (London 1937).\nWe can then assume to take two identical spherical colloids of radius \\(R\\) at distance \\(h\\) and that every volume element of such spheres interacts with the London dispersion force (see the semi-classical approach of Hamaker Hamaker (1937) for illustration). Integrating over all volume elements yields the collodial spheres Van der Waals attractive potential in the form\n\n\\[W_{wdW}(h)=-\\dfrac{A-H}{g}f(h/R)\\]\n\nwhere \\(A_H\\) is the Hamaker constant and \\(f(h/R)\\) is\n\\[f(h/R) = \\left[ \\frac{2R^2}{h^2 - 4R^2} + \\frac{2R^2}{h^2} + \\ln\\left( \\frac{h^2 - 4R^2}{h^2} \\right) \\right]\\]\n\n\n\n\n\n\nSketch of a derivation of colloid-colloid Van der Waals potential\n\n\n\n\n\nEach volume element in one sphere interacts with each in the other sphere via:\n\\[\n\\phi(r) = -\\frac{C}{r^6}\n\\]\nSo the total interaction energy is:\n\\[\nU(h) = -C \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\]\nLet:\n\nSphere 1 be centered at \\((0, 0, 0)\\),\nSphere 2 be centered at \\((0, 0, h)\\),\n\\(\\mathbf{r}_1 = \\mathbf{r}\\), \\(\\mathbf{r}_2 = \\mathbf{r}'\\)\n\nThen \\(|\\mathbf{r}_1 - \\mathbf{r}_2| = |\\mathbf{r} - \\mathbf{r}'|\\), and the integral becomes:\n\\[\nU(h) = -C \\iiint_{|\\mathbf{r}| \\leq R} \\iiint_{|\\mathbf{r}' - h\\hat{z}| \\leq R} \\frac{1}{|\\mathbf{r} - \\mathbf{r}'|^6} \\, d^3\\mathbf{r} \\, d^3\\mathbf{r}'\n\\]\nHamaker evaluated this by integrating over the densities of interacting atoms with number density \\(\\rho\\) in both spheres. The result:\n\\[\nU(h) = -\\rho^2 C \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\]\nDefining the Hamaker constant:\n\\[\nA_H = \\pi^2 \\rho^2 C\n\\]\nSo,\n\\[\nU(h) = -\\frac{A_H}{\\pi^2} \\iint \\frac{1}{|\\mathbf{r}_1 - \\mathbf{r}_2|^6} \\, dV_1 \\, dV_2\n\\]\nThis six-dimensional integral can be evaluated analytically for spheres, yielding:\n\\[\nU(h) = -\\frac{A_H}{6} \\left[ \\frac{2R^2}{h^2 - 4R^2} + \\frac{2R^2}{h^2} + \\ln\\left( \\frac{h^2 - 4R^2}{h^2} \\right) \\right]\n\\]\n\n\n\n\n\n\n\n\n\nVan der Waals interactions are considered to be short-range forces in the sense that their decay rate(e.g. London’s \\(1/r^6\\)) is much faster than Coulombic interactions (\\(1/r\\)). Importantly, since their origin resides in the fluctuation of charges on the colloids, their strength is not additive: simply summing all the pairwise interactions does not fully accurately account for man-body effects. We often rely on such two-body approximations, but we should be aware that they are a simplified scenario.\n\n\n15.2.3 Double-layer interaction\nColloids are often charged. The solution they are immersed also has an inhomogeneous distribution of ions: there will be\n\nco-ions (same charge as the colloid) that will be pushed away from the colloid surface, while\ncounter-ions (opposite charge) will accumulate at the surface.\n\nThese two different concentrations of oppositely charged ions form what is called a double layer and its property (such as its width) are obviously controlled by the number of ions in the solvent, which can be tuned by adding or removing, for example, salts.\nSuppose we now have two colloids of the same size and charges in the solvent. Th charges in their double layers will interact giving rise to a repulsive interaction. This interaction is referred to as screened-Coulomb (as the electrostatic interaction is screened by the presence of the ions or double layer repulsion. We are not going to derive it, but it can be shown that, for a colloid of radius \\(R\\) in solvent with salt density \\(n_s\\) it can approximated by an exponential decay\n\n\\[\nW_{DR}(h) = B\\dfrac{R}{\\lambda_B}\\exp{(-h/\\lambda_D)}\n\\]\n\nwhere \\(\\lambda_D\\) is called the Debye length\n\\[\n\\lambda_D = \\sqrt{\\dfrac{1}{8\\pi\\lambda_B n_s}}\n\\]\nwhile \\(\\lambda_B\\) is the Bjerrum length, which is itself derived from the characteristic distance at which two elementary charges have energy \\(k_B T\\) 2\n\\[\n\\lambda_B = \\dfrac{e^2}{4\\pi \\varepsilon_0\\varepsilon_r k_B T}\n\\]\nThe coefficient \\(B\\) is a material properties that depend on the surface potential. We are not going more into the details of these features, which are important for the design of colloidal experiments.\n\nFrom our point of view, what matters is that identical colloids in solution appear to have two interactions of opposite sign\n\na van der Waals components, typically attractive and emergent from induced dipole -dipole ineractions emerging from quantum fluctuations of the electronic clouds\na double layer component, repulsive in nature, and resulting from the electrostatic repulsions induced by ions and counterions\n\nThe sum of the two gives rise to the DLVO (Derjaguin–Landau–Verwey–Overbeek) interaction which is an elementary model for colloid stability and aggregation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity\n\n\n\nModify the script above and test graphically that:\n\nWhen the salt concentration is low, the double layer repulsion dominates the DVLO interaction\nThere are salt concentrations where two minima occur in the DLVO potential: on at very close distance (dominated by the Van der Waals attraction) and one, much shallower, at intermediate distances comparable to \\(2(R+\\lambda_D)\\) . This minimum can lead to weak aggregation of colloids\nLarge salt concentrations depress the DVLO maximum altogether and eventually the Van der Waals interaction dominates.\n\n\n\n\n\n15.2.4 Steric interactions and depletion interactions\nWe have seen that quantum fluctuations from the uncertainty principle can be recast via a semi-classical approach into effective short-range interactions (van-der Waals). Another fundamental quantum principle – Paulis’s exclusion principle – is also at the source of key effective interactions that can be understood in a semi-classical picture. Indeed, the main consequence of the exclusion principle is that since electrons cannot occupy the same quantum state, there are minimal distances below which atoms cannot be brought together.\nThis simply means that as we take a pair of atoms, below a certain distance they repel each other with very strong forces, even in the absence of double layer interactions. These repulsive interactions can be approximated in various ways: their strength depends on the details of the atoms and hence, in the case of colloids, on the details of the materials composing the colloids. We call these excluded volume interactions steric interactions3.\nThese fundamentally repulsive interactions mean that that, in a system with \\(N\\) colloids, an additional \\(N+1\\) colloid does not have access to the entire configuration space: there is a large excluded volume due to the presence of the other colloids.\nWhat is intriguing of these interactions is that, even if they are purely repulsive, they can collectively give rise to effective attractive interactions: attraction through repulsion.\n\n15.2.4.1 A simple example: particles in a one-dimensional line (hard rods)\nAs an introductory example, let us consider a very simple, idealised system of purely repulsive objects. Let us confine them along a one dimensional line, bounded by hard (repulsive and impenetrable) walls seperated by the distance \\(L\\). Assume the objects to be \\(N\\) spheres of diameter \\(d\\), or (equivalently) hard rods of length \\(d\\). They cannot overlap so, once they are placed along the line, their order cannot change.\nWhat we want to know is how these hard objects, that interact solely via repulsive interactions, distribute themselves along the line. The problem is a classic of statistical mechanics and thanks to its one-dimensional nature can be addrssed extensively using analytical methods.\nHere we take a more algorithmic approach, and directly sample the probability density distribution \\(\\rho(x)\\) of finding a particle center at position \\(x\\).\nTo do so, we do the following:\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nposition the particles in a valid configuration (no overlaps with other particles or the walls)\npick a particle at random\nmove it slightly along the line\ntest the if the new position is valid (no overlaps)\nif valid, accept the move otherwise, reject\ngo back to (2)\n\n\n\nThis very simple algorithm is based on a trial move and an acception/rejection step. This is the heart of a very popular molecular simulation method, named Metropolis Monte-Carlo Chain method.\nHere below you have a very simple implementation in python.\n\n\n\n\n\n\nThe code should produce a final probability distribution along the \\(x\\) axis. The main control parameter here is the packing fraction, i.e. \\(\\phi = \\dfrac{dN}{L}\\), the coverage of the line. If you take high values for the packing fraction you will obserbve an interesting effect: the distribution \\(\\rho(x)\\) displays distinctive modulations. Surely, these reflect the layering of the particles along the line, due to their hard-core interactions. However, the oscillations are even more interesting as we observe that the particles are morelikely to be found near the walls than away from them.\nReflect on this point. In the complete absence of any attractive interactions, we find that the hard particles are preferentially located close to the walls. It is as if the walls exerted an attractive force capable of pulling the particles close to them. In reality, the force is purely statistical in nature. It ismerely the result of the entropic advantage that the entire system acquire when the particles are closer to the walls: simply put,if the first (and last) particles are close to the walls, there is more space for the particles in the middle, hence a larger number of configurations and hence larger configurational entropy.\nEven if the force is statistical, it is not less real: in this simplified case, it leads to the layering of the density profile. Its generalisation to less idiealised conditions leads to a family of forces that are essential for the aggregation of soft matter which are called depletion interactions.\n\n\n\n15.2.5 Asakura-Oosawa depletion potential\n\n\n\nMixture of colloids (yellow) and polymers (squiggly lines inside red circles). The depletion layers are as thick as the polymer radius (red circles) and are indicated with the dashes around the colloids. When the two layers do not overlap, the osmotic pressure due to the polymers on the colloids is balanced. When there is overlap, there is a region inaccessible to the polymers (purple) and the pressure is unbalanced, leading to aggregation.\n\n\nThe simple unidimensional scenario can be extended to a more interesting situation of hard-core colloidal particles dispersed in a medium where other smaller, repulsive particles (e.g. coiled polymers) are also dispersed. It is not important atthistage to know the details of such polymeric structures. We will ignore their internal structure and we wil also ignore their mutual interactions. We will only consider for the moment how they interact with the colloids and how this mediates an interaction between the colloids. We call these idealised polymers penetrable hard spheres.\nThis assumption yield a great simplification: the polymers, on their own, are an ideal gas. Their chemical potnetial is given by\n\\[\\mu = k_BT \\ln \\eta_b \\tag{15.1}\\]\nWe assume also to work in an ensemble at fixed volume \\(V\\), temperature \\(T\\) and chemical potential \\(\\mu\\): this is the grand canonical ensemble. This means that we imagine that there is some ideal reservoir with which we can exchange polymers in order to maintain the chemical potential constant.\nThe grand potential for such ideal polymers is given by\n\\[\\Omega = -k_BT e^{\\mu/k_B T}V_{\\rm accessible}\\]\nwhere \\(V_{\\rm accessible}\\) is the accessible volume. In the absence of the colloids, the entire volume \\(V\\) is accessible.\nLet’s image to intrudce two colloids at separation \\(r\\). For all \\(r&gt; 2(R+\\delta)=R_d\\), the accessible volume is \\(V_{\\rm accessible}^{\\infty}=V-2V_{\\rm exclusion}\\), where \\(R_d\\) is the depletion radius and \\(V_{\\rm exclusion}\\) is the inaccessible region due to the colloid-polymer interaction around each colloid:\n\\[\\begin{aligned}\nV_{\\rm exclusion}& = V_{\\rm outer}-V_{\\rm inner}\\\\\n&= \\dfrac{4\\pi}{3}\\left((R+\\delta)^3-R^3\\right)\n\\end{aligned}\n\\]\nChanging the separation between the two colloids but maintaining \\(r&gt;2R_d\\) does not change the grand potential: there is no free energy advantage and hence no effective interaction.\nInstead, it is only when we take the colloids closer than \\(2R_d\\) that we see a free energy difference. When \\(r&lt;2R_d\\) (and, obviously, \\(r&gt;2R\\)) an overlap region is formed (the lens-shaped region of tha figur above). Its volume can be calculated simply from geometrical considerations and it is \\[\nV_{\\mathrm{overlap}}(r)=\\dfrac{4 \\pi}{3} R_d^3\\left[1-\\frac{3}{4} \\frac{r}{R_d}+\\frac{1}{16}\\left(\\frac{r}{R_d}\\right)^3\\right]\n\\tag{15.2}\\]\nTh new accessible volume is \\[V_{\\rm accessible}\\prime=V-2V_{\\rm exclusion}+V_{\\mathrm{overlap}}\\]. The interaction between the two colloids resulting from the free energy adgvantage is called potential of mean force \\(W(r)\\). It is expressed as\n\\[\n\\begin{aligned}\nW_{\\rm AO}(r) = \\Omega(r)-\\Omega^{\\infty} & =-k_BT e^{m/k_B T}\\left(V_{\\rm accessible}(r)-V_{\\rm accessible}(\\infty)\\right)\\\\\n& = -k_BT e^{m/k_B T}\\left[V-2V_{\\rm exclusion}+V_{\\mathrm{overlap}}(r)-(V-2V_{\\rm exclusion})\\right]\\\\\n& = -k_BT e^{m/k_B T} V_{\\mathrm{overlap}(r)}\n\\end{aligned}\n\\]\nWe can now re-use the ideal polymer chemical potential definition Equation 15.1 and the gemoetrical expression for \\(V_{\\mathrm{overlap}}\\) in Equation 15.2 to finally write the Asakura-Oosawa potential\n\n\\[\nW_{\\rm AO} (r) = - \\dfrac{4 \\pi \\eta_b k_B T}{3} (R+\\delta)^3\\left[1-\\dfrac{3}{4} \\dfrac{r}{R+\\delta}+\\frac{1}{16}\\left(\\dfrac{r}{R+\\delta}\\right)^3\\right] \\quad 2R\\leq r&lt; 2R+\\delta\n\\]\n\ntaking \\(q = \\delta / R\\) (the polymer-to-colloid size ratio), so \\(R+\\delta = R(1+q)\\) and \\(r/(R+\\delta) = r/[R(1+q)]\\) the AO potential can also be rewritten as:\n\\[\nW_{\\rm AO}(r) = -\\frac{4\\pi \\eta_b k_B T}{3} [R(1+q)]^3 \\left[1 - \\frac{3}{4} \\frac{r}{R(1+q)} + \\frac{1}{16} \\left(\\frac{r}{R(1+q)}\\right)^3 \\right] \\quad 2R \\leq r &lt; 2R(1+q)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nForce-based derivation of the AO interaction\n\n\n\n\n\nWhen the colloids are so close the polymers cannot enter the lens-shaped region between the two colloids. This gap leads therefore to an uniform distribution of polymers which results in an pressure difference: the outer polymers push the colloids together, producing an effective attractive force. Such pressure resulting from an uncompensated concentration gradient is called osmotic.\nThe lens-shaped overlap region between the two colloids consists of two identical spherical caps, each subtending an angle \\(\\theta_0\\) at the center of the colloid. From the given geometry, the angle is such that \\[\\cos\\theta_0 = \\dfrac{r}{2R_d}.\\]\nThe uncompensated pressure acts on such surface.\nFor symmetry reasons, only the forces along the axis connecting the wto spheres contribute to the total force. For a given angle \\(\\theta\\) the components is proportional to \\(P\\cos\\theta\\) where \\(P\\) is the pressure exerted by the ideal gas of polymers is simply \\(P=\\eta_b kT\\) where \\(\\eta_b\\) is the polymer concentration. The surface element on which this pressure acts for a small increment \\(d\\theta\\) is \\[dS = 2\\pi R_d^2\\sin\\theta d\\theta\\]. Integrating over the range \\([0,\\theta_0]\\) yields the total force \\(F_d\\)\n\\[\n\\begin{aligned}\nF_d(r) &= -2\\pi \\eta_b k_B T R_d^2 \\int_0^{\\theta_0} \\sin\\theta \\cos\\theta \\, d\\theta \\\\\n&= -\\pi R_d^2 \\eta_b k_B T \\left[1 - \\left(\\frac{r}{2R_d}\\right)^2\\right]\n\\end{aligned}\n\\]\nNotice the negative sign, chosen to reflect the fact that the force is attarctive.\nIntegrating the force yields the interaction potential.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of main colloid-colloid interactions\n\n\n\n\n\n\n\n\n\n\n\nInteraction Type\nDescription\nRange\n\n\n\n\nVan der Waals\nAttractive forces arising from induced dipoles between particles.\nShort-range\n\n\nDouble Layer\nElectrostatic repulsion due to overlapping electrical double layers around charged particles.\nLong-range\n\n\nDLVO\nCombination of van der Waals attraction and double layer repulsion.\nShort and long range\n\n\nDepletion\nTypically attractive interactions emerging from purely entropic interactions\nShort range",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#colloids-as-big-atoms",
    "href": "soft-matter/soft-matter_colloids.html#colloids-as-big-atoms",
    "title": "15  Colloids",
    "section": "15.3 Colloids as big atoms",
    "text": "15.3 Colloids as big atoms\nColloids can be viewed as “big atoms”: they are large particles suspended in a medium, exhibiting thermal motion and interactions in various ways analogous to atoms, but at much larger length and time scales. As we have seen above, the interactionsa can have statistical or even quanto-mechanical origina, but are ultimately cast in a classical form that is amenable to a classical treatment. At the same time, the large scales of colloids mean that via dedicatated microscopy techniques one is able to identify individual colloids, study theiur arrangements in detail, and follow their dynamics.\nAs mentionedearlier, there is a huge variety of colloids and a vast literature characterising their properties but also producing theoritical and computational models.\nHere we focus on the most elementary model of a colloid. The simplest such exammple is the purely repulsive hard-sphere colloid. While an idealisation, quasi-hard-sphere colloids can be prepared in the laboratory by various techniques (see Royall et al. (2024) ). This includes by synthesizing a spheres of bundled polymers (you will learn about polymers in the next chapter) such as polymethyl methacrylate (PMMA), but also simply silica (small sphere of non-crystalline \\(\\rm Si O_2\\)) micron-sized beads carefully treated to screen and minimize the electrostatic interactions that would lead to DLVO-like contributions.\n\n15.3.1 The archetype: hard-spheres\nA hard-sphere is an idealized particle model in which each particle is represented as a perfectly rigid sphere of fixed radius \\(R\\) and diameter \\(\\sigma=2R\\). Hard spheres interact only through excluded volume: they cannot overlap, but otherwise experience no attraction or repulsion. The interaction potential \\(U(r)\\) between two hard spheres separated by a center-to-center distance \\(r\\) is:\n\\[\nU(r) = \\begin{cases}\n\\infty & \\text{if } r &lt; \\sigma \\\\\n0 & \\text{if } r \\geq \\sigma\n\\end{cases}\n\\]\nThis model captures the essential physics of excluded volume which, as we said earlier, fundamentally emerges from Pauli’s exclusion principle (electrons cannot occupy the same quantum state so electronic clouds of different atoms exclude each other).\n\n\n   r  U(r)      2R    ∞  0  forbidden  Hard Sphere Potential\n\n\nPhase behaviour of hard spheres\nSince the interaction potential is only based on excluded volume, the energy of hard spheres is trivial: it is always zero. A naive interpretation of such trivial energetics may lead to conclude that nothing interesting happens to a collection of hard spheres, since they always are at their energy minimum (namely, zero). However, it is clear from the earlier discussion of depletion forces that the interaction energy is only a part of the picture for systems subject to thermal fluctuations: indeed, for any \\(T&gt;0\\), entropic contributions to the free energy are always present. In the specific case of a fixed number of hard spheres in a fixed volume, they are the only contribution to the free energy.\nIn this sense, hard-spheres are completely entropy-driven system. For a collection of identical (monodisperse) hard spheres the entropy is solely configurational and correspods to the number of possible arrangements. This constrained only by the accessible volume, of which we have seen an instance when considering the depletion interactions. Notice that changing the temperature does not really affect the statistics of the configurations: the Boltzmann factor \\(e^{U(\\mathbf{r}^N)/k_BT}\\) is always \\(1\\) for all valid configurations. The only way we can change the state of the system is by varying the accessible volume. For a system of \\(N\\) identical hard spheres in a volume \\(V\\) this can only be done in two ways\n\nby adding more spheres (of the same kind)\nby varying the volume V\n\nThe two routes essentially amount to varying one single parameter, which is the packing fraction (also called, volume fraction) of the system, defined as\n\\[\\phi = N\\dfrac{v_{\\rm sphere}}{V} = \\dfrac{\\pi \\sigma^6 N}{6V}\\]\nwhich can also be expressed in terms of the number density \\(\\rho=N/V\\) as \\(\\phi=\\pi\\sigma^3\\rho/6\\).\nTo change the phase behaviour of hard spheres we have a single control parameter, \\(\\phi\\) meaning that (differently from fluids like water) we are bound to have one-dimensional phase diagram.\nWe explore the phase behaviour of hard spheres by considering different regimes of packing fraction.\n\n\nLow packing fractions\nThe hard repulsion between hard spheres means that each sphere is surrounded by an excluded volume in which the center of other spheres cannot be placed. Since the distance of closest approach between two spheres is \\(\\sigma\\) such excluded volume per particle is simply \\(v_{\\rm ex} =4\\pi\\sigma^3/3\\) and it is much larger than the volume pert particle \\(v= V/N\\). A collection of \\(N\\) hard spheres has a total excluded volume \\(V_{\\rm ex}\\neq N v_{\\rm ex}\\) simply because the excluded volumes of individual particles can in general overlap, as we saw earlier with the case of depletion.\nAt very low densities, most hard spheres isolated. So, in this case, we can approximate the roral excluded volume as \\(V_{\\rm ex} \\rm N v_{\\rm ex}\\) so that the accessible volume (the volume not occupied by the spheres) is\n\\[V_{\\rm accessible} = V-Nv_{\\rm ex}\\]\nWe use this to perform an appropriate statistical mechanical calculation for the Helmholtz free energy of system \\(F\\), which we know is purely entropic \\(F=-TS\\). The partition function \\(\\mathcal{Z}\\) is\n\\[\n\\mathcal{Z} = \\frac{1}{N! \\Lambda^{3N}} \\int_{V_{\\rm accessible}} d\\mathbf{r}_1 \\ldots d\\mathbf{r}_N\n\\]\nwhere \\(\\Lambda\\) is the thermal de Broglie wavelength\n\\[\\Lambda = h/\\sqrt{2\\pi mk_B T},\\] and originates from the integration over the Maxwell-Boltzmann disribution of momenta for particles of mass \\(m\\), while \\(h\\) is Planck’s constant.\nFor hard spheres, the integral is over all configurations where no two spheres overlap (i.e., \\(|\\mathbf{r}_i - \\mathbf{r}_j| \\geq \\sigma\\) for all \\(i \\neq j\\)), and \\(V_{\\rm accessible}\\) is the total accessible volume. The results is that\n\\[\\mathcal{Z} = \\dfrac{(V-Nv_{\\rm ex}/2)^N}{N!\\Lambda^{3N}},\\]\nwith the \\(1/2\\) factor coming from the fact that we avoid double counting the excluded volume of pairs. The entropy is \\[\nS = k_B \\ln \\mathcal{Z} = k_B \\left[ N \\ln(V - N v_{\\rm ex}/2) - \\ln N! - 3N \\ln \\Lambda \\right]\n\\]\nFrom the partition function, we use Stirling’s formula \\(\\ln N! = N\\ln N -N\\) and obtain\n\\[\nS = k_B \\left[ N \\ln(V - N v_{\\rm ex}/2) - (N \\ln N - N) - 3N \\ln \\Lambda \\right]\n\\]\nwhich reads \\[\nS = N k_B \\left[ \\ln\\left( \\frac{V - N v_{\\rm ex}/2}{N \\Lambda^3} \\right) + 1 \\right]\n\\]\nThe phase behaviour is encoded in the equation of state (the equation that links the three thermodynamics variables \\(P,T\\) and \\(\\phi\\)). To obtain it, we calculate the pressure\n\\[P = -\\left(\\dfrac{\\partial F}{\\partial V}\\right)_{N,T} =T\\left(\\dfrac{\\partial S}{\\partial V}\\right)_{N,T}= \\dfrac{k_B T }{v-v_{\\rm ex}/2}\\]\nThis expression can be simplified (do it as an exercise) to obtain the quation of state\n\\[Z_{\\rm comp}= \\dfrac{PV}{Nk_BT}=\\dfrac{1}{1-4\\phi} \\quad (\\phi\\ll 1)\\]\nalso known as the compressibility factor \\(Z_{\\rm comp}\\). Since \\(\\phi\\) is very small the expression is in fact\n\n\nThe choice of the letter “Z” is unfortunate. Do not confuse the compressibility factor with the partition function!\n\\[Z_{\\rm comp} = 1+4\\phi+O(\\phi^2)\\]\nThis expression makes it apparent that the first term linear in \\(\\phi\\) is a correction to the ideal gas law. This is example (to very low order) of what is called the virial expansion. This, in general takes the form\n\\[Z_{\\rm comp}= 1 + B_2 \\rho + B_3 \\rho^2+ \\dots\\]\nwhere \\(B_2, B_3, \\dots\\) are the virial coefficients and for systems that are not hard spheres they depend also on the temperature, \\(B_2(T), B_3(T),\\dots\\). They are important as they encode the effects of correlations:\n\n\\(B_2\\) accounts for pairwise correlations (how the presence of one particle affects the probability of finding another nearby),\n\\(B_3\\) for three-body correlations, and so on.\n\nGiven an interaction potential the second virial coefficient \\(B_2\\) can be calculated independently from the equation state\n\\[\nB_2(T)=-\\frac{1}{2} \\int_0^{\\infty}\\left(\\exp \\left(-\\frac{U(r)}{k_B T}\\right)-1\\right) 4 \\pi r^2 d r\n\\]\nFor hard spheres this results in\n\\[\nB_2 =  \\frac{2\\pi}{3} \\sigma^3\n\\]\nwhich is exactly what is predicted by the equation of state above, once you recognise that \\(\\phi = \\frac{\\pi\\sigma^3}{6}\\rho\\).\n\n\nExercise: check this calculation.\nHigher-order coefficients become increasingly complex to compute and reflect more complex many-body corelations that can be established even if the interaction potential is purely twobody (as in the case of hard spheres). The higher order coefficients become mor eand more important as the packing fraction is increased. Eventually something surprising occurs at a sufficiently high volume fraction.\n\n\nDense packing: metastability and crystalisation\nAs we increase the packing fraction, the accessible (free) volume reduces rapidly. At some point, disordered packing of spheres are so tightly packed that thermal motion becomes extremely inefficient or even impossible. Such disordered (random) packing of spheres are described as jammed: they are so densely packed that they are no longer behaving like a fluid but they instead display some of the rigidity that we typically associate with solids. Such jammed configurations are indeed examples of amorphous solids. We will explore them more in detail in the chapter dedicated to arrested systems.\nIt is important to note, however, that the disordered packing at high volume fractions are not minima of the free energy. The densest possible packing of hard spheres is achieved by arranging them in a crystalline lattice. In three dimensions, the densest packings are the face-centered cubic (FCC) and hexagonal close-packed (HCP) structures, both of which have a maximum packing fraction of\n\\[\n\\phi_{\\text{max}} = \\frac{\\pi}{3\\sqrt{2}} \\approx 0.74\n\\]\nThis means that, at most, about 74% of the available volume can be filled by the spheres, with the remainder being empty space. This result was conjectured by Kepler in 1611 and proved rigorously only in 1998 (the Kepler conjecture).\nIn contrast, random close packing (the densest disordered arrangement of spheres) yields a lower packing fraction, typically around \\(\\phi_{\\text{rcp}} \\approx 0.64\\). Notice that this value is approximate and variations can be observed (due to randomness and, more importantky, to the method by which such packing are reached, i.e. the compression protocol). This means that the only way to compress spheres at very high packing has to lead (spontaneously) to the formation of crystals. We can indeed imagine to perform the following experiment:\n\nprepare a disordered assembly of colloidal hard spheres\ncompress them gradually to high and higher packing fraction, taking care to monitor the systems so that time correlations decay and the system is at thermal equilibrium\nmeasure the resulting packing fraction\n\nExperiments of this sort have been performed historically, leveraging for example the slow sedimentation of colloids, which on long time scales leads to the accumulation of dense packings of spheres. Such experiments reveal that beyond the so-called freezing packing fraction \\(\\phi_{f} 0.494\\) the system spontaneously forms small regions of crystals mixed with the fluid. At a the high melting packing fraction \\(\\phi_{m}=0.545\\) the entirety of the system is then crystalline and its optical properties consequently change.\n\n\n\nPhase behaviour of hard-sphere colloids from Pusey et al. (2009)\n\n\nA simple cell model can help us rationalise what is taking place. When the spheres are packed within their FCC cell, they can move very little beyond their own diameter \\(\\sigma\\). Assume that the volume per particle again is \\(v\\) and the (geometrically consrained) close packed volume is \\(v_{cp}\\).\nThe maximum dispalcement is \\[\n\\delta=\\frac{\\sigma}{\\sqrt{2}}\\left(\\left(\\frac{v}{v_{c p}}\\right)^{1 / 3}-1\\right)\n\\] The corresponding free volume is then \\(v_f=\\frac{4 \\pi}{3} \\delta^3\\) from which we can calculate the entropy\n\\[\nS = -N k_B T \\ln \\left(v_f / \\Lambda^3\\right)\n\\]\nand the resulting pressure\n\\[\nP=\\frac{N k_B T}{v_{c p}} \\frac{\\left(v / v_{c p}\\right)^{-2 / 3}}{\\left(v / v_{c p}\\right)^{1 / 3}-1}\n\\]\nRearranging and expressing everything in terms of packing fraction \\(\\phi = \\dfrac{\\pi\\sigma^3}{6v}\\) yields\n\\[\nZ_{\\rm comp}=\\frac{1}{1-\\left(\\phi / \\phi_{c p}\\right)^{1 / 3}}\n\\]\nWhat is noteable here is that the expression we have obtained is a completely different functional form compared to the low density fluid regime. Thisis indicative of a discontinuous, first-order phase transition between the fluid and the crystalline phases. First order phase transitions are characterised by coexisting regions where the system can be found in partial fluid and partial crystaline state, as illustrated in the experiments above. This also means that we can prepare a disordered packing at very high density: this will not be its equilibrium state (global minimum of the free energy), but will still be stable for some (finite) time. This local equilibrium state is called a *metastable state.\n\n\n\nSchematic free energy for hard spheres compressed at high pressures: the fluid branch becomes metastable and the free enrgy minimum is located in the crystal phase\n\n\nBut where does the free energy advantage of the crystal over the disrodered fluid come from? From the discussion above the answer is obvious: crystals can accommodate higher packings, meaning that they use the availale volume more efficiently. This in fact means that (on average) every hard sphere has more available volume if it arranged in the crystalline state compared to the fluid phase: the increased volume (available for thermal fluctuations and random particle displacements) is translated into an increased entropy. So, in fact the ordered, crystalline state has overall a higher entropy than the disordered fluid. This is important instance in which the conventional storytelling, where entropy is just disorder, simply does not hold. As we have seen with depletion forces earlier, entropy can lead to structure: in the case of hard spheres, it si the only mechanism leading to structure, and such structure is the most orderly one can think of: long-range, crystalline order.\nThe video below shows instead the results of a Monte-Carlo simulation at packing \\(\\phi=0.493\\) for a small system of 32 particles. Small systems have enhanced fluctuations, and since the overal packing fraction is very close to the freezing line, we see spontaneous freezing and unfreezing of the small system (wait until second 14 in the movie).\n\nIn conclusion, colloidal hard spheres have a one-dimensional phase diagram, that depends only on the packing fraction but with various distinct phases\n\n\n\nHard-spheres phase diagram\n\n\n\n\n\n15.3.2 Beyond hard-spheres: simple liquids\nA simple liquid is a system of particles interacting via short-range, spherically symmetric (isotropic) pair potentials. The most common model is the Lennard-Jones (LJ) potential, which captures both the short-range repulsion (due to Pauli exclusion) and longer-range van der Waals attraction:\n\\[\nU_{\\mathrm{LJ}}(r) = 4\\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\n\\]\nwhere \\(\\epsilon\\) sets the depth of the potential well (interaction strength) and \\(\\sigma\\) is the particle diameter (distance at which \\(U=0\\)). The \\(r^{-12}\\) term models steep repulsion, while \\(r^{-6}\\) describes the attractive tail.\nThe Lennard-Jones fluid exhibits rich phase behavior: at low temperatures and densities, it forms a gas; at intermediate conditions, a liquid; and at high densities, a solid. The LJ model is widely used to study atomic and molecular liquids, and serves as a reference for understanding real fluids and their phase transitions.\nIn the phase diagram below these different phases have been highlighted. It is clear that, compared to hard spheres which only display a fluid and a crystalline solid phase, systems like the Lennard-Jones fluid display an additional phase, the liquid. This phase emerges from the presence of the attractive well in the interaction potential. This allows for short-range attractive interactions that case a region of teh phase diagram in the save class of universality of the Ising model and the lattice gas.\n\n\n\nPhase diagram of the Lennard-Jones Fluid, adapted from Wikimedia.\n\n\nThe Lennard-Jones interaction has already been designed to model noble gases (e.g. Argon) but has over time been used to model many other systems due to its simplicity and computational convenince: in particular it is used tor construct coarse-grained models of macromolecules, as well as colloids and nanoparticles. It belongs to a wider class of, classical, effective pair-wise interaction models actensively used to claculate properties and phase diagrams of a variety of condensed matter systems. This are useful because they allow one to, for example, long molecular simulations that are normally unreachable when considering atomistics and electronic density calculations.\nThe following table provides you with a few example potentials and their functional forms:\n\n\n\n\n\n\n\n\nPotential Name\nMathematical Form\nTypical Systems/Features\n\n\n\n\nLennard-Jones (LJ)\n\\(U(r) = 4\\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]\\)\nSimple atomic fluids, coarse-grained molecular interactions\n\n\nHard Sphere\n\\(U(r) = \\begin{cases} \\infty & r &lt; \\sigma \\\\ 0 & r \\geq \\sigma \\end{cases}\\)\nColloids, granular materials, excluded volume\n\n\nYukawa (Screened Coulomb)\n\\(U(r) = \\epsilon \\frac{e^{-\\kappa r}}{r}\\)\nCharged colloids, plasmas, electrolytes\n\n\nDipolar\n\\(U(r) = \\frac{\\mu_0}{4\\pi} \\frac{\\mu_1 \\mu_2}{r^3} (1 - 3\\cos^2\\theta)\\)\nMagnetic colloids, polar molecules",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#characterisation-of-colloidal-systems",
    "href": "soft-matter/soft-matter_colloids.html#characterisation-of-colloidal-systems",
    "title": "15  Colloids",
    "section": "15.4 Characterisation of colloidal systems",
    "text": "15.4 Characterisation of colloidal systems\nWhen looking at colloidal systems, we typically focus on two main aspects of their physics:\n\ntheir structural features, chracterised by the spatial correlations between their constituents\ntheir dynamical features, characterised by the mobility of single particles or groups of particles.\n\nHere below, we briefly account of some main approaches to characterise these two dimensions.\n\n15.4.1 Structural properties: the radial distribution function\nStructural features of disordered (but also ordered) systems are described in terms of correlation functions. The underlying idea is that we are provided with an ensemble of stochastic variables (the positions of the colloids) which have a characteristic spatial distribution. for \\(N\\) particles we have an N-body probability distribution function \\(\\rho_N(\\mathbf{r}^N)\\) which contains all the necessary statistical mechanical information to describe the thermodyanmics of the system. However, this is very difficult to access directly. In experiments or theoretical calculations we typically only have access to some lower order marginalisation of the distribution, in terms of few-body distributions.\nOne of the simplest assumptions we can make when we describe a system is that its potential energy is expressed in terms of purely pairwise additive potential, meaning that\n\\[ U_N (\\mathbf{r}^N)=\\sum_{i=1}^{N-1}\\sum_{j=i+1}^N V(r_ij) = \\dfrac{1}{2}\\sum_{i\\neq j}V(r_{ij})\\]\nwhere \\(V(r_{ij})\\) is the pairwise, radial interaction potential that only depends on the distance between particle centres \\(r_{ij} = \\mathbf{r}_i-\\mathbf{r}_j\\).\nFor system with pariwise interactions, the natural spatial correlation function is a twobody correlation \\(g(\\mathbf{r}_i, \\mathbf{r}_j)\\), where \\(\\mathbf{r}_i\\) and \\(\\mathbf{r}_j\\) are two randomly chosen particles. In the case of non-crystalline, disordered systems the corrlation function has to be\n\ntranslationally invariant , so that it only depends on the difference \\(\\mathbf{r}_i-\\mathbf{r}_j\\)\nrotationally invariant, so that there is no angular component and hence only the distance \\(r_{ij} =| \\mathbf{r}_i-\\mathbf{r}_j|\\) matters\n\nThis function \\(g(r)\\) is called the radial distribution function and plays a crucial role in the characterisation of fluids, crystals, glasses and much more. For system with twobody interactions only, it contains in principle all the thermodynamic information necessary to reconstruct the free energy, as one can write\n\\[\n\\frac{F_{\\mathrm{ex}}}{k_B T}=2 \\pi \\rho N \\int_0^{\\infty}[g(r) \\ln g(r)-g(r)+1] r^2 d r+\\frac{\\rho N}{2 k_B T} \\int V(r) g(r) d^3 r\n\\] where the first term represent the entropic contributions and the second term represents the energetic contribution.\nBut how is it calculated? very simply. You can think of constructing a histogram of the distances between all of the particles and suitably normalising such histogram by the density of the system. Mathematically this reads as\n\n\\[\ng(r) = \\frac{1}{\\rho N} \\left\\langle \\sum_{i=1}^N \\sum_{j \\neq i} \\delta(|\\mathbf{r}_i - \\mathbf{r}_j| - r) \\right\\rangle\n\\]\n\nwhere \\(\\rho = N/V\\) is the number density, and the angle brackets denote an ensemble average and \\(\\delta\\) is a Dirac delta function.\n\nFor an ideal gas, \\(g(r) = 1\\) everywhere (no correlations).\nFor hard spheres, \\(g(r) = 0\\) for \\(r &lt; \\sigma\\) (no overlap), and \\(g(r)\\) shows oscillations at higher \\(r\\) due to packing effects.\n\nNotice that the radial distribution function is an instance of the more general class of pair-wise correlations that have been introduced earlier, see Section 2.2 and it is paired with its reciprocal space Fourier transform, the structure factor:\n\\[\nS(k) = 1 + \\rho \\int \\left[ g(r) - 1 \\right] e^{-i \\mathbf{k} \\cdot \\mathbf{r}} d\\mathbf{r}\n\\]\nFor isotropic systems, this reduces to:\n\\[\nS(k) = 1 + 4\\pi \\rho \\int_0^\\infty r^2 [g(r) - 1] \\frac{\\sin(kr)}{kr} dr\n\\]\nExperimentally, on can measure \\(S(k)\\) can be measured using scattering techniques or \\(g(r)\\) via direct imaging in colloidal systems. They provide insight into short-range order, local structure, and phase transitions in soft matter.\n\n\n\n\n\n\nThe radial distribution function \\(g(r)\\) has two possible interpretations\n\nIt represents the probability density of finding a particle at distance \\(r\\) away from a reference particle relative to the probability density of an idela gas at the same number density.\nIf a given refernce particle is taken at the origin, then the local average density at distance \\(r\\) is \\(\\rho g(r)\\).\n\n\n\n\nHere below we show two codes to calculate the radial distribution fucntion for an ideal gas (which is trivial) and then for an assembly of hard spheres\n\n\n\n\n\n\nWe can compare this with arrangements of hard spheres\n\n\n\n\n\n\nThe radial distribution function is the central object of much of liquid state theory, which aims to predict the shape of the correlation functions (such as \\(g(r)\\)) from the sole knowledge of the interaction potential between the elementary particles (see Santos (2016) for a gentle introduction to the topic). In particular, one can imagine the correlations between two particles \\(1\\) and \\(2\\) in a fluid to have two possible origins\n\na direct correlation between the two particles, mediated by direct interactions (e.g. collisions) between 1 and 2.\nan indirect correlation, mediated by other particle sin the fluid\n\nThe \\(g(r)\\) contains both direct and indirect correlations. We can assume that a suitable function \\(c(r)\\) exists to express the direct correlations. In such case, for twobody interactions, we cna write a hierarchy of equations via a central result of liquid state theory, the Ornestein-Zernicke (OZ) equation\n\\[\nh\\left(r_{12}\\right)=c\\left(r_{12}\\right)+\\rho \\int c\\left(r_{13}\\right) h\\left(r_{32}\\right) d \\mathbf{r}_3\n\\tag{15.3}\\]\nwhere we defined the total correlation function as \\(h(r)=g(r)-1\\). The OZ equation is an integral equation. In Fourier space (assuming the isotropicity of a fluid) it becomes an algebraic equation \\[\n\\tilde{h}(k)=\\frac{\\tilde{c}(k)}{1-\\rho \\tilde{c}(k)}\n\\] Since both \\(h(r)\\) and \\(c(r)\\) an additional relationship is known called a closure. These constructed through phsyical arguments. A common one is the so-caled Percus-Yevick closure\n\\[\nc(r)=[1+h(r)]\\left[1-e^{\\beta U(r)}\\right]\n\\] where the pairwise interaction enters explicitly.\nSolving the OZ equation with the Percus-Yevick closure produces realistic radial distribution functions in a wide range of packing fractions for hard spheres.\n\n\n\n\n\n\n\n\n\n\n\n15.4.2 Dynamics: single vs collective displacements\nWe have seen earlier that if we are given a number of particles evolving microscopically according to the Langevin equation of drag \\(\\gamma\\) and zero-mean noise \\(\\eta\\)\n\\[\nm \\frac{d \\vec{v}}{d t}=-\\gamma\\vec{v}+\\vec{\\eta}(t)\n\\]\nthen an ensemble of independent particles will follow the diffusion equation:\n\\[\\frac{\\partial \\rho(\\mathbf{r}, t)}{\\partial t} = D \\nabla^2 \\rho(\\mathbf{r}, t) \\]\nwhere \\(\\rho(\\mathbf{r},t)\\) is the probability of finding a particle at position \\(\\mathbf{r}\\) at time \\(t\\) with diffusivity \\(D\\). The diffusivity \\(D\\) is related to the microscopic Langevin equation parameters via the fluctuation-dissipation relation (or Einstein relation) \\[\nD = \\frac{k_B T}{\\gamma}\n\\] where \\(k_B\\) is Boltzmann’s constant, \\(T\\) is temperature, and \\(\\gamma\\) is the friction (drag) coefficient.\nIt is easy to show (see below) that in 1 dimension the general solution is\n\\[\n\\rho(x, t) = \\frac{1}{\\sqrt{4\\pi D t}} \\exp\\left(-\\frac{x^2}{4Dt}\\right)\n\\]\n\n\n\n\n\n\nSolution of the diffusion equation using Laplace transform\n\n\n\n\n\nThe 1D diffusion equation is: \\[\n\\frac{\\partial \\rho(x, t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}\n\\]\nSuppose the initial condition is a delta function at the origin: \\[\n\\rho(x, 0) = \\delta(x)\n\\]\nTake the Laplace transform in time: \\[\n\\tilde{\\rho}(x, s) = \\int_0^\\infty \\rho(x, t) e^{-st} dt\n\\]\nThe Laplace transform of the time derivative: \\[\n\\mathcal{L}\\left[\\frac{\\partial n}{\\partial t}\\right] = s\\tilde{\\rho}(x, s) - \\rho(x, 0)\n\\]\nSo the transformed equation is: \\[\ns\\tilde{\\rho}(x, s) - \\delta(x) = D \\frac{\\partial^2 \\tilde{\\rho}(x, s)}{\\partial x^2}\n\\]\nRearrange: \\[\nD \\frac{\\partial^2 \\tilde{\\rho}}{\\partial x^2} - s\\tilde{\\rho} = -\\delta(x)\n\\]\nFor \\(x \\neq 0\\), the homogeneous equation: \\[\nD \\frac{\\partial^2 \\tilde{\\rho}}{\\partial x^2} - s\\tilde{\\rho} = 0\n\\]\nGeneral solution: \\[\n\\tilde{\\rho}(x, s) = A e^{-\\lambda |x|}, \\quad \\lambda = \\sqrt{\\frac{s}{D}}\n\\]\nThe delta function at \\(x=0\\) gives a discontinuity in the derivative: \\[\n\\left.\\frac{\\partial \\tilde{\\rho}}{\\partial x}\\right|_{x=0^+} - \\left.\\frac{\\partial \\tilde{\\rho}}{\\partial x}\\right|_{x=0^-} = -\\frac{1}{D}\n\\]\nCompute derivatives: \\[\n\\frac{\\partial \\tilde{\\rho}}{\\partial x} = -A \\lambda \\, \\text{sgn}(x) e^{-\\lambda |x|}\n\\]\nSo at \\(x=0^+\\): \\(-A\\lambda\\), at \\(x=0^-\\): \\(A\\lambda\\). The jump is \\(-2A\\lambda\\).\nSet equal to \\(-1/D\\): \\[\n-2A\\lambda = -\\frac{1}{D} \\implies A = \\frac{1}{2D\\lambda}\n\\]\nSo: \\[\n\\tilde{\\rho}(x, s) = \\frac{1}{2D\\lambda} e^{-\\lambda |x|} = \\frac{1}{2\\sqrt{Ds}} e^{-|x|\\sqrt{\\frac{s}{D}}}\n\\]\nInvert the Laplace transform (using tables or convolution theorem): \\[\n\\rho(x, t) = \\frac{1}{\\sqrt{4\\pi D t}} \\exp\\left(-\\frac{x^2}{4Dt}\\right)\n\\]\nThis is the fundamental solution (Green’s function) of the diffusion equation.\n\n\n\nThe diffusion equation can also be read as a simple consequence of two requirements:\n\ncontinuity of the distribution of mass (no mass is lost during the transport), as expressed in the continuity equation \\[\n\\frac{\\partial \\rho(\\mathbf{r}, t)}{\\partial t} + \\nabla \\cdot \\mathbf{J}(\\mathbf{r}, t) = 0\n\\] where the divergence \\(\\nabla \\cdot \\mathbf{J}(\\mathbf{r}, t)\\) represents the net outflow of particles from a given region due to the flux \\(\\mathbf{J}\\).\nthe flux of particles is assumed to be proportional to the gradient in the density (or concnetration of the particles). This can be taken as an empirical assumption, a reasonable approximation (i.e. a perturbative approach) or a consequence of underlying Brownian dynamics. It is known as Fick’s law:\n\n\\[\n\\mathbf{J}(\\mathbf{r}, t) = -D \\nabla \\rho(\\mathbf{r}, t)\n\\]\nThe diffusivity constant \\(D\\) is therefore central for the diffusion. It is possible to link the microscopic motion of the particles to the collective behaviour of the density distribution \\(\\rho(x,t)\\) by exploting the connection provided bt the mean squared displacement, which is simply the variance of the distributione \\(\\rho(x,t)\\) at time \\(t\\).\nAs discussed earlier, in \\(d\\) dimensions this is equal to \\[\\langle r(t)^2\\rangle-\\langle r(t)\\rangle^2 = 2d D t\\]\nHence, measuring the average squared displacements is sufficient to recover the diffusivity and to reconstruct the distribution.\n\n15.4.2.1 Diffusion and interactions\nWe have up to now considered the dilute (or non-nteracting) limit, where collisions between the colloids are ignored. Let’s now consider instead simple colloids (again, hard spheres) and their dynamics.\nWe are interested in the mean squared displacement \\(\\left\\langle r^{2}(t)\\right\\rangle\\) as a function of time for different volume fractions. At low volume fractions, the particles undergo Brownian motion (random-walk diffusion) due to collisions with liquid molecules. The mean squared displacement (in three dimensions) is \\[\\left\\langle\\underline{r}^{2}(t)\\right\\rangle=6 D_{0} t\\] where the meaning of the subscript 0 will be apparent shortly.\nFor sufficiently dense hard spheres (e.g. above \\(\\phi \\sim 0.3\\)), however, different regimes are observed. At short times the particles diffuse with the short time (self) diffusion constant \\(D_{s}\\). This is determined from the short time limit and is smaller than the \\(D_{0}\\) measured for \\(\\phi \\rightarrow 0\\). The motion of the particles (self diffusion) is still driven by collisions with the liquid molecules, but in addition the interactions between particles become significant.\nWhile the particles are diffusing in their cages formed by their neighbours, the hydrodynamic interaction with the neighbours, transmitted through flows in the liquid, causes slowing down relative to the free diffusion at low concentrations. At intermediate times the particles encounter the neighbours and the interactions slow the motion down. To make further progress, the particle has to break out of the cage formed by its neighbours. Now the particles experience a further interaction, direct interactions (hard-sphere interactions), in addition to the hydrodynamic interactions. The long-time and long-ranged movement is also diffusive, i.e. we still have \\[\\langle r^{2}(t)\\rangle \\propto t\\] when the particles undergo large-scale random-walk diffusion through many cages.\nHowever, the motion is further slowed and a smaller diffusion constant relative to the motion in the short time limit is observed, the long time (self) diffusion constant \\(D_{L}\\).\n\n\n\n\n\n\n\n\n\nThe slowing down due to collisions eventually dominates the ohysics of hard spheres at high densities. This is more broadly true also for generic dense colloidal suspensions, where the short range interactions dominate on other mechanisms of motion (including the hydrodynamics). Eventually, for very dense packings one observes the emergence of a new physical regime where relaxation becomes anomalous and non-diffusive: this is the glassy regime, which we will revisit in a following chapter.\n\n\n\n\n15.4.3 Stokes-Einstein relation\nSuppose that the particles are subjected to an external force F in the x direction, e.g. gravity. In thermal equilibrium the Maxwell-Boltzmann distribution is valid, i.e. the particle density \\(n(x)\\) is given by\n\\[\nn(x)=n_{0} \\exp \\left(-U(x) / k_{B} T\\right)=n_{0} \\exp \\left(-F x / k_{B} T\\right)\n\\]\nwhere we assumed a constant force \\(F\\) in the last equation. In the case of gravity \\(F=m_{B} g\\) with \\(m_{B}\\) the buoyant mass. \\(n(x)\\) results from the balance between the motion of the particles due to the external force setting up a concentration gradient, and the resultant diffusion given by Fick’s law.\n\n{\n  const width = 600;\nconst height = 400;\nconst numCircles = 300;\n\nlet temperature = 300;    // Arbitrary scale\nlet viscosity = 0.02;     // Arbitrary scale\nconst particleRadius = 5; // pixels\nlet timeStep = 1;\n\nconst kB = 1; // reduced units\nlet gravityFactor = 0.1;\nlet gravity = 10 * gravityFactor;\n\n// Compute damping as velocity decay per timestep (Langevin friction)\nfunction computeDamping(viscosity, radius, dt) {\n  const gamma = 6 * Math.PI * viscosity * radius; // friction coeff\n  return Math.exp(-gamma * dt);\n}\n\nlet damping = computeDamping(viscosity, particleRadius, timeStep);\n\n// Create SVG\nconst svg = d3.create(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .style(\"border\", \"1px solid #ccc\")\n  .style(\"background\", \"#f9f9f9\");\n\n// Initialize circles\nconst circles = Array.from({ length: numCircles }, (_, i) =&gt; ({\n  id: i,\n  x: Math.random() * width,\n  y: Math.random() * height * 0.3,\n  vx: (Math.random() - 0.5) * 2,\n  vy: (Math.random() - 0.5) * 2,\n  radius: particleRadius,\n  color: d3.interpolateYlOrBr(Math.random() * 0.5 + 0.25)\n}));\n\n// Create circles in SVG\nconst circleElements = svg.selectAll(\"circle\")\n  .data(circles)\n  .enter()\n  .append(\"circle\")\n  .attr(\"r\", d =&gt; d.radius)\n  .attr(\"fill\", d =&gt; d.color)\n  .attr(\"opacity\", 0.8)\n  .attr(\"stroke\", \"#333\")\n  .attr(\"stroke-width\", 1);\n\n// Collision detection & response\nfunction handleCollisions() {\n  for (let i = 0; i &lt; circles.length; i++) {\n    for (let j = i + 1; j &lt; circles.length; j++) {\n      const c1 = circles[i];\n      const c2 = circles[j];\n      const dx = c2.x - c1.x;\n      const dy = c2.y - c1.y;\n      const dist = Math.hypot(dx, dy);\n      const minDist = c1.radius + c2.radius;\n\n      if (dist &lt; minDist && dist &gt; 0) {\n        const overlap = minDist - dist;\n        const nx = dx / dist;\n        const ny = dy / dist;\n\n        c1.x -= nx * overlap / 2;\n        c1.y -= ny * overlap / 2;\n        c2.x += nx * overlap / 2;\n        c2.y += ny * overlap / 2;\n\n        const dvx = c2.vx - c1.vx;\n        const dvy = c2.vy - c1.vy;\n        const vn = dvx * nx + dvy * ny;\n\n        if (vn &gt; 0) continue;\n\n        const restitution = 1.0;\n        const impulse = (-(1 + restitution) * vn) / 2;\n\n        c1.vx -= impulse * nx;\n        c1.vy -= impulse * ny;\n        c2.vx += impulse * nx;\n        c2.vy += impulse * ny;\n      }\n    }\n  }\n}\n\n// Update damping when viscosity or timestep changes\nfunction updateDamping() {\n  damping = computeDamping(viscosity, particleRadius, timeStep);\n}\n\n// Animate function\nfunction animate() {\n  const diffusion = (kB * temperature) / (6 * Math.PI * viscosity * particleRadius);\n\n  circles.forEach(c =&gt; {\n    // Apply gravity\n    c.vy += gravity * timeStep;\n\n    // Diffusion random kick scaled by sqrt(2*D/dt)\n    const diffusionForce = Math.sqrt(2 * diffusion / timeStep);\n    c.vx += (Math.random() - 0.5) * diffusionForce;\n    c.vy += (Math.random() - 0.5) * diffusionForce;\n\n    // Apply damping\n    c.vx *= damping;\n    c.vy *= damping;\n\n    // Update position\n    c.x += c.vx * timeStep;\n    c.y += c.vy * timeStep;\n\n    // Boundary collision\n    if (c.x - c.radius &lt; 0) {\n      c.x = c.radius;\n      c.vx *= -1;\n    }\n    if (c.x + c.radius &gt; width) {\n      c.x = width - c.radius;\n      c.vx *= -1;\n    }\n    if (c.y - c.radius &lt; 0) {\n      c.y = c.radius;\n      c.vy *= -1;\n    }\n    if (c.y + c.radius &gt; height) {\n      c.y = height - c.radius;\n      c.vy *= -1;\n    }\n  });\n\n  handleCollisions();\n\n  circleElements\n    .attr(\"cx\", d =&gt; d.x)\n    .attr(\"cy\", d =&gt; d.y);\n\n  diffusionDisplay.text(`Diffusion: ${diffusion.toFixed(4)}`);\n}\n\n// Start timer\nconst timer = d3.timer(animate);\n\n// Controls container\nconst controls = d3.create(\"div\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"padding\", \"10px\")\n  .style(\"background\", \"#f0f0f0\")\n  .style(\"border-radius\", \"8px\")\n  .style(\"width\", `${width}px`);\n\n// Slider factory\nfunction createSlider(labelText, min, max, initial, step, onChange) {\n  const container = controls.append(\"div\")\n    .style(\"margin-bottom\", \"10px\")\n    .style(\"display\", \"flex\")\n    .style(\"align-items\", \"center\")\n    .style(\"gap\", \"10px\");\n\n  container.append(\"label\")\n    .text(labelText)\n    .style(\"min-width\", \"120px\")\n    .style(\"font-weight\", \"bold\");\n\n  const slider = container.append(\"input\")\n    .attr(\"type\", \"range\")\n    .attr(\"min\", min)\n    .attr(\"max\", max)\n    .attr(\"step\", step)\n    .attr(\"value\", initial)\n    .style(\"flex\", \"1\");\n\n  const valueDisplay = container.append(\"span\")\n    .text(initial.toFixed(step &lt; 1 ? 4 : 0))\n    .style(\"width\", \"50px\")\n    .style(\"text-align\", \"right\")\n    .style(\"font-family\", \"monospace\");\n\n  slider.on(\"input\", function () {\n    const val = +this.value;\n    onChange(val);\n    valueDisplay.text(val.toFixed(step &lt; 1 ? 4 : 0));\n  });\n\n  return slider;\n}\n\n// Create sliders\ncreateSlider(\"Gravity\", -100, 100, gravity / gravityFactor, 10, v =&gt; gravity = v * gravityFactor);\n\ncreateSlider(\"Temperature\", 0, 1000, temperature, 10, v =&gt; temperature = v);\n\ncreateSlider(\"Viscosity\", 0.001, 0.1, viscosity, 0.001, v =&gt; {\n  viscosity = v;\n  updateDamping();\n});\n\n// createSlider(\"Time Step\", 0.1, 5, timeStep, 0.1, v =&gt; {\n//   timeStep = v;\n//   updateDamping();\n// });\n\n// Diffusion display\nconst diffusionDisplay = controls.append(\"div\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"font-family\", \"monospace\")\n  .style(\"font-weight\", \"bold\")\n  .text(\"\");\n\n// Reset button\ncontrols.append(\"button\")\n  .text(\"Reset Positions\")\n  .style(\"margin-top\", \"10px\")\n  .style(\"padding\", \"6px 12px\")\n  .on(\"click\", () =&gt; {\n    circles.forEach(c =&gt; {\n      c.x = Math.random() * width;\n      c.y = Math.random() * height * 0.3;\n      c.vx = (Math.random() - 0.5) * 2;\n      c.vy = (Math.random() - 0.5) * 2;\n    });\n  });\n\n// Start/Stop toggle\nconst startStopBtn = controls.append(\"button\")\n  .text(\"Stop\")\n  .style(\"margin-left\", \"10px\")\n  .style(\"padding\", \"6px 12px\")\n  .on(\"click\", function () {\n    if (timer._call) {\n      timer.stop();\n      this.textContent = \"Start\";\n    } else {\n      timer.restart(animate);\n      this.textContent = \"Stop\";\n    }\n  });\n\n// Container div\nconst container = d3.create(\"div\");\ncontainer.node().appendChild(svg.node());\ncontainer.node().appendChild(controls.node());\n\nreturn container.node();\n}\n\n\n\n\n\n\n\nIn the case of gravity, this leads to a sedimentation equilibrium. (a) Flux due to external force, \\(J_{F}\\)\nThe velocity of a particle under an applied force \\(F\\) in a viscous fluid can be written as \\(v=\\) \\(F / \\xi\\) which defines the friction coefficient \\(\\xi\\). Hence\n\\[\nJ_{F}=n(x) v=\\frac{n(x) F}{\\xi}\n\\]\n\nDiffusive flux, \\(J_{D}\\) \\(J_{D}\\) is given by Fick’s Law (see above):\n\n\\[\nJ_{D}(x)=-D \\frac{\\partial n(x)}{\\partial x}\n\\]\nEquating the two fluxes \\(J_{F}=J_{D}\\) we get\n\\[\n\\frac{n(x) F}{\\xi}=-D \\frac{\\partial n(x)}{d x}=+D \\frac{F}{k_{B} T} n(x)\n\\]\nThe second equation is obtained by differentiating the Maxwell-Boltzmann distribution. This gives the relation between the diffusion and friction coefficients:\n\\[\nD=\\frac{k_{B} T}{\\xi}=\\frac{k_{B} T}{6 \\pi \\eta R}\n\\]\nThe last equation applies to a spherical particle of radius \\(R\\) in a fluid of viscosity \\(\\eta\\), for which Stokes’s Law gives \\(\\xi=6 \\pi \\eta R\\) (which applies only at low Reynold’s number, \\(\\rho R v / \\eta\\) \\(\\ll 1\\) ) resulting in the Stokes-Einstein (and Sutherland) relation.\n\nThe Stokes-Einstein relation is a very deep result. It relates equilibrium fluctuations in a system to the energy dissipation when the system is driven off equilibrium. Here, the fluctuations in the fluid give rise to the diffusive motion of the suspended particle and \\(D\\) is therefore the ‘fluctuation’ part. A sheared fluid will dissipate energy because of its finite viscosity and thus \\(\\eta\\) represents the dissipative part.\nMore generally, Brownian motion sets a natural limit to the precision of physical measurements. Example from the Feynman Lectures:\n\nA mirror suspended on a torsion fibre reflects a spot of light onto a scale. The spot will jiggle due to the random impact of air molecules and the random motion of atoms in the quartz fibre. To reduce the jiggle, the apparatus has to be cooled. The relation between fluctuation and dissipation tells us where to cool. ‘This depends upon where [the mirror] is getting its ’kicks’ from. If it is through the fibre, we cool it … if the mirror is surrounded by a gas and is getting hit mostly by collisions in the gas, it is better to cool the gas. As a matter of fact, if we know where the damping of the oscillations comes from, it turns out that that is always the source of the fluctuations. (Adapted from Feynman, Chapter 41)\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nColloids are mixtures with dispersed particles (1 nm – 1 μm) that remain suspended due to Brownian motion.\nStability of colloids depends on the balance between attractive (van der Waals) and repulsive (double layer) interactions.\nThe DLVO theory combines van der Waals attraction and double layer repulsion to explain colloidal stability and aggregation.\nEntropic effects (e.g., depletion interactions) can induce effective attractions even in purely repulsive systems.\nHard-sphere colloids are a fundamental model: their phase behavior is controlled by packing fraction, leading to fluid, crystalline, and metastable (jammed) states.\nThe radial distribution function \\(g(r)\\) characterizes spatial correlations and structure in colloidal systems.\nDynamics of colloids are governed by Brownian motion, with diffusion slowed at higher densities due to interactions.\nThe Stokes-Einstein relation links diffusion to temperature, viscosity, and particle size.\nEntropy can drive ordering (e.g., crystallization of hard spheres), showing that entropy is not always associated with disorder.\nColloidal systems serve as accessible models for studying fundamental concepts in soft matter and statistical mechanics.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#references",
    "href": "soft-matter/soft-matter_colloids.html#references",
    "title": "15  Colloids",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHamaker, Hugo C. 1937. “The London—van Der Waals Attraction Between Spherical Particles.” Physica 4 (10): 1058–72. https://www.sciencedirect.com/science/article/pii/S0031891437802037.\n\n\nLekkerkerker, Henk NW, Remco Tuinier, and Mark Vis. 2024. Colloids and the Depletion Interaction. Springer Nature. https://bris.on.worldcat.org/oclc/1428319704.\n\n\nLondon, Fritz. 1937. “The General Theory of Molecular Forces.” Transactions of the Faraday Society 33: 8b–26.\n\n\nPusey, PN, E Zaccarelli, C Valeriani, E Sanz, Wilson CK Poon, and Michael E Cates. 2009. “Hard Spheres: Crystallization and Glass Formation.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 367 (1909): 4993–5011. https://bris.on.worldcat.org/oclc/8582157462.\n\n\nRoyall, C Patrick, Patrick Charbonneau, Marjolein Dijkstra, John Russo, Frank Smallenburg, Thomas Speck, and Chantal Valeriani. 2024. “Colloidal Hard Spheres: Triumphs, Challenges, and Mysteries.” Reviews of Modern Physics 96 (4): 045003. https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.96.045003.\n\n\nSantos, Andrés. 2016. “A Concise Course on the Theory of Classical Liquids.” Lecture Notes in Physics 923: 064601–4. https://bris.on.worldcat.org/oclc/949904359.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_colloids.html#footnotes",
    "href": "soft-matter/soft-matter_colloids.html#footnotes",
    "title": "15  Colloids",
    "section": "",
    "text": "This is clearly an instance of the idea of coarse graining that we have introduced earlier.↩︎\nAs you see, scaling by \\(k_B T\\) is a recurrent feature of soft matter.↩︎\nFrom the Greek στερεός, “solid, three-dimensional”.↩︎",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Colloids</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html",
    "href": "soft-matter/soft-matter_polymers.html",
    "title": "16  Polymers",
    "section": "",
    "text": "16.1 General molecular properties\nA polymer is a large molecule made up of many small, simple chemical units, joined together by chemical bonds. The basic unit of this sequence is called a monomer, and the number of units in the sequence is called the degree of polymerisation.\nIt is possible to have polymers containing over \\(10^{5}\\) units and there are naturally occurring polymers with a degree of polymerisation exceeding \\(10^{9}\\). For example, polystyrene with a degree of polymerisation of \\(10^{5}\\) has a molecular weight of about \\(10^{7} \\mathrm{~g} / \\mathrm{mol}\\) and, if fully stretched out, would be about 25 \\(\\mu \\mathrm{m}\\) long.\nPolymers play a central role in many fields, ranging from technology to biology. This is reflected in a huge number of different chemical structures. Given this manifold and the complexity of polymer molecules, the theories are astonishingly simple. This is possible because of the characteristic feature of polymers: The molecule itself is very large (compared to the individual units) and the macroscopic behaviour is dominated by this large-scale property of the molecule. Theories focus on such large-scale properties, whereas the small scale fine structure is typicaly resolved by computer simulations.\nPolymers exist in very different architectures, such as\nFurthermore, a large variation in the chemical structure may be achieved by combining different monomers (copolymerisation).\nFor simplicity, we will focus on linear homopolymers, i.e. with no branch points and with all identical subunits.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#general-molecular-properties",
    "href": "soft-matter/soft-matter_polymers.html#general-molecular-properties",
    "title": "16  Polymers",
    "section": "",
    "text": "linear: straight chains of repeating units (e.g., polyethylene).\nbranched: polymers with a main chain and side chains (branches) attached. The branching affects properties like density and melting point (e.g., low-density polyethylene).\nstar: polymers with multiple linear chains (arms) radiating from a central core. These structures can have unique properties like lower viscosity.\ncross-linked: polymers where chains are interconnected by covalent bonds, forming a network. This structure makes them rigid and heat-resistant (e.g., vulcanized rubber).\n\n\n\n\n16.1.1 Example structures\nHere below you can see the chemical structures of (sections) of a few common polymers. Hovering on the diagram should highlight the repeated units.\n\n\n\n\n\n\nHow to use the visualisers\n\n\n\n\n\nCick on the 🔧 symbol to customise the visualisation:\n\nyou should see a State Tree to your left\nclick on the last item (highlighted in light blue, by default Ball & Stick)\nclick on Update 3D representation and select a suitable Type. For example the Cartoon mode will correspond to the kinnd of coarse graining we will have in mind when talking about polymer chains.\n\n\n\n\n\nPolystyrene \\(\\rm (C_8H_8)_n\\)\n    \n    \n    \n\n\nPoly(methyl methacrylate) (PMMA) \\(\\rm (C_5H_8O_2)_n\\)\nThis polymer is also known as acrylic, acrylic glass, or plexiglass. This a very common material also to fabricate colloids (PMMA particles).\n    \n    \n    \n\n\nNatural rubber \\(\\rm (C_5H_8)_n\\)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#models-for-the-conformation-of-polymers",
    "href": "soft-matter/soft-matter_polymers.html#models-for-the-conformation-of-polymers",
    "title": "16  Polymers",
    "section": "16.2 Models for the conformation of polymers",
    "text": "16.2 Models for the conformation of polymers\nIn order to understand the properties of most substances, we must consider a large assembly of molecules. In the case of polymers, however, the molecules themselves are very large and due to their flexibility they can take up an enormous number of configurations by rotation of chemical bonds. The shape of the polymer can therefore only be usefully described statistically and one need to use statistical mechanics to calculate the characteristics of even an isolated polymer.\nTo be able to investigate the properties of a single polymer and to neglect interactions between polymers, the polymer is placed in a very dilute solution. In this chapter, we will theoretically investigate the properties of an isolated, single polymer chain in solution (which in addition is linear and consists of only one kind of monomers).\n\n16.2.1 Freely-jointed chain\nMany polymers are highly flexible and are coiled up in solution. In a simple model we thus describe a polymer with two assumptions :\n\nit is composed of a large number of segments freely joined up\nall angles between segments are assumed to be equally likely.\n\nThe momomers are located at positions \\(\\mathbf{R}_{\\mathrm{j}}\\) and connected by bonds \\(\\mathbf{r}_{j}=\\mathbf{R}_{j}-\\mathbf{R}_{j-1}\\) of length \\(\\left|\\mathbf{r}_{j}\\right|=b_{0}\\).\nThe end-to-end vector \\(\\mathbf{R}\\) is then simply the vector linking the head to the tail fo the chain:\n\\[\\mathbf{R} = \\mathbf{r}_1+\\mathbf{r}_2+\\dots+\\mathbf{r}_N = \\sum_{j=1}^N \\mathbf{r}_j.\\]\nAt any instant, the configuration (arrangement) of the polymer is one realisation of an N-step random walk in three dimensions.\n\n\n\n\n\n\n\nEnd-to-end vector\nIn time, the various segments undergo Brownian motion and the polymer fluctuates between all possible configurations of the random walk. Rememebr however that it does this with fixed inter-monomer distance.\nThe mean squared end-to-end distance is then simply \\[\\begin{aligned}\\langle\\mathbf{R}^2\\rangle &= \\left\\langle \\left( \\sum_{i=1}^N \\mathbf{r}_i\\right) \\cdot  \\left( \\sum_{j=1}^N \\mathbf{r}_j\\right) \\right\\rangle\\\\\n&= \\left\\langle \\sum_{i=1}^N \\sum_{j=1}^N  \\mathbf{r}_i \\mathbf{r}_j \\right\\rangle\\\\\n\\end{aligned}\\] We can split the sum into the terms where \\(i=j\\) and the rest. This yields in general\n\\[\\langle \\mathbf{R}^2\\rangle = Nb_0^2+\\langle \\mathbf{r}_i \\mathbf{r}_j  \\rangle\\]\nwhere the second term simply encodes the covariances between the (random) variables \\(\\mathbf{r}_i,\\mathbf{r}_j\\). Since we assumed (in this simplistic case) that they are independent, only the first term remains for the free-jointed chain\n\\[\\langle \\mathbf{R}^2\\rangle = Nb_0^2\\]\nThis is essentially the same result as for the mean squared displacement of a random walk, provided that we recognise that the number of monomers has taken the role played by time inbthe case of the wal. We had \\({\\rm MSD}\\propto t\\) and here we have\n\\[\\langle \\mathbf{R}^2\\rangle\\propto N\\]\nSince we formally inherit all the results holding for a random walk, we can also predict what happens to the distribution of possible end-to-end distances in the limit of long polymers, i.e. large \\(N\\).\nThe mean squared end-to-end distance can be decomposed as\n\\[\n\\left\\langle\\mathbf{R}^2\\right\\rangle=\\left\\langle R_x^2\\right\\rangle+\\left\\langle R_y^2\\right\\rangle+\\left\\langle R_z^2\\right\\rangle=3 \\sigma^2=N b_0^2 \\Rightarrow \\sigma^2=\\frac{N b_0^2}{3}\n\\] where \\(\\sigma\\) is the variance per component.\nWe know that the random walk converges to a Gaussian distribution in 3d with each component having variance \\(\\sigma\\). Thus, for large \\(N\\), the probability distribution for the end-to-end vector \\(\\mathbf{R}\\) is \\[\nP(\\mathbf{R}) = \\left( \\frac{3}{2\\pi N b_0^2} \\right)^{3/2} \\exp\\left( -\\frac{3 \\mathbf{R}^2}{2 N b_0^2} \\right)\n\\]\nThis means that, for long chains, the end-to-end distance is distributed as a 3D Gaussian, centered at zero, with variance proportional to \\(N\\). Also, we can\n\n\nRadius of gyration\nWhile the end-to-end distance represents a well-defined quantity for a linear chain, we need a more versatile measure of the size for more complicated architectures, such as branched or star-shaped polymers. This is provided by the so called radius of gyration, a measure of the (average) extent of the polymer chain.\nThe radius of gyration is a generic quantity that can be measured from any point cloud. It is closely linked to the (co)-variance of the set of points.\nWe define the center of mass as the average position\n\\[\\mathbf{R}_{\\rm CM}=\\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{R}_{j}\\]\nA general way to describe the spatial extent and shape of the polymer is to construct the gyration tensor (also called the configuration tensor):\n\\[\n\\mathbf{S} = \\frac{1}{N} \\sum_{j=1}^N (\\mathbf{R}_j - \\mathbf{R}_{\\rm CM}) \\otimes (\\mathbf{R}_j - \\mathbf{R}_{\\rm CM})\n\\]\nwhere \\(\\otimes\\) denotes the outer product, and \\(\\mathbf{S}\\) is a \\(3 \\times 3\\) symmetric matrix. The elements of \\(\\mathbf{S}\\) are given by\n\\[\nS_{\\alpha\\beta} = \\frac{1}{N} \\sum_{j=1}^N (R_{j,\\alpha} - R_{{\\rm CM},\\alpha})(R_{j,\\beta} - R_{{\\rm CM},\\beta})\n\\]\nwhere \\(\\alpha, \\beta \\in \\{x, y, z\\}\\).\nThe radius of gyration squared is then simply the trace of this tensor:\n\\[\nR_g^2 = \\mathrm{Tr}(\\mathbf{S}) = S_{xx} + S_{yy} + S_{zz}\n\\]\nThe eigenvalues and eigenvectors of \\(\\mathbf{S}\\) provide information about the principal axes and shape anisotropy of the polymer coil. The tensor of gyration correspondss to the covariance matrix of the positions \\(\\mathbf{R}_j\\).\nThe radius of gyration is direcly proportional to the end-to-end vector.\n\n\\[\n\\langle R_g^2 \\rangle = \\frac{1}{6} \\langle R^2 \\rangle\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe radius of gyrations is \\[\nR_g^2=\\frac{1}{N} \\sum_{j=1}^N\\left(\\mathbf{R}_j-\\mathbf{R}_G\\right)^2\n\\]\nWe can rewrite it as \\[\nR_g^2 = \\frac{1}{2N^2} \\sum_{j=1}^N \\sum_{k=1}^N (\\mathbf{R}_j - \\mathbf{R}_k)^2\n\\]\nand note that, from our previous result on the end-to-end distance, the mean squared distance between monomers \\(j\\) and \\(k\\) in a free jointed chain is \\[\n\\langle (\\mathbf{R}_j - \\mathbf{R}_k)^2 \\rangle = |j - k| b_0^2\n\\]\nSo, \\[\n\\langle R_g^2 \\rangle = \\frac{b_0^2}{2N^2} \\sum_{j=1}^N \\sum_{k=1}^N |j - k|\n\\]\nEvaluating the double sum gives: \\[\n\\sum_{j=1}^N \\sum_{k=1}^N |j - k| = \\frac{N^3 - N}{3}\n\\]\nThus, \\[\n\\langle R_g^2 \\rangle = \\frac{b_0^2}{2N^2} \\cdot \\frac{N^3 - N}{3} = \\frac{b_0^2}{6} (N - \\frac{1}{N})\n\\]\nFor large \\(N \\gg 1\\): \\[\n\\langle R_g^2 \\rangle \\approx \\frac{1}{6} N b_0^2\n\\]\n\n\n\n\n\n\n16.2.2 Freely-rotating chain\nIn a polymer molecule the bond angles are usually restricted, which leads to a limited flexibility of the molecule. Let us consider the case of \\(n\\)-butane:\n\\[\n\\mathrm{H}_{3} \\mathrm{C}-\\mathrm{CH}_{2}-\\mathrm{CH}_{2}-\\mathrm{CH}_{3}\n\\]\n\nN-butane\n    \n    \n    \nThe valence angle (also called the bond angle) is the angle formed between two adjacent chemical bonds originating from the same atom. In the context of polymers, it is the angle between two consecutive bonds along the polymer backbone. The value of the valence angle is determined by the chemical structure of the monomer and affects the flexibility and overall conformation of the polymer chain. For example, in \\(n\\)-butane, the C–C–C bond angle is about \\(112^\\circ\\). Still rotations about the C-C bond are possible.\n\n\n\nFour conformers of butane, from LibreText Chemistry\n\n\nIndeed, the potential energy of a polymer configuration depends on the valence (dihedral) angle.\n\n\n\nPotential energy of the above conformers\n\n\nAt low temperatures ( \\(k_{B} T &lt; {\\mathrm {\\text{configurational energy}}}\\)) the configuration will thus be predominantly of type A (an anti conformation). As the temperature is increased ( \\(k_{B} T \\sim\\) config. energy), there will also be C (gauche) configurations and at high temperatures ( \\(k_{B} T \\gg\\) config. energy), any angle will be possible.\nThis suggests that a good model of polymeric conformation can take fixed angles between bonds, but needs to include free rotation about the bonds. This model is called the freely-rotating chain.\n\n\n\nWe start with a fixed configuration of \\(\\mathbf{r}_{l}\\), \\(\\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\) and then add the next segment \\(\\mathbf{r}_{j}\\). While the bond angle \\(\\theta\\) is given by the chemistry of the molecule, the segment can still freely rotate about the axis defined by \\(r_{j-1}\\), i.e. \\(\\varphi\\) can take any value \\(0 \\leq \\varphi \\leq 2 \\pi\\). If we average \\(\\mathbf{r}_{j}\\) over \\(\\varphi\\), while keeping \\(\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\) fixed, only the component in \\(\\mathbf{r}_{j}\\) direction remains:\n\\[\\langle \\mathbf{r}_{j} \\rangle_{\\mathbf{r}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{j-1}\\quad {\\rm fixed}}=\\cos \\theta \\mathbf{r}_{j-1}\\]\nFor the calculation of \\(\\left\\langle R^{2}\\right\\rangle\\) we also need \\(\\left\\langle\\mathbf{r}_{j} \\cdot \\mathbf{r}_{k}\\right\\rangle\\), which we obtain by multiplying both sides with \\(\\mathbf{r}_{k}\\) and taking the average: \\(\\left\\langle r_{j} \\mathbf{x}_{k}\\right\\rangle=\\cos \\theta\\left\\langle r_{j-1} \\mathbf{x}_{k}\\right\\rangle=\\cos ^{2} \\theta\\left\\langle r_{j-2} \\mathbf{\\Sigma}_{k}\\right\\rangle=\\ldots=\\left(\\cos ^{|j-k|} \\theta\\right)\\left\\langle r_{k}{\\mathbf{x_{k}}}\\right\\rangle=\\left(\\cos ^{|j-k|} \\theta\\right) b_{0}^{2}\\)$\nSince \\(\\cos \\theta&lt;1\\), correlations between \\(\\mathbf{r}_{j}\\) and \\(\\mathbf{r}_{k}\\) decrease with increasing distance \\(|j-k|\\) between the links and the orientations of distant links become uncorrelated.\nThe end-to-end distance \\(\\left\\langle R^{2}\\right\\rangle\\) of a freely-rotating chain is hence \\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =\\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left\\langle\\mathbf{r}_{j} \\cdot \\mathbf{r}_{k}\\right\\rangle=b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\cos ^{(j-k \\mid} \\theta\\right)=N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\sum_{k=1}^{j-1} \\cos ^{(j-k)} \\theta \\\\\n& =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\theta \\sum_{k=1}^{j-1} \\cos ^{-k} \\theta\n\\end{aligned}\n\\]\nwhere we used the same argument as above to deal with \\(|j-k|\\). To calculate these two sums we consider the geometric progression:\n\\[\n\\begin{aligned}\n& S=\\sum_{m=1}^{M} x^{m}=x+x^{2}+\\ldots .+x^{M} \\quad \\therefore x S=x^{2}+x^{3}+\\ldots .+x^{M+1} \\quad \\therefore S-x S=x-x^{M+1} \\\\\n& \\therefore S=\\frac{x-x^{M+1}}{1-x}\n\\end{aligned}\n\\]\nWith the help of this formula we get\n\\[\n\\begin{aligned}\n\\left\\langle R^{2}\\right\\rangle & =N b_{0}^{2}+2 b_{0}^{2} \\sum_{j=1}^{N} \\cos ^{j} \\theta \\frac{\\frac{1}{\\cos \\theta}-\\frac{1}{\\cos ^{j} \\theta}}{1-\\frac{1}{\\cos \\theta}}=N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\theta-1}\\left(\\sum_{j=1}^{N} \\cos ^{j} \\theta-\\sum_{j=1}^{N} \\cos \\theta\\right) \\\\\n& =N b_{0}^{2}+\\frac{2 b_{0}^{2}}{\\cos \\theta-1}\\left(\\frac{\\cos \\theta-\\cos ^{N+1} \\theta}{1-\\cos \\theta}-N \\cos \\theta\\right)\n\\end{aligned}\n\\]\nFor large N this can be simplified\n\\[\\left\\langle R^{2}\\right\\rangle \\approx N b_{0}^{2}+\\frac{2 b_{0}^{2}}{1-\\cos \\theta} N \\cos \\theta=N b_{0}^{2}\\left(\\frac{1+\\cos \\theta}{1-\\cos \\theta}\\right)=C N b_{0}^{2}\\]\nwith \\(C=(1+\\cos \\theta) /(1-\\cos \\theta)\\).\nTo get a better idea of the effect of a fixed angle \\(\\theta\\), i.e. going from a freely-jointed to a freely-rotating chain, we look at a few special (but not necessarily very realistic) cases:\n\n\\(\\theta \\rightarrow 0 \\Rightarrow \\cos \\theta \\rightarrow 1-\\frac{\\theta^{2}}{2} \\Rightarrow C=\\frac{2-\\theta^{2} / 2}{\\theta^{2} / 2} \\approx \\frac{4}{\\theta^{2}} \\quad\\left(\\mathrm{C} \\approx 500\\right.\\) for \\(\\left.\\theta=5^{0}\\right)\\)\n\n\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle \\gg N b_{0}^{2}\n\\]\n\nThis corresponds to a nearly straight chain, i.e., a rigid rod. The end-to-end distance is much larger than that of a flexible chain with the same number of segments.\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\\[\n\\begin{aligned}\n\\theta \\rightarrow \\pi-\\delta \\quad & \\Rightarrow \\cos \\theta \\rightarrow-1-\\frac{\\delta^{2}}{2} \\Rightarrow C=\\frac{\\delta^{2} / 2}{2-\\delta^{2} / 2} \\approx \\frac{\\delta^{2}}{4} \\quad\\left(\\mathrm{C} \\approx 2 \\times 10^{-3} \\text { for } \\theta=175^{0}\\right) \\\\\n\\end{aligned}\n\\]\n\n\\[\\therefore\\left\\langle R^{2}\\right\\rangle \\ll N b_{0}^{2}\\]\n\nThis corresponds to the opposite limit, where the chain is compact and forms a globular, collapsed assembly. Examples include:\n\npolypeptides (the co nstituents of amino acids) with strong hydrophobic interactions (e.g. alanine, leucine, methionine)\npolystyrene in water\nchromatin (the genetic information condensed in the nucleus of cells)\n\n\n\n\n                            \n                                            \n\n\n\n\n\\(\\theta \\rightarrow \\pi / 2 \\Rightarrow \\cos \\theta \\rightarrow 0 \\Rightarrow C=1\\)\n\n\\[\n\\therefore\\left\\langle R^{2}\\right\\rangle=N b_{0}^{2}\\]\n\n\nThese are ideal conditions, where the random walk model of the free-jointed chain works exactly.\n\nAgain, we get \\(\\left\\langle R^{2}\\right\\rangle \\propto N\\), which suggests that a long freely-rotating chain can be represented by an effective freely-jointed chain with \\(N'\\) segments of length \\(b\\).\n\n\nThis is another example of coarse-graining.\nReal and effective chain must have the same actual length, i.e. \\[\nN b_0 = N' b\n\\] and the same end-to-end distance, \\[\nC N b_0^2 = N' b^2.\n\\] Solving these two equations gives \\(b = C b_0\\) and \\(N' = N / C\\).\nThese constraints result in \\(b=C b_{0}\\) and \\(N^{\\prime}=N / C\\). This has important consequences:\n\nAll sufficiently long flexible chains have identical behaviour as regards their dimensions: the chemical details are hidden in \\(N^{\\prime}\\) and \\(b.\\)\nWhile individual monomer pairs are not totally flexible, groups of monomers are\n\\(C\\) represents the number of monomers over which the orientational correlation is lost\n\\(b\\) is the so called “Kuhn” statistical segment length and defines a related characteristic known as the “persistence length” \\(l_{p}=b/2\\), also calculated directly via the correlation of bond vectors along the chain: \\[\n\\langle \\mathbf{r}_i \\cdot \\mathbf{r}_{i+n} \\rangle = b_0^2 \\, \\langle \\cos \\theta \\rangle^n = b_0^2 \\, e^{-n b_0 / l_p}\n\\] where \\(b_0\\) is the bond length, \\(\\theta\\) is the angle between consecutive bonds, and \\(n\\) is the number of bonds separating the two segments.\n\nFor the freely-rotating chain, \\[\nl_p = -\\frac{b_0}{\\ln \\langle \\cos \\theta \\rangle}\n\\]\nFor small angles, this simplifies to \\(l_p \\approx \\frac{b_0}{1 - \\langle \\cos \\theta \\rangle}\\).\n\n\nOriginal contour length: 200.0000\nCoarse-grained contour length: 163.8304\n\n\n                            \n                                            \n\n\n\n\n\nThis visualisation doesn’t strictly satisfy the equal length requirement we set out above, but satisfies the equal end-to-ende vector for the sake of visualisation. The Kuhn length should be thought of as a statistical quantity and teh coarse-graiend chain as a polymer with the equivalent statistical propeeties to a chain of subunits of the original chain.\n\n\n\n16.2.3 Excluded volume effects\nThe fact that two monomers cannot occupy the same space has consequences on different length scales.\n\nOn a local length scale this prevents neighbouring monomers from coming too close together. This effect is taken into account in terms of a restricted bond-angles, which prevent them from overlaping.\nNon-overlap, i.e. excluded volume, of distant monomers along the chain has also to be taken into account and can have surprisingly large effects.\n\nTo estimate the importance of this effect, we consider the fraction of coil volume actually occupied by monomers:\n\\[\\quad V_{N}=N V_{1} \\sim N b^{3},\\]\nwhere \\(\\mathrm{V}_{1}\\) is the volume of a monomer.\nThe overall volume occupied by the whole coil is\n\\[\nV_{\\text {coil }}=\\frac{4 \\pi}{3}\\langle R_{g}^{2}\\rangle^{3 / 2} \\sim \\frac{4 \\pi}{3} N^{3 / 2} b^{3}\n\\]\n\\[\\therefore \\frac{V_{N}}{V_{\\text {coil }}}=\\frac{N b^{3}}{(4 \\pi / 3) N^{3 / 2} b^{3}} \\sim N^{-1 / 2}\\]\nThis means that for \\(N=10^{4}\\) monomers occupy only about \\(1 \\%\\) of the whole coil volume.\nThe overall chain size as estimated by the rafdius of gyration \\(\\langle R_{g}^{2}\\rangle^{1 / 2}\\) is determined by the competition of two effects:\n\nEntropy (and chain connectivity) favour a compact chain and avoid the more unlikely stretched configurations\nRepulsive excluded volume interactions want to expand the chain to avoid overlap.\n\nBased on this balance we will estimate the effect of excluded volume in a very hand-waving way. (Due to a fortuitous cancellation of errors introduced by various approximations, the result is practically identical to more rigorous treatments, which are very involved.)\nWe consider the Helmholtz function of a single chain, which is regarded as an assembly of particles with constant volume \\(\\mathrm{NV}_{1}\\) at constant temperature T :\n\\[\nF=U-T S\n\\]\nThe entropy S is given by\n\\[\nS=k_{B} \\ln (\\text {number of configurations })\n\\]\nwhere for a given end-to-end vector \\(\\mathbf{R}\\) the number of configurations is expected to be proportional to\n\\[\nP(\\mathbf{R})=\\left(\\frac{3}{2 \\pi&lt;R^{2}&gt;}\\right)^{3 / 2} e^{-\\frac{3 R^{2}}{2&lt;R^{2}&gt;}}\n\\]\nand hence\n\\[\nS \\sim \\frac{-3 k_{B} R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nThe internal energy \\(U\\) includes the kinetic and potential energy. However, the kinetic energy is independent of the configuration and thus of \\(\\mathbf{R}\\) and we only have to consider the potential energy.\nTo estimate the potential energy, we disregard the connectivity of the chain and calculate the interaction energy of a ‘segment gas’ confined in a volume \\(R^{3}\\). The probability of a monomer to lie in this volume is given by the fraction of total coil volume occupied by monomers, which we estimated above to be \\(N V_{1} / R^{3}\\)\nThus the probability of monomer-monomer contacts is \\(N^{2} V_{1} / R^{3} \\sim N^{1 / 2}\\). With an energy \\(\\varepsilon\\) of a monomer-monomer contact, the potential energy \\(U \\sim \\varepsilon N^{2} V_{1} / R^{3}\\). We thus obtain\n\\[\nF=\\frac{\\varepsilon N^{2} V_{1}}{R^{3}}+\\frac{3 k_{B} T R^{2}}{2 N b^{2}}+\\text { terms indep. of } \\mathrm{R}\n\\]\nwhich can be minimized with respect to \\(R\\), i.e. \\(d F / d R=0\\), yielding\n\\[R^{5}=\\frac{\\varepsilon V_{1} b^{2}}{k_{B} T} N^{3} \\sim \\frac{\\varepsilon}{k_{B} T} N^{3} b^{5}\\]\n\\[ \\therefore R \\sim N^{3 / 5} b\\]\nSimulations give a very similar scaling, \\(\\mathrm{R} \\sim N^{0.588}\\). The chain can no longer be modelled by a random walk, but has to be described by a self-avoiding random walk. The distribution of end-to-end distances is also not Gaussian. Note that the exponent \\(3/5\\) here is an example of a critical exponent (though not in exactly the same sense that you saw when studying magnets and fluids). Inclusion of excluded volume effects changes the universality class of the scaling of the chain length and radius of gyration with \\(N\\) compared to the Gaussian chains that we looked at previously.\nAlthough the difference between an exponent of 0.5 (as is characteristic for the freely jointed and freely-rotating chains, i.e. a random walk) and 0.6 (excluded volume chain, i.e. self-avoiding random walk) seems small, it has a large effect at large N . For example, for \\(N=10^{4}, R=N^{0.5} b=100 b\\), while \\(R=N^{0.6} b=251 b\\), which corresponds to a swelling of the chain by a factor of 2.5.\n\n\nThis is an example of the importance of accurate scaling exponents and how they can impact our predictions.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#good-poor-and-theta-solvents",
    "href": "soft-matter/soft-matter_polymers.html#good-poor-and-theta-solvents",
    "title": "16  Polymers",
    "section": "16.3 Good, poor and theta solvents",
    "text": "16.3 Good, poor and theta solvents\nSo far we only considered monomer-monomer interactions, which we assumed to be purely repulsive, and neglected the influence of the solvent.\nHowever, the type of solvent has a great effect on the polymer size. If there is a high affinity with the solvent (‘good solvent’) the polymer swells, while it will shrink in a ‘poor solvent’.\nSolvent affinity refers to how strongly the solvent molecules interact with the polymer (or monomer) molecules compared to how they interact with themselves or with other monomers. Mathematically, this is captured by the interaction energy \\(\\varepsilon_{sp}\\) (solvent-monomer), compared to the average of solvent-solvent (\\(\\varepsilon_{ss}\\)) and monomer-monomer (\\(\\varepsilon_{pp}\\)) interactions.\nWe consider a lattice model, where each lattice site has \\(z\\) nearest neighbours and there are \\(N_{s}\\) solvent molecules and \\(N_{p}\\) monomers, with \\(\\mathrm{N}_{\\mathrm{sp}}\\) solvent-monomer contacts.\nThe energies of interaction are\n\n\\(\\varepsilon_{s s}\\) for the solvent-solvent\n\\(\\varepsilon_{\\mathrm{pp}}\\) for monomer-monomer\n\\(\\varepsilon_{s p}\\) for solvent-monomer interactions.\n\nThen the energy of mixing \\(\\Delta U_{\\operatorname{mix}}\\) is given by\n\\[\n\\Delta U_{\\operatorname{mix}}=U-\\left(U_{S}+U_{p}\\right)\n\\]\nwhere energy of pure solvent is \\[U_{s}=\\frac{z N_{s} \\varepsilon_{s s}}{2}\\]\nand the energy of pure polymer is \\[U_{p}=\\frac{z N_{p} \\varepsilon_{p p}}{2}\\]\n\nresulting in an energy of solution \\[U=N_{s p} \\varepsilon_{s p}+\\frac{\\left(z N_{s}-N_{s p}\\right) \\varepsilon_{s s}}{2}+\\frac{\\left(z N_{p}-N_{s p}\\right) \\varepsilon_{p p}}{2}\\]\nHence we obtain for the energy of mixing\n\\[\n\\Delta U_{\\mathrm{mix}}=N_{s p}\\left[\\varepsilon_{s p}-\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right)\\right]\n\\]\nwhich can be either positive or negative:\n\nGood solvent: \\(\\varepsilon_{s p}&lt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&lt;0\\)\n\n\nThis is the case of a ‘good solvent’, because the monomers prefer to be near the solvent molecules. Excluded volume effects then expand the chain.\n\nPoor solvent: \\(\\varepsilon_{s p}&gt;\\frac{1}{2}\\left(\\varepsilon_{s s}+\\varepsilon_{p p}\\right) \\quad \\therefore \\Delta \\mathrm{U}_{\\text {mix }}&gt;0\\)\n\nThis is the case of a ‘poor solvent’, because the monomers prefer to be near to each other (and similarly for the solvent molecules). The attraction between the different monomers offset the excluded volume effect.\nThe importance of the attractions generally depends on temperature. At very high temperatures the coil is expanded and the solvent quality is good. In contrast, at very low temperatures, the solvent quality is poor, attraction dominates, the coil collapses and phase separation is observed. In between these two limits, there is a temperature, the so-called theta temperature \\(\\theta\\), where the coil has ideal dimensions and the effects of excluded volume and attraction cancel each other. The solvent at \\(T=\\theta\\) is called a ‘theta solvent’. The stronger the attractions the higher \\(\\theta\\) will be, while for weak attractions \\(\\theta\\) is low. A full treatment of the coil expansion is rather involved and has to take into account excluded volume, attractions, configurational entropy and entropy of mixing. You can read more about the theta condition in Jones (2002).\n\n\n\n\n\n\nCheck your understanding\n\n\n\n\nA polymer has characteristic features on different length scales.\nOn a very global length scale, it has a molar mass \\(M\\) and an overall size which can be characterised by the root mean square end-to-end distance \\(\\langle R^{2}\\rangle^{1 / 2}\\) or radius of gyration \\(\\langle R g^{2}\\rangle^{1 / 2 }\\propto N^{\\nu} \\propto M^{\\nu}\\), where \\(\\nu=1 / 2\\) for a freely-jointed or freely-rotating chain (random walk) and \\(\\nu=3 / 5\\) for an excluded volume chain (self-avoiding random walk).\nOn a smaller length scale the behaviour will be dominated by the finite flexibility or persistence of the chain, which is characterised by the Kuhn length b. The chain will essentially behave like a stiff rod on this length scale. This rod typically has a constant mass per length, \\(M / L\\), and thus \\(M \\alpha L\\).\nFinally, the local cross-sectional structure will be observed on an even smaller length scale.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#concentrated-polymer-solutions",
    "href": "soft-matter/soft-matter_polymers.html#concentrated-polymer-solutions",
    "title": "16  Polymers",
    "section": "16.4 Concentrated polymer solutions",
    "text": "16.4 Concentrated polymer solutions\nUp to now we considered a single polymer in a very dilute solution. Now we increase the concentration in steps until we reach bulk polymers.\nThe most important regimes of concentration are:\n\nDilute regime\n\n\nThe polymer coils are well-separated on average. Call \\(c\\) the concentration expressed as mass per unit volume, then it satisfies\n\\[ \\dfrac{c}{M}N_A \\times \\dfrac{4\\pi}{3}R_g^3\\ll 1\\]\n\n\nOverlap concentration \\(c*\\) \n\nOverlap occurs when the volume fraction of coils reaches unity and thus\n\\[\n\\frac{c^{*}}{M} N_{A} \\frac{4 \\pi}{3} R_{g}^{3} \\sim 1 \\quad \\therefore c^{*}=\\frac{3 M}{4 \\pi N_{A} R_{g}^{3}}\n\\]\nusing \\(R g=&lt;R g^{2&gt;^{1 / 2}}=B M^{\\nu}\\) gives\n\\[\nc^{*}=\\frac{3}{4 \\pi N_{A} B^{3}} M^{1-3 v}\n\\]\nFor example, polystyrene with \\(M=10^{6} \\mathrm{~g} \\mathrm{~mol}^{-1}\\) in a good solvent ( \\(v=0.6\\) ) and \\(B=0.028 \\mathrm{~nm}\\left(\\mathrm{~g} \\mathrm{~mol}^{-1}\\right)^{-0.6}\\) leads to \\(\\mathrm{c}^{*}=0.29 \\mathrm{~kg} \\mathrm{~m}^{-3}=0.29 \\mathrm{mg} / \\mathrm{ml}\\). With the density of polystyrene \\(\\rho=1050 \\mathrm{~kg} \\mathrm{~m}^{-3}\\), the volume fraction of monomers is \\(\\mathrm{c}^{*} / \\rho=0.28 \\times 10^{-3} . \\mathrm{c}^{*}\\) can be very small for large polymers.\n\nSemi-dilute\n\n\nThe concentration is larger than the overlap concentration \\(\\mathrm{c}^{*}\\), but still much smaller than the bulk density. The coils interpenetrate and entangle, but the solution is still mostly solvent\n\nConcentrated\n\nIn this case the concentration is very close to the bulk density and the polymer monomers occupy a significant fraction of the total volume.\n\nBulk polymers\n\nA bulk polymer refers to a polymeric material in which the polymer chains occupy a significant fraction of the total volume, with little or no solvent present. In this regime, the properties of the material are dominated by polymer-polymer interactions rather than polymer-solvent interactions. Bulk polymers can be amorphous, semi-crystalline, or crystalline, and their mechanical, thermal, and optical properties are determined by the arrangement and mobility of the polymer chains.\nBulk polymers are divided into two main classes, characterised by whether they are cross-linked or not. There are elastomers or rubbers with a low degree of cross-linking and thermosets with a high degree of cross-linking. We will investigate the behaviour of rubbers in the next chapter. The second class are thermoplastics, which are not cross-linked. Most everyday plastic products are thermoplastics. We will briefly discuss their behaviour upon cooling, which shows similarities to the behaviour of colloids. At high temperature the free energy is dominated by the entropic terms. The melt resembles a random assembly of mobile, intertwined, flexible coils with a density similar to the density of the corresponding monomer liquid. Upon cooling the potential energy takes over and the bonds are restricted in their rotation leading to configurations which are more straightened out. Below the melting temperature \\(\\mathrm{T}_{\\mathrm{m}}\\), a crystal is the lowest free energy state. Crystallisation, however, requires significant ordering of the initially random melt and is only possible if cooling occurs slow enough. If the melt is rapidly cooled below the glass transition temperature \\(T_{g}\\left(&lt;T_{m}\\right)\\), then instead of a crystal a glass is formed, which represents an amorphous metastable, but long-lived state. Although the polymers can still vibrate, they can no longer move. Solid thermoplastics are frequently a mixture of crystalline and amorphous structures.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_polymers.html#references",
    "href": "soft-matter/soft-matter_polymers.html#references",
    "title": "16  Polymers",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Polymers</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html",
    "href": "soft-matter/soft-matter_liquidcrystals.html",
    "title": "17  Liquid crystals",
    "section": "",
    "text": "17.1 Anisotropic particles\nOrdered phases of matter are typically characterised by spatial correlations, which can be positional or orientational in nature.\nPositional order is the regular arrangement of particle positions in space, often forming a repeating lattice structure. Orientational order refers to the angular alignment of the neighbours of a particle, influenced for example by the anisotropy of the particle itself (e.g. rods tending to align with other rods).\nIn crystalline phases, both positional and orientational order are long-ranged, meaning that the arrangement and orientation of particles are correlated over macroscopic distances. In contrast, simple liquids exhibit only very short-range (positional and orientational) correlations.\nLiquid crystals represent an intermediate case, where orientational order can persist over long distances while positional order remains short-ranged or absent.\nThe decoupling between orientational and translational degrees of freedom is favoured by individual microscopic units (particles) that already have preferential axis of symmetry, i.e. anisotropic particles. Anisotropic particles have shapes that are not spherically symmetric, such as rods, ellipsoids, or plates. Their interactions depend not only on the distance between particles but also on their relative orientations.\nThere are many kinds of anisotropic particles. The table below contains a non-exhaustive list.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#anisotropic-particles",
    "href": "soft-matter/soft-matter_liquidcrystals.html#anisotropic-particles",
    "title": "17  Liquid crystals",
    "section": "",
    "text": "Particle Shape\nDescription\nExample Materials\nTypical Applications\n\n\n\n\nRods\nCylindrical, length &gt; width\nGold nanorods, Tobacco virus\nPhotothermal therapy, plasmonics\n\n\nEllipsoids\nElongated or flattened spheroids\nHematite ellipsoids\nAnisotropic optics, directed self-assembly\n\n\nPlates/Discs\nFlat, disk-like shapes\nGraphene oxide, clay platelets\nBarrier materials, viscosity control\n\n\nPolyhedra\nMulti-faceted, highly symmetric shapes\nGold nanocubes, silica polyhedra\nPhotonic crystals, catalysis\n\n\n\n\n\n\nVarious assemblies of anisotropic particles. Left to right: Golden nanorods (from Stanford Advanced Materials), tobacco virus rods (adapted from Knapp and Lewandowski (2001)), ellipsoidal silica-coated hematite particles (adapted from Sánchez-Ferrer et al. (2010)), hard platelets in the isotropic phase (from Atashpendar, Ingenbrand, and Schilling (2020)), and an arrangement of octahedra simulated using hard particle Monte Carlo with hoomdblue",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#liquid-crystal-phases",
    "href": "soft-matter/soft-matter_liquidcrystals.html#liquid-crystal-phases",
    "title": "17  Liquid crystals",
    "section": "17.2 Liquid crystal phases",
    "text": "17.2 Liquid crystal phases\nThe additional orientational degrees of freedom makes opens up the possibilities of phases that do not normally exist for spherical, isotropic particles. We progress from the most disordered to the most ordered phase:\n\nIsotropic fluid: this phase is not very different from the ordinary fluid phase of spherical particles. There is no long-range positional or orientational order and all correlations decay rapidly and on short lengthscales.\nNematic phase: still without translational order, the constituents arrange themselves spontaneously in a preferential, average direction, called the director. Since there is no positional order, the transition from isotropic to nematic can only be detected by taking into account the relative orientations between the constituents, as the individual centres of mass are as disordered in the nematic as they are in the isotropic. If the particles are themselves chiral (i.e., the particle differs from its mirror image) the director tends to form a helix due to the propensity of te molecules to align at some angle between each other. This leads to so called chiral nematic phase (also known as cholesteric).\nsmectic phase: in this phase, particles not only align along a common direction (as in the nematic phase) but also exhibit partial translational order. The centers of mass of the particles tend to form well-defined layers, with orientational order within each layer. However, within a layer, the positions of the particles remain disordered, similar to a liquid. Smectic phases can be further classified (e.g., smectic A, smectic C) depending on the relative orientation of the director with respect to the layer normal.\nColumnar phase: here, anisotropic particles (often disc-like) stack into columns, which then arrange themselves into a two-dimensional lattice. There is long-range positional order in two directions (within the plane perpendicular to the columns) and orientational order along the column axis.\nCrystal: the most ordered phase, where both positional and orientational order are long-ranged in all directions, forming a true three-dimensional periodic lattice.\n\n\n\n\n\n\n\n\n\nPhase\nPositional Order\nOrientational Order\n\n\n\n\nIsotropic fluid\nNo\nNo\n\n\nNematic\nNo\nYes (long-range)\n\n\nChiral nematic\nNo\nYes (helical)\n\n\nSmectic\nYes (1D: layered)\nYes (within layers)\n\n\nColumnar\nYes (2D: columns)\nYes (along columns)\n\n\nCrystal\nYes (3D: lattice)\nYes (long-range)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#orientational-correlations-and-the-isotropicnematic-transition",
    "href": "soft-matter/soft-matter_liquidcrystals.html#orientational-correlations-and-the-isotropicnematic-transition",
    "title": "17  Liquid crystals",
    "section": "17.3 Orientational correlations and the isotropic/nematic transition",
    "text": "17.3 Orientational correlations and the isotropic/nematic transition\nWhile for the spherical colloids we focused on positional correlation functions like the radial distribution function, for anisotropic particle we need to quantify orientational correlations.\nSuppose we have \\(N\\) rod-like particles, each with an orientation angle \\(\\theta_i\\) (in 2D) or a unit vector \\(\\mathbf{u}_i\\) (in 3D). We want to determine the director, i.e. the average direction of the long molecular axes of all molecules in the liquid crystal.\nThe rods are head-tail symmetric, so, we cannot truly distinguish which one is the head or the tail (e.g. an orientation of \\(\\  theta\\) in 2D is the same as an orientation of \\(\\theta+\\pi\\)). We cannot therefore just take the average of the vectors to extract the common orientation of the various vectors, otherwise we would incur into a cancellation.\nInstead, we construct a second moment tensor called the alignment tensor\n\n\\[\n\\mathbf{Q}=\\dfrac{d}{2}\\left\\langle\\mathbf{u}_i \\otimes \\mathbf{n}_i-\\dfrac{1}{d}\\mathbf{I}\\right\\rangle\n\\]\n\nwhere \\(d\\) is the dimensionality. The removed \\(\\dfrac{1}{d}\\mathbf{I}\\) term ensures that the tensor is traceless and does not include isotropic components.\nThe analysis of the tensor yield the main characteristics of the orientation of the system: - the largest eigenvalue of \\(\\mathbf{Q}\\) is the scalar nematic order parameter \\(\\mathcal{S}\\) - the eigenvector corresponding to the largest eigenvalues is called the director \\(\\mathbf{n}\\) and corresponds to the main collective orientation of the system.\nIn two dimensions, the director is simply characterised by the angle \\(\\psi\\) expressed as \\[\n\\psi = \\frac{1}{2} \\operatorname{atan2}(\\sin{2\\theta_i}, \\cos{2\\theta_i}).\n\\] and the nematic order parameter is simply\n\\[\n\\mathcal{S}=\\sqrt{\\left\\langle\\cos 2 \\theta_i\\right\\rangle^2+\\left\\langle\\sin 2 \\theta_i\\right\\rangle^2}\n\\]\n\n\nNotice that \\(\\mathcal{S}\\) is not the entropy \\(S\\)! These commonly used symbols should not be confused.\nwhile in three dimensions this gives\n\n\\[\n\\mathcal{S}=\\dfrac{1}{2} \\left\\langle 3\\cos ^2 \\theta_i-1\\right\\rangle\n\\]\n\n(the expression for the director in 3d requires explicit diagonalisation).\n\n\n\n\n\n\nDerivation of 2D director\n\n\n\n\n\nFor the purpose of illustrating how the calculations for the director are carried out, we provide a detailed example in two-dimensions.\nIn two dimensions, each particle has an orientation unit vector\n\\[\n\\mathbf{u}_i = \\begin{pmatrix} \\cos \\theta_i \\\\ \\sin \\theta_i \\end{pmatrix},\n\\]\nand the alignment tensor is defined as\n\\[\n\\mathbf{Q} = \\left\\langle \\mathbf{u}_i \\otimes \\mathbf{u}_i \\right\\rangle = \\begin{pmatrix}\n\\langle \\cos^2 \\theta_i \\rangle & \\langle \\cos \\theta_i \\sin \\theta_i \\rangle \\\\\n\\langle \\cos \\theta_i \\sin \\theta_i \\rangle & \\langle \\sin^2 \\theta_i \\rangle\n\\end{pmatrix}.\n\\]\nUsing double-angle trigonometric identities,\n\\[\n\\cos^2 \\theta = \\frac{1 + \\cos 2\\theta}{2}, \\quad \\sin^2 \\theta = \\frac{1 - \\cos 2\\theta}{2}, \\quad \\cos \\theta \\sin \\theta = \\frac{1}{2} \\sin 2\\theta,\n\\]\nwe rewrite \\(\\mathbf{Q}\\) as\n\\[\n\\mathbf{Q} = \\frac{1}{2} \\begin{pmatrix}\n1 + C & D \\\\\nD & 1 - C\n\\end{pmatrix},\n\\]\nwhere\n\\[\nC = \\langle \\cos 2\\theta_i \\rangle, \\quad D = \\langle \\sin 2\\theta_i \\rangle.\n\\]\nTo find the eigenvalues \\(\\lambda\\), solve\n\\[\n\\det(\\mathbf{Q} - \\lambda \\mathbf{I}) = 0,\n\\]\nwhich yields the largest eigenvalue\n\\[\n\\lambda_{\\max} = \\frac{1}{2} + \\frac{S}{2},\n\\]\nwith the scalar nematic order parameter\n\\[\n\\mathcal{S} = \\sqrt{C^2 + D^2}.\n\\]\nWhere \\(\\mathcal{S} \\to 1\\) indicates strong alignment and \\(\\mathcal{S} \\to 0\\) indicates isotropic order.\nThe director \\(\\mathbf{n}\\) is the eigenvector corresponding to \\(\\lambda_{\\max}\\), satisfying\n\\[\n(\\mathbf{Q} - \\lambda_{\\max} \\mathbf{I}) \\mathbf{v} = 0,\n\\]\nwhich gives\n\\[\n\\mathbf{Q} - \\lambda_{\\max} \\mathbf{I} = \\frac{1}{2} \\begin{pmatrix}\nC - S & D \\\\\nD & -C - S\n\\end{pmatrix}.\n\\]\nThe eigenvector is\n\\[\n\\mathbf{v} \\propto \\begin{pmatrix} 1 \\\\ \\frac{S - C}{D} \\end{pmatrix}.\n\\]\nNormalizing \\(\\mathbf{v}\\) gives the director \\(\\mathbf{n}\\).\nThe director angle \\(\\psi\\) is given by\n\\[\n\\tan \\psi = \\frac{S - C}{D}.\n\\]\nThis simplifies to\n\\[\n\\tan \\psi = \\frac{D}{S + C}.\n\\]\nUsing the double-angle formula,\n\\[\n\\psi = \\frac{1}{2} \\operatorname{atan2}(D, C).\n\\]\nThe angle \\(\\psi\\) represents the director’s orientation modulo \\(\\pi\\).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#landau-de-gennes-mean-field-theory",
    "href": "soft-matter/soft-matter_liquidcrystals.html#landau-de-gennes-mean-field-theory",
    "title": "17  Liquid crystals",
    "section": "17.4 Landau-de Gennes mean field theory",
    "text": "17.4 Landau-de Gennes mean field theory\nLet us consider the uniaxial 3D case, which is the most commonly discussed case (this focuses on nematic order, excluding cholesteric and smectic order) and attempt to construct a Landau theory centred on the order parameter in the spirit of what we saw in Section 5.5. If the system is uniaxial, then the order is characterized by a single director and the \\(\\mathbf{Q}\\) must be symmetric and traceless with a preferred axis \\(\\mathbf{n}\\).\nThe tensorial order parameter can be rewritten levaraging that the scalar order parameter is an eigenvalue of the director. i.e. \\(\\mathbf{Qn} = S\\mathbf{n}\\). The result is that\n\\[\n\\mathbf{Q}=\\mathcal{S}\\left(\\mathbf{n} \\otimes \\mathbf{n}-\\frac{1}{d} \\mathbf{I}\\right)\n\\]\nfor some scalar value \\(\\mathcal{S}\\) (the order parameter between 0 for the isotropic and 1 for the nematic). If we choose now the \\(z\\) axis along \\(\\mathbf{n}\\) we can write this explicitly as\n\\[\n\\mathbf{Q}=\\mathcal{S}\\left(\\begin{array}{ccc}\n-\\frac{1}{3} & 0 & 0 \\\\\n0 & -\\frac{1}{3} & 0 \\\\\n0 & 0 & \\frac{2}{3}\n\\end{array}\\right)\n\\]\nwith \\(\\mathcal{S}=\\frac{3}{2}\\left\\langle(\\mathbf{u} \\cdot \\mathbf{n})^2-\\frac{1}{3}\\right\\rangle\\) as defined earlier.\nWe can now make an extra step by considering how to express the scalar order parameter in terms of the probability distribution \\(p(\\Omega)\\) of finding a rod with pair of polar angles \\(\\Omega=(\\theta,\\phi)\\) in a coordinates system with polar axis along \\(\\mathbf{n}\\) (which is the reference frame we have just chosen). The infinitesimal probability is \\(p(\\Omega)d\\Omega\\).\nAs often in physics, we need to make a symmetry observation, and remark that the nematic phase has full cylindrical symmetry around the director. This means that, for us \\(p(\\Omega)=p(\\theta, \\phi)\\) is in fact independent of The azimuthal angle \\(\\phi\\) and only depends on the deviation from the director \\(\\theta\\).\nWe also note that the expression for the nematic order parameter satisfies head-tail symmetry bt employing the first non-trivial Legendre polynomial, \\(P_2(\\cos\\theta)\\) as\n\\[\\mathcal{S}=\\dfrac{3}{2}\\left\\langle(\\mathbf{u} \\cdot \\mathbf{n})^2-\\frac{1}{3}\\right\\rangle = \\dfrac{1}{2}\\langle 3 \\cos^2\\theta-1\\rangle=\\langle P_2(\\cos\\theta)\\rangle\\]\nIn terms of the probability distribution this is\n\\[\\mathcal{S}=2\\pi \\int_0^{\\pi}P_2(\\cos\\theta)p(\\theta)\\sin\\theta d\\theta\\]\nThe Landau free energy density for the isotropic-nematic transition is constructed by scalar combinations of the tensor \\(\\mathbf{Q}\\). The idea is to expand the free energy density \\(f\\) around the transition and write it as\n\\[\nf=f_0+\\frac{A}{2} Q_{i j} Q_{j i}-\\frac{B}{3} Q_{i j} Q_{j k} Q_{k i}+\\frac{C}{4}\\left(Q_{i j} Q_{i j}\\right)^2\n\\]\nor in more compact form\n\\[\nf=f_0+\\frac{A}{2} \\operatorname{tr} \\mathbf{Q}^2-\\frac{B}{3} \\operatorname{tr} \\mathbf{Q}^3+\\frac{C}{4}\\left(\\operatorname{tr} \\mathbf{Q}^2\\right)^2\n\\] where we stop the construction to terms of fourth order in \\(\\mathbf{Q}\\) and \\(A,B,C\\) are temperature dependent coefficients.\nWe note that given our definitions above the following identities hold \\[\n\\operatorname{Tr}\\left(\\mathbf{Q}^2\\right)=\\frac{2}{3} S^2 \\quad,\\quad\n\\operatorname{Tr}\\left(\\mathbf{Q}^3\\right)=\\frac{2}{9} S^3\n\\]\nso that\n\\[\nf=f_0+\\frac{A}{3}S^2-\\frac{2B}{27}S^3+\\frac{C}{9}S^4\n\\]\n\n\nWe can expand \\(A,B,C\\) around the reference temperature \\(T^\\ast\\) as \\[ A(T)=a\\left(T-T_*\\right)+\\cdots \\] and at zeroth order for both \\(B\\) and \\(C\\) as \\(B(T)= b+\\dots\\) , \\(C(T)=c+\\dots\\), with \\(a,b,c&gt;0\\) to preserve the most relevant terms.\nWe can follow the scheme used for Landau phase transitions and obtain the so-called Landau-de Gennes theory result\n\\[\nf-f_0=\\frac{a}{3}\\left(T-T^{\\ast}\\right) S^2-\\frac{2 b}{27} S^3+\\frac{c}{9} S^4 .\n\\]\nThis has the form of standard Landau free energy (combinations of power of S up to the power of 4 with real coefficients that depend on the temperature).  It can be shown that this corresponds to a first order phase transition without critical point (because the isotropic and nematic phase have different symmetry) with transition temperature\n\\[\nT_{N I}=T^{\\ast}+\\frac{b^2}{27 a c}\n\\] where \\(T^{\\ast}\\) is a material dependent temperature, identifying where the quadratic term of the free energy vanishes, and where the isotropic phase becomes unstable (spinodal).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#maier-saupe-theory",
    "href": "soft-matter/soft-matter_liquidcrystals.html#maier-saupe-theory",
    "title": "17  Liquid crystals",
    "section": "17.5 Maier-Saupe theory",
    "text": "17.5 Maier-Saupe theory\nThe Landau-de Gennes theory illustrates how the simple existence of a suitably defined nematic order is compatible with the construction of a theory that predicts a first order phase transition. A complementary approach is to consider the minimal physical ingredients that lead to the transition itself. As in the case of the colloids, we have essentially two classes\n\nattractive interactions that can favour the new symmetries (e.g. the nematic order)\nentropic contributions that make it easier to pack the rods in the new symmetry (i.e. when they are aligned)\n\nPhenomenologically, we can put these two together assuming a form for the energetic and entropic contributions to write down a free energy of the following form\n\\[f = f_0 - u\\dfrac{S^2}{2}+k_BT \\int_0^\\pi p(\\theta) \\ln (4\\pi p(\\theta)) \\sin\\theta d\\theta\\]\nwhere the \\(4\\pi\\) term assumes that \\(p(\\theta)\\) is correctly normalised on the sphere and \\(\\sin\\theta d\\theta\\) is the usual Jacobian. The two terms above identify two simple contributions:\n\nan energetic contribution that is a simple quadratic function of the order parameter (more order, lower energy, with the isotropic phase paying the highest cost). Near the isotropic phase the free energy needs to be even due to the head-tail symmetry.\na generic entropic contribution of the form \\(k_B\\int p(\\theta)\\ln p(\\theta) d\\theta\\) using the Gibbs entropy formula.\n\nThe minimization of the free energy with respect to \\(p(\\theta)\\), under the normalization constraint \\(\\int_0^\\pi p(\\theta) \\sin\\theta d\\theta = 1\\), is performed using a Lagrange multiplier \\(\\lambda\\). The variational problem is:\n\\[\n\\delta \\left[ f + \\lambda \\left( \\int_0^\\pi p(\\theta) \\sin\\theta d\\theta - 1 \\right) \\right] = 0\n\\]\nTaking the functional derivative with respect to \\(p(\\theta)\\) and setting it to zero yields:\n\\[\n\\frac{\\delta f}{\\delta p(\\theta)} + \\lambda \\sin\\theta = 0\n\\]\nPlugging in the expression for \\(f\\) and simplifying, we obtain the solution:\n\\[\np(\\theta) = \\frac{1}{Z} \\exp\\left( \\lambda P_2(\\cos\\theta) \\right)= \\frac{1}{Z} \\exp\\left( \\frac{u S}{k_B T} P_2(\\cos\\theta) \\right)\n\\]\nwhere \\(\\mathcal{S}\\) is the nematic order parameter to be determined self-consistently, and \\(Z\\) is the normalization constant (partition function):\n\\[\nZ = \\int_0^\\pi \\exp\\left( \\frac{u S}{k_B T} P_2(\\cos\\theta) \\right) \\sin\\theta d\\theta\n\\]\nThis is the Maier-Saupe self-consistent equation for the orientational distribution in the mean-field theory of nematic liquid crystals. In the code below, we can see how this leads to a non trivial energy profile which corresponds to the emergence of a ordered state (a minimum at high \\(\\mathcal{S}\\)) corresponding to the nematic phase.\nThe transition is weakly-first order, as two separate basins of stability (separated typicaly by a small barrier) are formed.\n\n\n\n\n\n\nThese profiles are example of simple free energy landscapes. We will see more about this later in Section 19.1.\n\n17.5.1 Lattice model: the Lebwohl-Lasher model\nThe Lebwohl-Lasher model is a lattice model designed to capture the orientational ordering of anisotropic particles, such as those found in nematic liquid crystals and is a lattice version of the Maier-Saupe mean field model. In this model, each lattice site \\(i\\) is associated with a unit vector \\(\\mathbf{n}_i\\) representing the local orientation (the “director”) of a particle at that site.\nThe Hamiltonian of the Lebwohl-Lasher model is given by:\n\\[\nH = -\\epsilon \\sum_{\\langle i, j \\rangle} \\left[ \\frac{3}{2} (\\mathbf{n}_i \\cdot \\mathbf{n}_j)^2 - \\frac{1}{2} \\right]\n\\]\nwhere: - \\(\\epsilon &gt; 0\\) is the coupling constant favoring alignment, - the sum \\(\\langle i, j \\rangle\\) runs over all pairs of nearest-neighbor sites, - \\((\\mathbf{n}_i \\cdot \\mathbf{n}_j)^2\\) measures the degree of alignment between neighboring directors.\nThis Hamiltonian favors parallel (or antiparallel) alignment of neighboring directors, capturing the essential physics of the isotropic-nematic transition in liquid crystals.\nIn two dimensions, the Lebwohl-Lasher model is similarly defined, but the directors \\(\\mathbf{n}_i\\) are restricted to lie in the plane. Each director can be represented by a unit vector \\(\\mathbf{n}_i = (\\cos\\theta_i, \\sin\\theta_i)\\), where \\(\\theta_i\\) is the orientation angle at site \\(i\\).\nThe Hamiltonian in 2D becomes:\n\\[\nH = -\\epsilon \\sum_{\\langle i, j \\rangle} \\left[ \\frac{3}{2} \\cos^2(\\theta_i - \\theta_j) - \\frac{1}{2} \\right]\n\\]\nwhere the sum is over nearest-neighbor pairs on a 2D lattice. This model captures the essential features of orientational ordering and the isotropic-nematic transition in two-dimensional systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity\n\n\n\nModify the script above to check the following\n\nThe high temperature regime (\\(T\\gg 0.5\\)$) is mostly disordered (isotropic)\nThe low temperature regime forms larger and larger patches with coherent orientations\nDefine and plot a global nematic order parameter. Τhe global nematic order parameter S is defined as:\n\n\\[ S = \\langle \\cos(2(\\theta - \\psi))\\rangle\\]\nwhere \\(\\psi\\) is the director (average orientation), which in 2D is\n\\[\\psi = \\dfrac{1}{2} {\\rm atan2}(\\langle \\sin(2\\theta)\\rangle, \\langle \\cos(2\\theta)\\rangle)\\]",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#splay-twist-and-bend",
    "href": "soft-matter/soft-matter_liquidcrystals.html#splay-twist-and-bend",
    "title": "17  Liquid crystals",
    "section": "17.6 Splay, twist and bend",
    "text": "17.6 Splay, twist and bend\nIn the continuum limit, the nematic liquid crystal can be characterised in terms of the deformation of a vector field \\(\\mathbf{n}(\\mathbf{r})\\) which represents the director at every point \\(\\mathbf{r}\\) (a local director, resulting from teh average over a large number of particlesion a single volume).\nThe Frank free energy density for distortions of the director field is\n\\[\nf = \\frac{1}{2} K_1 (\\nabla \\cdot \\mathbf{n})^2\n    + \\frac{1}{2} K_2 [\\mathbf{n} \\cdot (\\nabla \\times \\mathbf{n})]^2\n    + \\frac{1}{2} K_3 [\\mathbf{n} \\times (\\nabla \\times \\mathbf{n})]^2\n\\]\nwhere: - \\(K_1\\) is the splay elastic constant, - \\(K_2\\) is the twist elastic constant, - \\(K_3\\) is the bend elastic constant.\nEach term in the Frank free energy penalizes a specific type of distortion: splay, twist, or bend. The total elastic free energy is found by integrating \\(f\\) over the entire sample volume. This formulation is named after Charles Frank, a pioneer in the study of crystal dislocations (which are defects in crystalline solids) and what we call today soft matter at the University of Bristol, for whom the Frank lecture theatre is named.\n\n\n\nThe formulation allows one to model the behaviour of liquid crystals with finite element methods on much larger scales than the typical size of the elementary constituents. It also allows to phenomenologically add more deformation modes, to include for example the description of chirality.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#topological-defects",
    "href": "soft-matter/soft-matter_liquidcrystals.html#topological-defects",
    "title": "17  Liquid crystals",
    "section": "17.7 Topological defects",
    "text": "17.7 Topological defects\nThe continuum limit description of liqudi crystals allows us to anhance their description with the notion of topological defectes.\nTopological defects are singularities in the director field that can not be removed by a continuous deformation of the director field: they are topolgically protected. We can see this with a few examples in 2d by drawing lines of the director (remember that the head-tail symmetry means that the lines do not have any preferential direction but indicate local orientation):\n\n\nCode\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\ndef plot_defect(ax, charge, title):\n    # Grid\n    x = np.linspace(-1, 1, 10)\n    y = np.linspace(-1, 1, 10)\n    X, Y = np.meshgrid(x, y)\n    # Polar coordinates\n    theta = np.arctan2(Y, X)\n    # Director angle: n = [cos(q*theta/2), sin(q*theta/2)]\n    phi = charge * theta / 2\n    U = np.cos(phi)\n    V = np.sin(phi)\n    # Normalize for nematic symmetry (director, not vector)\n    U2 = np.where(U &lt; 0, -U, U)\n    V2 = np.where(U &lt; 0, -V, V)\n    # Plot director field as dashed lineså\n    ax.quiver(X, Y, U2, V2, pivot='middle', color='k', scale=11, headwidth=0, headlength=0, headaxislength=0, linewidth=0.85, linestyle='dashed')\n    # Defect core\n    ax.plot(0, 0, 'o', color='orange', markersize=14)\n    ax.set_title(title, fontsize=11)\n    ax.set_aspect('equal')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlim(-1.1, 1.1)\n    ax.set_ylim(-1.1, 1.1)\n    ax.axis('off')\n\nfig, axs = plt.subplots(1, 4, figsize=(8, 8))\ncharges = [1, -1, 0.5, -0.5]\ntitles = [r\"Topological charge $+1$\", r\"Topological charge $-1$\", r\"Topological charge $+\\frac{1}{2}$\", r\"Topological charge $-\\frac{1}{2}$\"]\n\nfor ax, charge, title in zip(axs.flat, charges, titles):\n    plot_defect(ax, charge, title)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese topological defects are places where the orientation changes discontinuously and are also called disclinations.\nIn two dimensions, the disclinations are characterized by a topological charge (or strength) \\(s\\), defined by the total rotation of the director around a closed loop enclosing the defect:\n\\[\n\\Delta \\theta = 2\\pi s\n\\]\n\n\n\n\n\n\n\n\n\nCommon disclination strengths in nematic liquid crystals are \\(s = \\pm \\frac{1}{2}\\) and \\(s = \\pm 1\\). For example, a \\(+1/2\\) disclination corresponds to a director field that rotates by \\(+\\pi\\) as one encircles the defect, while a \\(-1/2\\) disclination rotates by \\(-\\pi\\).\nWhen two disclinations of opposite charges meet, they annihilate. This means the defects can cancel each other out, restoring uniform orientational order in the region. The annihilation of defect pairs is a key mechanism by which nematic liquid crystals relax toward equilibrium after being disturbed. This is because the idealise equilibrium state should in principle be free of any such defects, as they cost energy: they deform the dirctor field on long distances, leading to a deformation cost, and they also engendre a complete loss of orientational order at the singularity point, which costs energy by itslef.\nIn three dimensions, disclinations are line defects rather than point defects. These line defects represent regions where the orientational order of the director field is singular along a curve, rather than at isolated points. The topology and dynamics of disclination lines in 3D nematic liquid crystals are richer and more complex than in 2D, allowing for phenomena such as defect loops, entanglements, and reconnections. Disclination lines can form closed loops, terminate at surfaces, or interact with other defects, and play a crucial role in the response of liquid crystals to external fields, boundary conditions, and during phase transitions.\nThe classification of disclination in 3D involves the rotation group SO(3)* (that generalises the construction we have seen in two dimensions) and are more complex in general. They include lines, closed loops, point-like features (hedgehogs and monopoles) and complex textures.\nYou can read more about liquid crystal’s defects and their coupling to the material properties in Jones (2002).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_liquidcrystals.html#references",
    "href": "soft-matter/soft-matter_liquidcrystals.html#references",
    "title": "17  Liquid crystals",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtashpendar, Arshia, Tim Ingenbrand, and Tanja Schilling. 2020. “Shape, Geometric Percolation, and Electrical Conductivity of Clusters in Suspensions of Hard Platelets.” Phys. Rev. E 101 (March): 032706. https://doi.org/10.1103/PhysRevE.101.032706.\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.\n\n\nKnapp, Elisabeth, and Dennis J Lewandowski. 2001. “Tobacco Mosaic Virus, Not Just a Single Component Virus Anymore.” Molecular Plant Pathology 2 (3): 117–23.\n\n\nSánchez-Ferrer, Antoni, Mathias Reufer, Raffaele Mezzenga, Peter Schurtenberger, and Hervé Dietsch. 2010. “Inorganic–Organic Elastomer Nanocomposites from Integrated Ellipsoidal Silica-Coatedhematite Nanoparticles as Crosslinking Agents.” Nanotechnology 21 (18): 185603.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Liquid crystals</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html",
    "href": "soft-matter/soft-matter_surfactants.html",
    "title": "16  Surfactants",
    "section": "",
    "text": "16.1 Hydrophobicity and amphiphiles\nThe hydrophobic interaction between solutes is a statistical force mediated by water (the solvent). Traditional explanations focus on the role played by hydrogen bonds (an attractive interaction due to the difference in electronegativity between oxygen and hydrogen in water) and the structure of water around solutes.\nPolar molecules have regions with partial positive and negative charges due to differences in electronegativity between atoms, resulting in an uneven distribution of electrons. This allows them to interact strongly with water (hydrophilic) and other polar substances.\nApolar (or nonpolar) molecules, instead, have a more even distribution of electrical charge, lacking distinct poles. They do not mix well with water and tend to aggregate with other nonpolar substances.\nIn this chapter we discuss the behaviour of a special class of molecules. In these molecules one end contains a hydrophilic (literally, water-loving) part, while the other end is hydrophobic (water-fearing). For their nature, they are called amphiphilic (loving both) molecules, which reflects their structure, or surfactants (from SURFace ACTive AgeNT), which refers to their behaviour in solution.\nFor very small solutes (a few atoms) the traditional picture of hydro-phobicity/philicity is that the energetic advantage due to hydrogen bonding favors configurations that minimally disrupt the local structure of water, and this means that multiple apolar solutes tend to come together to minimise such disruption since they cannot form such bonds. However, for larger molecules (such as large proteins or longer macromolecular chains) interfaces between the bulk water and the solutes are formed, to which we can associate characteristic density fluctuations and surface tensions. Phase separation between a local vapor-like layer and the bulk water becomes then important and aggregation due to hydrophobicty becomes very much a a many-body effect. This is an active topic of current research (even by some of us, see Wilding and Turci (2025) ) and demonstrates how even very foundational concepts in soft matter remain to be explored.\nIn the rest of the chapter, we will look mostly phenomenologically at the structure of surfactants and how they collectively come together to form meso-scale structures via the process of self-assembly, and assume that there exist solvent-mediated interactions such as hydrophobicity that control the free energies of these assemblies.\nA few chemical structures are shown below. They illustrates that the hydrophobic tail usually consits of hydrocarbon chains of different lengths",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#hydrophobicity-and-amphiphiles",
    "href": "soft-matter/soft-matter_surfactants.html#hydrophobicity-and-amphiphiles",
    "title": "16  Surfactants",
    "section": "",
    "text": "Schematic model of a surfactant (adapted).\n\n\n\n\n\n\nSDS (dodecyl sulfate)\nIn SDS (sodium dodecyl sulfate) the head is the hydrophilic sulfate group (\\({\\rm –OSO_3}\\)), which is negatively charged and water-attracting whil the tail is the long hydrophobic 12-carbon alkyl chain (dodecyl group), which is water-repelling.\n    \n    \n    \n\n\nCholesterol\nIn cholesterol the head is the small polar hydroxyl (-OH) group and the tail is the nonpolar hydrocarbon isooctyl side chain at the opposite end.\n    \n    \n    \nIn general, the hydrophilic head might either be positively or negatively charged, zwitterionic (with both charges but overall neutral) or uncharged.\nThe hydrocarbon chains are insoluble in water. The molecules are thus preferentially located at the surface, which allows the hydrophilic head to be surrounded by water and the hydrophobic chains to avoid contact with water. There is always an equilibrium between surfactants at the surface and in the bulk of the solution. The coverage of the surface leads to a reduction of the surface tension with increasing surfactant concentration.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#self-assembled-structures",
    "href": "soft-matter/soft-matter_surfactants.html#self-assembled-structures",
    "title": "16  Surfactants",
    "section": "16.2 Self-assembled structures",
    "text": "16.2 Self-assembled structures\nSelf-assembly is a fundamental property of surfactant molecules in solution. It also a highlight feature of soft-matter systems: under the influence of thermal fluctuations (typically mediated by a solvent), the constituents of soft matter systems self-organise into more complex, meso or macroscopic structures. This is typically due to driving thermodynamic forces that make initially unform systems find (local ord global) minima of their free energy which distinctive structural features. In this sense, soft matter systems explore free energy landscapes whose complexity is tuned by the strength of thermal fluctuations.\nIn many ways, we have already seen various forms of self assembly: crystallisation of colloids is itself self-organised but so is also the phase transition between the gasous and the liquid phase, or the isotropic-nematic transition in liquid crystals.\nFull phase separation, though, is much more demanding than self assembly: in full phase separation we eventually attain the (global) free energy minimum corresponding to the equilibrium state prescribed by the chosen thermodynamic variables (e.g. temperature and pressure). The notion of self assembly emphasises instead the propensity of the constituents to aggregate, to form structures at intermediate scales, structures which can often be only very locally stable, and whose existence may rely not only on the structure of the energy landscape but also on the kinetics of the constituents, such as their diffusion mechanism.\nIn the case of surfactants, we need to consider that these molecules are often very small—typically just a few nanometers in length. This small size means that thermal fluctuations play a significant role in their behavior, and their assembly is rapid due to their fast diffusion. For example, for an approximately spherical surfactant of size \\(R\\) in a viscous medium its diffusivity is given by the Stokes-Einstein relation (see )\n\n\\[\nD = \\frac{k_B T}{6\\pi \\eta R}\n\\]\n\nwhere \\(D\\) is the diffusion coefficient, \\(k_B\\) is Boltzmann’s constant, \\(T\\) is temperature, \\(\\eta\\) is the viscosity of the solvent, and \\(R\\) is the radius of the particle. Compared to a colloidal particle of \\(2 {\\rm \\mu m}\\), a single SDS surfactant molecule of approximately \\(2 {\\rm nm}\\) in size will be approximately 1000 times faster.\nThis leads to fast local equilibration and constant exchange between monomers and aggregates. As a result, the structures formed by surfactants are not static but exist in a dynamic equilibrium, with lifetimes and sizes that depend sensitively on temperature, concentration, and solvent conditions.\nDue to their amphiphilic nature, surfactants spontaneously organise into ordered structures without external guidance, but driven by forces such as hydrophobicity, described above.\nAs a result, surfactants minimize the system’s free energy by forming a variety of aggregates such as micelles, vesicles, or bilayers. The specific structure formed depends on the molecular geometry of the surfactant and the solution conditions. Self-assembly is in many cases reversible and fundamentally dynamic process, with aggregates constantly forming and dissociating in equilibrium with monomers in solution.\n\n\n16.2.1 Aggregation: general case\nSuppose we have a system where solvent particles are dispersed in a solvent and tend to aggregate due to their mutual interactions. Suppose that we know the quantity \\(\\epsilon_n\\) representing the free energy change when a specific particle is taken from the bulk and added to an aggregate of size \\(N\\).\nCall $_1 $ and \\(\\mu_N\\) the chemical potential of isolated particles and aggregates of size \\(N\\) respectively. In equilibrium, they must be equal, with value \\(\\mu\\).\nLet’s focus on an aggregate of size \\(N\\). We can express $$ in terms of\n\nthe interaction energy from being in the aggregate (\\(\\epsilon_N\\))\nthe (translational) entropy of the aggregate as a whole (\\(\\propto {\\rm number of aggregates}\\times \\ln {\\rm number fo aggregates}\\))\n\nAssume an overall volume fraction of surfactants \\(\\phi\\). Call the volume fraction of surfactants in an aggregate with \\(N\\) molecules \\(X_N\\), so that \\(\\sum_N X_N = \\phi\\). The numbers of aggregates of size \\(N\\) is then simply \\(X_N/N\\). This means that we can write the uniform chemical potential as\n\\[\\mu = \\epsilon_N + \\dfrac{k_B T}{N}\\ln{\\dfrac{X_N}{N}}\\]\nwe can rewrite this as\n\\[X_N = N \\exp{\\left( \\dfrac{N(\\mu-\\epsilon_N)}{k_BT}\\right)}\\]\nwe can eliminate \\(\\mu\\) by evaluating the expression for for \\(N=1\\) and plugging it back to get\n\\[X_N = N X_1^N \\exp{\\left( \\dfrac{N(\\epsilon_1-\\epsilon_N)}{k_BT}\\right)}\\]\nObviously, the equation show that one has a large fraction of the solutes in an aggregated state only if there is a free energy advantage at forming aggregates, i.e. \\(\\epsilon_1&gt;\\epsilon_N\\).\nThis means that knowning the form of \\(\\epsilon_N\\) allows us to predict the aggregation behaviour. For example, imagine we have an aggregate with \\(N\\) particles of total radius \\(r\\approx (Nv)^(1/3)\\) where v is the volume of a single particle. Then \\(\\epsilon_N\\) is the free energy per particle of the aggregate of size \\(N\\), \\(G_N=\\text{ bulk free energy}+\\text{surface free energy}\\). Assuming a surface tension \\(\\gamma\\) we can then write\n\\[\\epsilon_N = \\dfrac{G_N}{N} = \\epsilon_\\infty  +\\dfrac{1}{N}\\gamma r^2 =\\epsilon_\\infty +\\gamma\\left(\\dfrac{v^2}{N}\\right)^{1/3}\\]\nwhich is a monotonically decreasing function of \\(N\\). By defininit \\(\\alpha k_B T = \\gamma v^{2/3}\\) we can extract a relation between \\(X_N\\) and \\(X_1\\) parametrised solely by \\(\\alpha\\), i.e.\n\\[\\text{number of aggregates of size N per unit volume} = \\dfrac{X_N}{N}\\sim (X_1 e^{\\alpha})^N\\]\nThis should be read as follows: if we have very few isolated particles at a certain thermodynamic condition, then \\(X_1 e^{\\alpha} &lt; 1\\) and the exponential factor \\((X_1 e^{\\alpha})^N\\) becomes vanishingly small for large \\(N\\), leaving us with very few large aggregates. On the contrary, as \\(X_1\\) approaches \\(e^{-\\alpha}\\) from below, we reach the critical point where \\(X_1 e^{\\alpha} = 1\\), and aggregates of all sizes become equally probable. Since \\(\\epsilon_N\\) is rapidly decreasing in \\(N\\), this means that above a critical value of overall packing fraction \\(\\phi\\), the system cannot remain in a homogeneous state. Instead, it undergoes phase separation into a dilute phase of isolated monomers (with \\(X_1\\) pinned at \\(e^{-\\alpha}\\)) in coexistence with a dense phase consisting of one or very few aggregates of very large (effectively infinite) size.\nThe volume fraction \\(\\phi\\) at which this occurs is called critical aggregation concentration, or CAC.\n\n\n16.2.2 Aggregation: the surfactant case\nFor amphiphilic molecules like surfactants, the free energy change \\(\\epsilon_N\\) associated with adding a molecule to an aggregate is not a monotonically decreasing function of \\(N\\). Instead, \\(\\epsilon_N\\) typically exhibits a minimum at a characteristic aggregation number \\(N^*\\). This reflects the fact that aggregates (such as micelles) of a particular size are thermodynamically favored: too-small aggregates cannot sufficiently shield the hydrophobic tails from the solvent, while too-large aggregates become energetically unfavorable due to packing constraints or headgroup repulsion. See Jones (2002) for a expanded discussion of this.\n\n\n\nSurfactants in solution: normally, this is done in water in the presence of an interface with air. Due to their nature as amphiphiles, the surfactants typically sit at the air-water interface in a dynamical, equilibrium process that exchanges monomers between the bulk and the surface. The bulk surfactants, when the concentration is larger than a critical value, also self-assemble into micellar structures, which are at equilibrium with the isolated single surfactants.\n\n\nMathematically, this means that the distribution \\(X_N\\) of aggregates as a function of \\(N\\) is sharply peaked around \\(N^*\\), leading to a well-defined aggregate size in solution. The equilibrium is then characterized by a coexistence of monomers and aggregates of size \\(N^*\\), with very few intermediate-sized clusters. This is in contrast to the general case discussed above, where the aggregate size distribution can be broad or even diverge near a phase separation threshold.\nThis behavior underlies the concept of the critical micelle concentration (CMC): below the CMC, almost all surfactant molecules are present as monomers; above the CMC, additional surfactant molecules predominantly form micelles of size \\(N^*\\), while the monomer concentration remains nearly constant.\nAbove the critical micellar concentration surfactants self-assemble in solution spontaneously into larger structures. (In the following we will consider aqueous solutions, although the arguments also apply to other polar or non-polar (organic) solvents.) This allows the hydrophobic parts to crowd together while being ‘shielded’ by the hydrophilic heads. The density of the hydrophobic cores is very similar to the density of fluid hydro-carbons and the random arrangements of the chains resemble closely a fluid structure.\nThe surfactant assemblies are not held together by chemical bonds, but only by weak interactions ( \\(\\lesssim k B T\\) ). Their existence and properties are thus determined by a delicate balance between different effects, such as the transfer of hydrophobic chains into the core, interactions between the head group and the entropy of mixing. Small changes in control parameters, for example temperature, salt concentration or pH , thus have large effects on the characteristics of the surfactant aggregates. Nevertheless, for given conditions, they have very well-defined properties (shape, size etc.).\n\n\n16.2.3 Shape of surfactant assemblies\nSurfactants spontaneously self-assemble into a variety of different structures. We use packing considerations to understand and predict the shape of surfactant aggregates, leveraging what we have learned on colloids, polymers and liquid crystals.\nWe construct a geometric model where a surfactant molecule is described using the following parameters:\n\noptimal headgroup area \\(a_{0}\\): As discussed in the previous section, this depends on a delicate balance of forces and is thus not only controlled by the chemistry of the surfactant molecule, but also depends on different control parameters of the solution, such as salt concentration, pH or temperature. -volume \\(v\\) of the hydrophobic part: The hydrophobic part usually consists of hydrocarbon chains and for saturated hydrocarbons the volume \\(v\\) can be approximated by \\(v \\approx(27.4+26.9 \\mathrm{n}) \\times 10^{-3} \\mathrm{~nm}^{3}\\) where n is the number of carbon atoms.\ncritical chain length \\(\\boldsymbol{l}_{\\boldsymbol{c}}\\) : The maximum effective length of the hydrophobic chains is called the critical chain length \\(l_{c}\\), which has to be shorter than the fully extended molecular length of the chain \\(l_{\\max }\\). For saturated hydrocarbons the critical length can be estimated using \\(l_{c} \\leq l_{\\max } \\approx(0.154+0.1265 \\mathrm{n}) \\mathrm{nm}\\). The critical chain length heavily depends on the detailed chemical structure of the molecule, for example on the presence of double bonds or branching, as well as the temperature.\n\nThe structure which will be adopted is determined by a balance between entropy, which favours small aggregates, and energy considerations: A certain shape or size might only be possible by imposing a headgroup area \\(a&gt;a_{0}\\), which is energetically not favourable. We will now establish the criteria for the different shapes.\n\n\n\nDifferent types of self assembled structures: (A) spherical micelles, (B) cylindrical micelles and (C) bilayers.\n\n\nA. Spherical micelle\nFor a spherical micelle with aggregation number N , the total volume and surface area are given by\n\\[\n\\begin{gathered}\nN v=\\frac{4 \\pi}{3} R^{3} \\\\\nN a_{0}=4 \\pi R^{2} \\\\\n\\therefore \\frac{v}{a_{0}}=\\frac{R}{3}&lt;\\frac{l_{c}}{3}\n\\end{gathered}\n\\]\nwhere we used the fact that the radius \\(R\\) cannot be larger than the critical chain length \\(\\mathrm{I}_{\\mathrm{c}}\\). We thus obtain for the critical packing parameter \\(P\\)\n\\[\nP=\\frac{v}{a_{0} l_{c}}&lt;\\frac{1}{3}\n\\]\nB. Cylindrical micelles\nFor a cylindrical micelle the total volume and surface area are given by \\(N v=\\pi R^{2} L\\) \\(N a_{0}=2 \\pi R L\\) \\(\\therefore \\frac{v}{a_{0}}=\\frac{R}{2}&lt;\\frac{l_{c}}{2}\\) again using \\(R&lt;I_{C}\\).\nWe thus obtain for the critical packing parameter \\(P\\) \\(\\frac{1}{3}&lt;\\frac{v}{a_{0} l_{c}}&lt;\\frac{1}{2}\\)\nBelow the lower limit spherical micelles are formed.\nC. Bilayers\nFor a bilayer the total volume and surface area are given by\n\\[\n\\begin{aligned}\n& N v=A D \\\\\n& N a_{0}=2 A \\\\\n& \\frac{v}{a_{0}}=\\frac{D}{2}&lt;l_{c} \\quad\\left(\\text { using } D&lt;2 l_{c}\\right)\n\\end{aligned}\n\\]\nWe thus obtain for the critical packing parameter \\(P\\) \\(\\frac{1}{2}&lt;\\frac{v}{a_{0} l_{c}}&lt;1\\) Below the lower limit cylindrical micelles are formed.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#a-simple-on-lattice-model-for-micelle-formation",
    "href": "soft-matter/soft-matter_surfactants.html#a-simple-on-lattice-model-for-micelle-formation",
    "title": "16  Surfactants",
    "section": "16.3 A simple on-lattice model for micelle formation",
    "text": "16.3 A simple on-lattice model for micelle formation\nAggregation of surfactants has been studied in various ways, including numerical simulations. One can employ extremely realistic, all-atom molecular dynamics to glean the microscopic details or construct more coarse-grained, statistical mechanics models using for exmaple on lattice interactions.\nHere below you can find a Javascript implementation of a lattice-gas-like model where surfactants are representes by chains of 3 sites on a lattice, with one site for the solvophilic head and two for the solvophobic tail. Each lattice site is occupied by either an amphiphile segment or a solvent molecule.\nThere are only nearest-neighbor interactions and the hamiltonian is simply\n\\[\nH=n_{\\mathrm{HH}} E_{\\mathrm{HH}}+n_{\\mathrm{TS}} E_{\\mathrm{TS}}+n_{\\mathrm{HS}} E_{\\mathrm{HS}}+\\sum_{\\mathrm{i}} E_{\\mathrm{c}}\n\\]\nwhere \\(n_{\\mathrm{HH}}, n_{\\mathrm{TS}}\\) and \\(n_{\\mathrm{HS}}\\) are the total number of head-head, tail-solvent and head-solvent bonds, \\(E_{\\mathrm{HH}}, E_{\\mathrm{Ts}}\\) and \\(E_{\\mathrm{HS}}\\) are the head-head, tail-solvent and head-solvent interaction energies and \\(E_c\\) is the energy associated with the conformation of the \\(i\\) th molecule. The model has been discussed in detail in Care (1987).\nThe model is simplified by setting the head-head interaction to zero, the tail-solvent interaction hydrophobic \\(E_{\\rm TS}&gt;0\\), the head-solvent hydrophilic \\(E_{\\rm HS}&lt;0\\) and the chain completely flexible, \\(E_c^i=0\\).\n\nviewof temperature = Inputs.range([0.01, 2], {step: 0.05, label: \"Temperature (kT)\", value: 0.5})\n\n\n\n\n\n\n\nviewof numChains = Inputs.range([3, 300], {step: 10, label: \"Number of Chains\", value: 100})\n\n\n\n\n\n\n\n\nCode\n// This is ObservableJS code  \n// You can run at observableehq.com or convert it to an equivalent (and faster?) Python version if you like\nviewof simulation = {\n    // --- Parameters ---\n    const width = 40, height = 40;\n    const chainLength = 3;\n    const chainsToCreate = numChains;\n    const T = temperature;\n    // const numChains = 10;\n    const E_TS = 1.0;     // tail-solvent energy (solvophobic)\n    const E_HS = -1.5;    // head-solvent energy (solvophilic)\n    // const T = 0.1;        // temperature (kT units)\n\n    // --- Initialize grid and chains ---\n    let grid = Array.from({length: width}, () =&gt; Array(height).fill(null));\n    let chains = [];\n\n    // Periodic boundary helper\n    function mod(n, m) { return ((n % m) + m) % m; }\n\n    function getNeighbors(x, y) {\n        return [\n            {x: mod(x - 1, width), y: y},\n            {x: mod(x + 1, width), y: y},\n            {x: x, y: mod(y - 1, height)},\n            {x: x, y: mod(y + 1, height)}\n        ];\n    }\n\n    function placeChains() {\n        for (let id = 0; id &lt; numChains; id++) {\n            for (let tries = 0; tries &lt; 100; tries++) {\n                let x = Math.floor(Math.random() * width);\n                let y = Math.floor(Math.random() * height);\n                if (grid[x][y]) continue;\n\n                let chain = [{x, y, type: 'head'}];\n                grid[x][y] = {id, type: 'head'};\n                let ok = true;\n\n                for (let i = 1; i &lt; chainLength; i++) {\n                    let last = chain[chain.length - 1];\n                    let nbs = getNeighbors(last.x, last.y).filter(p =&gt; !grid[p.x][p.y]);\n                    if (nbs.length === 0) { ok = false; break; }\n                    let next = nbs[Math.floor(Math.random() * nbs.length)];\n                    chain.push({...next, type: 'tail'});\n                    grid[next.x][next.y] = {id, type: 'tail'};\n                }\n\n                if (ok) { chains.push({id, segments: chain}); break; }\n                else { chain.forEach(p =&gt; grid[p.x][p.y] = null); }\n            }\n        }\n    }\n\n    function energy(seg) {\n        let solventNbs = getNeighbors(seg.x, seg.y).filter(p =&gt; !grid[p.x][p.y]).length;\n        return seg.type === 'head' ? solventNbs * E_HS : solventNbs * E_TS;\n    }\n\n    function attemptMove(chain) {\n        const forward = Math.random() &lt; 0.5;\n        const tail = forward ? chain.segments[0] : chain.segments.at(-1);\n        const head = forward ? chain.segments.at(-1) : chain.segments[0];\n        const options = getNeighbors(head.x, head.y).filter(p =&gt; !grid[p.x][p.y]);\n\n        if (options.length === 0) return;\n        const next = options[Math.floor(Math.random() * options.length)];\n\n        const dE_old = energy(tail);\n        grid[tail.x][tail.y] = null;\n        const dE_new = energy({...next, type: tail.type});\n        grid[tail.x][tail.y] = {id: chain.id, type: tail.type};\n\n        const deltaU = dE_new - dE_old;\n        const accept = deltaU &lt; 0 || Math.random() &lt; Math.exp(-deltaU / T);\n\n        if (accept) {\n            grid[tail.x][tail.y] = null;\n            grid[next.x][next.y] = {id: chain.id, type: tail.type};\n            if (forward) {\n                chain.segments.shift();\n                chain.segments.push({...next, type: tail.type});\n            } else {\n                chain.segments.pop();\n                chain.segments.unshift({...next, type: tail.type});\n            }\n        }\n    }\n\n    // --- Visualization ---\n    const svg = d3.create(\"svg\")\n        .attr(\"viewBox\", `0 0 ${width} ${height}`)\n        .style(\"width\", \"400px\")\n        .style(\"height\", \"400px\")\n        .style(\"border\", \"1px solid #ccc\");\n\n    const g = svg.append(\"g\");\n\n    function draw() {\n        const data = chains.flatMap(c =&gt; c.segments);\n        g.selectAll(\"circle\")\n            .data(data, d =&gt; `${d.x}-${d.y}`)\n            .join(\"circle\")\n            .attr(\"cx\", d =&gt; d.x + 0.5)\n            .attr(\"cy\", d =&gt; d.y + 0.5)\n            .attr(\"r\", 0.45)\n            .attr(\"fill\", d =&gt; d.type === \"head\" ? \"blue\" : \"red\");\n    }\n\n    // --- Main Loop ---\n    placeChains();\n    draw();\n\n    let running = true;\n    const button = html`&lt;button&gt;⏸ Pause&lt;/button&gt;`;\n    button.onclick = () =&gt; {\n        running = !running;\n        button.textContent = running ? \"⏸ Pause\" : \"▶ Resume\";\n    };\n\n    (async () =&gt; {\n        while (true) {\n            if (running) {\n                for (let i = 0; i &lt; chains.length; i++) {\n                    const c = chains[Math.floor(Math.random() * chains.length)];\n                    attemptMove(c);\n                }\n                draw();\n            }\n            await new Promise(r =&gt; setTimeout(r, 50));\n        }\n    })();\n\n    return html`&lt;div&gt;${button}&lt;br&gt;${svg.node()}&lt;/div&gt;`;\n}",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_surfactants.html#references",
    "href": "soft-matter/soft-matter_surfactants.html#references",
    "title": "16  Surfactants",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCare, Christopher M. 1987. “Cluster Size Distribution in a Monte Carlo Simulation of the Micellar Phase of an Amphiphile and Solvent Mixture.” Journal of the Chemical Society, Faraday Transactions 1: Physical Chemistry in Condensed Phases 83 (9): 2905–12. https://pubs.rsc.org/en/content/articlehtml/1987/f1/f19878302905.\n\n\nJones, Richard AL. 2002. Soft Condensed Matter. Vol. 6. Oxford University Press. https://bris.on.worldcat.org/oclc/48753186.\n\n\nWilding, Nigel B., and Francesco Turci. 2025. “Origin of the Inverse Temperature Dependence of Hydrophobic Attraction.” Phys. Rev. Res. 7 (June): L022079. https://doi.org/10.1103/66lz-1yw9.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Surfactants</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html",
    "href": "soft-matter/soft-matter_glasses.html",
    "title": "19  Arrested states",
    "section": "",
    "text": "19.1 Energy landscapes\nIn these chapters we have been considering systems implicitly (or explicitly) in thermal equilibrium with a surrounding environment. This is typically some kind of dispersion medium (a solvent). But we also also been more demanding: we have required that the dispersed phase (the colloids, the polymers the anisotropic particles of liquid crystals) have explored exhaustively their free energy options and that they are truly in some macrostate corresponding to a global, stable thermal equilibrium state.\nEquilibrium signifies time reversibility.\nMany fluids, whether simple liquids like argon or complex liquids such as colloids and polymers, can be rapidly cooled (quenched) to temperatures well below their equilibrium freezing point without crystallization occurring on experimental timescales. In thermodynamic terms this is interpreted as a failure of the system to reach its true equilibrium (minimum free energy) state, namely the ordered crystalline phase. At these low temperatures, the dynamics slows down and the large scale structures remain disordered as the parent fluids. This slow dynamics is however characterised by a continuous drift away from equilibrium, with all of the characteristics of the systems (slowly) evolving during time as the system ages.\nThese relaxation dynamics are examples of nonequilibrium processes: the system spontaneously relaxes under the competing constraints of its kinetic rules (typically, diffusion mechanisms) and from the shape of its free energy profile.\nIn this chapter we are going to consider two paradigmatic cases of such nonequilibrium dynamics\nWhat is distinctive of both gels and glasses is that they are both examples of amorphous materials that behave like solids: they lack long-range atomic order yet resist deformation like crystalline solids. Indeed one can calculate properties that are distinctive of solids for both gels and glasses, including:\nThese features highlight that rigidity and solidity do not require crystalline order; amorphous materials like gels and glasses can be mechanically solid while remaining structurally disordered. Indeed, the border between solidity and fluidity has been questioned in recent research work, emphasizing how this depends on observational timescales and the notion of metastability, see Sausset, Biroli, and Kurchan (2010).\nThe free energy landscape is a conceptual framework used to describe the multitude of possible configurations (microstates) of a system and their associated free energies. Each point in this high-dimensional landscape corresponds to a particular arrangement of all the particles in the system, and the height at that point represents the free energy of that configuration. In particular, the landscapes is characterised by the presence of free energy minima (valleys) which correspond to stable or metastable states—configurations where the system tends to reside and local maxima (barrier) between the valleys, representing the so-called transition states.\nVarious minima can be clustered together when their energies are relatively close, i.e. when the barriers that separate them are of the order of the energy from thermal fluctuations \\(k_B T\\). These clusters of minima are known as (meta)-basins and are very important for amorphous systems: the same system at the same temperature can display similar macroscopic characteristic not because these reflect the properties of one particular minimum, but because they are the result of local averaging within a given metabasin.\nIn the context of supercooled liquids and glasses, the landscape is rugged, with many local minima separated by high barriers. At high temperatures, the system can easily hop between minima (exploring many configurations), but as the temperature decreases, it becomes trapped in deeper minima, leading to slow dynamics and eventual dynamical arrest (glass formation).\nSimilarly, in the case of gels, the free energy landscape is also rugged, but the system becomes arrested due to the formation of a percolating network of reversible bonds. Instead of being trapped in deep minima solely by energetic barriers (as in glasses), the system’s dynamics are constrained by the connectivity of the network. The system can only relax if enough bonds break and reform to allow large-scale rearrangements, which becomes increasingly unlikely as the network spans the system. Thus, the arrested state in gels is associated with the system being confined within a region of the landscape corresponding to networked, mechanically stable configurations, separated from other regions by high barriers related to breaking the network connectivity.\nThe free energy landscape is helpful as it suggests ways to enumerate distinct conformations of a disordered system: the various metabasins can be in principle enumerated. Their number \\(N_{\\rm minima}\\) allows us to define a configurational entropy:\n\\[\nS_{\\rm conf} = k_B \\ln N_{\\rm minima}\n\\]\nHow is this different from the total entropy? In principle, the total entropy \\(S\\) can be obtained from thermodynamic relations, for example as an integral over the pressure:\n\\[\nS(T) = S(T_0) + \\int_{T_0}^{T} \\frac{1}{T'} \\left( \\frac{\\partial P}{\\partial T'} \\right)_V dV\n\\]\nor, more commonly for liquids and glasses at constant volume,\n\\[\nS(T) = S(T_0) + \\int_{T_0}^{T} \\frac{C_V(T')}{T'} dT'\n\\]\nwhere \\(C_V\\) is the heat capacity at constant volume and \\(T_0\\) is a reference temperature. For a fluid like a gas, only the total entropy is well defined. Even for a liquid at high temperature this is the case, because there are no meaningful metabasins in the free energy profile.\nIt is only as we decrease the temperature further that the metabasins can be defined (via coarse-graining). These reference conformations can be used to split the total entropy into two contributions\n\\[S = S_{\\rm conf}+S_{\\rm vib}\\]\nwhere \\(S_{\\rm vib}\\) is the vibrational entropy around the reference conformations, and is mainly due to thermal fluctuatiosn. In this sense, we are describing the thermodynamics of a disordered system in a way closer to what we would do for a crystal, where the reference conformations are provided by the crustalline packings (e.g. FCC, HCP, BCC etc.).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#physical-gels",
    "href": "soft-matter/soft-matter_glasses.html#physical-gels",
    "title": "19  Arrested states",
    "section": "19.4 Physical gels",
    "text": "19.4 Physical gels\nThe word gel signifies many different things to different scientific communities. In the context of soft matter physics, a physical gel is typically understood as a system in which the constituent particles or polymers are connected via reversible, non-covalent bonds to form a percolating network that spans the entire sample. This network imparts solid-like mechanical properties to the material, even though the underlying structure remains disordered and fluid-like on a microscopic scale.\nPhysical gels differ from chemical gels, where the network is formed by irreversible covalent bonds. In physical gels, the bonds can break and reform dynamically, have an energy of oder \\(1 k_BT\\), allowing the system to respond to external stresses and sometimes to self-heal. Examples include gelatin desserts, agarose gels, and certain colloidal suspensions where attractive interactions lead to network formation, see Zaccarelli (2007) for a comprehensive review.\nThe transition from a fluid to a gel state is often associated with the appearance of a system-spanning cluster, which can be described using concepts from percolation theory. The mechanical rigidity of the gel arises when this cluster forms, leading to a dramatic increase in viscosity and the emergence of an elastic response.\n\n\n\n\n\n\nPercolation\n\n\n\n\n\nPercolation is the study of an apparently innocent mathematical problem: take a square grid if \\(L\\times L\\) sites and randomly label \\(N\\) of them; how does the likelihood of forming a cluster that spans across the grid (i.e. percolates) depend on the probability \\(p=N/L^2\\) of having a labelled square?\nThe problem is interesting for its various applications across domains of science as diverse as networks formation, transport, material science, epidemiologu and ecology. It is a meaneable to an analytical treatment and can be solved via direct simulations. Percolation is a classic example of a phase transition and critical phenomena. The probability \\(p_c\\) at which a spanning cluster first appears is called the percolation threshold. For a large 2D square lattice, \\(p_c \\approx 0.5927\\). Below \\(p_c\\), only small, disconnected clusters exist; above \\(p_c\\), a giant connected component spans the system. The transitions is continuous, i.e. second order.\nAnalytically, percolation is tractable and exhibits universal critical exponents near \\(p_c\\), making it a cornerstone of statistical physics and network theory.\nYou can play with percolation on a squared lattice with the code below.\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table illustrates that various values of the critical percolation probability are known for various lattices, either exactly or via simulation. Notice that this non-universal feature, the critical \\(p_c\\), decreases with increasing dimensionality: at higher dimensions, the connectivity is naturally higher and so a comparitively smaller fraction of the system is require in order to find a percolating path.\n\n\n\n\n\n\n\n\n\n\nLattice Type\nDimension\nCoordination \\(z\\)\n\\(p_c^{site}\\)\n\\(p_c^{bond}\\)\n\n\n\n\nSquare\n2\n4\n0.592746\n0.5\n\n\nTriangular\n2\n6\n0.5\n0.347296\n\n\nHoneycomb\n2\n3\n0.697043\n0.652703\n\n\nKagome\n2\n4\n0.6527\n0.5244\n\n\nUnion Jack\n2\n8\n0.379\n0.429\n\n\nCubic\n3\n6\n0.3116\n0.2488\n\n\nFCC\n3\n12\n0.199\n0.120\n\n\nBCC\n3\n8\n0.245\n0.180\n\n\nDiamond\n3\n4\n0.43\n0.389\n\n\nHypercubic (4D)\n4\n8\n0.197\n0.160\n\n\nHypercubic (5D)\n5\n10\n0.141\n0.118\n\n\nHypercubic (6D)\n6\n12\n0.109\n0.094\n\n\nHypercubic (7D)\n7\n14\n0.091\n0.079\n\n\nHypercubic (8D)\n8\n16\n0.078\n0.069\n\n\n\nInstead, other properties of percolation are universal: these are the critical exponents of observables on the lattice,and do not depend on the lattice detail, such as the probability to find a site in a percolating cluster \\(P_{\\infty}(p)\\propto (p-p_c)^{\\beta}\\)\n\n\n\nDimension \\(d\\)\n\\(\\beta\\) (Percolation)\n\n\n\n\n2\n5/36 ≈ 0.1389\n\n\n3\n≈ 0.41\n\n\n4\n≈ 0.65\n\n\n5\n≈ 0.81\n\n\n6\n≈ 0.96\n\n\n≥6 (Mean-field)\n1\n\n\n\n\n\n\nIn the case of physical gels, percolation is a required ingredient for mechanical stability: in the absence of a large spanning percolating network, it is not possible for the gel to sustain external stresses or even self-generated stresses such as its own weight under gravity.\n\n19.4.1 Colloid-polymer mixtures as an example\nA way to realise robust physical gels in experiments is to use colloid-polymer mixtures, which we have seen previously when we talked about depletion. When the colloids are significantly larger than the polymers, the resulting depletion interaction is attractive and very short ranged: it is a so-called sticky interaction, ideally suited to form sobust physical bonds where, the colloids cluster in a nonequilibrium, branched, dense phase.\nThe resulting phase diagrams different significantly for the ones of simple liquids (e.g., Lennard-Jones interactions): the liquid-gas coexistence curve (the binodal) becomes metastable.\n\n\n\nPhase diagrams of colloid-polymer mixtures for different values of the size ratio parameter \\(q=\\sigma_{\\rm polymer}/\\sigma_{\\rm colloid}\\). Left: very small size ratio make the gas liquid binodal (enclosing the yellow fluid-fluid region) metastable to gas-solid phase separation. Centre: as we increase the size ratio, the liquid pocket at high density starts developing. Right: for almost equal sizes the phase diagram is reminiscent of the one of simple fluids (e.g. Lennard-Jones) after noticing that the polymer reservoir packing fraction plays a role inverse to the temeprature. Adapted from Dijkstra, Brader, and Evans (1999).)\n\n\nGiven the very short range of the interactions, models of colloid polymer mixtures can approximate the Asakura-Oosawa potential (see Section 15.2.5) using even simpler pair-wise interactions. For example, for Monte-Carlo simulations qand theoretical calculations one often uses the square-well model,\nThe square-well potential \\(U(r)\\) is defined as:\n\\[\nU(r) =\n\\begin{cases}\n\\infty & r &lt; \\sigma \\\\\n-\\epsilon & \\sigma \\leq r &lt; \\lambda \\sigma \\\\\n0 & r \\geq \\lambda \\sigma\n\\end{cases}\n\\]\nwhere \\(\\sigma\\) is the particle diameter (hard core), \\(\\epsilon\\) is the well depth (attractive strength), and \\(\\lambda\\) controls the range of the attraction.\n\n\n\n\n\n\n\n\n\nFor molecular dynamics one often employs the so-called Morse potential, which combines fast exponentially decaying tails and a repulsive core in a simple analytical form\n\\[\nU_{\\mathrm{Morse}}(r) = D \\left[ e^{-2\\alpha (r - r_0)} - 2 e^{-\\alpha (r - r_0)} \\right]\n\\]\nwhere \\(D\\) is the well depth, \\(\\alpha\\) controls the range (steepness) of the potential, and \\(r_0\\) is the equilibrium bond distance.\n\n\n\n\n\n\n\n\n\n\nPair correlations\nIn many situation, pair-wise correlation functions are sufficient to capture gel formation. Both the radial distribution function and the structure factor are able to describe gel formation, but they focus (and are accurate) in different regimes:\nThe radial distribution function \\(g(r)\\) has best statistics at short distances, capturing the features of the clusters that form the branched network.\n\n\n\nThe changes in the radial distribution fucntion \\(g(r)\\) as time evolves and the frozen structure of a simulated colloidal gel emerges. The systems is at an overall dilute concetration, so clusters appear as very sharp peaks at short distances, and their structure in terms of nearest and second-nearest neighbour shells are reflected in the first few peaks. From Griffiths, Turci, and Royall (2017)\n\n\nIn gels near the percolation threshold, the structure factor \\(S(q)\\) exhibits scale-free (power-law) behavior at low \\(q\\):\n\\[\nS(q) \\sim q^{-D_f}\n\\]\nwhere \\(D_f\\) is the fractal dimension of the gel network. This power-law regime reflects the absence of a characteristic length scale in the structure—clusters are self-similar over a range of sizes.\n\nFor \\(q\\) much smaller than the inverse cluster size, \\(S(q)\\) flattens (finite-size effects).\nFor intermediate \\(q\\), \\(S(q) \\sim q^{-D_f}\\), indicating fractal geometry.\nFor large \\(q\\), \\(S(q)\\) reflects local (non-fractal) structure.\n\n\n\n\nThe same changes of the previous figures as seen in changes in the structure factor \\(S(q)\\) as time evolves and the frozen structure of a colloidal gel emerges. At small q, a chracteristic power law behaviour emerges, while at higher q the mistructure changes are subtly capture by the reorganisation or splitting of the peaks.\n\n\nThe exponent \\(D_f\\) is related to the spatial scaling of mass with size in the fractal cluster: \\(M(R) \\sim R^{D_f}\\). Typical values for percolation clusters in 3D are \\(D_f \\approx 2.5\\).\nThus, by analyzing the low-\\(q\\) behavior of \\(S(q)\\), one can extract the fractal dimension and confirm scale-free, self-similar structure in gels.\n\n\n\n\n\n\nMeasuring fractal dimensions: the box-counting algorithm\n\n\n\n\n\nFractal dimensions are in practice quite tricky to measure. A common method is the box-counting algorithm:\n\nOverlay a grid of boxes of size \\(\\ell\\) over the structure (e.g., a cluster or network).\nCount the number \\(N(\\ell)\\) of boxes that contain any part of the structure.\nRepeat for different box sizes \\(\\ell\\).\nPlot \\(\\log N(\\ell)\\) versus \\(\\log(1/\\ell)\\). For a fractal, this plot is linear over some range, and the slope gives the fractal dimension \\(D_f\\): \\[\nN(\\ell) \\sim \\ell^{-D_f}\n\\]\n\nThis method is widely used for experimental images and simulation data to estimate the fractal dimension of clusters, aggregates, or networks.\nBelow, illustrate this using a known fractal, the Sierpinski carpet, whose fractal dimensions is exactly \\(D_f = \\log{8}/\\log{3}\\approx 1.8929\\)\n\n# Example: Box-counting fractal dimension for the Sierpinski carpet\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sierpinski_carpet(n):\n    \"\"\"Generate a Sierpinski carpet of order n as a 2D boolean array.\"\"\"\n    size = 3**n\n    carpet = np.ones((size, size), dtype=bool)\n    for i in range(n):\n        step = 3**i\n        for x in range(0, size, 3*step):\n            for y in range(0, size, 3*step):\n                carpet[x+step:x+2*step, y+step:y+2*step] = False\n    return carpet\n\ndef box_count(img, box_sizes):\n    \"\"\"Count number of boxes containing any part of the structure for each box size using histogramdd.\"\"\"\n    N = []\n    coords = np.argwhere(img)\n    for box in box_sizes:\n        bins = [img.shape[0] // box, img.shape[1] // box]\n        # Assign each coordinate to a box\n        hist, _ = np.histogramdd(coords, bins=bins, range=[[0, img.shape[0]], [0, img.shape[1]]])\n        N.append(np.count_nonzero(hist))\n    return np.array(N)\n\n# Generate Sierpinski carpet\norder = 5\ncarpet = sierpinski_carpet(order)\n\n# Box sizes (powers of 3)\nbox_sizes = [3**i for i in range(order, 0, -1)]\nN_boxes = box_count(carpet, box_sizes)\n\n# Plot Sierpinski carpet\nplt.figure(figsize=(4,4))\nplt.imshow(carpet, cmap='binary')\nplt.title('Sierpinski carpet (order %d)' % order)\nplt.axis('off')\nplt.show()\n\n# Box-counting plot\nplt.figure(figsize=(5,4))\nplt.plot(np.log(1/np.array(box_sizes)), np.log(N_boxes), 'o-', label='Box counting')\n# Linear fit for slope (fractal dimension)\nslope, intercept = np.polyfit(np.log(1/np.array(box_sizes)), np.log(N_boxes), 1)\nplt.plot(np.log(1/np.array(box_sizes)), slope*np.log(1/np.array(box_sizes))+intercept, '--', label=f'Fit: $D_f$={slope:.4f}')\nplt.xlabel(r'$\\log(1/\\ell)$')\nplt.ylabel(r'$\\log N(\\ell)$')\nplt.title('Box-counting for Sierpinski carpet')\nplt.legend(frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n19.4.2 Arrested spinodal scenario\nBut how do physical gels come about? A viable scenario is the so-called arrested spinodal decomposition scenario. Here, a system is rapidly quenched into a region of its phase diagram where it would normally phase separate into two distinct phases (e.g., a dense and a dilute phase) via spinodal decomposition. However, before the phase separation can complete, the dynamics of the dense regions slow down dramatically—often due to glassy or jamming behavior—leading to a dynamically arrested, bicontinuous structure.\nThis process is relevant for colloid-polymer mixtures, protein solutions, and some polymer blends. The resulting gels are characterized by a network-like structure that reflects the early stages of spinodal decomposition, “frozen in” by the arrest of particle motion.\nFor the arested spinodal scenario to hold, the system evolves following these steps:\n\nThe system is quenched inside the spinodal region, where spontaneous fluctuations grow.\nDomains of different densities form, but the dense domains become dynamically arrested before macroscopic phase separation completes.\nThe final structure is a bicontinuous, percolating network with solid-like mechanical properties.\n\n\n\n\nThe arrested spinodal scenario mapped on the phase diagram of the dispersed particles (e.g. colloids), adapted from Zaccarelli (2007). We distinguish various lines: the liquid-gas coexistence (binodal) in red, the spinodal line (where phase separation is unstable) black dashed, the percolation line (across which a percolating cluster can be detected), the glass transition line (a dynamical line, where the relaxation time of the system exceeds a conbentional threshold, e.g. 100 seconds). If a system is cooled rapidly down from the fluid to the coexistence region, two scenari are possible: the fluid spontneusly phase separates into two equilibrium phases (liquid and gas, orange horizontal line); or, the cooling is so rapid that proper quilibrium phases cannot be formed and the systems separates into a disordered arrested dense phase and a vapor phase (green horizontal line). This second scenariocorresponds to physical gel formation via spinodal decomposition.\n\n\nFor more details, see Zaccarelli (2007).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBouchaud, Jean-Philippe, and Giulio Biroli. 2004. “On the Adam-Gibbs-Kirkpatrick-Thirumalai-Wolynes Scenario for the Viscosity Increase in Glasses.” The Journal of Chemical Physics 121 (15): 7347–54. https://bris.on.worldcat.org/oclc/110636005.\n\n\nChandler, David, and Juan P Garrahan. 2010. “Dynamics on the Way to Forming Glass: Bubbles in Space-Time.” Annual Review of Physical Chemistry 61 (1): 191–217. https://bris.on.worldcat.org/oclc/4761088692.\n\n\nCoslovich, Daniele, Misaki Ozawa, and Walter Kob. 2018. “Dynamic and Thermodynamic Crossover Scenarios in the Kob-Andersen Mixture: Insights from Multi-CPU and Multi-GPU Simulations.” The European Physical Journal E 41: 1–11. https://bris.on.worldcat.org/oclc/7634416384.\n\n\nDebenedetti, Pablo G, and Frank H Stillinger. 2001. “Supercooled Liquids and the Glass Transition.” Nature 410 (6825): 259–67. https://www.nature.com/articles/35065704.\n\n\nDijkstra, Marjolein, Joseph M Brader, and Robert Evans. 1999. “Phase Behaviour and Structure of Model Colloid-Polymer Mixtures.” Journal of Physics: Condensed Matter 11 (50): 10079. https://bris.on.worldcat.org/oclc/4843512718.\n\n\nGriffiths, Samuel, Francesco Turci, and C Patrick Royall. 2017. “Local Structure of Percolating Gels at Very Low Volume Fractions.” The Journal of Chemical Physics 146 (1). https://pubs.aip.org/aip/jcp/article/146/1/014501/313344/Local-structure-of-percolating-gels-at-very-low.\n\n\nHasyim, Muhammad R, and Kranthi K Mandadapu. 2024. “Emergent Facilitation and Glassy Dynamics in Supercooled Liquids.” Proceedings of the National Academy of Sciences 121 (23): e2322592121. https://www.pnas.org/doi/abs/10.1073/pnas.2322592121.\n\n\nRoyall, C Patrick, Francesco Turci, Soichi Tatsumi, John Russo, and Joshua Robinson. 2018. “The Race to the Bottom: Approaching the Ideal Glass?” Journal of Physics: Condensed Matter 30 (36): 363001. https://bris.on.worldcat.org/oclc/1098708850.\n\n\nSausset, F, G Biroli, and J Kurchan. 2010. “Do Solids Flow?” Journal of Statistical Physics 140: 718–27. https://bris.on.worldcat.org/oclc/5649060652.\n\n\nZaccarelli, Emanuela. 2007. “Colloidal Gels: Equilibrium and Non-Equilibrium Routes.” Journal of Physics: Condensed Matter 19 (32): 323101. https://doi.org/10.1088/0953-8984/19/32/323101.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#glasses",
    "href": "soft-matter/soft-matter_glasses.html#glasses",
    "title": "19  Arrested states",
    "section": "19.2 Glasses",
    "text": "19.2 Glasses\n\n19.2.1 Glass formation\nHow are glasses formed? The figure below illustrates the standard route. First of all, one selects systems for which crystal formation (via nucleation and growth) is hindred (or in other words, frustrated): this can be because of the overall composition (silicate glasses, i.e.e window glasses, have many components) ore because of competing interactions (binary mixtures may have interactions between the A and B components that favour mixing without crystallisation).\nThen, for a chosen composition, one cools down the liquid across the temperature where in principle (if given enough time) the system would crystallise (dubbed \\(T_m\\)). The liquid becomes a so-called supercooled liquid which is a state of local equilibrium that has access to many disordered basins, but not the crystaline one (see energy landscape picture above).\n\n\n\nStandard picture of glass formation: a (local) equilibrium state (the supercooled liquid) is cooled at a certain cooling rate \\(R\\). The cooling rate is fast enough to avoid crystallization (freezing) at \\(T_m\\). At the glass transition temperature \\(T_g\\) the structural relaxation time of the system exceeds the observation timescale and the system falls off equilibrium. The experimental glass transition temperature is not unique and depends on the protocol: fast cooling rate lead to higher glass transition temperatures.\n\n\nAs we keep on cooling the supercooled liquid to lower and lower temperature, the molecules or constituents move more and more slowly, so that the diffusivity is progressively reduced. This slowing down is typically characterized in terms of autocorrelation functions of the density.\nA common way to quantify this is through the (self-part) intermediate scattering function \\(F_s(q, t)\\), which measures how density fluctuations at a given wavevector \\(q\\) decay over time:\n\n\nThe intermediate scattering function \\(F_s(q, t)\\) can be decomposed into two contributions:\n\nSelf part \\(F_s^{\\text{self}}(q, t)\\): Measures the correlation of each particle with its own initial position. It captures single-particle dynamics (how far a particle moves from where it started).\nCollective part \\(F_s^{\\text{coll}}(q, t)\\): Measures correlations between different particles, reflecting how density fluctuations evolve collectively. The collective part is given by \\[\n  F_s^{\\text{coll}}(q, t) = \\frac{1}{N} \\left\\langle \\sum_{j,k=1}^N e^{i \\mathbf{q} \\cdot [\\mathbf{r}_j(t) - \\mathbf{r}_k(0)]} \\right\\rangle\n  \\] In glassy systems, the self part is sensitive to particle mobility (e.g., caging and hopping), while the collective part describes how groups of particles rearrange together. Both are important for understanding relaxation and dynamical arrest.\n\n\n\\[\nF_s^{\\text{self}}(q, t) = \\left\\langle \\frac{1}{N} \\sum_{j=1}^N e^{i \\mathbf{q} \\cdot [\\mathbf{r}_j(t) - \\mathbf{r}_j(0)]} \\right\\rangle\n\\]\n\nAt high temperatures, \\(F_s(q, t)\\) decays rapidly, indicating fast relaxation. As temperature decreases, the decay becomes much slower, often exhibiting a two-step relaxation: a rapid initial drop (the “\\(\\beta\\)-relaxation”) followed by a long plateau and then a slow final decay (the “\\(\\alpha\\)-relaxation”). The time at which \\(F_s(q, t)\\) decays to a certain fraction (e.g., \\(1/e\\)) defines the structural relaxation time \\(\\tau_\\alpha\\), which represents the typical time for a particle to move over a lengthscale \\(\\sim 2\\pi/q\\).\n\n\n\nIntermediate scattering functions for molecular dynamics simulations of a standard glassformer (a coarse grained metallic binary mixture) for various reduced temperatures. From Coslovich, Ozawa, and Kob (2018). One can identify the initial relaxation (\\(\\beta\\)) followed by a long plateau which is itself followed by the complete relaxation (\\(\\alpha\\)). Notice the logarithmic scale of the horizontal axis (in reduced time units).\n\n\nAs the supercooled liquid gets colder and colder, the relaxation time increases rapidly, eventually of many orders of magnitude. At some experimentally determined temperature the relaxation time is larger than the observation time, i.e.\n\\[\\tau_{\\alpha}&gt;t_{\\rm obs}\\]\nThis means that it is impossible to take time averages to estimate thermodynamic properties of the system, because the system does not satisfy any longer the minimal requirement of the ergodic hypothesis. This failure of ergodicity means that the system effectively falls out of equilibrium, because it does not explore all relevant microstates within the available time. The system has become a glass. The temperature at which this clearly non-ordinary (and non-thermodynamic!) transition occurs is called the experimental glass transition temperature \\(T_g\\).\nA key feature of the glass transition is that there is no unique glass transition temperature \\(T_g\\): the transition temperature itself depends on how fast we are cooling the liquid down. It is a protocol dependent property. We can understand this by referring back to the energy landscape: at high temperatures, the system has enough thermal energy to explore many minima in the landscape, hopping over barriers with ease. As the temperature decreases, the barriers become increasingly difficult to cross within the available observation time. If the cooling is slow, the system can equilibrate and find deeper minima, resulting in a lower \\(T_g\\). If the cooling is fast, the system becomes trapped in higher-energy, shallower minima, and \\(T_g\\) is higher. Therefore, the glass transition is not a sharp thermodynamic phase transition, but a kinetic phenomenon determined by the interplay between the system’s relaxation time and the timescale of the experimental protocol.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html",
    "href": "soft-matter/soft-matter_active.html",
    "title": "20  Active matter",
    "section": "",
    "text": "20.1 Beyond thermal systems\nAll the systems we have considered up to now are composed of naturally occurring or synthetic molecules (or atoms) thatfar are composed of naturally occurring or synthetic molecules (or atoms) thatso now are composed of naturally occurring or synthetic molecules (or atoms) thatfar are composed of naturally occurring or synthetic molecules (or atoms) that we have considered up to now are composed of naturally occurring or synthetic molecules (or atoms) which interact via conservative forces (potentials) and are subject to thermal fluctuations. Essentially, energy is either stored in the system as potential energy or exchanged with the environment as heat, leading to equilibrium dynamics governed by the laws of thermodynamics and statistical mechanics. In these systems, the random motion of particles arises from thermal noise, and the system eventually relaxes to a Boltzmann distribution.\nHowever, many systems in nature and technology are driven out of equilibrium by continuous energy consumption at the microscopic scale. These are known as active matter systems. Here, each constituent (such as a bacterium, a synthetic microswimmer, or a molecular motor) converts energy from its surroundings into a variety of mechanisms:\nThis is what is typically performed by many living organisms: by consuming energy locally (hence dissipating heat) they perform some function. Here we focus on of the most fundamental kins of functions which is motion. A bacterium uses energy to propel itself into space, explore it and eventually interact with the environment, including other bacteria.\nWe could, at this stage, say that these interactions and the behaviour emerging from them are inherently biological and governed solely by mechanisms that transcend the physical correlations (behavioural science). As physicists, we do not negate the existence of such dimensions (especially when communication and sensing are involved) but aim to run a different research programme: we want to gauge the level of biological surprise1, as nicely put by Andrea Cavagna, a complex system physicist.\nTo do so we need to understand how systems that dissipate energy locally to produced self-propelled motionm can lead to the emergence of non-trivial phase behaviour and how this differs from systems that are either at thermal equilibrium (e.g. colloidal fluids) or are slowwly relaxing towards it (e.g. glasses and gels).\nThe field of reserach that studies such systems is called physics of active matter and in this chapter we will describe some reference model systems and main results.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#life-inspired-motion-run-and-tumble",
    "href": "soft-matter/soft-matter_active.html#life-inspired-motion-run-and-tumble",
    "title": "20  Active matter",
    "section": "20.2 Life-inspired motion: run and tumble",
    "text": "20.2 Life-inspired motion: run and tumble\nThe run-and-tumble model describes the motion of active particles, such as bacteria, that alternate between two types of movement: “runs” (straight-line motion) and “tumbles” (random reorientation). This model captures the behavior of microorganisms like E. coli.\n\n\n\n\n\n\nFigure 20.1: Swimming Escherichia coli bacteria with fluorescently-labeled flagellar filaments, showing individual run-and-tumble events with flagellar filaments unbundling, cells turning, and flagellar filaments rejoining the bundle. The details of the experimental procedure are given in Turner, Ryu, and Berg (2000).\n\n\n\n\n\nCode\nviewof simulation = {\n    const n = 100, W = 1000, H = 300, dt = 0.5, v0 = 1.5;\n\n    const tumbleSlider = Inputs.range([0.01, 1.0\n    ], {step:0.01, value:0.1, label:\"Tumble rate λ\"});\n    const button = html`&lt;button&gt;Pause&lt;/button&gt;`;\n    let running = true;\n    button.onclick = () =&gt; {\n        running = !running;\n        button.textContent = running ? \"Pause\" : \"Resume\";\n    };\n\n    let state = Array.from({length: n}, () =&gt; ({\n        x: Math.random() * W,\n        y: Math.random() * H,\n        theta: Math.random() * 2 * Math.PI\n    }));\n\n    // Each particle gets a trace array\n    let traces = Array.from({length: n}, () =&gt; []);\n\n    // Reset traces when tumble rate changes\n    let lastLambda = tumbleSlider.value;\n    tumbleSlider.addEventListener(\"input\", () =&gt; {\n        traces = Array.from({length: n}, () =&gt; []);\n        lastLambda = tumbleSlider.value;\n    });\n\n    function step(state, λ) {\n        return state.map((p, i) =&gt; {\n            if (Math.random() &lt; λ * dt) p.theta = Math.random() * 2 * Math.PI;\n\n            // Predict next position\n            let nx = p.x + v0 * Math.cos(p.theta) * dt;\n            let ny = p.y + v0 * Math.sin(p.theta) * dt;\n\n            // Bounce at left/right walls\n            if (nx &lt; 0) {\n                p.x = 0;\n                p.theta = Math.PI - Math.random() * Math.PI; // random angle pointing right\n            } else if (nx &gt; W) {\n                p.x = W;\n                p.theta = Math.PI + Math.random() * Math.PI; // random angle pointing left\n            } else {\n                p.x = nx;\n            }\n\n            // Bounce at top/bottom walls\n            if (ny &lt; 0) {\n                p.y = 0;\n                p.theta = (Math.random() * Math.PI); // random angle pointing down\n            } else if (ny &gt; H) {\n                p.y = H;\n                p.theta = Math.PI + (Math.random() * Math.PI); // random angle pointing up\n            } else {\n                p.y = ny;\n            }\n\n            // Add current position to trace\n            traces[i].push([p.x, p.y]);\n            // Limit trace length for performance\n            if (traces[i].length &gt; 200) traces[i].shift();\n            return p;\n        });\n    }\n\n    const container = html`&lt;div&gt;&lt;/div&gt;`;\n    container.append(tumbleSlider, button);\n    const svg = d3.select(container).append(\"svg\")\n        .attr(\"width\", W)\n        .attr(\"height\", H)\n        .style(\"background\", \"#f8f8f8\");\n\n    while (true) {\n        const λ = tumbleSlider.value;\n        if (running) state = step(state, λ);\n\n        svg.selectAll(\"*\").remove();\n\n        // Draw traces\n        traces.forEach(trace =&gt; {\n            if (trace.length &gt; 1) {\n                svg.append(\"path\")\n                    .attr(\"d\", d3.line()(trace))\n                    .attr(\"stroke\", \"#90caf9\")\n                    .attr(\"stroke-width\", 1)\n                    .attr(\"fill\", \"none\");\n            }\n        });\n\n        // Draw particles\n        svg.selectAll(\"circle\")\n            .data(state)\n            .enter().append(\"circle\")\n            .attr(\"cx\", d =&gt; d.x)\n            .attr(\"cy\", d =&gt; d.y)\n            .attr(\"r\", 3)\n            .attr(\"fill\", \"#1976d2\");\n\n        svg.selectAll(\"line\")\n            .data(state)\n            .enter().append(\"line\")\n            .attr(\"x1\", d =&gt; d.x)\n            .attr(\"y1\", d =&gt; d.y)\n            .attr(\"x2\", d =&gt; d.x + 10 * Math.cos(d.theta))\n            .attr(\"y2\", d =&gt; d.y + 10 * Math.sin(d.theta))\n            .attr(\"stroke\", \"#1976d2\")\n            .attr(\"stroke-width\", 1.2);\n\n        yield container;\n        await Promises.delay(16);\n    }\n}\n\n\n\n\n\n\n\n\n\nFigure 20.2: Non-interacting run and tumble particles.\n\n\n\n\n\nThe model is deisgned to encode a minimal set of ingredients that make the characteristic trajectories of the bacteria quite different fro an ordinary random walk. We can formulate this kind of model via a simple algorithm.\n\nRun Phase: During a run, the particle moves in a straight line with constant velocity \\(v_0\\): \\[\n\\mathbf{r}(t + \\Delta t) = \\mathbf{r}(t) + v_0 \\hat{\\mathbf{n}}(t) \\Delta t\n\\]\nwhere:\n\n\\(\\mathbf{r}(t)\\) is the position of the particle at time \\(t\\),\n\\(v_0\\) is the constant speed,\n\\(\\hat{\\mathbf{n}}(t)\\) is the unit vector indicating the direction of motion.\n\nTumble Phase: During a tumble, the particle randomly reorients. The new direction \\(\\hat{\\mathbf{n}}(t)\\) is chosen from a uniform distribution over the unit sphere (in 3D) or circle (in 2D).\n\nWe can the imagine a schematic algorithm: - Initialize the particle’s position \\(\\mathbf{r}(0)\\) and direction \\(\\hat{\\mathbf{n}}(0)\\). - For each time step \\(\\Delta t\\): - With probability \\(\\lambda \\Delta t\\), perform a tumble (randomize \\(\\hat{\\mathbf{n}}\\)). - Otherwise, update the position using the run equation. - Repeat for the desired simulation duration.\nHere, \\(\\lambda\\) is the tumble rate, which determines the frequency of reorientation events.\nThe mean squared displacement (MSD) of run-and-tumble particles exhibits a characteristic three-phase behavior:\n\nBallistic regime (short times):\nAt very short times, before the first tumble occurs, particles move in straight lines at constant speed. The MSD grows quadratically with time: \\[\n\\langle \\Delta r^2(t) \\rangle \\sim v_0^2 t^2\n\\]\nDiffusive regime (long times):\nAt times much longer than the average run time (\\(t \\gg 1/\\lambda\\)), the direction of motion has been randomized many times, and the motion becomes diffusive: \\[\n\\langle \\Delta r^2(t) \\rangle \\sim 2 D_{\\text{eff}} t\n\\] where \\(D_{\\text{eff}} = \\frac{v_0^2}{2\\lambda}\\) in 2D.\n\nAt intermediate timescales, the MSD transitions smoothly from ballistic to diffusive behavior. This crossover is a hallmark of persistent random walks like run-and-tumble dynamics.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#coloured-noise",
    "href": "soft-matter/soft-matter_active.html#coloured-noise",
    "title": "20  Active matter",
    "section": "20.3 Coloured noise",
    "text": "20.3 Coloured noise\nColoured noise introduces temporal correlations into the random forces acting on a particle, unlike white noise, which is uncorrelated. This is often modeled using an Ornstein-Uhlenbeck process for the noise term.\nIn the context of active matter, coloured noise can be used to describe the dynamics of active particles, where the noise term \\(\\boldsymbol{\\eta}(t)\\) evolves as: \\[\n\\frac{d\\boldsymbol{\\eta}(t)}{dt} = -\\frac{\\boldsymbol{\\eta}(t)}{\\tau_c} + \\sqrt{\\frac{2D_c}{\\tau_c}} \\boldsymbol{\\xi}(t),\n\\]\nwhere:\n\n\\(\\tau_c\\) is the correlation time of the noise,\n\\(D_c\\) is the noise strength,\n\\(\\boldsymbol{\\xi}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nThe particle’s velocity \\(\\mathbf{v}(t)\\) is then given by: \\[\n\\mathbf{v}(t) = v_0 \\hat{\\mathbf{n}}(t) + \\boldsymbol{\\eta}(t),\n\\] where \\(\\hat{\\mathbf{n}}(t)\\) is the direction of self-propulsion.\nThe coloured noise description of active matter is a generalisation of the run and tumble dynamics. The run phase corresponds to the persistence of \\(\\hat{\\mathbf{n}}(t)\\) over time, governed by the correlation time \\(\\tau_c\\), whereas the tumble phase is analogous to a rapid decorrelation of \\(\\hat{\\mathbf{n}}(t)\\), which can be modeled by resetting \\(\\boldsymbol{\\eta}(t)\\) or introducing a large noise term.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#active-brownian-particle-and-motility-induced-phase-separation",
    "href": "soft-matter/soft-matter_active.html#active-brownian-particle-and-motility-induced-phase-separation",
    "title": "20  Active matter",
    "section": "20.4 Active Brownian particle and motility-induced phase separation",
    "text": "20.4 Active Brownian particle and motility-induced phase separation\nActive Brownian particles (ABPs) are a minimal model for self-propelled colloids, such as Janus particles, which move due to chemical reactions at their surfaces. For example, a colloid half-coated with platinum can catalyze the decomposition of hydrogen peroxide in solution, generating local gradients that propel the particle forward.\nThe ABP model captures the essential physics of these systems. Each particle moves with a constant speed in a direction that undergoes rotational diffusion. This leads to persistent motion at short times and diffusive behavior at long times, similar to the run-and-tumble model but with continuous reorientation.\nThe dynamics of an ABP can be described by the following equations:\n\nTranslational Motion: The position \\(\\mathbf{r}(t)\\) of the particle evolves as: \\[\n\\frac{d\\mathbf{r}(t)}{dt} = v_0 \\hat{\\mathbf{n}}(t) + \\sqrt{2D_t} \\boldsymbol{\\xi}(t),\n\\] where:\n\n\\(v_0\\) is the self-propulsion speed,\n\\(\\hat{\\mathbf{n}}(t)\\) is the unit vector indicating the particle’s orientation,\n\\(D_t\\) is the translational diffusion coefficient,\n\\(\\boldsymbol{\\xi}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nRotational Motion: The orientation \\(\\hat{\\mathbf{n}}(t)\\) undergoes rotational diffusion, described by: \\[\n\\frac{d\\hat{\\mathbf{n}}(t)}{dt} = \\sqrt{2D_r} \\boldsymbol{\\eta}(t),\n\\] where:\n\n\\(D_r\\) is the rotational diffusion coefficient,\n\\(\\boldsymbol{\\eta}(t)\\) is a Gaussian white noise term with zero mean and unit variance.\n\nIn 2D, the orientation \\(\\hat{\\mathbf{n}}(t)\\) can be expressed in terms of an angle \\(\\theta(t)\\): \\[\n\\hat{\\mathbf{n}}(t) = (\\cos\\theta(t), \\sin\\theta(t)),\n\\] and the rotational dynamics reduce to: \\[\n\\frac{d\\theta(t)}{dt} = \\sqrt{2D_r} \\eta_\\theta(t),\n\\] where \\(\\eta_\\theta(t)\\) is a scalar Gaussian white noise term.\n\nWe can analyse this a little more in detail by considering the characteristic features of the displacements of (dilute or noninteracting) active Brrownian particles.\n\n20.4.1 Mean squared displacement of Active Brownian Particles\nIndeed, it can be shown. (see below) that teh mean squared displacement for active Browninan particles displays three regimes: \\[\n\\mathrm{MSD}(\\tau) = \\left[4 D_T + 2 v^2 \\tau_R \\right] \\tau + 2 v^2 \\tau_R^2 \\left( e^{-\\tau / \\tau_R} - 1 \\right).\n\\]\nwhere: - \\(D_{\\mathrm{eff}} = \\frac{v_0^2}{2 D_r}\\) is the effective long-time diffusion coefficient,\nThis clearly shows that there are three main regimes:\n\na very short time diffusive regime \\(MSD(\\tau) = \\propto 4D_T\\tau\\)\nan intermediate regime around the reorientation time \\(\\tau_R\\) where \\(\\operatorname{MSD}(\\tau)=4 D_{\\mathrm{T}} \\tau+2 v^2 \\tau^2\\) leading to super-diffusive motion (ballistic)\na final regime where the reorientation has taken place and where a new diffusive regime is reached where the MSD is proportional to time, i.e. \\(\\operatorname{MSD}(\\tau)=\\left[4 D_{\\mathrm{T}}+2 v^2 \\tau_{\\mathrm{R}}\\right] \\tau\\) but with a new diffusion constant.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe derive the MSD expression for 2d Active Brownian particles.\nWe first integrate the velocity\n\\[\n\\mathbf{r}(\\tau) - \\mathbf{r}(0) = v \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds + \\sqrt{2 D_T} \\int_0^\\tau \\boldsymbol{\\eta}(s) ds.\n\\]\nSince \\(\\boldsymbol{\\eta}(t)\\) is independent from \\(\\hat{\\mathbf{n}}(t)\\),\n\\[\n\\langle \\Delta r^2(\\tau) \\rangle = v^2 \\left\\langle \\left| \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds \\right|^2 \\right\\rangle + 4 D_T \\tau,\n\\]\nusing \\(\\langle |\\int \\boldsymbol{\\eta}(s) ds|^2 \\rangle = 2 d D_T \\tau\\) with dimension \\(d=2\\).\nWe first need to evaluate the orientation correlation integral.\nWrite the first term as a double integral:\n\\[\n\\left\\langle \\left| \\int_0^\\tau \\hat{\\mathbf{n}}(s) ds \\right|^2 \\right\\rangle = \\int_0^\\tau ds \\int_0^\\tau ds' \\langle \\hat{\\mathbf{n}}(s) \\cdot \\hat{\\mathbf{n}}(s') \\rangle.\n\\]\nFor 2D rotational diffusion, the orientation correlation is exponential:\n\\[\n\\langle \\hat{\\mathbf{n}}(s) \\cdot \\hat{\\mathbf{n}}(s') \\rangle = e^{-|s - s'|/\\tau_R}.\n\\]\nWe calculate the following double integral\n\\[\nI(\\tau) = \\int_0^\\tau ds \\int_0^\\tau ds' e^{-|s - s'|/\\tau_R}.\n\\]\nUse symmetry: the integrand depends only on \\(|s - s'|\\). Express as\n\\[\nI(\\tau) = 2 \\int_0^\\tau ds \\int_0^s ds' e^{-(s - s')/\\tau_R} = 2 \\int_0^\\tau ds \\int_0^s du\\, e^{-u/\\tau_R},\n\\]\nwhere we set \\(u = s - s'\\).\nIntegrate over \\(u\\):\n\\[\n\\int_0^s e^{-u/\\tau_R} du = \\tau_R \\left(1 - e^{-s/\\tau_R}\\right).\n\\]\nSo\n\\[\nI(\\tau) = 2 \\tau_R \\int_0^\\tau \\left(1 - e^{-s/\\tau_R}\\right) ds = 2 \\tau_R \\left[ \\tau - \\int_0^\\tau e^{-s/\\tau_R} ds \\right].\n\\]\nBy integratingthe exponential we get\n\\[\n\\int_0^\\tau e^{-s/\\tau_R} ds = \\tau_R \\left(1 - e^{-\\tau/\\tau_R}\\right).\n\\]\nHence\n\\[\nI(\\tau) = 2 \\tau_R \\left[ \\tau - \\tau_R \\left(1 - e^{-\\tau/\\tau_R} \\right) \\right] = 2 \\tau_R \\tau - 2 \\tau_R^2 \\left(1 - e^{-\\tau/\\tau_R} \\right).\n\\]\nPutting it all together,\n\\[\n\\langle \\Delta r^2(\\tau) \\rangle = v^2 I(\\tau) + 4 D_T \\tau = v^2 \\left[ 2 \\tau_R \\tau - 2 \\tau_R^2 (1 - e^{-\\tau/\\tau_R}) \\right] + 4 D_T \\tau,\n\\]\nor equivalently\n\\[\n\\boxed{\n\\langle \\Delta r^2(\\tau) \\rangle = \\left(4 D_T + 2 v^2 \\tau_R \\right) \\tau + 2 v^2 \\tau_R^2 \\left( e^{-\\tau/\\tau_R} - 1 \\right).\n}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n20.4.2 Interacting ABPs and motility induced phase separation\nWhen teh active Brownian particles are actually inetarcting with each other, they can give rise to an exceptional nnequilibrium phenomenon of self organisation called motility-induced phase separation (MIPS).\nThe idea is to cosnider purely repulsive particles (i.e hard spheres) obeying the ABP dynamics. This means that, in equilibrium (i.e. in the absence of self-propulsion) no liquid-gas phase separation is possible (see Section 15.3.1).\nHowever, as we reduce the rotational diffusion, the persistent motion becomes more important, the system becomes more out of equilibrium and new physics comes to play.\nIn particular, the colloision between particles are no longer leading to quick decorrelation: on teh contrary, head-to-head collisions between the particles mean that there is a finite residence time for a pair of particles to stay in each other neighborhoods. This effect is amplified by multiple collisions, leading to many-body caging effects that promote density heterogeneities in the fluid.\nThe result is striking: for sufficiently low rotational diffusions, the fluid of active Brownian particles spontaneously phase separates into a dilute and a dense phase, akin to the gas and liquid phase of equilibrium systems. Before reaching this regime, the fluid also displays a critical like phenomenology, with enhcanced fluctuations terminating a critical point.\nThis has been examined in detail in coputer simulations, both in two dimensional systems and in three dimensional systems (where the physics is even richer and presents parallels with the situtation of colloid-polymer mixtures due to the very short range nature of the effective interactions between active particles).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurner, Linda, William S Ryu, and Howard C Berg. 2000. “Real-Time Imaging of Fluorescent Flagellar Filaments.” Journal of Bacteriology 182 (10): 2793–2801.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_readings.html",
    "href": "soft-matter/soft-matter_readings.html",
    "title": "20  Readings",
    "section": "",
    "text": "The best overall text is: R.A.L Jones, Soft Condensed Matter, Oxford University Press. Shelfmark [539.2 Jon].\nAdditionally, the following more specialised texts should also be useful. They can be found in the University Library under the stated shelfmark.\n\n20.0.1 Colloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet. VCH Publishers (1994). [541.18 Eva]\nR.J.Hunter: Introduction to Modern Colloid Science. Oxford University Press (1993). 541.18 Hun]\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions Cambridge University Press (1989).[541.18 Rus]\nD.H.Everett: Basic Principles of Colloid Science.\n\nRoyal Society of Chemistry Paperbacks (1988) [ 541.18 Eve]\n\n\n20.0.2 Polymers and surfactants\n\nR.J. Young and P.A. Lovell: Introduction to polymers [678 You].\nM. Doi: Introduction to polymer physics [541.64 Doi]\nJ.Israelachvili, Intermolecular and Surface Forces, Academic Press (1992), Chs. 16 and 17\n\n\n\n20.0.3 Glasses\n\nJ. Zarzycki; Glasses and the vitreous state. Cambridge University Press (1991).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Readings</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html",
    "href": "soft-matter/soft-matter_problems.html",
    "title": "21  Problems",
    "section": "",
    "text": "21.1 Relaxation time in atomic fluids\nSpherical colloidal particles of radius 30 nm and density \\(1.35 \\mathrm{~g} \\mathrm{~cm}^{-3}\\) are suspended in water (temperature \\(25^{\\circ} \\mathrm{C}\\), density \\(1.0 \\mathrm{~g} \\mathrm{~cm}^{-3}\\), viscosity \\(1.0 \\times 10^{-3}\\) Pa s ). Calculate the average distance from the origin along a given axis travelled by a particle in 1 minute due to Brownian motion and sedimentation, respectively. What are the displacements after 1 hour?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "revision_guide.html",
    "href": "revision_guide.html",
    "title": "Appendix A — Revision guide",
    "section": "",
    "text": "In addition to having worked through and being familiar with all the questions on the problem sheets, you are expected to:\n\nKnow the relationship between the free energy of a system and its partition function. Be able to differentiate the free energy to obtain thermodynamic observables and fluctuation relations for the heat capacity and magnetic susceptibility.\nKnow the forms of the singularities exhibited by key observables on the approach to criticality and be able to define the exponents \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\) and \\(\\nu\\).\nBe able to draw the phase diagrams of magnetic and fluid systems and define the critical exponents. You should know the key differences between first and second order phase transitions.\nUnderstand the isomorphism between the Ising model and lattice gas.\nIsing model: Write down the Hamiltonian of the Ising model and be familiar with its properties in \\(d = 1, 2, 3\\). Derive the Ising model free energy in \\(d = 1\\) both in zero magnetic field (kink method) and in a finite field (Transfer matrix method).\nMean field theory: Show that the effective Hamiltonian \\[ H_{\\text{eff}} = H + qJm \\] and derive the equation \\[ m = \\tanh(\\beta H_{\\text{eff}}). \\] From this, find the behaviour of \\(m\\) and \\(\\chi\\) near \\(T_c\\). Know the form of the spin-spin correlation function in mean field theory and use it to calculate the behaviour of the heat capacity near criticality. Describe the shortcomings of mean field theory and explain the underlying reasons.\nLandau theory: Given the Landau free energy, sketch and interpret the form of the free energy as a function of magnetisation for a range of temperatures around \\(T_c\\). Derive the values of the critical exponents \\(\\beta\\), \\(\\delta\\), \\(\\gamma\\), \\(\\alpha\\). Understand the genesis of first order phase transitions and metastability within Landau theory and the importance of symmetry considerations.\nScaling theory: Recast a generalised homogeneous function as a power law in one variable. Applying this to the free energy, deduce scaling forms for key observables and relationships between the scaling variables and the critical exponents. Deduce scaling laws relating the critical exponents. Describe experimental tests of scaling and its utility in the context of computer simulations of critical phenomena.\nRenormalization Group: Describe the main purpose and features of the RG formalism in the real-space (block variable) approach. Describe the relevance of the RG to scaling phenomena and universality. Know which essential qualitative features of a system delineate universality classes.\nClassical nucleation theory: Understand that in first-order phase transitions, like in the Ising model under a small external field, or a slightly oversaturated gas, a free energy barrier must be overcome for the stable phase to nucleate within the metastable phase. Describe in qualitative and mathematical terms how nucleation involves the formation of a critical-sized region of the new phase, which can then grow to complete the transition, with the balance between bulk energy gain and surface energy cost determining the critical size. Describe mathematically how growth dynamics after nucleation depend on factors like interface motion and external driving forces, shaping the overall evolution from the metastable to the stable phase.\nStochastic processes: Understand that many natural processes are stochastic, meaning their evolution over time involves inherent randomness, either from lack of microscopic detail (in classical physics) or intrinsic probabilistic nature (in quantum mechanics). Describe such systems, using coarse-grained probabilistic methods, focusing on the likelihood of different outcomes instead of exact trajectories, ie. the master equation. Derive macroscopic behaviors like diffusion by analyzing the balance of transition rates between states. Understand that the Langevin approach models particle motion using a stochastic differential equation, offering a trajectory-based view complementary to the probability-focused diffusion (Fokker-Planck) equation. Describe mathematically how in the Langevin equation, random forces and friction combine to determine particle displacements over small time intervals, capturing the randomness inherent in Brownian motion. Derive key statistical quantities such as the mean square displacement and the form of the diffusion constant and dampling constant. By analyzing the statistics of these random steps, one can derive the diffusion equation as the continuum limit of the random walk.\nDynamics of fluctuations: Understand and describe mathematically how fluctuations of thermodynamic variables, like local magnetization or density, can be analyzed through their time correlations when the system is in thermal equilibrium. Describe the role of the two-time correlation function \\(M_{xx}(t)\\), which measures how a fluctuation at one time is related to a fluctuation at a later time. Describe how in equilibrium, this correlation depends only on the time difference and typically decays exponentially with a characteristic correlation time \\(t_c\\).\n\nNote that some of the above items were mainly covered in the problem sheets.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Revision guide</span>"
    ]
  },
  {
    "objectID": "phase-transitions/concepts_map.html",
    "href": "phase-transitions/concepts_map.html",
    "title": "Appendix B — Mindmaps - Linking concept and methods",
    "section": "",
    "text": "mindmap\n  root((Phase Transitions))\n    TYPES\n      Order parameters\n        Continuous vs. discontinuous\n          First-order transitions            \n            eg., colloidal freezing\n            eg., boiling,melting\n            eg., Isotropic-nematic transitions\n          Second-order transitions\n            eg., ferromagnetic transition\n            eg., Vapor-liquid criticality\n            eg., Demixing of binary colloids\n    CRITICALITY\n      Singularities\n        Diverging correlation length\n        Diverging response functions\n        Critical exponents\n          Scaling laws\n          Universality classes   \n    PHASE TRANSITION DYNAMICS\n      Quenching and coarsening\n        Nucleation and growth\n        Spinodal decomposition\n        Simulation\n        Scattering experiments\n    MODELS\n      Ising model\n      Lattice gas model\n      Lennard-Jones fluid\n      XY model\n    METHODS\n      Mean-field theory\n      Renormalization group\n      Monte Carlo simulations\n      Finite-size scaling\n      Molecular dynamics\n      Experiments\n        X-ray and neutron scattering\n        Microscopy\n        Rheology\n\n\n\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Stochastic processes))\n    Master equation\n      Diffusion equation\n        Mean squared displacement\n        Colloidal motion\n          Microscopy\n        Arrested states\n      Detailed Balance\n        Monte Carlo simulation\n    Langevin equation\n      Random walks\n        Brownian motion\n        Polymer configurations\n          Scaling of polymer dimensions\n      Driven Systems\n        Active Matter\n    Fluctuation dynamics\n      Temporal correlations\n      Response to perturbation\n        Fluctuation dissipation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Concept maps</span>"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#we-have",
    "href": "phase-transitions/Solutions_to_problems.html#we-have",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "11. We have",
    "text": "11. We have\n\\[\nX(t) - X(0) = \\int_0^t u(t') dt'\n\\]\nand substituting this into the definition of \\(D\\), we find:\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\int_0^t dt_1 \\int_0^t dt_2 \\langle u(t_1) u(t_2) \\rangle\n\\]\nNote: the use of different dummy variables \\(t_2\\) and \\(t_2\\) allows us to write the square as a product. Now change variables \\(t_2 = t_1 + \\tau\\):\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{t} \\int_0^t dt_1 \\int_0^{t - t_1} d\\tau \\langle u(t_1) u(t_1 + \\tau) \\rangle\n\\]\nFor a stationary process,\n\\[\n\\langle u(t_1) u(t_1 + \\tau) \\rangle = \\langle u(t_0) u(t_0 + \\tau) \\rangle\n\\]\nfor any arbitrary \\(t_0\\).\nDoing the integral with respect to \\(t_1\\) we find:\n\\[\nD = \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle dt,\n\\]\nwhere we renamed \\(\\tau \\to t\\) and assume that \\(\\langle u u \\rangle\\) will fall off so fast that the upper limit doesn’t matter.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#einsteins-expression-for-the-diffusion-coefficient",
    "href": "phase-transitions/Solutions_to_problems.html#einsteins-expression-for-the-diffusion-coefficient",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "13. Einstein’s expression for the diffusion coefficient",
    "text": "13. Einstein’s expression for the diffusion coefficient\nWe have\n\\[\nX(t) - X(0) = \\int_0^t u(t') dt'\n\\]\nand substituting this into the definition of \\(D\\), we find:\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\int_0^t dt_1 \\int_0^t dt_2 \\langle u(t_1) u(t_2) \\rangle\n\\]\nNote: the use of different dummy variables \\(t_2\\) and \\(t_2\\) allows us to write the square as a product. Now change variables \\(t_2 = t_1 + \\tau\\):\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{t} \\int_0^t dt_1 \\int_0^{t - t_1} d\\tau \\langle u(t_1) u(t_1 + \\tau) \\rangle\n\\]\nIn thermal equilibrium the process is stationary, so the two‐time correlation depends only on the time‐difference:\n\\[\n\\langle u(t_1) u(t_1 + \\tau) \\rangle = \\langle u(t_0) u(t_0 + \\tau) \\rangle\n\\] for any arbitrary \\(t_0\\).\nDoing the integral with respect to \\(t_1\\) we find:\n\\[\nD = \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle dt,\n\\]\nwhere we renamed \\(\\tau \\to t\\) and assume that \\(\\langle u u \\rangle\\) will fall off so fast that the upper limit doesn’t matter.\n\n\n14. Life in one dimension\nIn the interval between \\(t\\) and \\(t + \\delta t\\),\n\\[\np_{n-1}(t) \\alpha \\delta t : \\quad n - 1 \\rightarrow n;\n\\]\n\\[\np_{n+1}(t) \\beta \\delta t : \\quad n + 1 \\rightarrow n.\n\\]\nThese are both gains. There is also a loss \\(p_n(t)(\\alpha + \\beta)\\delta t\\) which is the probability of the particle hopping from \\(n\\) to one of the neighbouring sites.\nThe master equation follows as:\n\\[\n\\dot{p}_n(t) = \\alpha p_{n-1}(t) + \\beta p_{n+1}(t) - (\\alpha + \\beta) p_n(t).\n\\]\nIntroduce the generating function \\(F(z,t)\\), thus:\n\\[\nF(z,t) = \\sum_{n=-\\infty}^\\infty p_n(t) z^n,\n\\]\nwhere initial and boundary conditions imply \\(F(1,t) = 1\\) and \\(F(z,0) = 1\\).\nMultiply the master equation through by \\(z^n\\) and sum over \\(n\\) to obtain the equation for \\(F\\):\n\\[\n\\frac{\\partial F}{\\partial t} = [\\alpha z + \\beta z^{-1} - (\\alpha + \\beta)] F\n\\]\nwith easy solution:\n\\[\nF(z,t) = e^{-(\\alpha + \\beta)t} e^{\\alpha z t} e^{\\beta z^{-1} t} F(z,0)\n\\]\nwhere the initial condition gives us \\(F(z,0) = 1\\).\nWe obtain the \\(p_n(t)\\) by expanding the exponentials in (respectively) powers of \\(z\\) and \\(z^{-1}\\); thus:\n\\[\nF(z,t) = e^{-(\\alpha + \\beta)t} \\sum_{k=0}^\\infty \\sum_{l=0}^\\infty \\frac{\\alpha^k \\beta^l t^{k+l}}{k!l!} z^{k - l}.\n\\]\nThe probability \\(p_n\\) is the coefficient of \\(z^n\\). Consider two cases:\nCase 1 \\(n \\ge 0\\), set \\(k = n + l\\).\n\\[\np_n(t) = e^{-(\\alpha + \\beta)t} (\\alpha t)^n \\sum_{l=0}^\\infty \\frac{(\\alpha \\beta t^2)^l}{l!(l+n)!}\n\\]\nCase 2 \\(n \\le 0\\), set \\(l = k - n\\).\n\\[\np_n(t) = e^{-(\\alpha + \\beta)t} (\\beta t)^{-n} \\sum_{k=0}^\\infty \\frac{(\\alpha \\beta t^2)^k}{k!(k - n)!}\n\\]\nFind the mean and rms deviation by direct use of the generating function.\n\\[\n\\langle n \\rangle = \\sum_{n=-\\infty}^{\\infty} n p_n(t) = \\left. \\frac{\\partial F}{\\partial z} \\right|_{z=1} = (\\alpha - \\beta)t\n\\]\n\\[\n\\langle n^2 \\rangle = \\sum_{n=-\\infty}^{\\infty} n^2 p_n(t) = \\left. \\left( \\frac{\\partial}{\\partial z} \\left( z \\frac{\\partial F}{\\partial z} \\right) \\right) \\right|_{z=1} = (\\alpha + \\beta)t + (\\alpha - \\beta)^2 t^2\n\\]\n\n\n\n15. Master equation\nThe rates of change of \\(n_+\\) and \\(n_-\\) are clearly given by the master equations:\n\\[\n\\frac{dn_+}{dt} = -n_+ v_{+-} + n_- v_{-+};\n\\]\n\\[\n\\frac{dn_-}{dt} = n_+ v_{+-} - n_- v_{-+}.\n\\]\nAt equilibrium the right-hand sides of the above master equations must vanish. Hence the ratio of the transition probabilities is given by:\n\\[\n\\left. \\frac{v_{-+}}{v_{+-}} = \\frac{n_+}{n_-} \\right|_{\\text{eq}} = e^{-2\\beta \\varepsilon}.\n\\]\nwith \\(\\beta = 1/k\\tau\\), as usual.\nSubtracting one of the above master equations from the other, we obtain:\n\\[\n\\frac{dn}{dt} = 2n_+ v_{+-} - 2n_- v_{-+}\n\\]\nThen, noting that \\(N = n_- + n_+\\) and \\(n = n_- - n_+\\) implies that:\n\\[\nn_- = \\frac{N + n}{2} \\quad \\text{and} \\quad n_+ = \\frac{N - n}{2},\n\\]\nwe have:\n\\[\n\\frac{dn}{dt} = -n(v_{+-} + v_{-+}) + N(v_{+-} - v_{-+})\n\\]\nwhich can be written in the suggestive form:\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) + \\frac{N(v_{+-} - v_{-+})}{v_{+-} + v_{-+}} \\right]\n\\]\nwhere\n\\[\n\\tau = \\frac{1}{v_{+-} + v_{-+}}.\n\\]\nThis can further be written as\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) - n_{\\text{eq}} \\right],\n\\]\nwhere the equilibrium value of \\(n(t)\\) is:\n\\[\nn_{\\text{eq}} = N \\frac{v_{+-} - v_{-+}}{v_{+-} + v_{-+}} = N \\tanh(\\beta \\varepsilon).\n\\]\nThe solution is:\n\\[\nn(t) = n_{\\text{eq}} + e^{-t/\\tau} [n(0) - n_{\\text{eq}}].\n\\]\nYou should note the behaviour of this solution as \\(t \\to 0\\) and \\(t \\to \\infty\\).\n\n\n\n16. Detailed balance\n(a) For individual states, the rates of transition are:\n\\[\n1 \\rightarrow 2 \\quad = \\quad v_{12} p_1\n\\]\n\\[\n2 \\rightarrow 1 \\quad = \\quad v_{21} p_2\n\\]\nDefine groups of states:\nGroup A: \\(\\alpha = 1, \\cdots, N\\)\nGroup B: \\(\\beta = 1, \\cdots, M\\)\nThen\n\\[\n\\sum_{\\alpha=1}^N p_\\alpha = p_A \\quad ; \\quad \\sum_{\\beta=1}^M p_\\beta = p_B\n\\]\n(Note: \\(p_A \\ne p_B\\))\nOverall rate\n\\[\nA \\rightarrow B = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^m v_{\\alpha \\beta} p_\\alpha\n\\]\nOverall rate\n\\[\nB \\rightarrow A = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^m v_{\\beta \\alpha} p_\\beta\n\\]\nwhere\n\\(v_{\\alpha \\beta} = v_{\\beta \\alpha}\\) : jump rate symmetry\n\\(p_\\alpha = p_\\beta\\) : principle of equal a priori probabilities (for microstates)\nHence:\n\\[\n\\text{rate}(A \\rightarrow B) \\equiv v_{AB} p_A^{eq}\n\\]\n\\[\n\\text{rate}(B \\rightarrow A) \\equiv v_{BA} p_B^{eq}\n\\]\nare equal in equilibrium.\n(b) From problem 13, the probability \\(p_A\\) of a group of states \\(A\\) is given by\n\\[\np_A = \\frac{e^{-\\beta F_A}}{Z},\n\\]\nfor the canonical ensemble, and a similar result may be written for group \\(B\\).\nAs this system is in the canonical ensemble, we can assume that it is in a large heat bath, with the ‘system + heat bath’ being treated as ‘isolated’.\nTake \\(A\\) to be the microstate \\(i\\) of the system + any state of the heat bath, such that\n\\[\nE_{system} + E_{reservoir} = E \\, \\text{(fixed)}.\n\\]\nFor a large reservoir,\n\\[\np_i = \\frac{e^{-\\beta E_i}}{z} = \\textrm{``$p_A$''},\n\\]\nwith a similar result for \\(B\\).\nHence the result of part (a), that\n\\[\n\\text{Rate}(A \\rightarrow B) = \\text{Rate}(B \\rightarrow A)\n\\]\nmeans here that\n\\[\nv_{ij} p_i = v_{ji} p_j,\n\\]\neven though, for states of different system energy \\(E_i\\),\n\\[\np_i \\ne p_j \\quad \\text{and} \\quad v_{ij} \\ne v_{ji}.\n\\]\nThis implies:\n\\[\n\\frac{v_{ij}}{v_{ji}} = \\frac{p_j}{p_i} = e^{\\beta(E_j - E_i)}\n\\]\nThis result has important implications for Monte Carlo simulation\n\n\n\n17. Jump processes\n(a) Label by \\(p_i(t)\\) the probability to be in state \\(i\\) at time \\(t\\). From the question:\n\nFrom state 1 one can only jump to 2 with rate \\(\\lambda_0\\).\nFrom state 2 one can jump to 1 or to 3, each with rate \\(\\lambda_0\\).\nFrom state 3 one can only jump to 2 with rate \\(\\lambda_0\\).\n\nHence the gain–loss equations are \\[\n\\begin{aligned}\n\\dot p_1 &= -\\lambda_0\\,p_1 + \\lambda_0\\,p_2,\\\\\n\\dot p_2 &= \\lambda_0\\,p_1 - 2\\lambda_0\\,p_2 + \\lambda_0\\,p_3,\\\\\n\\dot p_3 &= \\lambda_0\\,p_2 - \\lambda_0\\,p_3.\n\\end{aligned}\n\\]\nWriting \\(\\mathbf p=(p_1,p_2,p_3)^{\\!T}\\), one checks immediately that \\[\n\\dot{\\mathbf p}\n\\;=\\;\n\\lambda_0\n\\begin{pmatrix}\n-1 & 1 & 0\\\\\n1 & -2 & 1\\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\mathbf p\n\\;=\\;M\\,\\mathbf p,\n\\] as claimed. Note that the columns of \\(M\\) sum to zero, ensuring \\(\\sum_i\\dot p_i=0\\) (probability conservation).\n(b) An equilibrium state \\(\\mathbf p^{\\rm eq}\\) satisfies \\[\nM\\,\\mathbf p^{\\rm eq}=0,\n\\quad\n\\sum_{i=1}^3 p^{\\rm eq}_i=1.\n\\] From the first row of \\(M\\,\\mathbf p=0\\) we get \\[\n-\\,p^{\\rm eq}_1 + p^{\\rm eq}_2 =0\n\\;\\Longrightarrow\\;\np^{\\rm eq}_1 = p^{\\rm eq}_2.\n\\] From the third row, \\[\np^{\\rm eq}_2 - p^{\\rm eq}_3=0\n\\;\\Longrightarrow\\;\np^{\\rm eq}_2 = p^{\\rm eq}_3.\n\\] Hence all three components are equal. Normalizing, \\[\np^{\\rm eq}_1 = p^{\\rm eq}_2 = p^{\\rm eq}_3 = \\tfrac13,\n\\] i.e.  \\[\n\\mathbf p^{\\rm eq} = \\bigl(\\tfrac13,\\tfrac13,\\tfrac13\\bigr).\n\\]\n(c) The matrix \\[\nM = \\lambda_0\n\\begin{pmatrix}\n-1 & 1 & 0\\\\\n1 & -2 & 1\\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\] is (up to \\(\\lambda_0\\)) the negative of the graph-Laplace operator of the path \\(1\\!-\\!2\\!-\\!3\\). Its characteristic polynomial is \\[\n\\det(M -\\mu I)\n=-\\mu(\\mu+\\lambda_0)(\\mu+3\\lambda_0),\n\\] so the eigenvalues are \\[\n\\mu_1=0,\\quad \\mu_2=-\\lambda_0,\\quad \\mu_3=-3\\lambda_0.\n\\] Since the zero‐eigenvalue has multiplicity one, the null space of \\(M\\) is one-dimensional. Imposing \\(\\sum_i p_i=1\\) picks out a unique vector in that null space. Hence the equilibrium \\((\\tfrac13,\\tfrac13,\\tfrac13)\\) is the only steady-state probability vector.",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#life-in-one-dimension",
    "href": "phase-transitions/Solutions_to_problems.html#life-in-one-dimension",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "12. Life in one dimension",
    "text": "12. Life in one dimension\n\nIn the interval between \\(t\\) and \\(t + \\delta t\\),\n\n\\[\np_{n-1}(t) \\alpha \\delta t : \\quad n - 1 \\rightarrow n;\n\\]\n\\[\np_{n+1}(t) \\beta \\delta t : \\quad n + 1 \\rightarrow n.\n\\]\nThese are both gains. There is also a loss \\(p_n(t)(\\alpha + \\beta)\\delta t\\) which is the probability of the particle hopping from \\(n\\) to one of the neighbouring sites.\nThe master equation follows as:\n\\[\n\\dot{p}_n(t) = \\alpha p_{n-1}(t) + \\beta p_{n+1}(t) - (\\alpha + \\beta) p_n(t).\n\\]\nIntroduce the generating function \\(F(z,t)\\), thus:\n\\[\nF(z,t) = \\sum_{n=-\\infty}^\\infty p_n(t) z^n,\n\\]\nwhere initial and boundary conditions imply \\(F(1,t) = 1\\) and \\(F(z,0) = 1\\).\nMultiply the master equation through by \\(z^n\\) and sum over \\(n\\) to obtain the equation for \\(F\\):\n\\[\n\\frac{\\partial F}{\\partial t} = [\\alpha z + \\beta z^{-1} - (\\alpha + \\beta)] F\n\\]\nwith easy solution:\n\\[\nF(z,t) = e^{-(\\alpha + \\beta)t} e^{\\alpha z t} e^{\\beta z^{-1} t} F(z,0)\n\\]\nwhere the initial condition gives us \\(F(z,0) = 1\\).\nWe obtain the \\(p_n(t)\\) by expanding the exponentials in (respectively) powers of \\(z\\) and \\(z^{-1}\\); thus:\n\\[\nF(z,t) = e^{-(\\alpha + \\beta)t} \\sum_{k=0}^\\infty \\sum_{l=0}^\\infty \\frac{\\alpha^k \\beta^l t^{k+l}}{k!l!} z^{k - l}.\n\\]\nThe probability \\(p_n\\) is the coefficient of \\(z^n\\). Consider two cases:\nCase 1 \\(n \\ge 0\\), set \\(k = n + l\\).\n\\[\np_n(t) = e^{-(\\alpha + \\beta)t} (\\alpha t)^n \\sum_{l=0}^\\infty \\frac{(\\alpha \\beta t^2)^l}{l!(l+n)!}\n\\]\nCase 2 \\(n \\le 0\\), set \\(l = k - n\\).\n\\[\np_n(t) = e^{-(\\alpha + \\beta)t} (\\beta t)^{-n} \\sum_{k=0}^\\infty \\frac{(\\alpha \\beta t^2)^k}{k!(k - n)!}\n\\]\nFind the mean and rms deviation by direct use of the generating function.\n\\[\n\\langle n \\rangle = \\sum_{n=-\\infty}^{\\infty} n p_n(t) = \\left. \\frac{\\partial F}{\\partial z} \\right|_{z=1} = (\\alpha - \\beta)t\n\\]\n\\[\n\\langle n^2 \\rangle = \\sum_{n=-\\infty}^{\\infty} n^2 p_n(t) = \\left. \\left( \\frac{\\partial}{\\partial z} \\left( z \\frac{\\partial F}{\\partial z} \\right) \\right) \\right|_{z=1} = (\\alpha + \\beta)t + (\\alpha - \\beta)^2 t^2\n\\]",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/Solutions_to_problems.html#master-equation",
    "href": "phase-transitions/Solutions_to_problems.html#master-equation",
    "title": "Unifying concepts: outline solutions to problems",
    "section": "13. Master equation",
    "text": "13. Master equation\nThe rates of change of \\(n_+\\) and \\(n_-\\) are clearly given by the master equations:\n\\[\n\\frac{dn_+}{dt} = -n_+ v_{+-} + n_- v_{-+};\n\\]\n\\[\n\\frac{dn_-}{dt} = n_+ v_{+-} - n_- v_{-+}.\n\\]\nAt equilibrium the right-hand sides of the above master equations must vanish. Hence the ratio of the transition probabilities is given by:\n\\[\n\\left. \\frac{v_{-+}}{v_{+-}} = \\frac{n_+}{n_-} \\right|_{\\text{eq}} = e^{-2\\beta \\varepsilon}.\n\\]\nwith \\(\\beta = 1/k\\tau\\), as usual.\nSubtracting one of the above master equations from the other, we obtain:\n\\[\n\\frac{dn}{dt} = 2n_+ v_{+-} - 2n_- v_{-+}\n\\]\nThen, noting that \\(N = n_- + n_+\\) and \\(n = n_- - n_+\\) implies that:\n\\[\nn_- = \\frac{N + n}{2} \\quad \\text{and} \\quad n_+ = \\frac{N - n}{2},\n\\]\nwe have:\n\\[\n\\frac{dn}{dt} = -n(v_{+-} + v_{-+}) + N(v_{+-} - v_{-+})\n\\]\nwhich can be written in the suggestive form:\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) + \\frac{N(v_{+-} - v_{-+})}{v_{+-} + v_{-+}} \\right]\n\\]\nwhere\n\\[\n\\tau = \\frac{1}{v_{+-} + v_{-+}}.\n\\]\nThis can further be written as\n\\[\n\\frac{dn}{dt} = -\\frac{1}{\\tau} \\left[ n(t) - n_{\\text{eq}} \\right],\n\\]\nwhere the equilibrium value of \\(n(t)\\) is:\n\\[\nn_{\\text{eq}} = N \\frac{v_{+-} - v_{-+}}{v_{+-} + v_{-+}} = N \\tanh(\\beta \\varepsilon).\n\\]\nThe solution is:\n\\[\nn(t) = n_{\\text{eq}} + e^{-t/\\tau} [n(0) - n_{\\text{eq}}].\n\\]\nYou should note the behaviour of this solution as \\(t \\to 0\\) and \\(t \\to \\infty\\).",
    "crumbs": [
      "Unifying concepts",
      "Solutions"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html#existence-of-a-phase-transition-in-d2.",
    "href": "phase-transitions/problems.html#existence-of-a-phase-transition-in-d2.",
    "title": "Unifying theoretical concepts: Problems",
    "section": "",
    "text": "(Hint: Model the domain wall as a non-reversing $N$-step random\nwalk on the lattice and find an expression for its energy and -from\nthe number of random walk configurations- its entropy.)\n\n\nCorrelation Length\n(Challenging) For a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n\\[\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m\\] where \\(m\\) is the number of domain walls between \\(i\\) and \\(j\\) and \\(p_m\\) is the probability of finding \\(m\\) domain walls between them.\nHence show that when \\(R_{ij}=|i-j|a\\) is large (with \\(a\\) the lattice spacing) and the temperature is small, that\n\\[\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)\\] with \\(\\xi=a/2p\\) and \\(p\\) the probability of finding a domain wall on a bond.\nHint: In the second part note that \\(p_m\\) is given by a binomial distribution because there is a probability \\(p\\) of each bond containing a domain wall and \\((1-p)\\) that it doesn’t. What special type of distribution does \\(p_m\\) tend to when \\(p\\) is small (as occurs at low \\(T\\))?\n\n\n\nA model fluid\n(Straightforward) The van der Waals (vdW) equation of state (recall PH10002) is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nUse a graph-plotting program such as “Excel” to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of P vs V on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\nMean field theory of the Ising model heat capacity\n(Straightforward) Using results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle\n  E\\rangle/\\partial T\\) has the behaviour\n\\[\\begin{aligned}\nC_H=& 0 ~~~~ T&gt;T_c\\\\\nC_H=& 3Nk_B/2 ~~~~ T\\le T_c\n\\end{aligned}\\]\n\n\n\nMagnetisation and fluctuations\n(Slightly tricky) A system of spins on a lattice, has, in the absence of an applied field, a Hamiltonian \\({\\cal H}\\). In the presence of a field \\(h\\) the Hamiltonian becomes \\[\\tilde {\\cal H}={\\cal H}-hM\\] where \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\nSpin-1 Ising model\n(Straightforward) A set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j + h\\sum_i s_i\\]\nFind the partition function and hence show that in the mean field approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\nTransfer Matrix.\n(Straightforward strategy but some lengthy algebra required)\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\nLandau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\nScaling laws\n(Straightforward.)Using the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\\]\nHence derive the scaling laws among the critical exponents:\n\\[\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\\]\n(Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 9: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\nColloidal diffusion\n\n\n\n\n\n\n\n\n\nEinstein’s expression for the diffusion coefficient\n\n\n\n\n\n\n\n\n\n\n\nLife in one dimension\n\n\n\n\n\n\n\n\nMaster equation\n\n\n\n\n\n\nDetailed balance\n\n\n\n\n\n\n\nJump processes",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "phase-transitions/problems.html#correlation-length",
    "href": "phase-transitions/problems.html#correlation-length",
    "title": "Unifying theoretical concepts: Problems",
    "section": "2. Correlation Length",
    "text": "2. Correlation Length\n(Challenging) For a 1D Ising model, show that the correlation between the spins at sites \\(i\\) and \\(j\\), is\n$$\\langle s_i s_j\\rangle =\\sum_m p_m(-1)^m$$ where $m$ is the number\nof domain walls between $i$ and $j$ and $p_m$ is the probability of\nfinding $m$ domain walls between them.\n\nHence show that when $R_{ij}=|i-j|a$ is large (with $a$ the lattice\nspacing) and the temperature is small, that\n\n$$\\langle s_i s_j\\rangle =\\exp(-R_{ij}/\\xi)$$ with $\\xi=a/2p$ and\n$p$ the probability of finding a domain wall on a bond.\n\n*Hint: In the second part note that $p_m$ is given by a binomial\ndistribution because there is a probability $p$ of each bond\ncontaining a domain wall and $(1-p)$ that it doesn't. What special\ntype of distribution does $p_m$ tend to when $p$ is small (as occurs\nat low $T$)?*\n\n\nA model fluid\n(Straightforward) The van der Waals (vdW) equation of state (recall PH10002) is essentially a mean field theory for fluids. It relates the pressure and the volume of a fluid to the temperature:\n\\[\\left(P+\\frac{a}{V^2}\\right)(V-b)=N_Ak_BT\\] where \\(a\\) and \\(b\\) are constants and \\(N_A\\) is Avogadro’s number.\nThe critical point of a fluid corresponds to the point at which the isothermal compressibility diverges, that is\n\\[\\left(\\frac{\\partial P}{\\partial V}\\right)_T=0\\] Additionally, one finds that isotherms of \\(P\\) versus \\(V\\) exhibit a point of inflection at the critical point, that is\n\\[\\left(\\frac{\\partial^2 P}{\\partial V^2}\\right)_T=0\\]\n\nUse these two requirements to show that the critical point of the vdW fluid is located at\n\\[V_c=3b, ~~~ P_c=\\frac{a}{27b^2},~~~ N_AK_BT_c=\\frac{8a}{27b}\\]\nHence show that when written in terms of reduced variables\n\\[p=\\frac{P}{P_c}, ~~~~ v=\\frac{V}{V_c} ~~~~ t=\\frac{T}{T_c}\\]\nthe equation takes the form\n\\[\\left(p+\\frac{3}{v^2}\\right)(v-\\frac{1}{3})=\\frac{8t}{3}\\]\nUse a graph-plotting program such as “Excel” to plot a selection of isotherms close to the critical temperature (you will need to choose suitable units for your axes). Plot also the gradient and second derivative of P vs V on the critical isotherm and confirm numerically that it exhibits a point of inflection at the critical pressure and temperature.\nObtain the value of the critical exponent \\(\\gamma\\) of the vdW model and confirm that it takes a mean-field value.\n\n\n\n\nMean field theory of the Ising model heat capacity\n(Straightforward) Using results derived in lectures, obtain an expression for the mean energy \\(\\langle E\\rangle\\) of the Ising model in zero field, within the simplest mean field approximation \\(\\langle\n  s_is_j\\rangle=\\langle s_i\\rangle\\langle s_j\\rangle=m^2\\). Hence show that for \\(H=0\\) the heat capacity \\(\\partial \\langle\n  E\\rangle/\\partial T\\) has the behaviour\n\\[\\begin{aligned}\nC_H=& 0 ~~~~ T&gt;T_c\\\\\nC_H=& 3Nk_B/2 ~~~~ T\\le T_c\n\\end{aligned}\\]\n\n\n\nMagnetisation and fluctuations\n(Slightly tricky) A system of spins on a lattice, has, in the absence of an applied field, a Hamiltonian \\({\\cal H}\\). In the presence of a field \\(h\\) the Hamiltonian becomes \\[\\tilde {\\cal H}={\\cal H}-hM\\] where \\(M\\) is the total magnetisation and \\(h\\) is the magnetic field. By considering the partition function \\(Z(T,h)\\) and its relationship to the free energy \\(F\\) show that in general\n\\[\\langle M \\rangle=-\\left(\\frac{\\partial F}{\\partial h}\\right)_T\\]\nShow also that the variance of the magnetisation fluctuations is\n\\[\\langle M^2\\rangle-\\langle M\\rangle^2=-k_BT\\left(\\frac{\\partial^2 F}{\\partial h^2}\\right)_T\\]\n(Hint: This is an important standard derivation found in many text books on Statistical Mechanics. You will need to differentiate \\(F\\) (twice) and use the product and chain rules.)\n\n\n\nSpin-1 Ising model\n(Straightforward) A set of spins on a lattice of coordination number \\(q\\) can take values \\((-1,0,1)\\), as opposed to just \\((-1,1)\\) as in the spin-1/2 Ising model. The Hamiltonian is\n\\[{\\cal H}=-J\\sum_{&lt;ij&gt;}s_is_j + h\\sum_i s_i\\]\nFind the partition function and hence show that in the mean field approximation, the magnetisation per site obeys\n\\[m=\\frac{2\\sinh[\\beta(Jqm+h)]}{2\\cosh[\\beta(Jqm+h)]+1}\\]\nand find the critical temperature \\(T_c\\) at which the net magnetisation vanishes.\n\n\n\nTransfer Matrix.\n(Straightforward strategy but some lengthy algebra required)\nVerify the calculation of the free energy of the 1D periodic chain Ising model in a field outlined in lectures using the Transfer Matrix method.\nUse your results to show that the spontaneous magnetisation is:\n\\[m=\\frac{\\sinh \\beta H}{\\sqrt{\\sinh^2\\beta H+\\exp{-4\\beta J}}}\\] Comment on the value of \\(m\\) in zero field.\n(Hint: Follow the prescription given in lectures. Depending on your approach you may need to use the trigonometrical identities \\(\\cosh^2x-\\sinh^2x=1\\), \\(\\cosh(2x)=2\\cosh^2x-1\\).)\n\n\n\nLandau theory\nCheck and complete the Landau theory calculations, given in lectures, for the critical exponents \\(\\gamma=1\\) and \\(\\alpha=0\\) of the Ising model. For the latter, you should first prove the result\n\\[C_H =-T\\frac{\\partial^2 F}{\\partial T^2}\\] starting from the classical theormodynamics expression for changes in the free energy of a magnet \\(dF=-SdT-MdH\\).\n(Hint: If you get stuck with the proof see standard thermodynamics text books. To get the susceptibility exponent in Landau theory add a term \\(-Hm\\) to the Hamiltonian.)\n\n\n\nScaling laws\n(Straightforward.)Using the generalised homogeneous form for the free energy given in lectures, take appropriate derivatives to find the relationships to the critical exponents:\n\\[\\beta=\\frac{1-b}{a}; ~~ \\gamma=\\frac{2b-1}{a};~~ \\delta= \\frac{b}{1-b}; ~~~ \\alpha=2-\\frac{1}{a}.\\]\nHence derive the scaling laws among the critical exponents:\n\\[\\begin{aligned}\n\\alpha+\\beta(\\delta+1)=2 \\\\\n\\alpha+2\\beta+\\gamma=2\\\\\n%\\gamma=\\beta(\\delta-1)\n\\end{aligned}\\]\n(Hint: For the heat capacity exponent \\(\\alpha\\) use the result from problem 9: \\(C_H=-T\\left(\\frac{\\partial^2F}{\\partial T^2}\\right)_{h=0}\\))\nColloidal diffusion\n\nA large colloidal particle of mass \\(M\\) moves in a fluid under the influence of a random force \\(F(t)\\) and a coefficient of Stokes friction drag \\(\\gamma\\), both per unit mass. If the solution of the corresponding Langevin equation for the velocity of the colloidal particle is given by\n\\[\nu = u_0 e^{-\\gamma t} + e^{-\\gamma t} \\int_0^t dt' \\, e^{\\gamma t'} F(t'),\n\\]\nwhere \\(u_0\\) is the velocity at \\(t = 0\\), show that for long times the velocity of the particle satisfies the relation\n\\[\n\\langle u^2 \\rangle = \\frac{kT}{M} + \\left( u_0^2 - \\frac{kT}{M} \\right) e^{-2\\gamma t},\n\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the absolute temperature.\nState clearly any assumptions that you make.\n\n\nEinstein’s expression for the diffusion coefficient\n\nIn 1905, Einstein showed that the friction coefficient \\(\\gamma\\) (per unit mass) of a colloidal particle must be related to the diffusion coefficient \\(D\\) of the particle by\n\\[\nD = \\frac{kT}{\\gamma}.\n\\]\nIf a marked particle covers a distance \\(X\\) in a given time \\(t\\) (assuming a one-dimensional random walk), the diffusion coefficient is defined to be\n\\[\nD = \\lim_{t \\to \\infty} \\frac{1}{2t} \\langle \\{ X(t) - X(0) \\}^2 \\rangle,\n\\]\nwhere the average \\(\\langle \\cdot \\rangle\\) is taken over an ensemble in thermal equilibrium.\nShow that the Einstein relation may be written as\n\\[\n\\gamma = \\frac{1}{\\mu} = \\frac{D}{kT} = \\frac{1}{kT} \\int_0^\\infty \\langle u(t_0) u(t_0 + t) \\rangle \\, dt,\n\\]\nwhere \\(\\mu\\) is known as the mobility of the particle and \\(t_0\\) is any arbitrarily chosen time.\n\n\nLife in one dimension\n\nA particle lives on the sites of a one-dimensional lattice. At any instant it has probability \\(\\alpha\\) per unit time that it will hop to the site on its right and probability \\(\\alpha\\) per unit time of hopping to the site on its left.\nWrite down the master equation for the set of probabilities \\(p_n(t)\\) of finding the particle at the \\(n^{\\text{th}}\\) site, where \\(-\\infty &lt; n &lt; \\infty\\).\nSolve the master equation for the \\(p_n\\), subject to the initial condition that the particle was at the site \\(n = 0\\) at time \\(t = 0\\). Hence obtain the mean position \\(\\langle n \\rangle\\) and root mean square deviation from the mean, both as functions of time.\nHint: The second part of the question is most easily done by introducing the generating function\n\\[\nF(z, t) = \\sum_{n=-\\infty}^{\\infty} p_n(t) z^n.\n\\]\n\n\nMaster equation\n\nA system of \\(N\\) atoms, each having two energy levels \\(E = \\pm \\epsilon\\), is brought into contact with a heat bath at temperature \\(T\\). The atoms do not interact with each other, but each atom interacts with the heat bath to have a probability \\(\\lambda_{-\\to+}(T)\\) per unit time of transition from lower to higher level, and a probability \\(\\lambda_{+\\to-}(T)\\) per unit time of the reverse transition.\nIf at any time \\(t\\) there are \\(n_+(t)\\) atoms at the higher level and \\(n_-(t)\\) at the lower level, then \\(n(t) = n_-(t) - n_+(t)\\) is a convenient measure of the non-equilibrium state.\nObtain the master equation for \\(n(t)\\) and hence the relaxation time \\(\\tau\\) which characterizes the exponential approach of the system to equilibrium.\n\n\nDetailed balance\n\n(a) Starting from the principle of detailed balance for an isolated system, show that for two groups of states within it, \\(A\\) and \\(B\\), the overall rate of transitions from group \\(A\\) to group \\(B\\) is balanced, in equilibrium, by those from \\(B\\) to \\(A\\):\n\\[\n\\lambda_{A \\to B} p^{\\text{eq}}_A = \\lambda_{B \\to A} p^{\\text{eq}}_B\n\\]\n(b) Deduce that the principle applies to microstates in the canonical ensemble, and hence that the jump rates between states of a subsystem (of fixed number of particles) connected to a heat bath must obey\n\\[\n\\frac{\\lambda_{i \\to j}}{\\lambda_{j \\to i}} = e^{-(E_j - E_i)/kT}.\n\\]\n\n\nJump processes\n\nAn isolated system can occupy three possible states of the same energy. The kinetics are such that it can jump from state 1 to 2 and 2 to 3 but not directly from 1 to 3. Per unit time, there is a probability \\(\\lambda_0\\) that the system makes a jump, from the state it is in, into (each of) the other state(s) it can reach.\n(a) Show that the occupancy probabilities \\(p = (p_1, p_2, p_3)\\) of the three states obey the master equation\n\\[\n\\dot{p} = M \\cdot p\n\\]\nwhere the transition matrix is\n\\[\nM = \\lambda_0 \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n1 & -2 & 1 \\\\\n0 & 1 & -1\n\\end{bmatrix}\n\\]\n(b) Confirm that an equilibrium state is \\(p = (1, 1, 1)/3\\).\n(c) Prove this equilibrium state is unique.\nHint: For part (c), consider the eigenvalues of \\(M\\).",
    "crumbs": [
      "Unifying concepts",
      "Problems"
    ]
  },
  {
    "objectID": "reading.html",
    "href": "reading.html",
    "title": "Recommended texts and literature",
    "section": "",
    "text": "One motivation for supplying you with detailed notes for this course course is the absence of a wholly ideal text book. However, it should be stressed that while these notes approach (in places) the detail of a book, the notes are not fully comprehensive and should be regarded as the ‘bare bones’ of the course, to be fleshed out via your own reading and supplementary note taking.\n\nRevision on thermodynamics and statistical mechanics\n\nF. Mandl: Statistical Physics.\n\n\n\nPhase transitions and critical phenomena\nA good book at the right level for the phase transitions and critical phenomena part of the course is\n\nJ.M. Yeomans: Statistical Mechanics of Phase Transitions\n\nA good book covering all aspects of this part of the course including non-equilibrium systems is\n\nD. Chandler: Introduction to Modern Statistical Mechanics\n\nYou might also wish to dip into the introductory chapters of the following more advanced texts\n\nN Goldenfeld: Lectures on Phase Transitions and the Renormalization Group\nJ.J. Binney, N.J. Dowrick, A.J.Fisher and M.E.J. Newman: The Theory of Critical Phenomena\n\n\n\nStochastic dynamics\n\nN.G. van Kampen: Stochastic processess in Physics and Chemistry\n\n\n\nComplex disordered matter\nThe best overall text for part 2 of the course is: R.A.L Jones, Soft Condensed Matter.\nAdditionally, the following more specialised texts should also be useful. They can be found in the University Library.\n\nColloids\n\nD.F.Evans, H.Wennerström: The Colloidal Domain - Where Physics, Chemistry, Biology, and Technology Meet\nR.J.Hunter: Introduction to Modern Colloid Science\nW.B.Russel, D.A.Saville, W.R.Schowalter: Colloidal Dispersions\nD.H.Everett: Basic Principles of Colloid Science\n\n\n\nPolymers and surfactants\nR.J. Young and P.A. Lovell: Introduction to polymers\nM. Doi: Introduction to polymer physics\nJ.Israelachvili, Intermolecular and Surface Forces\n\n\nGlasses\nJ. Zarzycki; Glasses and the vitreous state",
    "crumbs": [
      "Recommended texts"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#sec-energy-landscapes",
    "href": "soft-matter/soft-matter_glasses.html#sec-energy-landscapes",
    "title": "19  Arrested states",
    "section": "",
    "text": "The free energy landscape for supercooled liquids, adapted from Royall et al. (2018).",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#references",
    "href": "soft-matter/soft-matter_glasses.html#references",
    "title": "18  Arrested states",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBouchaud, Jean-Philippe, and Giulio Biroli. 2004. “On the Adam-Gibbs-Kirkpatrick-Thirumalai-Wolynes Scenario for the Viscosity Increase in Glasses.” The Journal of Chemical Physics 121 (15): 7347–54. https://bris.on.worldcat.org/oclc/110636005.\n\n\nChandler, David, and Juan P Garrahan. 2010. “Dynamics on the Way to Forming Glass: Bubbles in Space-Time.” Annual Review of Physical Chemistry 61 (1): 191–217. https://bris.on.worldcat.org/oclc/4761088692.\n\n\nCoslovich, Daniele, Misaki Ozawa, and Walter Kob. 2018. “Dynamic and Thermodynamic Crossover Scenarios in the Kob-Andersen Mixture: Insights from Multi-CPU and Multi-GPU Simulations.” The European Physical Journal E 41: 1–11. https://bris.on.worldcat.org/oclc/7634416384.\n\n\nDebenedetti, Pablo G, and Frank H Stillinger. 2001. “Supercooled Liquids and the Glass Transition.” Nature 410 (6825): 259–67. https://www.nature.com/articles/35065704.\n\n\nDijkstra, Marjolein, Joseph M Brader, and Robert Evans. 1999. “Phase Behaviour and Structure of Model Colloid-Polymer Mixtures.” Journal of Physics: Condensed Matter 11 (50): 10079. https://bris.on.worldcat.org/oclc/4843512718.\n\n\nGriffiths, Samuel, Francesco Turci, and C Patrick Royall. 2017. “Local Structure of Percolating Gels at Very Low Volume Fractions.” The Journal of Chemical Physics 146 (1). https://pubs.aip.org/aip/jcp/article/146/1/014501/313344/Local-structure-of-percolating-gels-at-very-low.\n\n\nHasyim, Muhammad R, and Kranthi K Mandadapu. 2024. “Emergent Facilitation and Glassy Dynamics in Supercooled Liquids.” Proceedings of the National Academy of Sciences 121 (23): e2322592121. https://www.pnas.org/doi/abs/10.1073/pnas.2322592121.\n\n\nRoyall, C Patrick, Francesco Turci, Soichi Tatsumi, John Russo, and Joshua Robinson. 2018. “The Race to the Bottom: Approaching the Ideal Glass?” Journal of Physics: Condensed Matter 30 (36): 363001. https://bris.on.worldcat.org/oclc/1098708850.\n\n\nSausset, F, G Biroli, and J Kurchan. 2010. “Do Solids Flow?” Journal of Statistical Physics 140: 718–27. https://bris.on.worldcat.org/oclc/5649060652.\n\n\nZaccarelli, Emanuela. 2007. “Colloidal Gels: Equilibrium and Non-Equilibrium Routes.” Journal of Physics: Condensed Matter 19 (32): 323101. https://doi.org/10.1088/0953-8984/19/32/323101.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "phase-transitions/nucleation-and-growth.html#sec-cnt",
    "href": "phase-transitions/nucleation-and-growth.html#sec-cnt",
    "title": "8  Dynamics of first order phase transitions: nucleation, growth and spinodal decomposition",
    "section": "8.2 Classical Nucleation Theory: Homogeneous Nucleation",
    "text": "8.2 Classical Nucleation Theory: Homogeneous Nucleation\nWe now present the framework of classical nucleation theory (CNT) for the case of homogeneous nucleation, in which the nucleation of the stable phase occurs spontaneously and uniformly throughout the bulk of the metastable phase, without the aid of impurities, defects, or surfaces.\nLet us consider a droplet of the stable (spin-down) phase of radius \\(R\\) embedded within the metastable (spin-up) background. The total change in free energy \\(\\Delta F(R)\\) associated with forming such a droplet consists of two competing contributions:\n\nBulk free energy gain: The interior of the droplet consists of \\(V \\sim R^d\\) spins aligned with the external field \\(H &lt; 0\\), leading to a volume free energy change\n\\[\n\\Delta F_{\\text{bulk}}(R) = -|\\Delta f| \\, R^d,\n\\]\nwhere \\(|\\Delta f| \\propto |H|\\) is the free energy density difference between the metastable and stable phases, and \\(d\\) is the spatial dimensionality of the system.\nInterfacial free energy cost: The boundary between the two phases has a surface area scaling as \\(R^{d-1}\\), and incurs a free energy cost proportional to the surface tension \\(\\sigma\\):\n\\[\n\\Delta F_{\\text{surface}}(R) = \\sigma \\, S_d \\, R^{d-1},\n\\]\nwhere \\(S_d\\) is a geometrical factor (e.g., \\(S_2 = 2\\pi\\) in 2D and \\(S_3 = 4\\pi\\) in 3D).\n\nThe total free energy change is therefore given by\n\\[\n\\Delta F(R) = \\sigma \\, S_d \\, R^{d-1} - |\\Delta f| \\, V_d \\, R^d,\n\\]\nwhere \\(V_d\\) is another dimension-dependent constant. This expression (Figure 8.1) exhibits a characteristic maximum at a critical droplet radius \\(R_c\\), obtained by extremizing \\(\\Delta F(R)\\) with respect to \\(R\\):\n\\[\n\\frac{d \\Delta F}{dR} = 0 \\quad \\Rightarrow \\quad R_c = \\frac{(d-1)\\sigma S_d}{d |\\Delta f| V_d}.\n\\]\n\n\n\n\n\n\nFigure 8.1: Free energy barrier \\(\\Delta F(r)\\) for nucleation of a spherical droplet as a function of radius \\(R\\) (schematic)\n\n\n\nThe corresponding free energy barrier for nucleation is\n\\[\n\\Delta F_c = \\Delta F(R_c) = \\frac{(d-1)^{d-1}}{d^d} \\cdot \\frac{(S_d)^d \\, \\sigma^d}{(|\\Delta f|)^{d-1} \\, (V_d)^{d-1}}.\n\\]\nThis barrier must be surmounted by thermal fluctuations in order for a critical nucleus to form and grow. The nucleation rate per unit volume is given (in the Arrhenius approximation) by\n\\[\nI \\sim I_0 \\exp\\left( -\\frac{\\Delta F_c}{k_B T} \\right),\n\\]\nwhere \\(I_0\\) is a prefactor determined by microscopic kinetics, and \\(k_B\\) is Boltzmann’s constant.\n\n8.2.1 Interpretation and Scaling Behavior\nSeveral key features emerge from this analysis:\n\nBarrier scaling: The nucleation barrier \\(\\Delta F_c \\sim \\sigma^d / |\\Delta f|^{d-1}\\) diverges as \\(H \\to 0\\), reflecting the increasing stability of the metastable phase near the coexistence point.\nCritical radius: The critical droplet size \\(R_c \\sim \\sigma / |\\Delta f|\\) also diverges as \\(|\\Delta f| \\to 0\\), indicating that larger fluctuations are required to initiate nucleation close to the coexistence line.\nDimensional dependence: Both \\(\\Delta F_c\\) and \\(R_c\\) exhibit strong dependence on the spatial dimension \\(d\\), with nucleation becoming increasingly suppressed in higher dimensions due to the dominance of interfacial cost.\n\nIn summary, homogeneous nucleation in a first-order transition is governed by a delicate balance between surface tension and bulk free energy gain. Only droplets exceeding a critical size can overcome the barrier and initiate a transition. This sets an intrinsic timescale for the dynamics of phase transformation, which can become extremely long near coexistence due to the exponentially small nucleation rate.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Nucleation, growth and spinoidal decomposition</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_glasses.html#viscosity-and-relaxation-times",
    "href": "soft-matter/soft-matter_glasses.html#viscosity-and-relaxation-times",
    "title": "19  Arrested states",
    "section": "19.3 Viscosity and relaxation times",
    "text": "19.3 Viscosity and relaxation times\nThe structural relaxation time extracted from correlation functions such as the intermediate scattering function is proportional to a macroscopic property, the viscosity\n\\[\\tau_\\alpha \\propto \\eta\\]\nand hence the viscosity at the glass transition increases rapidly (diverges) like \\(\\tau_\\alpha\\).\n\n\n\n\n\n\nRelationship between viscosity and structural relaxation time\n\n\n\n\n\nThe proportionality between viscosity \\(\\eta\\) and the structural relaxation time \\(\\tau_\\alpha\\) can be understood using linear response theory and the Green-Kubo relations.\nThe viscosity is given by the Green-Kubo formula as an integral of the stress autocorrelation function: \\[\n\\eta = \\frac{1}{k_B T V} \\int_0^\\infty \\langle \\sigma_{xy}(0) \\sigma_{xy}(t) \\rangle dt\n\\] where \\(\\sigma_{xy}\\) is the off-diagonal component of the stress tensor.\nIn supercooled liquids and glasses, the decay of the stress autocorrelation function is governed by the same slow structural relaxation processes that control \\(\\tau_\\alpha\\). Thus, the integral is dominated by timescales of order \\(\\tau_\\alpha\\), leading to: \\[\n\\eta \\sim G_\\infty \\tau_\\alpha\n\\] where \\(G_\\infty\\) is the instantaneous (high-frequency) shear modulus.\nThis proportionality holds in the regime where the relaxation is dominated by structural rearrangements (i.e., near the glass transition), and is supported by both experiments and simulations. Therefore, the dramatic increase in viscosity as the glass transition is approached directly reflects the growth of the structural relaxation time.\n\n\n\n\n\n\nViscosity as a function of the inverse temperature scaled by the experimental glass transition temperature (Angell plot). One observes an increase of several orders of magnitude between the high temperature and the low temperature regime. Also one can distinguish different classes of glassformers, conventionally termed as strong and fragile. Adapted from Debenedetti and Stillinger (2001).\n\n\nThis dramatic increase in relaxation time is a hallmark of glassy dynamics and underlies the kinetic arrest observed in glasses.\nPresently, we do not have a single unified theory of the glass transition: there is no universal law capable to predict the viscosity (or the relaxation times) of glassy systems from specific micrscopic parameters. To date, the origin of the glass transition remains one of the major unsolved problems condensed matter physics.\nFirst steps are typically phenomenological: glasses are classified as strong or fragile based on how their viscosity (or relaxation time) increases as temperature decreases toward the glass transition temperature \\(T_g\\). We have seen earlier in Section 8.2 that activated process are often governed by what are called Arrhenius laws, with a rate \\(R\\propto \\exp\\left[-\\Delta E/k_B T\\right]\\) where \\(\\Delta E\\) is an energy barrier. Viscosity curves can be fitted by empirical models inspired the activated picture:\n\nStrong glasses (e.g., silica, SiO\\(_2\\)) follow an Arrhenius law : \\[\n  \\eta(T) = \\eta_0 \\exp\\left(\\frac{E_A}{k_B T}\\right)\n  \\] where \\(E_A\\) is an activation energy. The plot of \\(\\log \\eta\\) vs \\(1/T\\) is a straight line.\nFragile glasses (e.g., o-terphenyl, many polymers) show super-Arrhenius behavior, often described by the Vogel-Fulcher-Tammann (VFT) equation: \\[\n  \\eta(T) = \\eta_0 \\exp\\left(\\frac{B}{T - T_0}\\right)\n  \\] where \\(B\\) and \\(T_0\\) are empirical parameters. The plot of \\(\\log \\eta\\) vs \\(1/T\\) is highly curved.\n\nThe distinction is visualized in the so-called Angell plot, where strong glasses show nearly linear behavior, while fragile glasses show a dramatic upturn as \\(T \\to T_g\\).\nThese models are suggestive of a thermodynamic origin of the glass transition. Indeed, it is possible to thinkto think of the emerging barrier \\(E_A\\) and \\(B\\) as the result of some free energy expression from the sampling of the energy landscapes in various basins.\n\n19.3.1 Connection between VFT and configurational entropy: the Adam-Gibbs model\nA key theoretical link between the dramatic slowdown of dynamics (as described by the VFT law) and the underlying thermodynamics is provided by the Adam-Gibbs model, details in Bouchaud and Biroli (2004).\nThis model proposes that the structural relaxation time \\(\\tau_\\alpha\\) is controlled by the configurational entropy \\(S_{\\rm conf}\\), which counts the number of distinct amorphous basins available to the system.\nThe Adam-Gibbs relation reads: \\[\n\\tau_\\alpha(T) = \\tau_0 \\exp\\left(\\frac{A}{T S_{\\rm conf}(T)}\\right)\n\\] where \\(A\\) is a constant. As temperature decreases, \\(S_{\\rm conf}\\) drops, leading to a rapid increase in \\(\\tau_\\alpha\\). If \\(S_{\\rm conf}\\) vanishes at a finite temperature \\(T_K\\) (the so-called Kauzmann temperature), the relaxation time diverges, reproducing the VFT form: \\[\n\\tau_\\alpha(T) \\sim \\exp\\left(\\frac{B}{T - T_0}\\right)\n\\] with \\(T_0 \\approx T_K\\). Thus, the Adam-Gibbs model provides a thermodynamic interpretation of the VFT law, connecting the kinetic slowdown to the loss of configurational entropy as the glass transition is approached.\n\n\n19.3.2 Alternative perspective: Dynamical facilitation and the parabolic law\nWhile thermodynamic models like Adam-Gibbs relate the glass transition to configurational entropy, a radical alternative approach is the dynamical facilitation theory, see Chandler and Garrahan (2010) for more details. This framework emphasizes that glassy slowdown arises from the dynamics themselves, rather than underlying thermodynamic changes.\nIn dynamical facilitation, mobility is sparse at low temperatures: regions of the system can only relax if they are adjacent to already mobile regions—mobility “facilitates” further mobility. This leads to hierarchical, cooperative dynamics without invoking a thermodynamic singularity.\nA schematic illustration:\n\nAt high \\(T\\), mobile regions are abundant and relaxation is fast.\nAs \\(T\\) decreases, mobile regions become rare, and relaxation requires the creation and propagation of mobility, which is a rare event.\nThe relaxation time grows rapidly due to the need for cooperative rearrangements.\n\nThis scenario predicts a parabolic law for the relaxation time:\n\\[\n\\log \\tau_\\alpha(T) \\sim J^2 \\left( \\frac{1}{T} - \\frac{1}{T_0} \\right)^2\n\\]\nwhere \\(J\\) is an energy scale and \\(T_0\\) is an onset temperature. Unlike the VFT law, the parabolic law does not diverge at finite \\(T\\) but still captures the super-Arrhenius growth of relaxation times.\nInterestingly, both the two perspective fit the viscosity data wall within their regimes of validity (and recent research suggests that close to the glass transition temperature the microscopic mechanisms resemble dynamical facilitation).\nHere below is a the visual representation of the time evolution of the magnitude of the displacement field in a model of glass governed by dynamical facilitation, see Hasyim and Mandadapu (2024) for more details.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Arrested states</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#beyond-thermal-systems",
    "href": "soft-matter/soft-matter_active.html#beyond-thermal-systems",
    "title": "20  Active matter",
    "section": "",
    "text": "motion\nsensing\nprocessing\ngrowth and deformation",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_active.html#footnotes",
    "href": "soft-matter/soft-matter_active.html#footnotes",
    "title": "20  Active matter",
    "section": "",
    "text": "Andrea Cavagna, a complex system physicist, describes “biological surprise” as the extent to which observed behaviour cannot be explained by physical interactions alone.↩︎",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Active matter</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg-effective-coupling.html",
    "href": "phase-transitions/rg-effective-coupling.html",
    "title": "8  The renormalisation group: effective coupling viewpoint",
    "section": "",
    "text": "8.1 A simple example\nLet us begin by returning to our fundamental Equation 2.1, which we rewrite as\n\\[p = Z^{-1}e^{-{\\cal H}}\\] where \\({\\cal H}\\equiv E/k_BT\\).\nThe first step is then to imagine that we generate, by a computer simulation procedure for example, a sequence of configurations with relative probability \\(\\exp(-{\\cal H})\\). We next adopt some coarse-graining procedure which produces from these original configurations a set of coarse-grained configurations. We then ask the question: what is the energy function \\({\\cal H}^\\prime\\) of the coarse-grained variables which would produce these coarse-grained configurations with the correct relative probability \\(\\exp(-{\\cal H}^\\prime)\\)? Clearly the form of \\({\\cal H}^\\prime\\) depends on the form of \\({\\cal H}\\) thus we can write symbolically\n\\[{\\cal H}^\\prime=R({\\cal H})\\]\nThe operation \\(R\\), which defines the coarse-grained configurational energy in terms of the microscopic configurational energy function is known as a renormalisation group transformation (RGT). What it does is to replace a hard problem by a less hard problem. Specifically, suppose that our system is near a critical point and that we wish to calculate its large-distance properties. If we address this task by utilizing the configurational energy and appealing to the basic machinery of statistical mechanics set out in Equation 2.1 and Equation 2.2, the problem is hard. It is hard because the system has fluctuations on all the (many) length scales intermediate between the correlation length \\(\\xi\\) and the minimum length scale \\(L_\\textrm{min}\\).\nHowever, the task may instead be addressed by tackling the coarse-grained system described by the energy \\({\\cal H}^\\prime\\). The large-distance properties of this system are the same as the large-distance properties of the physical system, since coarse-graining operation preserves large-scale configurational structure. In this representation the problem is a little easier: while the \\(\\xi\\) associated with \\({\\cal H}^\\prime\\) is the same as the \\(\\xi\\) associated with \\({\\cal H}\\), the minimum length scale of \\({\\cal H}^\\prime\\) is bigger than that of \\({\\cal H}\\). Thus the statistical mechanics of \\({\\cal H}^\\prime\\) poses a not-quite-so-many-length-scale problem, a problem which is effectively a little less critical and is thus a little easier to solve. The benefits accruing from this procedure may be amplified by repeating it. Repeated application of \\(R\\) will eventually result in a coarse- grained energy function describing configurations in which the \\(\\xi\\) is no bigger than the minimum length scale. The associated system is far from criticality and its properties may be reliably computed by any of a wide variety of approximation schemes. These properties are the desired large-distance properties of the physical system. As explicit reference to fluctuations of a given scale is eliminated by coarse-graining, their effects are carried forward implicitly in the parameters of the coarse-grained energy.\nIn order to setup the framework for a simple illustrative example, let is return to the lattice Ising model for which the energy function depended only on the product of nearest neighbour spins. The coefficient of this product in the energy is the exchange coupling, \\(J\\). In principle, however, other kinds of interactions are also allowed; for example, we may have a product of second neighbour spins with strength \\(J_2\\) or, perhaps, a product of four spins (at sites forming a square whose side is the lattice spacing), with strength \\(J_3\\). Such interactions in a real magnet have their origin in the quantum mechanics of the atoms and electrons and clearly depend upon the details of the system. For generality therefore we will allow a family of exchange couplings \\(J_1\\),\\(J_2\\),\\(J_3,\\dots\\), or \\(J_a, a =\n1,2,\\dots\\) In reduced units, the equivalent coupling strengths are \\(K_a =J_a/k_BT\\). Their values determine uniquely the energy for any given configuration.\nNow consider the coarse-graining procedure. Let us suppose that this procedure takes the form of a ‘majority rule’ operation in which the new spins are assigned values \\(+1\\) or \\(-1\\) according to the signs of the magnetic moments of the blocks with which they are associated. The new energy function \\({\\cal H}^\\prime\\) will be expressible in terms of some new coupling strengths \\(K^\\prime\\) describing the interactions amongst the new spin variables (and thus, in effect, the interactions between blocks of the original spin variables). The RGT simply states that these new couplings depend on the old couplings: \\(K_1^\\prime\\) is some function \\(f_1\\) of all the original couplings, and generally\n\\[K^\\prime_a=f_a(K_1,K_2,\\dots) =f_a({\\bf K}),~~~~ a= 1, 2,\\dots\n\\tag{8.1}\\] where K is shorthand for the set \\(K_1, K_2,\\dots\\)\nThis example illustrates how one can perform the RG transformation Equation 8.1 directly, without recourse to a ‘sequence of typical configurations’. The calculation involves a very crude approximation which has the advantage that it simplifies the subsequent analysis.\nConsider an Ising model in two dimensions, with only nearest neighbour interactions as shown in Figure 8.1. We have divided the spins into two sets, the spins \\(\\{s^\\prime\\}\\) form a square lattice of spacing \\(2\\), the others being denoted by \\(\\{\\tilde{s}\\}\\). One then defines an effective energy function \\({\\cal H^\\prime}\\) for the \\(s^\\prime\\) spins by performing an average over all the possible arrangements of the \\(\\tilde{s}\\) spins\n\\[\n\\exp(-{\\cal H}^\\prime)=\\sum_{\\{\\tilde {s}\\}} \\exp(-{\\cal H}).\n\\tag{8.2}\\]\nThis particular coarse-graining scheme is called ‘decimation’ because a certain fraction (not necessarily one-tenth!) of spins on the lattice is eliminated. This formulation of a new energy function realizes two basic aims of the RG method: the long-distance physics of the ‘original’ system, described by \\({\\cal H}\\), is contained in that of the ‘new’ system, described by \\({\\cal H}^\\prime\\) (indeed the partition functions are the same as one can see by summing both sides over \\(s^\\prime\\)) and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing (‘minimum length scale’) has been reduced by a factor of \\(1/2\\) (the ratio of the lattice spacings of the two systems). We must now face the question of how to perform the configuration sum in Equation 8.2. This cannot in general be done exactly, so we must resort to some approximation scheme. The particular approximation which we invoke is the high temperature series expansion. In its simplest mathematical form, since \\({\\cal H}\\) contains a factor \\(1/k_BT\\), it involves the expansion of \\(\\exp(-{\\cal H})\\) as a power series:\n\\[\\exp(-{\\cal H}/k_BT)=1-{\\cal H}/k_BT +\\frac{1}{2!}({\\cal H}/k_BT)^2+.....\\]\nWe substitute this expansion into the right hand side of Equation 8.2 and proceed to look for terms which depend on the \\(s^\\prime\\) spins after the sum over the possible arrangements of the \\(\\tilde{s}\\) spins is performed. This sum extends over all the possible (\\(\\pm 1\\)) values of all the \\(\\tilde{s}\\) spins. The first term (the 1) in the expansion of the exponential is clearly independent of the values of the \\(s^\\prime\\) spins. The second term (\\({\\cal H}\\)) is a function of the \\(s^\\prime\\) spins, but gives zero when the sum over the \\(s^\\prime\\) spins is performed because only a single factor of any \\(s^\\prime\\) ever appears, and \\(+ 1 - 1 = 0\\). The third term (\\({\\cal H}^2/2\\)) does contribute. If one writes out explicitly the form of \\({\\cal H}^2/2\\) one finds terms of the form \\(K^2s_1^\\prime\\tilde{s}\\tilde{s}s_2^\\prime=K^2s_1^\\prime s_2^\\prime\\), where \\(s_1^\\prime\\) and \\(s_2^\\prime\\) denote two spins at nearest neighbour sites on the lattice of \\(s^\\prime\\) spins and \\(\\tilde{s}\\) is the spin (in the other set) which lies between them. Now, in the corresponding expansion of the left hand side of Equation 8.2, we find terms of the form \\(K^\\prime s_1^\\prime s_2^\\prime\\), where \\(K^\\prime\\) is the nearest neighbour coupling for the \\(s^\\prime\\) spins. We conclude (with a little more thought than we detail here) that\n\\[\nK^\\prime=K^2\n\\tag{8.3}\\]\nOf course many other terms and couplings are generated by the higher orders of the high temperature expansion and it is necssary to include these if one wishes reliable values for the critical temperature and exponents, However, our aim here is to use this simple calculation to illustrate the RG method. Let us therefore close our eyes, forget about the higher order terms and show how the RGT Equation 8.3 can be used to obtain information on the phase transition.\nThe first point to note is that that mathematically Equation 8.3 has the fixed point \\(K^*= 1\\); if \\(K= 1\\) then the new effective coupling \\(K^\\prime\\) has the same value \\(1\\). Further, if \\(K\\) is just larger than \\(1\\), then \\(K^\\prime\\) is larger than \\(K\\), i.e. further away from \\(1\\). Similarly, if \\(K\\) is less than \\(1\\), \\(K^\\prime\\) is less than \\(K\\). We say that the fixed point is unstable: the flow of couplings under repeated iteration of Equation 8.3 is away from the fixed point, as illustrated in Figure 8.2. The physical significance of this is as follows: suppose that the original system is at its critical point so that the ratio of \\(\\xi\\) to lattice spacing is infinite. After one application of the decimation transformation, the effective lattice spacing has increased by a factor of two, but this ratio remains infinite; the new system is therefore also at its critical point. Within the approximations inherent in Equation 8.3, the original system is an Ising model with nearest neighbour coupling \\(K\\) and the new system is an Ising model with nearest neighbour coupling \\(K^\\prime\\). If these two systems are going to be at a common critically, we must identify \\(K^\\prime=\nK\\). The fixed point \\(K^*= 1\\) is therefore a candidate for the critical point \\(K_c\\), where the phase transition occurs. This interpretation is reinforced by considering the case where the original system is close to, but not at, criticality. Then \\(\\xi\\) is finite and the new system is further from critically because the ratio of \\(\\xi\\) to lattice spacing is reduced by a factor of two. This instability of a fixed point to deviations of \\(K\\) from \\(K^*\\) is a further necessary condition for its interpretation as a critical point of the system. In summary then we make the prediction\n\\[\nK_c=J/k_BT_c=1\n\\tag{8.4}\\]\nWe can obtain further information about the behaviour of the system close to its critical point. In order to do so, we rewrite the transformation (Equation 8.3) in terms of the deviation of the coupling from its fixed point value. A Taylor expansion of the function \\(K^\\prime=K^2\\) yields \\[\n\\begin{aligned}\nK^\\prime =& (K^*)^2 +(K-K^*)\\left.\\frac{\\partial K^\\prime}{\\partial K}\\right|_{K=K^*}+\\frac{1}{2}(K-K^*)^2\\left.\\frac{\\partial^2 K^\\prime}{\\partial K^2}\\right|_{K=K^*}+\\ldots\\nonumber\\\\\nK^\\prime - K^* =& 2 (K - K^*)+ (K - K^*)^2\n\\end{aligned}\n\\]\nwhere in the second line we have used the fact that the first derivative evaluates to \\(2K^*=2\\) and \\((K^*)^2=K^*\\).\nFor a system sufficiently close to its critical temperature the final term can be neglected. The deviation of the coupling from its fixed point (critical) value is thus bigger for the new system than it is for the old by a factor of two. This means that the reduced temperature is also bigger by a factor of two:\n\\[t^\\prime= 2t\\]\nBut \\(\\xi\\) (in units of the appropriate lattice spacing) is smaller by a factor of \\(1/2\\):\n\\[\\xi^\\prime= \\xi/2\\]\nThus, when we double \\(t\\), we halve \\(\\xi\\), implying that\n\\[\\xi\\propto t^{-1}\\]\nfor \\(T\\) close to \\(T_c\\). Thus we see that the RGT predicts scaling behaviour with calculable critical exponents. In this simple calculation we estimate the critical exponent \\(\\nu=1\\) for the square lattice Ising model. This prediction is actually in agreement with the exactly established value. The agreement is fortuitous- the prediction in Eq. refeq:Kc for \\(K_c\\), is larger than the exactly established value by a factor of more than two. In order to obtain reliable estimates more sophisticated and systematic methods must be used.\nThe crude approximation in the calculation above produced a transformation, Equation 8.3, involving only the nearest neighbour coupling, with the subsequent advantages of simple algebra. We pay a penalty for this simplicity in two ways: the results obtained for critical properties are in rather poor agreement with accepted values, and we gain no insight into the origin of universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>RG- the effective coupling viewpoint</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg-effective-coupling.html#a-simple-example",
    "href": "phase-transitions/rg-effective-coupling.html#a-simple-example",
    "title": "8  The renormalisation group: effective coupling viewpoint",
    "section": "",
    "text": "Figure 8.1: Coarse graining by decimation. The spins on the original lattice are divided into two sets \\(\\{s^\\prime\\}\\) and \\(\\{\\tilde{s}\\}\\). The \\(\\{s^\\prime\\}\\) spins occupy a lattice whose spacing is twice that of the original. The effective coupling interaction between the \\(\\{s^\\prime\\}\\) spins is obtained by performing the configurational average over the \\(\\{\\tilde{s}\\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Coupling flow under the decimation transformation described in the text.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>RG- the effective coupling viewpoint</span>"
    ]
  },
  {
    "objectID": "phase-transitions/rg-effective-coupling.html#universality-and-scaling",
    "href": "phase-transitions/rg-effective-coupling.html#universality-and-scaling",
    "title": "8  The renormalisation group: effective coupling viewpoint",
    "section": "8.2 Universality and scaling",
    "text": "8.2 Universality and scaling\nIn order to expose how universality can arise, we should from the start allow for several different kinds of coupling \\(J_a\\), and show how the systems with different \\(J_a\\) can have the same critical behaviour.\n\n\n\n\n\n\nFigure 8.3: General flow in coupling space\n\n\n\nFigure 8.3 is a representation of the space of all coupling strengths \\(K_a\\) in the energy function \\({\\cal H}/k_BT\\). This is of course actually a space of infinite dimension, but representing three of these, as we have done, enables us to illustrate all the important aspects. First let us be clear what the points in this space represent. Suppose we have some magnetic material which is described by a given set of exchange constants \\(J_1,J_2,J_3.....\\) As the temperature \\(T\\) varies, the coupling strengths \\(K_a=J_a/k_BT\\) trace out a straight line, or ray, from the origin of the space in the direction (\\(J_1,J_2,J_3 ....\\) ). Points on this ray close to the origin represent this magnet at high temperatures, and conversely points far from the origin represent the magnet at low temperatures. The critical point of the magnet is represented by a specific point on this ray, \\(K_a=\nJ_a/k_BT, a= 1,2,\\dots\\) The set of critical points on all of the possible rays forms a surface, the critical surface. Formally, it is defined by the set of all possible models (of the Ising type) which have infinite \\(\\xi\\). It is shown schematically as the shaded surface in Figure 8.3. (In the figure it is a two-dimensional surface; more generally it has one dimension less than the full coupling constant space, dividing all models into high and low temperature phases.)\nOur immediate goal then is to understand how the RGT can explain why different physical systems near this critical surface have the same behaviour. Let us turn now to the schematic representation of the RG flow in Figure 8.3. Suppose we start with a physical system, with coupling strengths \\(K_a,  a= 1,2, \\dots\\). What the RGT does is generate a new point in the figure, at the coupling strengths \\(K_a^{(1)}=f_a({\\bf K})\\); these are the couplings appearing in the effective energy function describing the coarse-grained system. If we repeat the transformation, the new energy function has coupling strengths \\(K_a^{(2)}=f_a({\\bf K})\\). Thus repeated application of the transformation generates a flow of points in the figure: \\({\\bf K\\to\nK^{(1)}\\to\\dots\\to K^{(n)}}\\) where the superscript (\\(n\\)) labels the effective couplings after \\(n\\) coarse-graining steps. if the change in coarse-graining scale is \\(b\\) (\\(&gt; 1\\)) at each step, the total change in coarse-graining scale is \\(b^n\\) after \\(n\\) steps. In the process, therefore, the ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor of \\(b^{-n}\\). The dots in Figure 8.3 identify three lines of RG flow starting from three systems differing only in their temperature. (The flow lines are schematic but display the essential features revealed in detailed calculations.)\nConsider first the red dots which start from the nearest neighbour Ising model at its critical point. The ratio of \\(\\xi\\) to coarse-graining scale is reduced by a factor b at each step, but, since it starts infinite, it remains infinite after any finite number of steps. In this case we can in principle generate an unbounded number of dots, \\({\\bf K^{(1)}, K^{(2)},\\dots,K^{(n)}}\\), all of which lie in the critical surface. The simplest behaviour of such a sequence as \\(n\\) increases is to tend to a limit, \\(K^*\\), say. In such a case\n\\[K^*_a=f_a(K^*)~~~~ a= 1,2 .....\\]\nThis point \\({\\bf K^*} \\equiv K_1^*, K_2^*, \\dots\\) is therefore a fixed point which lies in the critical surface.\nBy contrast, consider the same magnet as before, now at temperature \\(T\\) just greater than \\(T_c\\), its couplings \\(K_a\\), will be close to the first red dot (in fact they will be slightly smaller) and so will the effective couplings \\(K_a^{(1)},K_2^{(2)},\\dots\\) of the corresponding coarse-grained systems. The new flow will therefore appear initially to follow the red dots towards the same fixed point. However, the flow must eventually move away from the fixed point because each coarse-graining now produces a model further from criticality. The resulting flow is represented schematically by one set of black dots. The other set of black dots shows the expected flow starting from the same magnet slightly below its critical temperature.\nWe are now in a position to understand both universality and scaling within this framework. We will suppose that there exists a single fixed point in the critical surface which sucks in all flows starting from a point in that surface. Then any system at its critical point will exhibit large-length scale physics (large-block spin behaviour) described by the single set of fixed point coupling constants. The uniqueness of this limiting set of coupling constants is the essence of critical point universality. It is, of course, the algebraic counterpart of the unique limiting spectrum of coarse-grained configurations, discussed in Section 8.5. Similarly the scale-invariance of the critical point configuration spectrum (viewed on large enough length scales) is expressed in the invariance of the couplings under iteration of the transformation (after a number of iterations large enough to secure convergence to the fixed point).\nTo understand the behaviour of systems near but not precisely at critically we must make a further assumption (again widely justified by explicit studies). The flow line stemming from any such system will, we have argued, be borne towards the fixed point before ultimately deviating from it after a number of iterations large enough to expose the system’s noncritical character. We assume that (as indicated schematically in the streams of red and blue lines in Figure 8.3 the deviations lie along a single line through the fixed point, the direction followed along this line differing according to the sign of the temperature deviation \\(T-T_c\\). Since any two sets of coupling constants on the line (on the same side of the fixed point) are related by a suitable coarse-graining operation, this picture implies that the large-length-scale physics of all near- critical systems differs only in the matter of a length scale. This is the essence of near-critical point universality.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>RG- the effective coupling viewpoint</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#equipartition",
    "href": "soft-matter/soft-matter_problems.html#equipartition",
    "title": "21  Problems",
    "section": "21.2 Equipartition",
    "text": "21.2 Equipartition\nThe equipartition theorem states that in thermal equilibrium each generalised co-ordinate which occurs in the total system energy only as a quadratic term contributes \\((1 / 2) \\mathrm{k}_{\\mathrm{B}} \\mathrm{T}\\) to the mean energy of the system. Use the theorem to estimate the typical time it rakes atoms (or small molecules) in an atomic liquid to move distances of the order of their own size, i.e. estimate the relaxation time.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#gravitational-length",
    "href": "soft-matter/soft-matter_problems.html#gravitational-length",
    "title": "21  Problems",
    "section": "21.3 Gravitational length",
    "text": "21.3 Gravitational length\n\nFind the typical length scale (‘gravitational length’) of the distribution of polystyrene particles (density \\(\\rho_{\\mathrm{p}}=1.05 \\times 10^{3} \\mathrm{~kg} \\mathrm{~m}^{-3}\\) ) of radius 50 nm and \\(1 \\mu \\mathrm{~m}\\) respectively, suspended in water (density \\(\\rho_{\\mathrm{w}}=1.00 \\times 10^{3} \\mathrm{~kg} \\mathrm{~m}^{-3}\\), viscosity \\(1.0 \\times 10^{-3} \\mathrm{~Pa} \\mathrm{~s}\\) ) at room temperature. Hint: The gravitational length is the height at which the concentration falls to \\(1 / \\mathrm{e}\\) of its greatest value.\nIf a container of height \\(\\mathrm{H}=4 \\mathrm{~cm}\\) contains a dilute suspension of these particles at an average concentration of \\(n_{a}\\) particles per unit volume, what are the actual concentrations (in terms of \\(n_{a}\\) ) at the top and bottom of the sample when sedimentation equilibrium has been reached.\nIf the above sample is shaken up to give an initially uniform concentration and then left undisturbed, estimate how long it will take the concentration profile to reach its equilibrium state (A surprisingly long time!)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#peclet-number",
    "href": "soft-matter/soft-matter_problems.html#peclet-number",
    "title": "21  Problems",
    "section": "21.4 Peclet number",
    "text": "21.4 Peclet number\nA definition of a colloid is a particle small enough that its Brownian motion is not dominated by gravity. Thus if the particle is raised a distance equal to its radius, the increase in its gravitational potential energy must be less than the thermal energy \\((3 / 2) \\mathrm{k}_{\\mathrm{B}} T\\). Another measure of the relative importance of the effects of Brownian motion and gravity, is the Peclet number \\(\\mathrm{Pe}=\\tau_{R} / \\tau_{\\text {sed }}\\). Here \\(\\tau_{\\mathrm{R}}\\) is the time taken by a particle to diffuse a distance equal to its radius, and \\(\\tau_{\\text {sed }}\\) is the time taken by it to sediment the same distance. When \\(\\mathrm{Pe} \\ll 1\\), Brownian motion dominates; when \\(\\mathrm{Pe} \\gg 1\\), gravity dominates. Show that the condition \\(\\mathrm{Pe}&lt;1\\) is more or less equivalent to the definition of a colloid given above.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#diffusion-equation",
    "href": "soft-matter/soft-matter_problems.html#diffusion-equation",
    "title": "21  Problems",
    "section": "21.5 Diffusion equation",
    "text": "21.5 Diffusion equation\nProve by direct substitution that \\(n(x, t)=C(4 \\pi D t)^{-1 / 2} \\exp \\left(-x^{2} / 4 D t\\right)\\) is indeed a solution to the 1 D diffusion equation \\(\\partial n / \\partial t=D^{\\partial^{2} n} / \\partial x^{2}\\). Discuss the meaning of the constant C .",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#mean-square-displacement",
    "href": "soft-matter/soft-matter_problems.html#mean-square-displacement",
    "title": "21  Problems",
    "section": "21.6 Mean square displacement",
    "text": "21.6 Mean square displacement\nStarting from the diffusion equation, deduce the mean square displacement, \\(\\left\\langle x^{2}\\right\\rangle=2 D t\\), without using the full solution to the diffusion equation. Note that \\(n(x, t)\\) is the number density, from which the probability density of finding a particle in the region \\([\\mathrm{x}, \\mathrm{x}+\\mathrm{dx}]\\) during the time interval \\([\\mathrm{t}, \\mathrm{t}+\\mathrm{dt}]\\), can be obtained by normalising with \\(\\int n(x, t) d x=N\\) (the total number of particles). Having this in mind, the mean square displacement is \\(\\left\\langle x^{2}(t)\\right\\rangle=\\frac{\\int_{-\\infty}^{\\infty} x^{2} n(x, t) d x}{\\int_{-\\infty}^{\\infty} n(x, t) d x}=\\frac{1}{N} \\int_{-\\infty}^{\\infty} x^{2} n(x, t) d x\\) Hint: Multiply both sides of the 1D diffusion equation by \\(x^{2}\\) and integrate by parts over \\(x\\) to get an o.d.e for \\(\\left\\langle x^{2}\\right\\rangle\\) with t as the independent variable.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#relaxation-time-in-colloidal-suspensions",
    "href": "soft-matter/soft-matter_problems.html#relaxation-time-in-colloidal-suspensions",
    "title": "21  Problems",
    "section": "21.7 Relaxation time in colloidal suspensions",
    "text": "21.7 Relaxation time in colloidal suspensions\nUse \\(\\left\\langle\\underline{r}^{2}\\right\\rangle=6 D t\\) and the Stokes-Einstein relation to show that the time \\(\\tau_{\\mathrm{R}}\\) for a particle to diffuse its own radius \\(R\\), scales as \\(R^{3}\\). The relaxation time \\(\\tau_{R}\\) is a good estimate for the time taken to return to equilibrium after a disturbance. Estimate \\(\\tau_{\\mathrm{R}}\\) for a suspension of spheres of radius \\(\\mathrm{R} \\approx 1 \\mu \\mathrm{~m}\\), i.e. at the upper limit of the colloidal size range.",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#colloidal-crystals",
    "href": "soft-matter/soft-matter_problems.html#colloidal-crystals",
    "title": "21  Problems",
    "section": "21.8 Colloidal crystals",
    "text": "21.8 Colloidal crystals\nColloidal crystals are observed for volume fractions \\(\\phi\\) between 0.545 and 0.74 . Show that the upper limit of \\(\\phi=0.74\\) corresponds to the closest packing in a fcc crystal. What is the average distance between the colloidal particles for the lower limit \\(\\phi=0.545\\) ? Hint: to get the average consider a simple cubic lattice of the same volume fraction",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#lagranges-expression-for-the-radius-of-gyration",
    "href": "soft-matter/soft-matter_problems.html#lagranges-expression-for-the-radius-of-gyration",
    "title": "21  Problems",
    "section": "21.9 Lagrange’s expression for the radius of gyration",
    "text": "21.9 Lagrange’s expression for the radius of gyration\nFor N equal point masses at positions \\(\\left\\{\\underline{R}_{j}\\right\\}\\), the radius of gyration is defined by \\(R_{g}^{2}=\\frac{1}{N} \\sum_{j=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{G}\\right)^{2}\\) where \\(\\underline{R}_{G}=\\frac{1}{N} \\sum_{j=1}^{N} \\underline{R}_{j}\\) is the position of the centre of mass. Show that\n\\[\nR_{g}^{2}=\\frac{1}{2 N^{2}} \\sum_{j=1}^{N} \\sum_{k=1}^{N}\\left(\\underline{R}_{j}-\\underline{R}_{k}\\right)^{2}\n\\]\nHint: Note that \\(\\frac{1}{N} \\sum_{j=1}^{N} 1=1\\)",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#mean-length",
    "href": "soft-matter/soft-matter_problems.html#mean-length",
    "title": "21  Problems",
    "section": "21.10 Mean length",
    "text": "21.10 Mean length\nCalculate the mean end-to end length of a freely jointed polymer consisting of \\(10^{5}\\) monomers of length \\(4.322 \\AA\\). How does the end-to-end length change if the valence angle of the chain is fixed at \\(108^{\\circ}\\) ?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#real-data",
    "href": "soft-matter/soft-matter_problems.html#real-data",
    "title": "21  Problems",
    "section": "21.11 Real data",
    "text": "21.11 Real data\nLight scattering measurements on dilute solutions of polystyrene, of different molar masses M , at the theta temperature give the following results for the mean square radius of gyration \\(\\left\\langle R_{g}^{2}\\right\\rangle\\) :\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{M}\\left(10^{6} \\mathrm{~g} \\mathrm{~mol}^{-1}\\right)\\)\n4.04\n1.56\n1.20\n1.06\n0.626\n0.394\n\n\n\n\n\\(&lt;R_{g}^{2}&gt;\\left(\\mathrm{nm}^{2}\\right)\\)\n3260\n1210\n928\n770\n484\n305\n\n\n\n\nShow that these results are consistent with \\(\\left\\langle R_{g}^{2}\\right\\rangle^{1 / 2}=B M^{1 / 2} \\quad\\left(M=N M_{M}\\right.\\) where \\(N\\) is the number of monomers in the chain and \\(M_{M}\\) is the molar mass of one monomer; \\(M_{M}=100 \\mathrm{~g} \\mathrm{~mol}^{-1}\\) ) and estimate the value of \\(B\\).\nGiven that the ‘size’ of a styrene monomer is about \\(\\mathrm{b}_{0}=0.23 \\mathrm{~nm}\\), calculate the factor \\(C\\) which represents the effect of restricted bond angles in \\(\\left\\langle R^{2}\\right\\rangle=C N b_{0}^{2}\\). From \\(C\\), calculate the bond angle \\(\\theta\\) which would apply if polystyrene can be represented by a freely-rotating chain. [Remember \\(\\left.\\left\\langle R_{g}^{2}\\right\\rangle=\\frac{1}{6}&lt;R^{2}\\right\\rangle\\) ]\nCalculate \\(\\left.&lt;R_{g}^{2}\\right\\rangle^{1 / 2}\\) for a polystyrene chain of molar mass \\(1 \\times 10^{7} \\mathrm{~g} \\mathrm{~mol}^{-1}\\) at the theta temperature.\nFor the chain of (c), estimate the fraction of the volume \\(\\left.(4 \\pi / 3)&lt;R_{g}^{2}\\right\\rangle^{3 / 2}\\) which is actually occupied by styrene monomers (assume that the density of styrene when polymerised is about \\(1 \\mathrm{~g} \\mathrm{~cm}^{-3}\\) ). What is the overlap concentration \\(c^{*}\\) (in mass per volume)? How do you expect \\(c^{*}\\) to change upon an increase (decrease) in temperature?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#micelles",
    "href": "soft-matter/soft-matter_problems.html#micelles",
    "title": "21  Problems",
    "section": "21.12 Micelles",
    "text": "21.12 Micelles\nThe volume of a linear hydrocarbon chain with \\(n\\) carbon atoms is given by \\(v=(27.4+26.9 n) \\times 10^{-3} \\mathrm{~nm}^{3}\\), and its critical chain length is \\(l_{c}=(0.154+0.1265 n)\\) \\(n m\\). An amphiphile has an anionic head group with an optimum head group area in aqueous solution of \\(a_{0}=0.65 \\mathrm{~nm}^{2}\\). (i) What shape micelles are formed by amphiphiles with linear hydrocarbon chains having \\(\\mathrm{n}=10\\) ? (ii) What is the average size and aggregation number of each micelle?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#viscosity",
    "href": "soft-matter/soft-matter_problems.html#viscosity",
    "title": "21  Problems",
    "section": "21.13 Viscosity",
    "text": "21.13 Viscosity\nFor polystyrene, the variation of viscosity with temperature follows the Vogel-Fulcher law \\(B=710\\) and \\(T_{0}=50^{\\circ} \\mathrm{C}\\). Plot the function \\(\\eta / \\eta_{0}\\) in the temperature range \\(80-150^{\\circ} \\mathrm{C}\\). By what factor does the viscosity and relaxation time vary between the temperatures of \\(100-140^{\\circ} \\mathrm{C}\\) ?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "soft-matter/soft-matter_problems.html#the-glass-transition",
    "href": "soft-matter/soft-matter_problems.html#the-glass-transition",
    "title": "21  Problems",
    "section": "21.14 The glass transition",
    "text": "21.14 The glass transition\nFor polystyrene, a relaxation time associated with configurational rearrangements, \\(\\tau_{\\text {config, }}\\), follows a Vogel-Fulcher law,\n\\[\n\\tau_{\\text {confg }}=\\tau_{0} \\exp \\left(\\frac{B}{T-T_{0}}\\right)\n\\]\nWhere \\(\\tau_{0}, \\mathrm{~B}=710\\) and \\(T_{0}=50^{\\circ} \\mathrm{C}\\) are constants. A value of the experimental glass transition temperature is measured with an experiment carried out at an effective timescale \\(\\tau_{\\text {exp }}=1000\\) s and found to be \\(101.4^{\\circ} \\mathrm{C}\\). (a) Another experiment is carried out at an effective timescale of \\(10^{5} \\mathrm{~s}\\). What is the value of the glass transition temperature obtained from this experiment? (b) On what timescale must an experiment be carried out if it is to measure a glass transition temperature within \\(10^{\\circ} \\mathrm{C}\\) of the Vogel-Fulcher temperature \\(T_{0}\\). Is this practically possible?",
    "crumbs": [
      "Complex disordered systems",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Problems</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html",
    "href": "phase-transitions/lattice-gas.html",
    "title": "6  The Lattice Gas model",
    "section": "",
    "text": "6.1 Mapping between Ising model and lattice gas\nA crude representation of a fluid is the lattice gas model. Here particles can occupy the sites of a hypercubic lattice. The occupancy of a site \\(i\\) is specified by the variable \\(c_i=1\\) (occupied) or \\(c_i=0\\) vacant. The complete list of these occupancies \\(\\{c\\}\\) specifies a microstate. The average particle number density (fraction of occupied sites) is given by\n\\[c=L^{-d}\\sum_i c_i \\]\nwhere \\(L\\) is the linear extent of the lattice and \\(d\\) its dimensionality.\nThe Hamiltonian of the lattice gas model is\n\\[{\\cal H}_{LG}=-\\epsilon\\sum_{&lt;i,j&gt;}c_ic_j - \\mu\\sum_ic_i\\]\nwhere \\(\\epsilon\\) is an attraction energy between a pair of particles on adjacent (nearest neighbouring) sites and \\(\\mu\\) is a field known as the chemical potential, which couples to the particle density which is assumed to fluctuate around a mean value controlled by the prescribed chemical potential. This representation of the model in which the overall density fluctuates is known as the Grand Canonical ensemble.\nThe lattice gas model is interesting because whilst being a plausible model for a fluid, it maps onto the Ising model. This extends the applicability of the Ising model. To expose the mapping we write the grand partition function of the lattice gas:\n\\[ \\Xi=\\sum_\\textrm{ state}\\exp-\\beta{\\cal H}_{LG}=\\sum_{\\{c\\}}\\exp\\left[\\beta \\epsilon\\sum_{&lt;i,j&gt;}c_ic_j +\\beta\\mu\\sum_ic_i\\right] \\] where the sum is an unrestricted sum over the occupancies of the lattice sites. We now change variables to\n\\[c_i=(1+s_i)/2; ~~~~ J=\\frac{\\epsilon}{4} ~~~~\nh=\\frac{\\epsilon q+2\\mu}{4}\\] Hence\n\\[{\\cal H}_{LG}={\\cal H}_\\textrm{ I} + \\textrm{ constant}\\] Since the last term does not depend on the configuration, it feeds through as an additive constant in the free energy; and since all observables feature as derivatives of the free energy, the constant has no physical implications.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html#phase-diagram",
    "href": "phase-transitions/lattice-gas.html#phase-diagram",
    "title": "6  The Lattice Gas model",
    "section": "6.2 Phase diagram",
    "text": "6.2 Phase diagram\nUsing these translation rules we can plot the phase diagram of the lattice gas in the number density-temperature plane.\n\n\n\n\n\n\nFigure 6.1: Phase diagram of the lattice gas model in the density-temperature plane.\n\n\n\nIn the \\(\\mu-T\\) plane there is a line of first order phase transitions terminating at a critical point. The first order line means that if \\(T&lt;T_c\\) we smoothly increase the chemical potential through the coexistence value of \\(\\mu\\), the density of particles on our lattice \\(\\rho=N/L^d\\) jumps discontinuously from a low to a high value.\n\\[\\rho_\\textrm{ gas}=\\frac{1-m^\\star}{2} \\to \\rho_\\textrm{ liquid}=\\frac{1+m^\\star}{2}\n\\] These values merge at \\(T_c\\), the gas-liquid critical point. At higher temperatures, the distinction between the phases disappears.",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  },
  {
    "objectID": "phase-transitions/lattice-gas.html#real-fluids",
    "href": "phase-transitions/lattice-gas.html#real-fluids",
    "title": "6  The Lattice Gas model",
    "section": "6.3 Real Fluids",
    "text": "6.3 Real Fluids\nYou may wish to compare this with the results of (say) van der Waals equation (see recommended textbooks for the required phase diagram). The main difference is that the lattice gas has so-called “particle-hole” symmetry, \\(\\rho\\to 1-\\rho\\) (inherited from the up-down symmetry of the Ising model) which is not present for a real fluid. Accordingly, the phase diagram in a real fluid looks like a lopsided version of the above picture as shown in Figure 6.2. See here for some real experimental data showing the asymmetry of the coexistence curve in liquid metals.\n\n\n\n\n\n\nFigure 6.2: Schematic of the liquid-gas phase diagram in the \\(\\rho-T\\) plane for a realistic fluid .",
    "crumbs": [
      "Unifying concepts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The lattice gas</span>"
    ]
  }
]